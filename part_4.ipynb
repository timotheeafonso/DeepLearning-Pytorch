{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1099e37",
   "metadata": {},
   "source": [
    "# Deep Learning avec PyTorch\n",
    "\n",
    "\n",
    "## Deep learning pour le texte\n",
    "L'objectif est d'apprivoiser les embeddings de mots, de tokens, ainsi que les architectures Transformer : \n",
    "- Chargement des embeddings pré-entrainés\n",
    "- Manipulation des embeddings\n",
    "- Visualuation des embeddings\n",
    "- Classification de phrases courtes, en fonction de différentes représentations et architectures\n",
    "- Classification de textes : Finetuning d'un Transformer pre-entraîné sur le corpus IMDB \n",
    "\n",
    "Prérequis (en plus de pytorch et numpy): \n",
    "- conda install pandas\n",
    "- conda install matplotlib\n",
    "- conda install -c anaconda scikit-learn\n",
    "- conda install ipywidgets\n",
    "- conda install gensim\n",
    "- conda install datasets \n",
    "- conda install -c huggingface transformers\n",
    "- conda install tqdm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2235348f",
   "metadata": {},
   "source": [
    " ## Configuration environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c52e4b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# pour colab, decommenter:    \n",
    "# !pip install torch==1.8.0+cu111 torchtext==0.9.0 torchvision==0.9.0+cu111 torchaudio==0.8.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93fba3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_81539/449048064.py:8: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.31.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Progrès\n",
    "from tqdm import tqdm\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cca82b1",
   "metadata": {},
   "source": [
    "## Chargement d'embeddings et calculs de similarité\n",
    "\n",
    "Pour commencer à travailler sur le texte, nous allons dans un premier temps utiliser un ensemble de vecteurs d'embeddings appris via le modèle Glove (similaire à Word2Vec vu en cours), que nous allons charger via la librairie gensim (il s'agit d'un ensemble d'embeddings parmi d'autres, de nombreux autres, appris sur des corpus différents existent dans gensim ou d'autres librairies comme spacy). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1dcc32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0076543  0.93456   -0.73189   -0.55162    0.76977    0.35925\n",
      " -1.1365    -1.1632     0.34214    0.29145   -0.8711     0.9197\n",
      " -0.47069   -0.22834    1.4777    -0.81714   -0.17466   -0.51093\n",
      " -0.28354    0.23292    0.71832    0.23414    0.49443    0.35483\n",
      "  0.76889   -1.4374    -1.7457    -0.28994   -0.10156   -0.36959\n",
      "  2.5502    -1.0581    -0.049416  -0.25524   -0.63303    0.02671\n",
      " -0.18733    0.20206   -0.26288   -0.41418    0.83473   -0.14227\n",
      " -0.28125    0.098155  -0.17096    0.52408    0.31851   -0.089847\n",
      " -0.27223   -0.0088736]\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "embeds = api.load(\"glove-wiki-gigaword-50\")\n",
    "\n",
    "# affichage de l'embedding du mot \"book\"\n",
    "print(embeds['book']) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c340e517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method most_similar in module gensim.models.keyedvectors:\n",
      "\n",
      "most_similar(positive=None, negative=None, topn=10, clip_start=0, clip_end=None, restrict_vocab=None, indexer=None) method of gensim.models.keyedvectors.KeyedVectors instance\n",
      "    Find the top-N most similar keys.\n",
      "    Positive keys contribute positively towards the similarity, negative keys negatively.\n",
      "\n",
      "    This method computes cosine similarity between a simple mean of the projection\n",
      "    weight vectors of the given keys and the vectors for each key in the model.\n",
      "    The method corresponds to the `word-analogy` and `distance` scripts in the original\n",
      "    word2vec implementation.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    positive : list of (str or int or ndarray) or list of ((str,float) or (int,float) or (ndarray,float)), optional\n",
      "        List of keys that contribute positively. If tuple, second element specifies the weight (default `1.0`)\n",
      "    negative : list of (str or int or ndarray) or list of ((str,float) or (int,float) or (ndarray,float)), optional\n",
      "        List of keys that contribute negatively. If tuple, second element specifies the weight (default `-1.0`)\n",
      "    topn : int or None, optional\n",
      "        Number of top-N similar keys to return, when `topn` is int. When `topn` is None,\n",
      "        then similarities for all keys are returned.\n",
      "    clip_start : int\n",
      "        Start clipping index.\n",
      "    clip_end : int\n",
      "        End clipping index.\n",
      "    restrict_vocab : int, optional\n",
      "        Optional integer which limits the range of vectors which\n",
      "        are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      "        only check the first 10000 key vectors in the vocabulary order. (This may be\n",
      "        meaningful if you've sorted the vocabulary by descending frequency.) If\n",
      "        specified, overrides any values of ``clip_start`` or ``clip_end``.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    list of (str, float) or numpy.array\n",
      "        When `topn` is int, a sequence of (key, similarity) is returned.\n",
      "        When `topn` is None, then similarities for all keys are returned as a\n",
      "        one-dimensional numpy array with the size of the vocabulary.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Methode most_similar permet d'obtenir les mots les plus proches d'un mot dans l'espace d'embeddings. sa specification peut être obtenue en executant la ligne suivante : \n",
    "help(embeds.most_similar) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f09ced",
   "metadata": {},
   "source": [
    "Afficher les 5 mots les plus proches de \"cat\" via la méthode model.most_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6aa034e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dog', 0.9218006134033203),\n",
       " ('rabbit', 0.8487821221351624),\n",
       " ('monkey', 0.8041081428527832),\n",
       " ('rat', 0.7891963124275208),\n",
       " ('cats', 0.7865270972251892)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#[[Student/]]\n",
    "embeds.most_similar([\"cat\"],topn=5)\n",
    "#[[/Student]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "472f7436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5607928\n",
      "0.4138724\n"
     ]
    }
   ],
   "source": [
    "# La similarité est calculée selon une mesure de cosinus dans l'espace des embeddings\n",
    "# par exemple\n",
    "print(embeds.similarity(\"apple\", \"banana\"))\n",
    "print(embeds.similarity(\"apple\", \"dog\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc58f9d",
   "metadata": {},
   "source": [
    "### Out Of Distribution \n",
    "Attention, bien sûr tous les mots possibles ne sont pas inclus dans le dictionnaire d'embeddings, leur sémantique dépend notamment fortement du corpus sur lequel ils ont été appris.\n",
    "\n",
    "Par exemple le mot \"covid\" n'est pas présent, l'execution de model['covid'] ferait planter l'execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7372fd",
   "metadata": {},
   "source": [
    "Pour éviter ce genre de problème par exemple pour le traitement d'un texte ne contenant pas certains mots, ils convient de vérifier leur présence dans le vocabulaire (et alors ignorer les mots correspondants). Ceci peut se faire via vocab.keys comme ci-dessous. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "826df032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oov\n"
     ]
    }
   ],
   "source": [
    "# Le vocabulaire du modèle peut être obtenus via vocab.keys, qui retourne l'ensemble des tokens (mots) pour lesquels il existe un embedding dans le modèle\n",
    "vocab = embeds.key_to_index.keys()\n",
    "np.random.choice(list(vocab), 5)\n",
    "\n",
    "x=\"covid\"\n",
    "if x in vocab:\n",
    "    print(embeds[x])\n",
    "else: print(\"oov\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c69e3e",
   "metadata": {},
   "source": [
    "### Visualisation des embeddings\n",
    "\n",
    "On souhaite maintenant visualiser les embeddings dans un espace en 2D. Pour cela, on utilise t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e85c8d6d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x1600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "words=['cat','mouse','dog','car','truck','motorcycle','bike','lion','plane','deers']\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def tsne_plot(model, words,n_components=2,perplexity=40):\n",
    "    \"Creates and TSNE model and plots it\"\n",
    "    labels = []\n",
    "    tokens = []\n",
    "    vocab = model.key_to_index.keys()\n",
    "    \n",
    "    for word in words:\n",
    "        if word in vocab: \n",
    "            tokens.append(model[word])\n",
    "            labels.append(word)\n",
    "\n",
    "    tsne_model = TSNE(perplexity=perplexity, n_components=n_components, init='pca', n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(np.array(tokens))\n",
    "    fig = plt.figure(figsize=(16, 16))\n",
    "    if n_components==3:\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.scatter(new_values[:,0],new_values[:,1],new_values[:,2],c=\"r\",marker=\"o\")\n",
    "        for i in range(len(new_values)):\n",
    "            ax.text(new_values[i][0],new_values[i][1],new_values[i][2],labels[i])\n",
    "    else:\n",
    "        plt.scatter(new_values[:,0],new_values[:,1])\n",
    "        for i in range(len(new_values)):\n",
    "            plt.annotate(labels[i],\n",
    "                        xy=(new_values[i][0],new_values[i][1]),\n",
    "                        xytext=(5, 2),\n",
    "                        textcoords='offset points',\n",
    "                        ha='right',\n",
    "                        va='bottom')\n",
    "    return new_values,labels\n",
    "\n",
    "new_values,labels = tsne_plot(embeds,words,n_components=2,perplexity=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90ddc86",
   "metadata": {},
   "source": [
    "### Index des Embeddings\n",
    "\n",
    "Lors de l'établissement de modèles utilisant les embeddings du vocabulaire, il est utile de considérer des indices des mots dans le vocabulaire plutôt que les mots eux mêmes (pour les traiter par exemple dans des tenseurs avant de les remplacer par leurs vecteurs de poids). Ceci se fait dans gensim par l'utilisation des instructions suivantes:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0493c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "539\n",
      "book\n",
      "[-0.0076543  0.93456   -0.73189   -0.55162    0.76977    0.35925\n",
      " -1.1365    -1.1632     0.34214    0.29145   -0.8711     0.9197\n",
      " -0.47069   -0.22834    1.4777    -0.81714   -0.17466   -0.51093\n",
      " -0.28354    0.23292    0.71832    0.23414    0.49443    0.35483\n",
      "  0.76889   -1.4374    -1.7457    -0.28994   -0.10156   -0.36959\n",
      "  2.5502    -1.0581    -0.049416  -0.25524   -0.63303    0.02671\n",
      " -0.18733    0.20206   -0.26288   -0.41418    0.83473   -0.14227\n",
      " -0.28125    0.098155  -0.17096    0.52408    0.31851   -0.089847\n",
      " -0.27223   -0.0088736]\n",
      "[-0.0076543  0.93456   -0.73189   -0.55162    0.76977    0.35925\n",
      " -1.1365    -1.1632     0.34214    0.29145   -0.8711     0.9197\n",
      " -0.47069   -0.22834    1.4777    -0.81714   -0.17466   -0.51093\n",
      " -0.28354    0.23292    0.71832    0.23414    0.49443    0.35483\n",
      "  0.76889   -1.4374    -1.7457    -0.28994   -0.10156   -0.36959\n",
      "  2.5502    -1.0581    -0.049416  -0.25524   -0.63303    0.02671\n",
      " -0.18733    0.20206   -0.26288   -0.41418    0.83473   -0.14227\n",
      " -0.28125    0.098155  -0.17096    0.52408    0.31851   -0.089847\n",
      " -0.27223   -0.0088736]\n",
      "(400000, 50)\n"
     ]
    }
   ],
   "source": [
    "print(embeds.key_to_index[\"book\"])\n",
    "print(embeds.index_to_key[539])\n",
    "\n",
    "# Deux manières equivalentes de recupérer les poids\n",
    "print(embeds[\"book\"])   # via le mot \n",
    "print(embeds.vectors[embeds.key_to_index[\"book\"]]) #via l'index\n",
    "\n",
    "\n",
    "# taille du vocabulaire: \n",
    "print(embeds.vectors.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3818f91c",
   "metadata": {},
   "source": [
    "#### Similarité de phrases\n",
    "\n",
    "On souhaite calculer la matrice de similarité de différentes phrases du dataset suivant:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "607bb700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the road is straight</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the black cat plays with a ball</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a big dog with a ball</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dog and cat are together</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>traffic jam on the 6th road</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>white bird on a big tree</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>a big truck</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>two cars crashed</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>two deers in a field</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I like ridding my bike</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>a lion in the savane</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>a motorcycle rides on the road</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>it is a bike, it is not a flamingo</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>it is not a bike, it is a flamingo</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>a mouse bitten by a cat</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>two pigs in the mood</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>take a plane is sometimes slower than taking t...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>take the highway</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text label\n",
       "0                                the road is straight     Y\n",
       "1                     the black cat plays with a ball     N\n",
       "2                               a big dog with a ball     N\n",
       "3                            dog and cat are together     N\n",
       "4                         traffic jam on the 6th road     Y\n",
       "5                            white bird on a big tree     N\n",
       "6                                         a big truck     Y\n",
       "7                                    two cars crashed     Y\n",
       "8                                two deers in a field     N\n",
       "9                              I like ridding my bike     Y\n",
       "10                               a lion in the savane     N\n",
       "11                     a motorcycle rides on the road     Y\n",
       "12                 it is a bike, it is not a flamingo     Y\n",
       "13                 it is not a bike, it is a flamingo     N\n",
       "14                            a mouse bitten by a cat     N\n",
       "15                               two pigs in the mood     N\n",
       "16  take a plane is sometimes slower than taking t...     Y\n",
       "17                                   take the highway     Y"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdf = pd.DataFrame([\n",
    "    ['the road is straight', 'Y'],\n",
    "    ['the black cat plays with a ball', 'N'],\n",
    "    ['a big dog with a ball', 'N'],\n",
    "    ['dog and cat are together', 'N'],\n",
    "    ['traffic jam on the 6th road', 'Y'],\n",
    "    ['white bird on a big tree', 'N'],\n",
    "    ['a big truck', 'Y'],\n",
    "    ['two cars crashed', 'Y'],\n",
    "    ['two deers in a field', 'N'],\n",
    "    ['I like ridding my bike','Y'],\n",
    "    ['a lion in the savane','N'],\n",
    "    ['a motorcycle rides on the road','Y'],\n",
    "    ['it is a bike, it is not a flamingo', 'Y'], \n",
    "    ['it is not a bike, it is a flamingo', 'N'],\n",
    "    ['a mouse bitten by a cat','N'],\n",
    "    ['two pigs in the mood','N'],\n",
    "    ['take a plane is sometimes slower than taking train','Y'],\n",
    "    ['take the highway','Y']\n",
    "], columns=['text', 'label'])\n",
    "tdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4b3abb",
   "metadata": {},
   "source": [
    "Pour chaque phrase, on va moyenner les embeddings de ces mots. Chaque vecteur de phrase sera normalisé (x.norm()). Les phrases pourront être comparées deux à deux par un produit scalaire.\n",
    "\n",
    "- chaque phrase doit être découpée : (\"the\", \"cat\", \"is\" , \"on\", \"the\",\"bank\")\n",
    "- on définit une fonction getvectors qui à partir de la liste de phrases découpées : moyenne les vecteurs d'embeddings (pensez à utiliser les tenseurs de torch), normalise le resultat et le retourne. \n",
    "- on peut ensuite calculer la matrice de similarité qui sera donnée à la fonction visual_similarity_matrix() fournie ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57f3927a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'road', 'is', 'straight'], ['the', 'black', 'cat', 'plays', 'with', 'a', 'ball'], ['a', 'big', 'dog', 'with', 'a', 'ball'], ['dog', 'and', 'cat', 'are', 'together'], ['traffic', 'jam', 'on', 'the', 'th', 'road'], ['white', 'bird', 'on', 'a', 'big', 'tree'], ['a', 'big', 'truck'], ['two', 'cars', 'crashed'], ['two', 'deers', 'in', 'a', 'field'], ['I', 'like', 'ridding', 'my', 'bike'], ['a', 'lion', 'in', 'the', 'savane'], ['a', 'motorcycle', 'rides', 'on', 'the', 'road'], ['it', 'is', 'a', 'bike', 'it', 'is', 'not', 'a', 'flamingo'], ['it', 'is', 'not', 'a', 'bike', 'it', 'is', 'a', 'flamingo'], ['a', 'mouse', 'bitten', 'by', 'a', 'cat'], ['two', 'pigs', 'in', 'the', 'mood'], ['take', 'a', 'plane', 'is', 'sometimes', 'slower', 'than', 'taking', 'train'], ['take', 'the', 'highway']]\n",
      "tensor([[-2.0428e-03,  1.2294e-01, -2.5053e-02,  3.8993e-02,  3.7583e-02,\n",
      "          2.6519e-02, -1.1185e-01, -2.6247e-02,  2.7497e-02, -9.6953e-02,\n",
      "         -6.0345e-02, -6.9913e-02, -1.8141e-01, -5.1170e-02, -3.2547e-02,\n",
      "          1.5009e-02,  7.2446e-02, -7.5682e-03, -1.6532e-01, -3.6185e-02,\n",
      "         -5.5564e-02, -5.6831e-02, -2.6280e-02,  5.6929e-02,  4.6426e-02,\n",
      "         -3.6241e-01, -3.9693e-02,  1.0887e-01,  5.8831e-02, -1.0784e-01,\n",
      "          7.9753e-01, -2.2809e-02,  1.1595e-02,  1.3907e-03,  7.4593e-02,\n",
      "         -3.1810e-02,  5.9722e-02,  5.6344e-02, -2.9517e-02, -2.5317e-02,\n",
      "         -8.4087e-02, -4.6965e-02, -4.0396e-02, -1.2824e-02, -8.2437e-02,\n",
      "          4.7296e-02,  5.0943e-02, -1.4947e-01,  6.4213e-02, -7.0726e-02],\n",
      "        [-4.7767e-02,  4.5056e-02, -1.1619e-01, -7.5715e-03,  1.1945e-01,\n",
      "          1.2060e-01, -1.4952e-01, -6.9987e-02, -3.7955e-02, -9.4739e-03,\n",
      "          2.0197e-02,  5.0031e-02, -7.0797e-02,  8.6741e-02,  2.5296e-02,\n",
      "         -5.7494e-02,  5.2715e-02,  2.4480e-02, -2.3326e-01, -9.7552e-02,\n",
      "         -8.3775e-02,  5.2929e-02,  4.0196e-02,  2.7240e-02, -7.2953e-03,\n",
      "         -4.0391e-01, -7.5160e-02,  9.5593e-02,  3.0136e-02, -1.5233e-01,\n",
      "          7.4303e-01,  2.1237e-02, -4.1597e-02,  2.0649e-02,  2.8402e-02,\n",
      "          8.5709e-02, -2.4475e-03,  3.4634e-02,  3.0157e-02, -1.1022e-01,\n",
      "          2.7955e-02,  1.1985e-01, -6.8278e-02, -1.7812e-03,  1.6729e-02,\n",
      "          1.3932e-02,  5.6093e-02, -1.4155e-01,  2.2468e-02, -4.9819e-02],\n",
      "        [-2.1516e-02,  2.2308e-02, -5.4702e-02,  4.7502e-03,  1.4957e-01,\n",
      "          1.0408e-01, -1.6986e-01, -4.5854e-02,  3.6416e-02,  1.5337e-02,\n",
      "         -5.2876e-02,  2.9560e-02, -9.1246e-02,  1.0263e-01,  4.4066e-02,\n",
      "         -6.1577e-02,  6.6561e-02,  3.6338e-02, -2.0928e-01, -1.2754e-01,\n",
      "         -4.7528e-02,  3.1014e-02,  8.1490e-03, -7.2373e-03,  2.0625e-02,\n",
      "         -4.1907e-01, -6.5325e-02,  1.1504e-01,  9.0616e-02, -1.2045e-01,\n",
      "          7.3562e-01,  2.2412e-02, -4.8732e-02,  8.8678e-02,  2.5877e-02,\n",
      "          6.7347e-02,  1.6472e-02,  6.1936e-02,  6.7795e-02, -1.0501e-01,\n",
      "          9.7703e-04,  9.9276e-02, -1.2099e-01,  5.6562e-02,  4.1615e-02,\n",
      "         -1.7535e-02,  2.9117e-02, -7.4214e-02,  6.0848e-02,  4.6968e-03],\n",
      "        [ 1.1717e-01, -3.3876e-02, -5.8636e-02, -2.9467e-02,  1.1407e-01,\n",
      "          1.5284e-01, -1.7129e-01, -1.3522e-01, -1.2309e-03, -6.7334e-02,\n",
      "          4.3212e-02,  8.7681e-02,  3.8449e-02,  4.5291e-02,  8.0334e-02,\n",
      "          4.7649e-02,  6.2867e-02,  8.5045e-02, -1.1555e-01, -1.5578e-01,\n",
      "         -2.0916e-02,  7.5515e-02,  1.8923e-01,  9.0180e-02,  5.4009e-02,\n",
      "         -2.8715e-01, -1.5074e-01,  1.2207e-02,  2.8892e-02, -1.7857e-01,\n",
      "          7.3643e-01,  1.0713e-01, -4.2510e-02, -8.7247e-03,  2.1424e-02,\n",
      "          6.8594e-02, -3.3846e-02,  3.5209e-03,  1.5958e-02, -5.5030e-02,\n",
      "         -6.7328e-02,  4.1051e-02, -4.1598e-04,  9.5740e-02,  1.3421e-01,\n",
      "         -2.0369e-02, -3.5347e-02, -1.7119e-01,  1.1921e-02, -2.8620e-02],\n",
      "        [ 1.7494e-02,  1.3201e-01,  5.9241e-02,  5.4146e-02, -2.1494e-02,\n",
      "         -6.5809e-02, -1.1817e-01,  3.2448e-03,  9.5685e-02, -7.7847e-02,\n",
      "         -5.9182e-02, -1.1624e-01, -1.2027e-01, -4.3171e-02,  1.8128e-02,\n",
      "         -3.2012e-02, -7.3302e-05,  2.4579e-02, -1.7031e-01, -7.7464e-02,\n",
      "          8.7305e-02, -8.7622e-02, -1.8844e-02,  1.0297e-01,  3.6225e-02,\n",
      "         -2.7146e-01, -1.8441e-02,  6.3301e-02,  1.4163e-01, -8.5846e-02,\n",
      "          7.9530e-01,  1.2043e-02, -3.8877e-02,  1.1657e-02,  3.3195e-02,\n",
      "          7.6418e-03,  1.4206e-01, -9.9085e-02, -3.8684e-02,  1.4953e-01,\n",
      "         -4.1468e-02, -2.3948e-02, -1.1937e-01, -5.7051e-02, -6.5689e-02,\n",
      "          1.2132e-02,  6.4865e-02, -1.2309e-01,  9.9649e-03, -7.4020e-02],\n",
      "        [ 3.7095e-02,  1.2562e-01, -7.0502e-02,  3.6771e-02,  1.3715e-01,\n",
      "          7.9876e-02, -2.1457e-01, -1.5543e-01,  4.6288e-02, -8.5927e-02,\n",
      "         -8.6042e-02,  2.6807e-04,  4.0781e-02,  4.0247e-02,  6.2005e-02,\n",
      "          2.1459e-02,  2.8446e-02, -1.7747e-02, -1.6746e-01, -1.2861e-01,\n",
      "         -8.4730e-02, -4.6951e-02,  3.0706e-02, -6.2356e-02,  7.5014e-03,\n",
      "         -3.5215e-01, -5.6273e-02,  1.5982e-01,  6.2147e-02, -1.0334e-01,\n",
      "          7.3890e-01, -3.2230e-02, -6.6677e-02,  1.5690e-02,  5.6325e-04,\n",
      "          1.5543e-02,  6.5639e-03, -4.2584e-02,  3.2913e-02, -1.0780e-01,\n",
      "         -9.0471e-02,  3.6242e-02, -7.1307e-02,  1.2288e-02,  1.6472e-01,\n",
      "          3.6378e-02, -4.6258e-03, -1.2156e-01,  5.9653e-02, -3.6234e-02],\n",
      "        [ 1.7953e-02,  3.0340e-02,  9.7666e-02,  2.9695e-02,  1.0671e-01,\n",
      "          9.8160e-02, -2.2114e-01, -1.8827e-02,  6.5775e-02, -1.0671e-02,\n",
      "         -6.2941e-02, -8.1064e-02, -1.2051e-01,  8.9965e-02,  7.0223e-02,\n",
      "         -7.0626e-02,  3.1642e-02,  1.5764e-01, -1.4392e-01, -2.0419e-01,\n",
      "          3.1266e-02, -3.1587e-02, -1.1218e-01,  3.0081e-02, -2.4316e-02,\n",
      "         -3.9722e-01, -1.2281e-01,  1.4010e-01,  1.8748e-01, -6.5029e-02,\n",
      "          6.8128e-01,  7.8327e-03, -4.5163e-02,  1.2299e-01,  1.0960e-01,\n",
      "          4.2589e-02,  1.9026e-03, -1.2921e-02,  8.0507e-02, -4.6692e-02,\n",
      "         -6.7960e-02,  5.1087e-02, -7.0773e-02, -6.9185e-02,  5.3638e-02,\n",
      "         -1.9482e-02, -2.2735e-02, -3.3203e-02,  9.1528e-02,  7.4829e-03],\n",
      "        [ 1.5745e-01, -3.3956e-02,  2.2064e-01,  3.1427e-02, -2.9020e-02,\n",
      "          5.9676e-02, -2.6040e-01, -1.4855e-02, -3.7834e-02, -1.4373e-01,\n",
      "          2.8493e-02, -1.4573e-01, -1.9564e-01,  8.6262e-02,  5.9082e-02,\n",
      "         -4.4722e-02, -1.0576e-01,  1.3536e-01, -2.5401e-01, -2.6187e-01,\n",
      "          2.6667e-03,  4.2914e-02, -7.4598e-02,  3.9770e-02,  1.1238e-02,\n",
      "         -2.3891e-01, -5.7561e-03,  7.7220e-02,  1.2250e-01, -3.1257e-02,\n",
      "          5.6776e-01,  7.2514e-02, -2.5488e-03,  7.3015e-02,  2.1834e-01,\n",
      "          5.3148e-02,  6.5352e-02,  2.3739e-02, -8.6529e-02,  1.1969e-01,\n",
      "         -8.8478e-02, -3.5492e-03,  1.6848e-01, -6.1676e-02,  1.7299e-02,\n",
      "          2.4120e-02, -5.6596e-02, -9.5655e-02,  5.4733e-02, -1.9870e-01],\n",
      "        [ 1.0687e-02,  5.6268e-02, -4.8091e-02,  7.8142e-02,  1.1457e-01,\n",
      "          8.4640e-02, -1.9167e-01, -5.8494e-02, -2.5981e-03, -6.7234e-02,\n",
      "          3.7308e-03, -1.0773e-01, -8.1622e-02,  1.6216e-02,  1.3148e-02,\n",
      "         -7.8865e-02,  8.8670e-02, -2.7219e-02, -2.6462e-01, -4.4879e-02,\n",
      "         -4.4603e-02, -5.8808e-02,  6.1142e-02, -3.4407e-02, -7.6189e-02,\n",
      "         -3.1373e-01,  4.9185e-02,  3.1527e-02,  5.0034e-03, -1.3086e-01,\n",
      "          7.8649e-01, -2.1406e-02, -4.6731e-02, -8.0046e-02,  7.4224e-02,\n",
      "          6.4255e-02, -5.4320e-04,  1.1356e-01,  3.8423e-02,  4.5872e-02,\n",
      "         -6.3193e-02,  5.9765e-03, -5.4789e-02,  6.7320e-02,  1.6221e-02,\n",
      "         -7.3728e-02,  3.6952e-02, -4.1618e-02, -1.5057e-02, -9.0896e-02],\n",
      "        [ 4.4792e-02,  9.1339e-02, -6.7452e-03, -1.2377e-01,  1.9065e-02,\n",
      "          1.5518e-02, -9.1324e-02, -7.1038e-02,  1.2714e-01,  4.5309e-02,\n",
      "         -5.2611e-02,  7.0835e-02, -2.1882e-01,  9.8066e-03,  5.9192e-02,\n",
      "          4.7648e-02,  6.0517e-02,  1.9774e-01,  6.6348e-02, -1.7626e-01,\n",
      "         -1.4285e-02,  1.0892e-01, -3.7805e-02,  1.0012e-01,  1.5107e-01,\n",
      "         -3.8960e-01, -1.9351e-01,  1.0201e-01,  2.2253e-01, -1.9412e-01,\n",
      "          5.7859e-01,  1.9292e-01, -1.0302e-01,  5.8425e-02, -3.8031e-02,\n",
      "          4.8810e-02, -3.6488e-02,  1.0079e-01, -3.1819e-03, -3.5478e-02,\n",
      "         -3.8674e-02, -7.3198e-02, -2.4336e-02,  9.0759e-02,  9.6293e-02,\n",
      "         -2.3884e-02,  6.7503e-02, -1.8839e-01, -2.6520e-03, -3.0704e-02],\n",
      "        [ 1.7525e-01,  2.0391e-02, -1.0415e-01,  2.9919e-02,  4.9772e-02,\n",
      "          3.1821e-02, -4.2238e-02, -8.6865e-02,  7.8785e-02, -2.0560e-02,\n",
      "          3.6692e-02, -3.4691e-02, -3.3633e-02,  1.9025e-02, -5.4436e-02,\n",
      "         -3.6142e-02,  1.4178e-02,  9.5948e-02, -2.8431e-01,  4.3846e-02,\n",
      "         -8.7872e-03, -1.5486e-02, -1.0310e-02, -5.5008e-02, -4.3035e-02,\n",
      "         -3.8728e-01, -1.6373e-01,  7.3815e-02, -1.9489e-02,  6.6261e-03,\n",
      "          7.5930e-01, -3.1577e-02, -7.6739e-02,  7.3414e-02,  4.4959e-02,\n",
      "          1.0642e-02,  3.0965e-02,  4.4450e-03,  1.0431e-02, -2.4953e-02,\n",
      "         -2.4554e-02,  5.5401e-02, -8.3044e-02, -1.1067e-01,  1.3516e-03,\n",
      "         -5.2609e-02,  7.3520e-03, -1.6164e-01,  6.7646e-02, -1.5224e-02],\n",
      "        [ 5.6254e-02,  7.7692e-02, -4.6202e-02,  3.9060e-02, -2.3961e-02,\n",
      "          5.9582e-02, -2.1305e-01, -7.0430e-02,  8.5252e-02, -1.1224e-01,\n",
      "         -4.3824e-02, -9.6219e-02, -1.5517e-01, -2.0649e-02,  2.9648e-02,\n",
      "         -4.9297e-02, -8.0194e-03,  1.2303e-01, -2.2120e-01, -1.4633e-01,\n",
      "          5.6785e-02,  2.2762e-02, -1.5007e-01,  8.0970e-02,  4.6461e-02,\n",
      "         -3.5887e-01, -5.2668e-02,  8.6948e-02,  6.3324e-02, -6.9861e-02,\n",
      "          7.1410e-01,  2.0519e-02, -9.7354e-02,  5.6058e-02,  8.6854e-02,\n",
      "          1.6411e-02,  8.6099e-02, -2.9234e-02, -5.8213e-02,  2.6738e-02,\n",
      "         -5.5876e-02, -2.3981e-02, -6.6246e-02, -1.1534e-02, -5.7439e-03,\n",
      "         -7.0813e-02,  3.2858e-02, -1.5455e-01,  1.0920e-01, -1.0932e-01],\n",
      "        [ 1.0259e-01,  7.6997e-02, -7.0159e-02,  3.9231e-02,  1.1118e-01,\n",
      "          9.6408e-02, -9.1618e-02, -8.8430e-02,  4.9855e-02,  2.9569e-02,\n",
      "          1.2829e-02,  8.7557e-03, -6.2494e-02, -2.5563e-02,  9.7296e-02,\n",
      "          6.9517e-02,  6.6574e-02,  9.8062e-02, -1.0929e-01, -9.6086e-02,\n",
      "         -4.6963e-02,  5.4743e-04, -5.0111e-02,  5.9691e-02,  3.3881e-02,\n",
      "         -4.1510e-01, -8.4216e-02,  1.3051e-01,  1.0628e-01, -7.2463e-02,\n",
      "          7.8010e-01, -6.6654e-02, -8.9029e-02, -8.9152e-03,  1.3426e-02,\n",
      "         -6.3856e-02,  5.0349e-02,  2.3092e-02,  1.9220e-02, -6.7176e-02,\n",
      "         -1.9107e-02,  2.1782e-02, -4.3602e-02,  5.1928e-02, -6.7759e-02,\n",
      "         -2.8218e-02, -1.2995e-02, -8.8694e-02,  6.7289e-02,  5.0301e-02],\n",
      "        [ 1.0259e-01,  7.6997e-02, -7.0159e-02,  3.9231e-02,  1.1118e-01,\n",
      "          9.6408e-02, -9.1618e-02, -8.8430e-02,  4.9855e-02,  2.9569e-02,\n",
      "          1.2829e-02,  8.7557e-03, -6.2494e-02, -2.5563e-02,  9.7296e-02,\n",
      "          6.9517e-02,  6.6574e-02,  9.8062e-02, -1.0929e-01, -9.6086e-02,\n",
      "         -4.6963e-02,  5.4743e-04, -5.0111e-02,  5.9691e-02,  3.3881e-02,\n",
      "         -4.1510e-01, -8.4216e-02,  1.3051e-01,  1.0628e-01, -7.2463e-02,\n",
      "          7.8010e-01, -6.6654e-02, -8.9029e-02, -8.9152e-03,  1.3426e-02,\n",
      "         -6.3856e-02,  5.0349e-02,  2.3092e-02,  1.9220e-02, -6.7176e-02,\n",
      "         -1.9107e-02,  2.1782e-02, -4.3602e-02,  5.1928e-02, -6.7759e-02,\n",
      "         -2.8218e-02, -1.2995e-02, -8.8694e-02,  6.7289e-02,  5.0301e-02],\n",
      "        [ 1.2060e-01, -1.0915e-01, -3.2787e-02, -7.8019e-03,  1.0579e-01,\n",
      "          1.8072e-01, -1.5630e-01, -1.3650e-01,  7.8640e-02,  5.2373e-03,\n",
      "          5.2342e-02,  5.3139e-02,  3.2873e-02,  1.0281e-01,  7.5533e-02,\n",
      "         -6.8514e-03, -3.5827e-02,  6.9487e-02, -2.4115e-01, -3.2996e-02,\n",
      "         -1.0379e-01,  1.7333e-02,  4.4909e-02,  2.0754e-02,  9.3955e-02,\n",
      "         -4.1216e-01, -9.7297e-02,  8.2701e-02,  1.9816e-03, -1.0698e-01,\n",
      "          6.5592e-01, -8.5844e-02, -5.3892e-02,  5.6689e-02, -5.0494e-02,\n",
      "          6.9143e-02,  2.4456e-02, -3.7971e-03,  6.2767e-02, -1.3319e-01,\n",
      "          8.0445e-03,  1.2780e-01, -1.4491e-01,  8.4489e-02,  1.0506e-01,\n",
      "         -7.1797e-02,  6.3970e-02, -1.6220e-01,  8.0360e-02, -7.8457e-03],\n",
      "        [ 1.1473e-01, -2.2198e-03, -9.1275e-02, -6.5268e-02,  5.1490e-02,\n",
      "          2.0587e-02, -1.0889e-01, -5.5191e-02, -3.5948e-02, -2.3651e-03,\n",
      "          2.9300e-02, -9.5658e-02, -2.0474e-02,  2.8922e-02,  5.9528e-02,\n",
      "          2.4459e-02, -4.0253e-02, -4.0310e-02, -2.0467e-01, -1.4406e-02,\n",
      "         -2.3067e-02,  3.3743e-03,  1.2811e-01, -1.7171e-02, -3.3959e-02,\n",
      "         -3.4785e-01, -1.1618e-01,  5.9473e-02,  2.8616e-02, -1.8529e-02,\n",
      "          8.1600e-01,  8.6017e-02, -2.8398e-02, -5.3241e-02,  6.3993e-02,\n",
      "         -4.5396e-03,  3.1791e-02,  2.3818e-02, -1.9619e-02,  3.4443e-02,\n",
      "         -1.5121e-01,  1.0209e-02, -2.3889e-02,  1.6179e-02,  1.0134e-01,\n",
      "          5.1946e-02, -3.2542e-02, -6.8885e-02, -9.9377e-03, -9.8888e-02],\n",
      "        [ 1.7051e-01,  1.1243e-02,  4.7905e-02, -7.1926e-02,  6.8444e-02,\n",
      "          4.6548e-02, -9.0206e-02, -5.3671e-03,  1.4310e-02,  1.1166e-03,\n",
      "          3.0686e-02, -3.9473e-02, -5.7214e-02,  3.7092e-02,  7.7468e-02,\n",
      "          2.0374e-02, -2.3979e-02,  5.6100e-02, -1.4596e-01, -1.7938e-01,\n",
      "         -3.8048e-03,  3.1624e-02, -1.0339e-02,  5.8773e-02,  1.1158e-01,\n",
      "         -3.9930e-01, -3.6435e-02,  6.1463e-02,  1.1878e-01,  5.0433e-03,\n",
      "          8.0565e-01,  6.2784e-02, -4.2598e-02, -1.3902e-02,  7.5711e-02,\n",
      "         -4.6224e-02,  3.0281e-02,  3.9166e-02, -3.8495e-02,  9.5680e-03,\n",
      "         -5.2301e-02,  6.0923e-02,  5.1085e-02,  3.5596e-02, -4.1508e-03,\n",
      "         -2.6140e-02,  4.4051e-02, -2.3916e-02,  1.6978e-02,  2.9975e-02],\n",
      "        [ 8.7447e-02,  8.2487e-02,  7.5323e-02, -4.5403e-03, -5.0060e-02,\n",
      "         -1.2066e-02, -9.9451e-02, -2.0650e-02,  5.0168e-02, -1.5253e-01,\n",
      "         -1.0442e-01, -2.8762e-02, -1.0935e-01, -2.5260e-02, -4.7565e-02,\n",
      "          1.7025e-02,  4.6997e-02,  2.4968e-02, -9.4502e-02, -5.6541e-02,\n",
      "          2.7308e-02, -2.0510e-02, -2.5150e-02,  4.0474e-02,  5.1280e-02,\n",
      "         -3.9860e-01, -2.3165e-02,  4.5795e-02,  4.6330e-02, -1.1768e-01,\n",
      "          7.7982e-01,  2.5424e-02, -1.1562e-01, -2.6463e-02,  1.0269e-01,\n",
      "         -3.9777e-02,  6.1297e-02, -6.3707e-02, -3.5692e-02,  5.3288e-02,\n",
      "         -1.3968e-01, -5.7884e-03,  6.7142e-02, -4.9438e-02, -1.6182e-01,\n",
      "          5.0549e-02, -1.5713e-02, -6.0351e-02,  8.3464e-02, -5.2063e-02]])\n",
      "['the road is straight', 'the black cat plays with a ball', 'a big dog with a ball', 'dog and cat are together', 'traffic jam on the th road', 'white bird on a big tree', 'a big truck', 'two cars crashed', 'two deers in a field', 'I like ridding my bike', 'a lion in the savane', 'a motorcycle rides on the road', 'it is a bike it is not a flamingo', 'it is not a bike it is a flamingo', 'a mouse bitten by a cat', 'two pigs in the mood', 'take a plane is sometimes slower than taking train', 'take the highway']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Premiere possibilité :\n",
    "def getlistwordsentence(text):\n",
    "    ret=[]\n",
    "    for sentence in text:\n",
    "        print(sentence)\n",
    "        ret.append(sentence.split(\" \"))\n",
    "    return ret\n",
    "\n",
    "\n",
    "# Deuxième possibilité (plus robuste, gère la ponctuation, etc.):\n",
    "import gensim\n",
    "def getlistwordsentence2(text):\n",
    "    ret=[]\n",
    "    for sentence in text:\n",
    "        ret.append(list(gensim.utils.tokenize(sentence)))\n",
    "    return ret\n",
    "\n",
    "text_wordlist = getlistwordsentence2(tdf['text'])\n",
    "print(text_wordlist)\n",
    "\n",
    "def getvectors(wordslist, normalize=True):\n",
    "    l = []\n",
    "    s = []\n",
    "    knon_words = embeds.key_to_index.keys()\n",
    "    for words in wordslist:\n",
    "        x = torch.cat(([torch.Tensor(embeds[x]).view(1,-1) for x in words if x in knon_words]),0)\n",
    "        x = x.mean(0)\n",
    "        if normalize:\n",
    "            x/=x.norm(dim=-1)\n",
    "        l.append(x)\n",
    "        s.append(\" \".join(words))\n",
    "    return torch.stack(l), s\n",
    "    \n",
    "\n",
    "x, sentences = getvectors(text_wordlist,True)\n",
    "\n",
    "print(x)\n",
    "print(sentences)\n",
    "\n",
    "def getSims(x):\n",
    "    return x@x.t()\n",
    "\n",
    "innerproducts=getSims(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c265e3e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 3850x3850 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visual_similarity_matrix(innerproducts):\n",
    "  fig, ax = plt.subplots()\n",
    "  im = ax.imshow(innerproducts)\n",
    "\n",
    "  # We want to show all ticks...\n",
    "  ax.set_xticks(np.arange(len(sentences)))\n",
    "  ax.set_yticks(np.arange(len(sentences)))\n",
    "  # ... and label them with the respective list entries\n",
    "  ax.set_xticklabels(sentences)\n",
    "  ax.set_yticklabels(sentences)\n",
    "\n",
    "  # Rotate the tick labels and set their alignment.\n",
    "  plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "\n",
    "  # Loop over data dimensions and create text annotations.\n",
    "  for i in range(len(sentences)):\n",
    "    for j in range(len(sentences)):\n",
    "      text = ax.text(j, i, \"%.2f\" % innerproducts[i, j],\n",
    "                       ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "  ax.set_title(\"Produit scalaire des phrases\")\n",
    "  fig.tight_layout()\n",
    "  fig.set_size_inches(38.5, 38.5)\n",
    "  plt.show()\n",
    "\n",
    "visual_similarity_matrix(innerproducts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac350f2",
   "metadata": {},
   "source": [
    "## Utilisation des embeddings pour une tâche de classification\n",
    "\n",
    "On s'intéresse maintenant à apprendre un classifieur sur les données de l'exemple jouet défini ci dessus. On utilisera le jeu de données suivant comme jeu de validation :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24b90129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the bike drives on the road</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a lion and a cat in a tree</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>two cars crashed</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i always go to work by bike</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i have no animal at home</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dogs like cheese</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>a pink flamingo</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>trucks</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>truckks</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>truckmegatruck</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>a text about trucks, not animals</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>a text about animals, not trucks</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>doggs</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                text label\n",
       "0        the bike drives on the road     Y\n",
       "1         a lion and a cat in a tree     N\n",
       "2                   two cars crashed     Y\n",
       "3        i always go to work by bike     Y\n",
       "4           i have no animal at home     N\n",
       "5                   dogs like cheese     N\n",
       "6                    a pink flamingo     N\n",
       "7                             trucks     Y\n",
       "8                            truckks     Y\n",
       "9                     truckmegatruck     Y\n",
       "10  a text about trucks, not animals     Y\n",
       "11  a text about animals, not trucks     N\n",
       "12                             doggs     N"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vdf = pd.DataFrame([\n",
    "    ['the bike drives on the road', 'Y'],\n",
    "    ['a lion and a cat in a tree', 'N'],\n",
    "    ['two cars crashed', 'Y'],\n",
    "    ['i always go to work by bike', 'Y'],\n",
    "    ['i have no animal at home', 'N'],\n",
    "    ['dogs like cheese', 'N'], \n",
    "    ['a pink flamingo','N'],\n",
    "    ['trucks','Y'],\n",
    "    ['truckks','Y'],\n",
    "    ['truckmegatruck', 'Y'], \n",
    "    ['a text about trucks, not animals','Y'], \n",
    "    ['a text about animals, not trucks','N'],\n",
    "    ['doggs','N']\n",
    "], columns=['text', 'label'])\n",
    "vdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21637614",
   "metadata": {},
   "source": [
    "La première étape  consiste à transformer les données (label numérique, tokenization, construction d'un batch.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7054c753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 18\n",
      "}) Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 13\n",
      "})\n",
      "['the road is straight', 'the black cat plays with a ball', 'a big dog with a ball', 'dog and cat are together', 'traffic jam on the 6th road', 'white bird on a big tree', 'a big truck', 'two cars crashed', 'two deers in a field', 'I like ridding my bike', 'a lion in the savane', 'a motorcycle rides on the road', 'it is a bike, it is not a flamingo', 'it is not a bike, it is a flamingo', 'a mouse bitten by a cat', 'two pigs in the mood', 'take a plane is sometimes slower than taking train', 'take the highway']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 18/18 [00:00<00:00, 2759.51 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'labels'],\n",
      "    num_rows: 18\n",
      "})\n",
      "[[0, 586, 14, 1362], [0, 521, 5450, 1381, 17, 7, 1083], [7, 365, 2926, 17, 7, 1083], [2926, 5, 5450, 32, 600], [1596, 8202, 13, 0, 14358, 586], [298, 2632, 13, 7, 365, 2654], [7, 365, 2575], [55, 1277, 3732], [55, 134183, 6, 7, 610], [400000, 117, 40085, 192, 7696], [7, 6657, 6, 0, 232439], [7, 7213, 9371, 13, 0, 586], [20, 14, 7, 7696, 20, 14, 36, 7, 35115], [20, 14, 36, 7, 7696, 20, 14, 7, 35115], [7, 7571, 22022, 21, 7, 5450], [55, 9609, 6, 0, 5030], [190, 7, 1313, 14, 1071, 6914, 73, 582, 1470], [190, 0, 1883]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 13/13 [00:00<00:00, 7574.10 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'labels'],\n",
      "    num_rows: 13\n",
      "})\n",
      "[[0, 7696, 5919, 13, 0, 586], [7, 6657, 5, 7, 5450, 6, 7, 2654], [55, 1277, 3732], [41, 690, 242, 4, 161, 21, 7696], [41, 33, 84, 2694, 22, 163], [3876, 117, 5795], [7, 5491, 35115], [3597], [400000], [400000], [7, 2829, 59, 3597, 36, 2430], [7, 2829, 59, 2430, 36, 3597], [400000]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train_dataset = Dataset.from_pandas(tdf)\n",
    "validation_dataset = Dataset.from_pandas(vdf)\n",
    "print(train_dataset,validation_dataset)\n",
    "print(train_dataset[\"text\"])\n",
    "\n",
    "# Creation d'un index de padding pour les positions a ignorer par le modèle (e.g., mots inconnus)\n",
    "pad_idx=embeds.vectors.shape[0]\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    vocab=embeds.key_to_index\n",
    "    inputs= getlistwordsentence2(examples[\"text\"])\n",
    "    \n",
    "    inputs = [[(vocab[word] if word in vocab else pad_idx) for word in sentence] for sentence in inputs] \n",
    "    \n",
    "    labels = [(1 if l=='Y' else 0) for l in examples[\"label\"]] \n",
    "    ret = {}\n",
    "    ret[\"input_ids\"]=inputs\n",
    "    ret[\"labels\"]=labels\n",
    "    \n",
    "    return ret\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True, remove_columns=train_dataset.features.keys(), load_from_cache_file=True)\n",
    "print(train_dataset)\n",
    "print(train_dataset[\"input_ids\"])\n",
    "validation_dataset = validation_dataset.map(preprocess_function, batched=True, remove_columns=validation_dataset.features.keys(), load_from_cache_file=True)\n",
    "print(validation_dataset)\n",
    "print(validation_dataset[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa696ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': tensor([1, 0, 1, 0]), 'input_ids': tensor([[   190,      7,   1313,     14,   1071,   6914,     73,    582,   1470],\n",
      "        [    20,     14,     36,      7,   7696,     20,     14,      7,  35115],\n",
      "        [    20,     14,      7,   7696,     20,     14,     36,      7,  35115],\n",
      "        [     0,    521,   5450,   1381,     17,      7,   1083, 400000, 400000]])}\n",
      "{'labels': tensor([0, 0, 1, 0]), 'input_ids': tensor([[    7,   365,  2926,    17,     7,  1083],\n",
      "        [  298,  2632,    13,     7,   365,  2654],\n",
      "        [    7,  7213,  9371,    13,     0,   586],\n",
      "        [    7,  7571, 22022,    21,     7,  5450]])}\n",
      "{'labels': tensor([1, 0, 0, 0]), 'input_ids': tensor([[  1596,   8202,     13,      0,  14358,    586],\n",
      "        [    55, 134183,      6,      7,    610, 400000],\n",
      "        [     7,   6657,      6,      0, 232439, 400000],\n",
      "        [    55,   9609,      6,      0,   5030, 400000]])}\n",
      "{'labels': tensor([0, 1, 1, 1]), 'input_ids': tensor([[  2926,      5,   5450,     32,    600],\n",
      "        [400000,    117,  40085,    192,   7696],\n",
      "        [     0,    586,     14,   1362, 400000],\n",
      "        [   190,      0,   1883, 400000, 400000]])}\n",
      "{'labels': tensor([1, 1]), 'input_ids': tensor([[   7,  365, 2575],\n",
      "        [  55, 1277, 3732]])}\n",
      "{'labels': tensor([0, 1, 0, 1]), 'input_ids': tensor([[     7,   6657,      5,      7,   5450,      6,      7,   2654],\n",
      "        [    41,    690,    242,      4,    161,     21,   7696, 400000],\n",
      "        [     7,   2829,     59,   2430,     36,   3597, 400000, 400000],\n",
      "        [     7,   2829,     59,   3597,     36,   2430, 400000, 400000]])}\n",
      "{'labels': tensor([1, 0, 0, 1]), 'input_ids': tensor([[     0,   7696,   5919,     13,      0,    586],\n",
      "        [    41,     33,     84,   2694,     22,    163],\n",
      "        [  3876,    117,   5795, 400000, 400000, 400000],\n",
      "        [    55,   1277,   3732, 400000, 400000, 400000]])}\n",
      "{'labels': tensor([0, 1, 1, 0]), 'input_ids': tensor([[     7,   5491,  35115],\n",
      "        [400000, 400000, 400000],\n",
      "        [400000, 400000, 400000],\n",
      "        [400000, 400000, 400000]])}\n",
      "{'labels': tensor([1]), 'input_ids': tensor([[3597]])}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers.trainer_pt_utils import LengthGroupedSampler,Sampler,get_length_grouped_indices\n",
    "\n",
    "batchsize=4\n",
    "megabatch_mul=16\n",
    "\n",
    "\n",
    "# Un sampler permet de générer les batchs en selectionnant des indices d'échantillons de manière aléatoire\n",
    "# dans les données d'entrée\n",
    "# Ici on souhaite minimiser le padding, donc on choisit de regrouper au maximum les sequences par longueur \n",
    "# dans les batchs\n",
    "# C'est fait selon les étapes suivantes :  \n",
    "#     1 - Tirage d'une permutation des indices de manière aléatoire\n",
    "#     2 - Découpage de la liste en megabatchs de taille batch_size*megabatch_mul\n",
    "#     3 - Tri des éléments par ordre de longueur déscendante à l'intérieur de chaque megabatch\n",
    "#     4 - Concaténation des listes d'indices retriées localement \n",
    "#     5 - Découpage en batch de taille batchsize\n",
    "#\n",
    "# Ainsi:\n",
    "#     - si megabatch_mul trop grand, alors aucun aléatoire (ce qui peu être gênant pour l'apprentissage dans certains cas)\n",
    "#                                   ==> les batchs seront toujours les mêmes (données triées par longueur, minimisation optimale du padding)\n",
    "#     - si megabatch_mul trop petit (e.g. = 1), alors batchs complètement aléatoires (séquences simplement ordonnées à l'intérieur du batch, pas d'optimisation sur la minimisation du padding) \n",
    "#                                   ==> beaucoup de padding possible (rajoute de la complexité à l'apprentissage)\n",
    "#\n",
    "# Il s'agit de trouver un bon compromis, jouer en tp avec les valeurs de megabatch_mul et observer les effets\n",
    "class LengthGroupedSampler(Sampler):\n",
    "    r\"\"\"\n",
    "    Sampler that samples indices in a way that groups together features of the dataset of roughly the same length while\n",
    "    keeping a bit of randomness.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,batch_size,dataset, megabatch_mul=None):\n",
    "        self.batch_size = batch_size\n",
    "        lengths = [len(sample[\"input_ids\"]) for sample in dataset]\n",
    "        self.lengths = lengths\n",
    "        self.megabatch_mul=megabatch_mul\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lengths)\n",
    "\n",
    "    def __iter__(self):\n",
    "        indices = get_length_grouped_indices(self.lengths, self.batch_size, self.megabatch_mul)\n",
    "        return iter(indices)\n",
    "\n",
    "    \n",
    "# Fonction qui produit des tenseurs pytorch a partir d'un batch de données issu du sampler\n",
    "#\n",
    "# ici on crée deux tenseurs par batch :\n",
    "#         - un tenseur pour les labels\n",
    "#         - un tenseur pour les index de mots des séquences, complétées avec l'idex de padding pad_idx pour les séquences plus courtes (afin d'avoir des données de même taille sur chaque ligne du tenseur, ce qui est requis pour leur création) \n",
    "def data_collator(batch):\n",
    "    first = batch[0]\n",
    "    ret = {}\n",
    "    dtype = torch.long if type(first[\"labels\"]) is int else torch.float\n",
    "    ret[\"labels\"] = torch.tensor([f[\"labels\"] for f in batch], dtype=dtype)\n",
    "    longest=max([len(l[\"input_ids\"]) for l in batch])\n",
    "    s = np.stack([np.pad(x[\"input_ids\"], (0, longest - len(x[\"input_ids\"])), constant_values=pad_idx) for x in batch])\n",
    "    ret[\"input_ids\"] = torch.tensor(s)\n",
    "    return ret\n",
    "\n",
    "\n",
    "train_sampler = LengthGroupedSampler(batchsize, train_dataset, megabatch_mul)\n",
    "\n",
    "train_loader=DataLoader(train_dataset,batchsize,sampler=train_sampler,collate_fn=data_collator,pin_memory=True, shuffle=False, num_workers=0)\n",
    "\n",
    "for data in train_loader:\n",
    "    print(data)\n",
    "\n",
    "test_sampler = LengthGroupedSampler(batchsize, validation_dataset, megabatch_mul)    \n",
    "    \n",
    "test_loader=DataLoader(validation_dataset,batchsize,sampler=test_sampler,collate_fn=data_collator,pin_memory=True, shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "for data in test_loader:\n",
    "    print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a976a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': tensor([0, 1, 1, 0]), 'input_ids': tensor([[     7,   6657,      5,      7,   5450,      6,      7,   2654],\n",
      "        [    41,    690,    242,      4,    161,     21,   7696, 400000],\n",
      "        [     0,   7696,   5919,     13,      0,    586, 400000, 400000],\n",
      "        [     7,   2829,     59,   2430,     36,   3597, 400000, 400000]])}\n",
      "['a lion and a cat in a tree', 'i always go to work by bike', 'the bike drives on the road', 'a text about animals not trucks']\n",
      "{'labels': tensor([1, 0, 1, 0]), 'input_ids': tensor([[     7,   2829,     59,   3597,     36,   2430],\n",
      "        [    41,     33,     84,   2694,     22,    163],\n",
      "        [    55,   1277,   3732, 400000, 400000, 400000],\n",
      "        [  3876,    117,   5795, 400000, 400000, 400000]])}\n",
      "['a text about trucks not animals', 'i have no animal at home', 'two cars crashed', 'dogs like cheese']\n",
      "{'labels': tensor([0, 0, 1, 1]), 'input_ids': tensor([[     7,   5491,  35115],\n",
      "        [400000, 400000, 400000],\n",
      "        [400000, 400000, 400000],\n",
      "        [400000, 400000, 400000]])}\n",
      "['a pink flamingo', '', '', '']\n",
      "{'labels': tensor([1]), 'input_ids': tensor([[3597]])}\n",
      "['trucks']\n"
     ]
    }
   ],
   "source": [
    "### On peut créer une fonction de décodage des batchs\n",
    "def batch_decode(input_ids):\n",
    "    x=input_ids.data\n",
    "    x=[\" \".join([embeds.index_to_key[i] for i in s if i!=pad_idx]) for s in x]\n",
    "    return x\n",
    "\n",
    "for data in test_loader:\n",
    "    print(data)\n",
    "    print(batch_decode(data[\"input_ids\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b42c9e",
   "metadata": {},
   "source": [
    "On note que le dernier texte \"truckmegatruck\" est décodé sous la chaîne vide: il ne contient que des mots inconnus, donc ignorés... \n",
    "\n",
    "### Modèle de classifcation simple\n",
    "\n",
    "On construit ensuite le modèle de classification avec torch. La couche d'entrée correspond aux embeddings, implémentés par nn.Embedding(). nn.Embedding() permet de construire la matrice d'embeddings sur l'ensemble du vocabulaire. Ils sont ensuite \"activés\" en fonction du texte d'entrée. Le modèle à construire fait une simple moyenne des embeddings des mots des textes, applique une activation tanh et envoie le resultat à travers un Linear à deux sorties. Penser à indiquer l'index de padding à la construction de nn.Embedding, car çà permet de ne pas les prendre en compte dans les calculs de gradients (et de conserver ces embeddings vides à zeros pour ne pas en dépendre => invariance par rapport à la longueur). Penser aussi à ajouter de la l2 sur les embeddings (très importants pour que les représentations ne s'éparpillent pas aux confins de l'espace de représentation!). On pourra aussi ajouter du Dropout. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9c1ea3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, padding_idx,dropout=0):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        self.linear = nn.Linear(embedding_dim, 2)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.tanh = nn.Tanh()        \n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        avg = embedded.mean(dim=1)\n",
    "        avg = self.dropout(avg)\n",
    "        activated = self.tanh(avg)\n",
    "        x = self.linear(activated)\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbe0f84",
   "metadata": {},
   "source": [
    "On entraîne et teste le modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ad8b49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(train_iter, test_iter, model, loss_function, optimizer, epochs, clip=-1):\n",
    "  for epoch in range(epochs):\n",
    "      epoch_loss = 0\n",
    "      epoch_accuracy = 0\n",
    "      model.train()\n",
    "      nb_samples=0\n",
    "      for batch in train_iter:\n",
    "          optimizer.zero_grad()\n",
    "          #print(\"text shape \",batch.text.T.shape)\n",
    "          prediction = model(batch[\"input_ids\"])\n",
    "          if not isinstance(prediction,torch.Tensor):  \n",
    "                prediction = prediction[\"logits\"]\n",
    "          #print(prediction)\n",
    "          loss = loss_function(prediction, batch[\"labels\"])\n",
    "\n",
    "          loss.backward()\n",
    "          if clip>0:\n",
    "              torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "          optimizer.step()\n",
    "          nb_samples+=prediction.shape[0]\n",
    "          epoch_loss+=loss.item()*prediction.shape[0]\n",
    "          preds=(prediction[:,1]>prediction[:,0])*1.0\n",
    "          accuracy=(preds==batch[\"labels\"]).sum()\n",
    "          epoch_accuracy+=accuracy.item()\n",
    "      print('train loss on epoch {} : {:.3f}'.format(epoch, epoch_loss/nb_samples))\n",
    "      print('train accuracy on epoch {}: {:.3f}'.format(epoch, epoch_accuracy/nb_samples))\n",
    "      \n",
    "      model.eval()\n",
    "      test_loss = 0\n",
    "      test_accuracy = 0\n",
    "      nb_samples=0\n",
    "      accuracy=0\n",
    "      for batch in test_iter:\n",
    "          #print(\"test \",batch)\n",
    "          with torch.no_grad():\n",
    "              optimizer.zero_grad()\n",
    "              prediction = model(batch[\"input_ids\"])\n",
    "              if not isinstance(prediction,torch.Tensor):\n",
    "                    prediction = prediction[\"logits\"]\n",
    "              loss = loss_function(prediction, batch[\"labels\"])\n",
    "              nb_samples+=prediction.shape[0]\n",
    "              test_loss+=loss.item()*prediction.shape[0]\n",
    "              #print(batch_decode(batch[\"input_ids\"]))\n",
    "              preds=(prediction[:,1]>prediction[:,0])*1.0\n",
    "              accuracy=(preds==batch[\"labels\"]).sum()\n",
    "              test_accuracy+=accuracy.item()  \n",
    "              #print(preds,accuracy)\n",
    "      print('test loss on epoch {}: {:.3f}'.format(epoch, test_loss/nb_samples))\n",
    "      print('test accuracy on epoch {}: {:.3f}'.format(epoch, test_accuracy/nb_samples))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1051f628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss on epoch 0 : 0.730\n",
      "train accuracy on epoch 0: 0.500\n",
      "test loss on epoch 0: 0.642\n",
      "test accuracy on epoch 0: 0.692\n",
      "train loss on epoch 1 : 0.622\n",
      "train accuracy on epoch 1: 0.778\n",
      "test loss on epoch 1: 0.629\n",
      "test accuracy on epoch 1: 0.692\n",
      "train loss on epoch 2 : 0.563\n",
      "train accuracy on epoch 2: 0.778\n",
      "test loss on epoch 2: 0.618\n",
      "test accuracy on epoch 2: 0.615\n",
      "train loss on epoch 3 : 0.500\n",
      "train accuracy on epoch 3: 0.889\n",
      "test loss on epoch 3: 0.605\n",
      "test accuracy on epoch 3: 0.769\n",
      "train loss on epoch 4 : 0.504\n",
      "train accuracy on epoch 4: 0.833\n",
      "test loss on epoch 4: 0.572\n",
      "test accuracy on epoch 4: 0.769\n",
      "train loss on epoch 5 : 0.537\n",
      "train accuracy on epoch 5: 0.722\n",
      "test loss on epoch 5: 0.568\n",
      "test accuracy on epoch 5: 0.769\n",
      "train loss on epoch 6 : 0.478\n",
      "train accuracy on epoch 6: 0.833\n",
      "test loss on epoch 6: 0.552\n",
      "test accuracy on epoch 6: 0.769\n",
      "train loss on epoch 7 : 0.416\n",
      "train accuracy on epoch 7: 0.889\n",
      "test loss on epoch 7: 0.545\n",
      "test accuracy on epoch 7: 0.769\n",
      "train loss on epoch 8 : 0.484\n",
      "train accuracy on epoch 8: 0.889\n",
      "test loss on epoch 8: 0.568\n",
      "test accuracy on epoch 8: 0.769\n",
      "train loss on epoch 9 : 0.420\n",
      "train accuracy on epoch 9: 0.778\n",
      "test loss on epoch 9: 0.572\n",
      "test accuracy on epoch 9: 0.769\n",
      "train loss on epoch 10 : 0.390\n",
      "train accuracy on epoch 10: 0.944\n",
      "test loss on epoch 10: 0.569\n",
      "test accuracy on epoch 10: 0.769\n",
      "train loss on epoch 11 : 0.429\n",
      "train accuracy on epoch 11: 0.889\n",
      "test loss on epoch 11: 0.548\n",
      "test accuracy on epoch 11: 0.769\n",
      "train loss on epoch 12 : 0.397\n",
      "train accuracy on epoch 12: 0.778\n",
      "test loss on epoch 12: 0.571\n",
      "test accuracy on epoch 12: 0.769\n",
      "train loss on epoch 13 : 0.315\n",
      "train accuracy on epoch 13: 0.944\n",
      "test loss on epoch 13: 0.562\n",
      "test accuracy on epoch 13: 0.692\n",
      "train loss on epoch 14 : 0.345\n",
      "train accuracy on epoch 14: 1.000\n",
      "test loss on epoch 14: 0.595\n",
      "test accuracy on epoch 14: 0.615\n",
      "train loss on epoch 15 : 0.398\n",
      "train accuracy on epoch 15: 0.778\n",
      "test loss on epoch 15: 0.561\n",
      "test accuracy on epoch 15: 0.692\n",
      "train loss on epoch 16 : 0.308\n",
      "train accuracy on epoch 16: 0.944\n",
      "test loss on epoch 16: 0.594\n",
      "test accuracy on epoch 16: 0.538\n",
      "train loss on epoch 17 : 0.346\n",
      "train accuracy on epoch 17: 0.889\n",
      "test loss on epoch 17: 0.583\n",
      "test accuracy on epoch 17: 0.692\n",
      "train loss on epoch 18 : 0.283\n",
      "train accuracy on epoch 18: 0.944\n",
      "test loss on epoch 18: 0.553\n",
      "test accuracy on epoch 18: 0.615\n",
      "train loss on epoch 19 : 0.332\n",
      "train accuracy on epoch 19: 0.889\n",
      "test loss on epoch 19: 0.591\n",
      "test accuracy on epoch 19: 0.692\n",
      "train loss on epoch 20 : 0.344\n",
      "train accuracy on epoch 20: 0.944\n",
      "test loss on epoch 20: 0.583\n",
      "test accuracy on epoch 20: 0.692\n",
      "train loss on epoch 21 : 0.279\n",
      "train accuracy on epoch 21: 0.944\n",
      "test loss on epoch 21: 0.539\n",
      "test accuracy on epoch 21: 0.692\n",
      "train loss on epoch 22 : 0.248\n",
      "train accuracy on epoch 22: 0.944\n",
      "test loss on epoch 22: 0.541\n",
      "test accuracy on epoch 22: 0.692\n",
      "train loss on epoch 23 : 0.285\n",
      "train accuracy on epoch 23: 0.944\n",
      "test loss on epoch 23: 0.538\n",
      "test accuracy on epoch 23: 0.692\n",
      "train loss on epoch 24 : 0.225\n",
      "train accuracy on epoch 24: 0.944\n",
      "test loss on epoch 24: 0.542\n",
      "test accuracy on epoch 24: 0.692\n",
      "train loss on epoch 25 : 0.203\n",
      "train accuracy on epoch 25: 1.000\n",
      "test loss on epoch 25: 0.545\n",
      "test accuracy on epoch 25: 0.692\n",
      "train loss on epoch 26 : 0.238\n",
      "train accuracy on epoch 26: 0.944\n",
      "test loss on epoch 26: 0.541\n",
      "test accuracy on epoch 26: 0.692\n",
      "train loss on epoch 27 : 0.251\n",
      "train accuracy on epoch 27: 0.889\n",
      "test loss on epoch 27: 0.570\n",
      "test accuracy on epoch 27: 0.692\n",
      "train loss on epoch 28 : 0.258\n",
      "train accuracy on epoch 28: 0.889\n",
      "test loss on epoch 28: 0.568\n",
      "test accuracy on epoch 28: 0.692\n",
      "train loss on epoch 29 : 0.187\n",
      "train accuracy on epoch 29: 0.944\n",
      "test loss on epoch 29: 0.569\n",
      "test accuracy on epoch 29: 0.692\n",
      "train loss on epoch 30 : 0.240\n",
      "train accuracy on epoch 30: 0.889\n",
      "test loss on epoch 30: 0.536\n",
      "test accuracy on epoch 30: 0.692\n",
      "train loss on epoch 31 : 0.229\n",
      "train accuracy on epoch 31: 0.944\n",
      "test loss on epoch 31: 0.530\n",
      "test accuracy on epoch 31: 0.692\n",
      "train loss on epoch 32 : 0.212\n",
      "train accuracy on epoch 32: 0.944\n",
      "test loss on epoch 32: 0.539\n",
      "test accuracy on epoch 32: 0.692\n",
      "train loss on epoch 33 : 0.216\n",
      "train accuracy on epoch 33: 0.944\n",
      "test loss on epoch 33: 0.536\n",
      "test accuracy on epoch 33: 0.692\n",
      "train loss on epoch 34 : 0.210\n",
      "train accuracy on epoch 34: 0.944\n",
      "test loss on epoch 34: 0.537\n",
      "test accuracy on epoch 34: 0.692\n",
      "train loss on epoch 35 : 0.216\n",
      "train accuracy on epoch 35: 0.944\n",
      "test loss on epoch 35: 0.579\n",
      "test accuracy on epoch 35: 0.692\n",
      "train loss on epoch 36 : 0.249\n",
      "train accuracy on epoch 36: 0.944\n",
      "test loss on epoch 36: 0.539\n",
      "test accuracy on epoch 36: 0.692\n",
      "train loss on epoch 37 : 0.233\n",
      "train accuracy on epoch 37: 0.889\n",
      "test loss on epoch 37: 0.583\n",
      "test accuracy on epoch 37: 0.692\n",
      "train loss on epoch 38 : 0.194\n",
      "train accuracy on epoch 38: 0.889\n",
      "test loss on epoch 38: 0.530\n",
      "test accuracy on epoch 38: 0.692\n",
      "train loss on epoch 39 : 0.204\n",
      "train accuracy on epoch 39: 0.944\n",
      "test loss on epoch 39: 0.531\n",
      "test accuracy on epoch 39: 0.692\n",
      "train loss on epoch 40 : 0.180\n",
      "train accuracy on epoch 40: 1.000\n",
      "test loss on epoch 40: 0.576\n",
      "test accuracy on epoch 40: 0.692\n",
      "train loss on epoch 41 : 0.226\n",
      "train accuracy on epoch 41: 0.944\n",
      "test loss on epoch 41: 0.542\n",
      "test accuracy on epoch 41: 0.692\n",
      "train loss on epoch 42 : 0.213\n",
      "train accuracy on epoch 42: 0.944\n",
      "test loss on epoch 42: 0.538\n",
      "test accuracy on epoch 42: 0.692\n",
      "train loss on epoch 43 : 0.209\n",
      "train accuracy on epoch 43: 0.889\n",
      "test loss on epoch 43: 0.575\n",
      "test accuracy on epoch 43: 0.692\n",
      "train loss on epoch 44 : 0.171\n",
      "train accuracy on epoch 44: 0.944\n",
      "test loss on epoch 44: 0.537\n",
      "test accuracy on epoch 44: 0.692\n",
      "train loss on epoch 45 : 0.193\n",
      "train accuracy on epoch 45: 0.944\n",
      "test loss on epoch 45: 0.542\n",
      "test accuracy on epoch 45: 0.769\n",
      "train loss on epoch 46 : 0.187\n",
      "train accuracy on epoch 46: 0.944\n",
      "test loss on epoch 46: 0.542\n",
      "test accuracy on epoch 46: 0.769\n",
      "train loss on epoch 47 : 0.201\n",
      "train accuracy on epoch 47: 0.944\n",
      "test loss on epoch 47: 0.541\n",
      "test accuracy on epoch 47: 0.769\n",
      "train loss on epoch 48 : 0.196\n",
      "train accuracy on epoch 48: 0.944\n",
      "test loss on epoch 48: 0.525\n",
      "test accuracy on epoch 48: 0.769\n",
      "train loss on epoch 49 : 0.157\n",
      "train accuracy on epoch 49: 1.000\n",
      "test loss on epoch 49: 0.569\n",
      "test accuracy on epoch 49: 0.769\n",
      "train loss on epoch 50 : 0.177\n",
      "train accuracy on epoch 50: 1.000\n",
      "test loss on epoch 50: 0.534\n",
      "test accuracy on epoch 50: 0.769\n",
      "train loss on epoch 51 : 0.199\n",
      "train accuracy on epoch 51: 0.889\n",
      "test loss on epoch 51: 0.540\n",
      "test accuracy on epoch 51: 0.769\n",
      "train loss on epoch 52 : 0.202\n",
      "train accuracy on epoch 52: 1.000\n",
      "test loss on epoch 52: 0.533\n",
      "test accuracy on epoch 52: 0.769\n",
      "train loss on epoch 53 : 0.171\n",
      "train accuracy on epoch 53: 1.000\n",
      "test loss on epoch 53: 0.540\n",
      "test accuracy on epoch 53: 0.769\n",
      "train loss on epoch 54 : 0.193\n",
      "train accuracy on epoch 54: 0.889\n",
      "test loss on epoch 54: 0.535\n",
      "test accuracy on epoch 54: 0.769\n",
      "train loss on epoch 55 : 0.196\n",
      "train accuracy on epoch 55: 0.889\n",
      "test loss on epoch 55: 0.541\n",
      "test accuracy on epoch 55: 0.769\n",
      "train loss on epoch 56 : 0.182\n",
      "train accuracy on epoch 56: 0.944\n",
      "test loss on epoch 56: 0.531\n",
      "test accuracy on epoch 56: 0.769\n",
      "train loss on epoch 57 : 0.183\n",
      "train accuracy on epoch 57: 0.889\n",
      "test loss on epoch 57: 0.535\n",
      "test accuracy on epoch 57: 0.769\n",
      "train loss on epoch 58 : 0.180\n",
      "train accuracy on epoch 58: 0.944\n",
      "test loss on epoch 58: 0.540\n",
      "test accuracy on epoch 58: 0.769\n",
      "train loss on epoch 59 : 0.196\n",
      "train accuracy on epoch 59: 1.000\n",
      "test loss on epoch 59: 0.534\n",
      "test accuracy on epoch 59: 0.769\n",
      "train loss on epoch 60 : 0.158\n",
      "train accuracy on epoch 60: 0.944\n",
      "test loss on epoch 60: 0.530\n",
      "test accuracy on epoch 60: 0.769\n",
      "train loss on epoch 61 : 0.176\n",
      "train accuracy on epoch 61: 1.000\n",
      "test loss on epoch 61: 0.539\n",
      "test accuracy on epoch 61: 0.769\n",
      "train loss on epoch 62 : 0.209\n",
      "train accuracy on epoch 62: 0.944\n",
      "test loss on epoch 62: 0.533\n",
      "test accuracy on epoch 62: 0.769\n",
      "train loss on epoch 63 : 0.185\n",
      "train accuracy on epoch 63: 0.944\n",
      "test loss on epoch 63: 0.519\n",
      "test accuracy on epoch 63: 0.769\n",
      "train loss on epoch 64 : 0.185\n",
      "train accuracy on epoch 64: 0.944\n",
      "test loss on epoch 64: 0.572\n",
      "test accuracy on epoch 64: 0.769\n",
      "train loss on epoch 65 : 0.205\n",
      "train accuracy on epoch 65: 0.889\n",
      "test loss on epoch 65: 0.534\n",
      "test accuracy on epoch 65: 0.769\n",
      "train loss on epoch 66 : 0.205\n",
      "train accuracy on epoch 66: 0.889\n",
      "test loss on epoch 66: 0.571\n",
      "test accuracy on epoch 66: 0.769\n",
      "train loss on epoch 67 : 0.182\n",
      "train accuracy on epoch 67: 0.889\n",
      "test loss on epoch 67: 0.570\n",
      "test accuracy on epoch 67: 0.692\n",
      "train loss on epoch 68 : 0.193\n",
      "train accuracy on epoch 68: 0.944\n",
      "test loss on epoch 68: 0.538\n",
      "test accuracy on epoch 68: 0.769\n",
      "train loss on epoch 69 : 0.180\n",
      "train accuracy on epoch 69: 0.944\n",
      "test loss on epoch 69: 0.573\n",
      "test accuracy on epoch 69: 0.769\n",
      "train loss on epoch 70 : 0.148\n",
      "train accuracy on epoch 70: 1.000\n",
      "test loss on epoch 70: 0.568\n",
      "test accuracy on epoch 70: 0.769\n",
      "train loss on epoch 71 : 0.184\n",
      "train accuracy on epoch 71: 0.944\n",
      "test loss on epoch 71: 0.534\n",
      "test accuracy on epoch 71: 0.692\n",
      "train loss on epoch 72 : 0.178\n",
      "train accuracy on epoch 72: 0.944\n",
      "test loss on epoch 72: 0.547\n",
      "test accuracy on epoch 72: 0.692\n",
      "train loss on epoch 73 : 0.164\n",
      "train accuracy on epoch 73: 1.000\n",
      "test loss on epoch 73: 0.524\n",
      "test accuracy on epoch 73: 0.692\n",
      "train loss on epoch 74 : 0.158\n",
      "train accuracy on epoch 74: 1.000\n",
      "test loss on epoch 74: 0.531\n",
      "test accuracy on epoch 74: 0.692\n",
      "train loss on epoch 75 : 0.171\n",
      "train accuracy on epoch 75: 0.944\n",
      "test loss on epoch 75: 0.542\n",
      "test accuracy on epoch 75: 0.692\n",
      "train loss on epoch 76 : 0.156\n",
      "train accuracy on epoch 76: 1.000\n",
      "test loss on epoch 76: 0.535\n",
      "test accuracy on epoch 76: 0.692\n",
      "train loss on epoch 77 : 0.178\n",
      "train accuracy on epoch 77: 0.944\n",
      "test loss on epoch 77: 0.567\n",
      "test accuracy on epoch 77: 0.692\n",
      "train loss on epoch 78 : 0.156\n",
      "train accuracy on epoch 78: 0.944\n",
      "test loss on epoch 78: 0.522\n",
      "test accuracy on epoch 78: 0.692\n",
      "train loss on epoch 79 : 0.159\n",
      "train accuracy on epoch 79: 1.000\n",
      "test loss on epoch 79: 0.546\n",
      "test accuracy on epoch 79: 0.692\n",
      "train loss on epoch 80 : 0.159\n",
      "train accuracy on epoch 80: 1.000\n",
      "test loss on epoch 80: 0.569\n",
      "test accuracy on epoch 80: 0.692\n",
      "train loss on epoch 81 : 0.186\n",
      "train accuracy on epoch 81: 0.944\n",
      "test loss on epoch 81: 0.542\n",
      "test accuracy on epoch 81: 0.692\n",
      "train loss on epoch 82 : 0.178\n",
      "train accuracy on epoch 82: 0.889\n",
      "test loss on epoch 82: 0.538\n",
      "test accuracy on epoch 82: 0.692\n",
      "train loss on epoch 83 : 0.204\n",
      "train accuracy on epoch 83: 0.889\n",
      "test loss on epoch 83: 0.580\n",
      "test accuracy on epoch 83: 0.692\n",
      "train loss on epoch 84 : 0.164\n",
      "train accuracy on epoch 84: 1.000\n",
      "test loss on epoch 84: 0.540\n",
      "test accuracy on epoch 84: 0.692\n",
      "train loss on epoch 85 : 0.158\n",
      "train accuracy on epoch 85: 0.944\n",
      "test loss on epoch 85: 0.531\n",
      "test accuracy on epoch 85: 0.692\n",
      "train loss on epoch 86 : 0.195\n",
      "train accuracy on epoch 86: 0.889\n",
      "test loss on epoch 86: 0.526\n",
      "test accuracy on epoch 86: 0.692\n",
      "train loss on epoch 87 : 0.176\n",
      "train accuracy on epoch 87: 0.889\n",
      "test loss on epoch 87: 0.579\n",
      "test accuracy on epoch 87: 0.692\n",
      "train loss on epoch 88 : 0.143\n",
      "train accuracy on epoch 88: 1.000\n",
      "test loss on epoch 88: 0.535\n",
      "test accuracy on epoch 88: 0.692\n",
      "train loss on epoch 89 : 0.191\n",
      "train accuracy on epoch 89: 0.889\n",
      "test loss on epoch 89: 0.536\n",
      "test accuracy on epoch 89: 0.692\n",
      "train loss on epoch 90 : 0.206\n",
      "train accuracy on epoch 90: 0.889\n",
      "test loss on epoch 90: 0.528\n",
      "test accuracy on epoch 90: 0.692\n",
      "train loss on epoch 91 : 0.122\n",
      "train accuracy on epoch 91: 1.000\n",
      "test loss on epoch 91: 0.544\n",
      "test accuracy on epoch 91: 0.692\n",
      "train loss on epoch 92 : 0.156\n",
      "train accuracy on epoch 92: 0.944\n",
      "test loss on epoch 92: 0.573\n",
      "test accuracy on epoch 92: 0.692\n",
      "train loss on epoch 93 : 0.141\n",
      "train accuracy on epoch 93: 1.000\n",
      "test loss on epoch 93: 0.574\n",
      "test accuracy on epoch 93: 0.692\n",
      "train loss on epoch 94 : 0.184\n",
      "train accuracy on epoch 94: 0.944\n",
      "test loss on epoch 94: 0.534\n",
      "test accuracy on epoch 94: 0.692\n",
      "train loss on epoch 95 : 0.195\n",
      "train accuracy on epoch 95: 0.889\n",
      "test loss on epoch 95: 0.575\n",
      "test accuracy on epoch 95: 0.692\n",
      "train loss on epoch 96 : 0.166\n",
      "train accuracy on epoch 96: 0.944\n",
      "test loss on epoch 96: 0.526\n",
      "test accuracy on epoch 96: 0.692\n",
      "train loss on epoch 97 : 0.175\n",
      "train accuracy on epoch 97: 0.944\n",
      "test loss on epoch 97: 0.574\n",
      "test accuracy on epoch 97: 0.692\n",
      "train loss on epoch 98 : 0.201\n",
      "train accuracy on epoch 98: 0.889\n",
      "test loss on epoch 98: 0.576\n",
      "test accuracy on epoch 98: 0.692\n",
      "train loss on epoch 99 : 0.172\n",
      "train accuracy on epoch 99: 0.889\n",
      "test loss on epoch 99: 0.547\n",
      "test accuracy on epoch 99: 0.692\n"
     ]
    }
   ],
   "source": [
    "# penser à ajouter un embedding pour pad_idx        \n",
    "net = MyModel(pad_idx+1, 50, pad_idx,dropout=0.5)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(net.parameters(), lr=0.01,weight_decay=0.01)\n",
    "epochs = 100\n",
    "train_test(train_loader, test_loader, net, loss_function, optimizer, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "384b3da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the bike drives on the road {'logits': tensor([[-1.1689,  1.3239]]), 'prediction': tensor([1.])} truth= 1\n",
      "a lion and a cat in a tree {'logits': tensor([[ 2.0828, -1.8948]]), 'prediction': tensor([0.])} truth= 0\n",
      "two cars crashed {'logits': tensor([[-1.5177,  1.6628]]), 'prediction': tensor([1.])} truth= 1\n",
      "i always go to work by bike {'logits': tensor([[-0.1045,  0.2694]]), 'prediction': tensor([1.])} truth= 1\n",
      "i have no animal at home {'logits': tensor([[-0.0293,  0.1937]]), 'prediction': tensor([1.])} truth= 0\n",
      "dogs like cheese {'logits': tensor([[-0.5369,  0.7009]]), 'prediction': tensor([1.])} truth= 0\n",
      "a pink flamingo {'logits': tensor([[ 0.6736, -0.5018]]), 'prediction': tensor([0.])} truth= 0\n",
      "trucks {'logits': tensor([[-0.1065,  0.2673]]), 'prediction': tensor([1.])} truth= 1\n",
      "truckks {'logits': tensor([[-0.0521,  0.2196]]), 'prediction': tensor([1.])} truth= 1\n",
      "truckmegatruck {'logits': tensor([[-0.0521,  0.2196]]), 'prediction': tensor([1.])} truth= 1\n",
      "a text about trucks, not animals {'logits': tensor([[ 0.2644, -0.0968]]), 'prediction': tensor([0.])} truth= 1\n",
      "a text about animals, not trucks {'logits': tensor([[ 0.2644, -0.0968]]), 'prediction': tensor([0.])} truth= 0\n",
      "doggs {'logits': tensor([[-0.0521,  0.2196]]), 'prediction': tensor([1.])} truth= 0\n",
      "tensor([0.6923])\n"
     ]
    }
   ],
   "source": [
    "def predict(sentence,model):\n",
    "    examples={}\n",
    "    examples[\"text\"]=[sentence]\n",
    "    examples[\"label\"]=[\"Y\"]\n",
    "    data=preprocess_function(examples)\n",
    "    #print(data)\n",
    "    data=data_collator([data])\n",
    "    #print(data[\"input_ids\"])\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prediction = model(data[\"input_ids\"][0])\n",
    "        if not isinstance(prediction,torch.Tensor):\n",
    "            prediction = prediction[\"logits\"]\n",
    "        preds=(prediction[:,1]>prediction[:,0])*1.0\n",
    "    return {\"logits\":prediction,\"prediction\":preds}\n",
    "    \n",
    "def predict_from_pandas(datap,net):\n",
    "    data=Dataset.from_pandas(vdf)\n",
    "    accuracy=0\n",
    "    size=len(data[\"text\"])\n",
    "    for i in range(size):\n",
    "        s=data[\"text\"][i]\n",
    "        l=data[\"label\"][i]\n",
    "        l=(1 if l=='Y' else 0)\n",
    "        p=predict(s,net)\n",
    "        print(s,p, \"truth=\",l)\n",
    "        accuracy+=(p[\"prediction\"]==l)\n",
    "    return accuracy/size\n",
    "\n",
    "print(predict_from_pandas(vdf,net))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97509ac7",
   "metadata": {},
   "source": [
    "On observe beaucoup de sur-apprentissage. Normal étant donné la taille du corpus d'apprentissage...\n",
    "\n",
    "\n",
    "### Même modèle mais avec des embeddings pré-entraînés\n",
    "\n",
    " Voyons ce que celà donne avec les représentations pre-entrainées (que l'on freeze) \n",
    "\n",
    "#### Méthode 1 : directement fournir les embeddings construits à partir du vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9367343e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss on epoch 0 : 0.699\n",
      "train accuracy on epoch 0: 0.556\n",
      "test loss on epoch 0: 0.638\n",
      "test accuracy on epoch 0: 0.692\n",
      "train loss on epoch 1 : 0.600\n",
      "train accuracy on epoch 1: 0.778\n",
      "test loss on epoch 1: 0.593\n",
      "test accuracy on epoch 1: 0.692\n",
      "train loss on epoch 2 : 0.482\n",
      "train accuracy on epoch 2: 0.889\n",
      "test loss on epoch 2: 0.554\n",
      "test accuracy on epoch 2: 0.692\n",
      "train loss on epoch 3 : 0.447\n",
      "train accuracy on epoch 3: 0.944\n",
      "test loss on epoch 3: 0.518\n",
      "test accuracy on epoch 3: 0.846\n",
      "train loss on epoch 4 : 0.453\n",
      "train accuracy on epoch 4: 0.889\n",
      "test loss on epoch 4: 0.480\n",
      "test accuracy on epoch 4: 0.846\n",
      "train loss on epoch 5 : 0.421\n",
      "train accuracy on epoch 5: 0.889\n",
      "test loss on epoch 5: 0.480\n",
      "test accuracy on epoch 5: 0.846\n",
      "train loss on epoch 6 : 0.412\n",
      "train accuracy on epoch 6: 0.889\n",
      "test loss on epoch 6: 0.466\n",
      "test accuracy on epoch 6: 0.846\n",
      "train loss on epoch 7 : 0.319\n",
      "train accuracy on epoch 7: 0.889\n",
      "test loss on epoch 7: 0.448\n",
      "test accuracy on epoch 7: 0.846\n",
      "train loss on epoch 8 : 0.418\n",
      "train accuracy on epoch 8: 0.944\n",
      "test loss on epoch 8: 0.441\n",
      "test accuracy on epoch 8: 0.846\n",
      "train loss on epoch 9 : 0.388\n",
      "train accuracy on epoch 9: 0.833\n",
      "test loss on epoch 9: 0.430\n",
      "test accuracy on epoch 9: 0.846\n",
      "train loss on epoch 10 : 0.360\n",
      "train accuracy on epoch 10: 0.889\n",
      "test loss on epoch 10: 0.420\n",
      "test accuracy on epoch 10: 0.846\n",
      "train loss on epoch 11 : 0.282\n",
      "train accuracy on epoch 11: 0.889\n",
      "test loss on epoch 11: 0.409\n",
      "test accuracy on epoch 11: 0.846\n",
      "train loss on epoch 12 : 0.297\n",
      "train accuracy on epoch 12: 0.944\n",
      "test loss on epoch 12: 0.397\n",
      "test accuracy on epoch 12: 0.846\n",
      "train loss on epoch 13 : 0.289\n",
      "train accuracy on epoch 13: 0.944\n",
      "test loss on epoch 13: 0.394\n",
      "test accuracy on epoch 13: 0.846\n",
      "train loss on epoch 14 : 0.340\n",
      "train accuracy on epoch 14: 0.944\n",
      "test loss on epoch 14: 0.391\n",
      "test accuracy on epoch 14: 0.846\n",
      "train loss on epoch 15 : 0.283\n",
      "train accuracy on epoch 15: 0.944\n",
      "test loss on epoch 15: 0.389\n",
      "test accuracy on epoch 15: 0.769\n",
      "train loss on epoch 16 : 0.296\n",
      "train accuracy on epoch 16: 0.944\n",
      "test loss on epoch 16: 0.375\n",
      "test accuracy on epoch 16: 0.769\n",
      "train loss on epoch 17 : 0.286\n",
      "train accuracy on epoch 17: 0.889\n",
      "test loss on epoch 17: 0.375\n",
      "test accuracy on epoch 17: 0.769\n",
      "train loss on epoch 18 : 0.296\n",
      "train accuracy on epoch 18: 0.889\n",
      "test loss on epoch 18: 0.374\n",
      "test accuracy on epoch 18: 0.923\n",
      "train loss on epoch 19 : 0.290\n",
      "train accuracy on epoch 19: 0.889\n",
      "test loss on epoch 19: 0.376\n",
      "test accuracy on epoch 19: 0.846\n",
      "train loss on epoch 20 : 0.214\n",
      "train accuracy on epoch 20: 0.944\n",
      "test loss on epoch 20: 0.361\n",
      "test accuracy on epoch 20: 0.846\n",
      "train loss on epoch 21 : 0.236\n",
      "train accuracy on epoch 21: 1.000\n",
      "test loss on epoch 21: 0.366\n",
      "test accuracy on epoch 21: 0.846\n",
      "train loss on epoch 22 : 0.249\n",
      "train accuracy on epoch 22: 0.944\n",
      "test loss on epoch 22: 0.360\n",
      "test accuracy on epoch 22: 0.846\n",
      "train loss on epoch 23 : 0.228\n",
      "train accuracy on epoch 23: 0.944\n",
      "test loss on epoch 23: 0.357\n",
      "test accuracy on epoch 23: 0.846\n",
      "train loss on epoch 24 : 0.256\n",
      "train accuracy on epoch 24: 0.889\n",
      "test loss on epoch 24: 0.356\n",
      "test accuracy on epoch 24: 0.846\n",
      "train loss on epoch 25 : 0.252\n",
      "train accuracy on epoch 25: 0.944\n",
      "test loss on epoch 25: 0.357\n",
      "test accuracy on epoch 25: 0.846\n",
      "train loss on epoch 26 : 0.178\n",
      "train accuracy on epoch 26: 1.000\n",
      "test loss on epoch 26: 0.350\n",
      "test accuracy on epoch 26: 0.846\n",
      "train loss on epoch 27 : 0.273\n",
      "train accuracy on epoch 27: 0.889\n",
      "test loss on epoch 27: 0.353\n",
      "test accuracy on epoch 27: 0.846\n",
      "train loss on epoch 28 : 0.265\n",
      "train accuracy on epoch 28: 0.889\n",
      "test loss on epoch 28: 0.351\n",
      "test accuracy on epoch 28: 0.846\n",
      "train loss on epoch 29 : 0.284\n",
      "train accuracy on epoch 29: 0.833\n",
      "test loss on epoch 29: 0.351\n",
      "test accuracy on epoch 29: 0.846\n",
      "train loss on epoch 30 : 0.288\n",
      "train accuracy on epoch 30: 0.944\n",
      "test loss on epoch 30: 0.344\n",
      "test accuracy on epoch 30: 0.846\n",
      "train loss on epoch 31 : 0.156\n",
      "train accuracy on epoch 31: 1.000\n",
      "test loss on epoch 31: 0.346\n",
      "test accuracy on epoch 31: 0.846\n",
      "train loss on epoch 32 : 0.262\n",
      "train accuracy on epoch 32: 0.889\n",
      "test loss on epoch 32: 0.342\n",
      "test accuracy on epoch 32: 0.769\n",
      "train loss on epoch 33 : 0.130\n",
      "train accuracy on epoch 33: 1.000\n",
      "test loss on epoch 33: 0.341\n",
      "test accuracy on epoch 33: 0.769\n",
      "train loss on epoch 34 : 0.211\n",
      "train accuracy on epoch 34: 0.944\n",
      "test loss on epoch 34: 0.341\n",
      "test accuracy on epoch 34: 0.769\n",
      "train loss on epoch 35 : 0.223\n",
      "train accuracy on epoch 35: 0.889\n",
      "test loss on epoch 35: 0.338\n",
      "test accuracy on epoch 35: 0.769\n",
      "train loss on epoch 36 : 0.243\n",
      "train accuracy on epoch 36: 0.944\n",
      "test loss on epoch 36: 0.346\n",
      "test accuracy on epoch 36: 0.769\n",
      "train loss on epoch 37 : 0.245\n",
      "train accuracy on epoch 37: 0.889\n",
      "test loss on epoch 37: 0.344\n",
      "test accuracy on epoch 37: 0.769\n",
      "train loss on epoch 38 : 0.189\n",
      "train accuracy on epoch 38: 0.944\n",
      "test loss on epoch 38: 0.337\n",
      "test accuracy on epoch 38: 0.846\n",
      "train loss on epoch 39 : 0.267\n",
      "train accuracy on epoch 39: 0.889\n",
      "test loss on epoch 39: 0.333\n",
      "test accuracy on epoch 39: 0.846\n",
      "train loss on epoch 40 : 0.193\n",
      "train accuracy on epoch 40: 1.000\n",
      "test loss on epoch 40: 0.342\n",
      "test accuracy on epoch 40: 0.769\n",
      "train loss on epoch 41 : 0.178\n",
      "train accuracy on epoch 41: 0.944\n",
      "test loss on epoch 41: 0.337\n",
      "test accuracy on epoch 41: 0.769\n",
      "train loss on epoch 42 : 0.192\n",
      "train accuracy on epoch 42: 0.944\n",
      "test loss on epoch 42: 0.335\n",
      "test accuracy on epoch 42: 0.769\n",
      "train loss on epoch 43 : 0.205\n",
      "train accuracy on epoch 43: 0.889\n",
      "test loss on epoch 43: 0.330\n",
      "test accuracy on epoch 43: 0.769\n",
      "train loss on epoch 44 : 0.150\n",
      "train accuracy on epoch 44: 1.000\n",
      "test loss on epoch 44: 0.331\n",
      "test accuracy on epoch 44: 0.769\n",
      "train loss on epoch 45 : 0.199\n",
      "train accuracy on epoch 45: 0.889\n",
      "test loss on epoch 45: 0.331\n",
      "test accuracy on epoch 45: 0.769\n",
      "train loss on epoch 46 : 0.118\n",
      "train accuracy on epoch 46: 1.000\n",
      "test loss on epoch 46: 0.334\n",
      "test accuracy on epoch 46: 0.769\n",
      "train loss on epoch 47 : 0.233\n",
      "train accuracy on epoch 47: 0.944\n",
      "test loss on epoch 47: 0.337\n",
      "test accuracy on epoch 47: 0.769\n",
      "train loss on epoch 48 : 0.230\n",
      "train accuracy on epoch 48: 0.889\n",
      "test loss on epoch 48: 0.344\n",
      "test accuracy on epoch 48: 0.769\n",
      "train loss on epoch 49 : 0.175\n",
      "train accuracy on epoch 49: 0.944\n",
      "test loss on epoch 49: 0.338\n",
      "test accuracy on epoch 49: 0.846\n",
      "train loss on epoch 50 : 0.277\n",
      "train accuracy on epoch 50: 0.833\n",
      "test loss on epoch 50: 0.341\n",
      "test accuracy on epoch 50: 0.769\n",
      "train loss on epoch 51 : 0.218\n",
      "train accuracy on epoch 51: 0.944\n",
      "test loss on epoch 51: 0.332\n",
      "test accuracy on epoch 51: 0.769\n",
      "train loss on epoch 52 : 0.139\n",
      "train accuracy on epoch 52: 1.000\n",
      "test loss on epoch 52: 0.335\n",
      "test accuracy on epoch 52: 0.769\n",
      "train loss on epoch 53 : 0.199\n",
      "train accuracy on epoch 53: 0.944\n",
      "test loss on epoch 53: 0.334\n",
      "test accuracy on epoch 53: 0.769\n",
      "train loss on epoch 54 : 0.196\n",
      "train accuracy on epoch 54: 0.889\n",
      "test loss on epoch 54: 0.331\n",
      "test accuracy on epoch 54: 0.769\n",
      "train loss on epoch 55 : 0.229\n",
      "train accuracy on epoch 55: 0.944\n",
      "test loss on epoch 55: 0.331\n",
      "test accuracy on epoch 55: 0.769\n",
      "train loss on epoch 56 : 0.183\n",
      "train accuracy on epoch 56: 0.944\n",
      "test loss on epoch 56: 0.329\n",
      "test accuracy on epoch 56: 0.769\n",
      "train loss on epoch 57 : 0.180\n",
      "train accuracy on epoch 57: 0.889\n",
      "test loss on epoch 57: 0.322\n",
      "test accuracy on epoch 57: 0.846\n",
      "train loss on epoch 58 : 0.166\n",
      "train accuracy on epoch 58: 1.000\n",
      "test loss on epoch 58: 0.327\n",
      "test accuracy on epoch 58: 0.769\n",
      "train loss on epoch 59 : 0.294\n",
      "train accuracy on epoch 59: 0.833\n",
      "test loss on epoch 59: 0.322\n",
      "test accuracy on epoch 59: 0.846\n",
      "train loss on epoch 60 : 0.206\n",
      "train accuracy on epoch 60: 0.889\n",
      "test loss on epoch 60: 0.328\n",
      "test accuracy on epoch 60: 0.769\n",
      "train loss on epoch 61 : 0.324\n",
      "train accuracy on epoch 61: 0.889\n",
      "test loss on epoch 61: 0.323\n",
      "test accuracy on epoch 61: 0.846\n",
      "train loss on epoch 62 : 0.159\n",
      "train accuracy on epoch 62: 0.944\n",
      "test loss on epoch 62: 0.322\n",
      "test accuracy on epoch 62: 0.846\n",
      "train loss on epoch 63 : 0.267\n",
      "train accuracy on epoch 63: 0.889\n",
      "test loss on epoch 63: 0.321\n",
      "test accuracy on epoch 63: 0.769\n",
      "train loss on epoch 64 : 0.247\n",
      "train accuracy on epoch 64: 0.889\n",
      "test loss on epoch 64: 0.324\n",
      "test accuracy on epoch 64: 0.769\n",
      "train loss on epoch 65 : 0.192\n",
      "train accuracy on epoch 65: 0.944\n",
      "test loss on epoch 65: 0.319\n",
      "test accuracy on epoch 65: 0.769\n",
      "train loss on epoch 66 : 0.288\n",
      "train accuracy on epoch 66: 0.889\n",
      "test loss on epoch 66: 0.322\n",
      "test accuracy on epoch 66: 0.769\n",
      "train loss on epoch 67 : 0.102\n",
      "train accuracy on epoch 67: 0.944\n",
      "test loss on epoch 67: 0.317\n",
      "test accuracy on epoch 67: 0.769\n",
      "train loss on epoch 68 : 0.164\n",
      "train accuracy on epoch 68: 0.889\n",
      "test loss on epoch 68: 0.321\n",
      "test accuracy on epoch 68: 0.769\n",
      "train loss on epoch 69 : 0.116\n",
      "train accuracy on epoch 69: 1.000\n",
      "test loss on epoch 69: 0.324\n",
      "test accuracy on epoch 69: 0.769\n",
      "train loss on epoch 70 : 0.108\n",
      "train accuracy on epoch 70: 0.944\n",
      "test loss on epoch 70: 0.324\n",
      "test accuracy on epoch 70: 0.769\n",
      "train loss on epoch 71 : 0.225\n",
      "train accuracy on epoch 71: 0.944\n",
      "test loss on epoch 71: 0.317\n",
      "test accuracy on epoch 71: 0.769\n",
      "train loss on epoch 72 : 0.133\n",
      "train accuracy on epoch 72: 0.944\n",
      "test loss on epoch 72: 0.317\n",
      "test accuracy on epoch 72: 0.769\n",
      "train loss on epoch 73 : 0.380\n",
      "train accuracy on epoch 73: 0.833\n",
      "test loss on epoch 73: 0.317\n",
      "test accuracy on epoch 73: 0.769\n",
      "train loss on epoch 74 : 0.161\n",
      "train accuracy on epoch 74: 1.000\n",
      "test loss on epoch 74: 0.316\n",
      "test accuracy on epoch 74: 0.769\n",
      "train loss on epoch 75 : 0.195\n",
      "train accuracy on epoch 75: 0.889\n",
      "test loss on epoch 75: 0.321\n",
      "test accuracy on epoch 75: 0.769\n",
      "train loss on epoch 76 : 0.130\n",
      "train accuracy on epoch 76: 0.944\n",
      "test loss on epoch 76: 0.318\n",
      "test accuracy on epoch 76: 0.769\n",
      "train loss on epoch 77 : 0.082\n",
      "train accuracy on epoch 77: 1.000\n",
      "test loss on epoch 77: 0.320\n",
      "test accuracy on epoch 77: 0.769\n",
      "train loss on epoch 78 : 0.113\n",
      "train accuracy on epoch 78: 1.000\n",
      "test loss on epoch 78: 0.314\n",
      "test accuracy on epoch 78: 0.769\n",
      "train loss on epoch 79 : 0.239\n",
      "train accuracy on epoch 79: 0.889\n",
      "test loss on epoch 79: 0.316\n",
      "test accuracy on epoch 79: 0.769\n",
      "train loss on epoch 80 : 0.271\n",
      "train accuracy on epoch 80: 0.889\n",
      "test loss on epoch 80: 0.318\n",
      "test accuracy on epoch 80: 0.769\n",
      "train loss on epoch 81 : 0.150\n",
      "train accuracy on epoch 81: 0.944\n",
      "test loss on epoch 81: 0.314\n",
      "test accuracy on epoch 81: 0.769\n",
      "train loss on epoch 82 : 0.206\n",
      "train accuracy on epoch 82: 0.889\n",
      "test loss on epoch 82: 0.315\n",
      "test accuracy on epoch 82: 0.769\n",
      "train loss on epoch 83 : 0.247\n",
      "train accuracy on epoch 83: 0.944\n",
      "test loss on epoch 83: 0.309\n",
      "test accuracy on epoch 83: 0.769\n",
      "train loss on epoch 84 : 0.085\n",
      "train accuracy on epoch 84: 1.000\n",
      "test loss on epoch 84: 0.313\n",
      "test accuracy on epoch 84: 0.769\n",
      "train loss on epoch 85 : 0.328\n",
      "train accuracy on epoch 85: 0.944\n",
      "test loss on epoch 85: 0.314\n",
      "test accuracy on epoch 85: 0.769\n",
      "train loss on epoch 86 : 0.318\n",
      "train accuracy on epoch 86: 0.889\n",
      "test loss on epoch 86: 0.320\n",
      "test accuracy on epoch 86: 0.769\n",
      "train loss on epoch 87 : 0.148\n",
      "train accuracy on epoch 87: 0.944\n",
      "test loss on epoch 87: 0.319\n",
      "test accuracy on epoch 87: 0.769\n",
      "train loss on epoch 88 : 0.183\n",
      "train accuracy on epoch 88: 1.000\n",
      "test loss on epoch 88: 0.313\n",
      "test accuracy on epoch 88: 0.769\n",
      "train loss on epoch 89 : 0.141\n",
      "train accuracy on epoch 89: 0.944\n",
      "test loss on epoch 89: 0.316\n",
      "test accuracy on epoch 89: 0.769\n",
      "train loss on epoch 90 : 0.091\n",
      "train accuracy on epoch 90: 1.000\n",
      "test loss on epoch 90: 0.321\n",
      "test accuracy on epoch 90: 0.769\n",
      "train loss on epoch 91 : 0.267\n",
      "train accuracy on epoch 91: 0.889\n",
      "test loss on epoch 91: 0.317\n",
      "test accuracy on epoch 91: 0.769\n",
      "train loss on epoch 92 : 0.153\n",
      "train accuracy on epoch 92: 0.944\n",
      "test loss on epoch 92: 0.311\n",
      "test accuracy on epoch 92: 0.769\n",
      "train loss on epoch 93 : 0.179\n",
      "train accuracy on epoch 93: 0.944\n",
      "test loss on epoch 93: 0.313\n",
      "test accuracy on epoch 93: 0.692\n",
      "train loss on epoch 94 : 0.146\n",
      "train accuracy on epoch 94: 0.944\n",
      "test loss on epoch 94: 0.309\n",
      "test accuracy on epoch 94: 0.769\n",
      "train loss on epoch 95 : 0.157\n",
      "train accuracy on epoch 95: 0.944\n",
      "test loss on epoch 95: 0.311\n",
      "test accuracy on epoch 95: 0.769\n",
      "train loss on epoch 96 : 0.176\n",
      "train accuracy on epoch 96: 0.944\n",
      "test loss on epoch 96: 0.309\n",
      "test accuracy on epoch 96: 0.769\n",
      "train loss on epoch 97 : 0.120\n",
      "train accuracy on epoch 97: 1.000\n",
      "test loss on epoch 97: 0.311\n",
      "test accuracy on epoch 97: 0.769\n",
      "train loss on epoch 98 : 0.122\n",
      "train accuracy on epoch 98: 1.000\n",
      "test loss on epoch 98: 0.313\n",
      "test accuracy on epoch 98: 0.769\n",
      "train loss on epoch 99 : 0.295\n",
      "train accuracy on epoch 99: 0.833\n",
      "test loss on epoch 99: 0.309\n",
      "test accuracy on epoch 99: 0.769\n",
      "train loss on epoch 100 : 0.199\n",
      "train accuracy on epoch 100: 0.833\n",
      "test loss on epoch 100: 0.309\n",
      "test accuracy on epoch 100: 0.769\n",
      "train loss on epoch 101 : 0.124\n",
      "train accuracy on epoch 101: 0.944\n",
      "test loss on epoch 101: 0.310\n",
      "test accuracy on epoch 101: 0.769\n",
      "train loss on epoch 102 : 0.118\n",
      "train accuracy on epoch 102: 0.944\n",
      "test loss on epoch 102: 0.303\n",
      "test accuracy on epoch 102: 0.769\n",
      "train loss on epoch 103 : 0.256\n",
      "train accuracy on epoch 103: 0.889\n",
      "test loss on epoch 103: 0.301\n",
      "test accuracy on epoch 103: 0.769\n",
      "train loss on epoch 104 : 0.158\n",
      "train accuracy on epoch 104: 0.944\n",
      "test loss on epoch 104: 0.306\n",
      "test accuracy on epoch 104: 0.769\n",
      "train loss on epoch 105 : 0.146\n",
      "train accuracy on epoch 105: 0.889\n",
      "test loss on epoch 105: 0.307\n",
      "test accuracy on epoch 105: 0.769\n",
      "train loss on epoch 106 : 0.296\n",
      "train accuracy on epoch 106: 0.833\n",
      "test loss on epoch 106: 0.303\n",
      "test accuracy on epoch 106: 0.769\n",
      "train loss on epoch 107 : 0.314\n",
      "train accuracy on epoch 107: 0.889\n",
      "test loss on epoch 107: 0.307\n",
      "test accuracy on epoch 107: 0.769\n",
      "train loss on epoch 108 : 0.122\n",
      "train accuracy on epoch 108: 0.944\n",
      "test loss on epoch 108: 0.301\n",
      "test accuracy on epoch 108: 0.769\n",
      "train loss on epoch 109 : 0.150\n",
      "train accuracy on epoch 109: 0.944\n",
      "test loss on epoch 109: 0.307\n",
      "test accuracy on epoch 109: 0.692\n",
      "train loss on epoch 110 : 0.328\n",
      "train accuracy on epoch 110: 0.889\n",
      "test loss on epoch 110: 0.302\n",
      "test accuracy on epoch 110: 0.769\n",
      "train loss on epoch 111 : 0.224\n",
      "train accuracy on epoch 111: 0.889\n",
      "test loss on epoch 111: 0.307\n",
      "test accuracy on epoch 111: 0.769\n",
      "train loss on epoch 112 : 0.132\n",
      "train accuracy on epoch 112: 0.944\n",
      "test loss on epoch 112: 0.307\n",
      "test accuracy on epoch 112: 0.769\n",
      "train loss on epoch 113 : 0.200\n",
      "train accuracy on epoch 113: 0.944\n",
      "test loss on epoch 113: 0.304\n",
      "test accuracy on epoch 113: 0.769\n",
      "train loss on epoch 114 : 0.275\n",
      "train accuracy on epoch 114: 0.833\n",
      "test loss on epoch 114: 0.306\n",
      "test accuracy on epoch 114: 0.769\n",
      "train loss on epoch 115 : 0.117\n",
      "train accuracy on epoch 115: 1.000\n",
      "test loss on epoch 115: 0.300\n",
      "test accuracy on epoch 115: 0.769\n",
      "train loss on epoch 116 : 0.199\n",
      "train accuracy on epoch 116: 0.944\n",
      "test loss on epoch 116: 0.302\n",
      "test accuracy on epoch 116: 0.769\n",
      "train loss on epoch 117 : 0.259\n",
      "train accuracy on epoch 117: 0.889\n",
      "test loss on epoch 117: 0.304\n",
      "test accuracy on epoch 117: 0.769\n",
      "train loss on epoch 118 : 0.289\n",
      "train accuracy on epoch 118: 0.889\n",
      "test loss on epoch 118: 0.302\n",
      "test accuracy on epoch 118: 0.769\n",
      "train loss on epoch 119 : 0.222\n",
      "train accuracy on epoch 119: 0.889\n",
      "test loss on epoch 119: 0.307\n",
      "test accuracy on epoch 119: 0.769\n",
      "train loss on epoch 120 : 0.327\n",
      "train accuracy on epoch 120: 0.944\n",
      "test loss on epoch 120: 0.300\n",
      "test accuracy on epoch 120: 0.769\n",
      "train loss on epoch 121 : 0.137\n",
      "train accuracy on epoch 121: 0.944\n",
      "test loss on epoch 121: 0.302\n",
      "test accuracy on epoch 121: 0.769\n",
      "train loss on epoch 122 : 0.121\n",
      "train accuracy on epoch 122: 1.000\n",
      "test loss on epoch 122: 0.307\n",
      "test accuracy on epoch 122: 0.769\n",
      "train loss on epoch 123 : 0.219\n",
      "train accuracy on epoch 123: 0.889\n",
      "test loss on epoch 123: 0.303\n",
      "test accuracy on epoch 123: 0.846\n",
      "train loss on epoch 124 : 0.142\n",
      "train accuracy on epoch 124: 0.944\n",
      "test loss on epoch 124: 0.298\n",
      "test accuracy on epoch 124: 0.846\n",
      "train loss on epoch 125 : 0.059\n",
      "train accuracy on epoch 125: 1.000\n",
      "test loss on epoch 125: 0.306\n",
      "test accuracy on epoch 125: 0.769\n",
      "train loss on epoch 126 : 0.317\n",
      "train accuracy on epoch 126: 0.889\n",
      "test loss on epoch 126: 0.305\n",
      "test accuracy on epoch 126: 0.769\n",
      "train loss on epoch 127 : 0.162\n",
      "train accuracy on epoch 127: 0.944\n",
      "test loss on epoch 127: 0.306\n",
      "test accuracy on epoch 127: 0.769\n",
      "train loss on epoch 128 : 0.213\n",
      "train accuracy on epoch 128: 0.889\n",
      "test loss on epoch 128: 0.304\n",
      "test accuracy on epoch 128: 0.769\n",
      "train loss on epoch 129 : 0.064\n",
      "train accuracy on epoch 129: 1.000\n",
      "test loss on epoch 129: 0.305\n",
      "test accuracy on epoch 129: 0.769\n",
      "train loss on epoch 130 : 0.091\n",
      "train accuracy on epoch 130: 1.000\n",
      "test loss on epoch 130: 0.306\n",
      "test accuracy on epoch 130: 0.769\n",
      "train loss on epoch 131 : 0.177\n",
      "train accuracy on epoch 131: 0.944\n",
      "test loss on epoch 131: 0.303\n",
      "test accuracy on epoch 131: 0.769\n",
      "train loss on epoch 132 : 0.262\n",
      "train accuracy on epoch 132: 0.944\n",
      "test loss on epoch 132: 0.299\n",
      "test accuracy on epoch 132: 0.769\n",
      "train loss on epoch 133 : 0.176\n",
      "train accuracy on epoch 133: 0.889\n",
      "test loss on epoch 133: 0.306\n",
      "test accuracy on epoch 133: 0.769\n",
      "train loss on epoch 134 : 0.152\n",
      "train accuracy on epoch 134: 0.944\n",
      "test loss on epoch 134: 0.310\n",
      "test accuracy on epoch 134: 0.692\n",
      "train loss on epoch 135 : 0.174\n",
      "train accuracy on epoch 135: 0.944\n",
      "test loss on epoch 135: 0.301\n",
      "test accuracy on epoch 135: 0.769\n",
      "train loss on epoch 136 : 0.242\n",
      "train accuracy on epoch 136: 0.889\n",
      "test loss on epoch 136: 0.306\n",
      "test accuracy on epoch 136: 0.769\n",
      "train loss on epoch 137 : 0.129\n",
      "train accuracy on epoch 137: 0.944\n",
      "test loss on epoch 137: 0.298\n",
      "test accuracy on epoch 137: 0.769\n",
      "train loss on epoch 138 : 0.233\n",
      "train accuracy on epoch 138: 0.889\n",
      "test loss on epoch 138: 0.308\n",
      "test accuracy on epoch 138: 0.769\n",
      "train loss on epoch 139 : 0.221\n",
      "train accuracy on epoch 139: 0.944\n",
      "test loss on epoch 139: 0.304\n",
      "test accuracy on epoch 139: 0.769\n",
      "train loss on epoch 140 : 0.179\n",
      "train accuracy on epoch 140: 0.944\n",
      "test loss on epoch 140: 0.313\n",
      "test accuracy on epoch 140: 0.769\n",
      "train loss on epoch 141 : 0.283\n",
      "train accuracy on epoch 141: 0.889\n",
      "test loss on epoch 141: 0.311\n",
      "test accuracy on epoch 141: 0.769\n",
      "train loss on epoch 142 : 0.100\n",
      "train accuracy on epoch 142: 1.000\n",
      "test loss on epoch 142: 0.304\n",
      "test accuracy on epoch 142: 0.769\n",
      "train loss on epoch 143 : 0.209\n",
      "train accuracy on epoch 143: 0.944\n",
      "test loss on epoch 143: 0.309\n",
      "test accuracy on epoch 143: 0.769\n",
      "train loss on epoch 144 : 0.118\n",
      "train accuracy on epoch 144: 0.944\n",
      "test loss on epoch 144: 0.303\n",
      "test accuracy on epoch 144: 0.769\n",
      "train loss on epoch 145 : 0.194\n",
      "train accuracy on epoch 145: 0.889\n",
      "test loss on epoch 145: 0.310\n",
      "test accuracy on epoch 145: 0.769\n",
      "train loss on epoch 146 : 0.158\n",
      "train accuracy on epoch 146: 0.944\n",
      "test loss on epoch 146: 0.311\n",
      "test accuracy on epoch 146: 0.769\n",
      "train loss on epoch 147 : 0.306\n",
      "train accuracy on epoch 147: 0.833\n",
      "test loss on epoch 147: 0.314\n",
      "test accuracy on epoch 147: 0.769\n",
      "train loss on epoch 148 : 0.087\n",
      "train accuracy on epoch 148: 1.000\n",
      "test loss on epoch 148: 0.314\n",
      "test accuracy on epoch 148: 0.769\n",
      "train loss on epoch 149 : 0.090\n",
      "train accuracy on epoch 149: 1.000\n",
      "test loss on epoch 149: 0.314\n",
      "test accuracy on epoch 149: 0.769\n",
      "train loss on epoch 150 : 0.116\n",
      "train accuracy on epoch 150: 0.944\n",
      "test loss on epoch 150: 0.313\n",
      "test accuracy on epoch 150: 0.769\n",
      "train loss on epoch 151 : 0.178\n",
      "train accuracy on epoch 151: 0.889\n",
      "test loss on epoch 151: 0.303\n",
      "test accuracy on epoch 151: 0.769\n",
      "train loss on epoch 152 : 0.156\n",
      "train accuracy on epoch 152: 0.889\n",
      "test loss on epoch 152: 0.304\n",
      "test accuracy on epoch 152: 0.769\n",
      "train loss on epoch 153 : 0.243\n",
      "train accuracy on epoch 153: 0.889\n",
      "test loss on epoch 153: 0.300\n",
      "test accuracy on epoch 153: 0.769\n",
      "train loss on epoch 154 : 0.060\n",
      "train accuracy on epoch 154: 1.000\n",
      "test loss on epoch 154: 0.305\n",
      "test accuracy on epoch 154: 0.769\n",
      "train loss on epoch 155 : 0.163\n",
      "train accuracy on epoch 155: 0.889\n",
      "test loss on epoch 155: 0.307\n",
      "test accuracy on epoch 155: 0.769\n",
      "train loss on epoch 156 : 0.195\n",
      "train accuracy on epoch 156: 0.944\n",
      "test loss on epoch 156: 0.306\n",
      "test accuracy on epoch 156: 0.769\n",
      "train loss on epoch 157 : 0.180\n",
      "train accuracy on epoch 157: 0.944\n",
      "test loss on epoch 157: 0.306\n",
      "test accuracy on epoch 157: 0.692\n",
      "train loss on epoch 158 : 0.082\n",
      "train accuracy on epoch 158: 1.000\n",
      "test loss on epoch 158: 0.303\n",
      "test accuracy on epoch 158: 0.769\n",
      "train loss on epoch 159 : 0.122\n",
      "train accuracy on epoch 159: 1.000\n",
      "test loss on epoch 159: 0.303\n",
      "test accuracy on epoch 159: 0.769\n",
      "train loss on epoch 160 : 0.094\n",
      "train accuracy on epoch 160: 0.944\n",
      "test loss on epoch 160: 0.302\n",
      "test accuracy on epoch 160: 0.769\n",
      "train loss on epoch 161 : 0.314\n",
      "train accuracy on epoch 161: 0.833\n",
      "test loss on epoch 161: 0.307\n",
      "test accuracy on epoch 161: 0.769\n",
      "train loss on epoch 162 : 0.378\n",
      "train accuracy on epoch 162: 0.889\n",
      "test loss on epoch 162: 0.307\n",
      "test accuracy on epoch 162: 0.769\n",
      "train loss on epoch 163 : 0.328\n",
      "train accuracy on epoch 163: 0.889\n",
      "test loss on epoch 163: 0.305\n",
      "test accuracy on epoch 163: 0.769\n",
      "train loss on epoch 164 : 0.070\n",
      "train accuracy on epoch 164: 1.000\n",
      "test loss on epoch 164: 0.306\n",
      "test accuracy on epoch 164: 0.769\n",
      "train loss on epoch 165 : 0.230\n",
      "train accuracy on epoch 165: 0.889\n",
      "test loss on epoch 165: 0.304\n",
      "test accuracy on epoch 165: 0.769\n",
      "train loss on epoch 166 : 0.112\n",
      "train accuracy on epoch 166: 0.944\n",
      "test loss on epoch 166: 0.300\n",
      "test accuracy on epoch 166: 0.769\n",
      "train loss on epoch 167 : 0.486\n",
      "train accuracy on epoch 167: 0.833\n",
      "test loss on epoch 167: 0.298\n",
      "test accuracy on epoch 167: 0.769\n",
      "train loss on epoch 168 : 0.116\n",
      "train accuracy on epoch 168: 1.000\n",
      "test loss on epoch 168: 0.298\n",
      "test accuracy on epoch 168: 0.769\n",
      "train loss on epoch 169 : 0.201\n",
      "train accuracy on epoch 169: 0.944\n",
      "test loss on epoch 169: 0.312\n",
      "test accuracy on epoch 169: 0.769\n",
      "train loss on epoch 170 : 0.225\n",
      "train accuracy on epoch 170: 0.889\n",
      "test loss on epoch 170: 0.314\n",
      "test accuracy on epoch 170: 0.769\n",
      "train loss on epoch 171 : 0.108\n",
      "train accuracy on epoch 171: 0.944\n",
      "test loss on epoch 171: 0.313\n",
      "test accuracy on epoch 171: 0.769\n",
      "train loss on epoch 172 : 0.204\n",
      "train accuracy on epoch 172: 0.833\n",
      "test loss on epoch 172: 0.309\n",
      "test accuracy on epoch 172: 0.769\n",
      "train loss on epoch 173 : 0.135\n",
      "train accuracy on epoch 173: 0.944\n",
      "test loss on epoch 173: 0.317\n",
      "test accuracy on epoch 173: 0.769\n",
      "train loss on epoch 174 : 0.225\n",
      "train accuracy on epoch 174: 0.889\n",
      "test loss on epoch 174: 0.305\n",
      "test accuracy on epoch 174: 0.769\n",
      "train loss on epoch 175 : 0.201\n",
      "train accuracy on epoch 175: 0.889\n",
      "test loss on epoch 175: 0.312\n",
      "test accuracy on epoch 175: 0.769\n",
      "train loss on epoch 176 : 0.108\n",
      "train accuracy on epoch 176: 0.944\n",
      "test loss on epoch 176: 0.318\n",
      "test accuracy on epoch 176: 0.769\n",
      "train loss on epoch 177 : 0.152\n",
      "train accuracy on epoch 177: 0.889\n",
      "test loss on epoch 177: 0.305\n",
      "test accuracy on epoch 177: 0.769\n",
      "train loss on epoch 178 : 0.149\n",
      "train accuracy on epoch 178: 0.944\n",
      "test loss on epoch 178: 0.304\n",
      "test accuracy on epoch 178: 0.769\n",
      "train loss on epoch 179 : 0.142\n",
      "train accuracy on epoch 179: 0.889\n",
      "test loss on epoch 179: 0.309\n",
      "test accuracy on epoch 179: 0.769\n",
      "train loss on epoch 180 : 0.281\n",
      "train accuracy on epoch 180: 0.944\n",
      "test loss on epoch 180: 0.301\n",
      "test accuracy on epoch 180: 0.769\n",
      "train loss on epoch 181 : 0.085\n",
      "train accuracy on epoch 181: 0.944\n",
      "test loss on epoch 181: 0.307\n",
      "test accuracy on epoch 181: 0.769\n",
      "train loss on epoch 182 : 0.204\n",
      "train accuracy on epoch 182: 0.833\n",
      "test loss on epoch 182: 0.314\n",
      "test accuracy on epoch 182: 0.769\n",
      "train loss on epoch 183 : 0.169\n",
      "train accuracy on epoch 183: 0.944\n",
      "test loss on epoch 183: 0.303\n",
      "test accuracy on epoch 183: 0.769\n",
      "train loss on epoch 184 : 0.108\n",
      "train accuracy on epoch 184: 0.944\n",
      "test loss on epoch 184: 0.317\n",
      "test accuracy on epoch 184: 0.769\n",
      "train loss on epoch 185 : 0.229\n",
      "train accuracy on epoch 185: 0.889\n",
      "test loss on epoch 185: 0.304\n",
      "test accuracy on epoch 185: 0.769\n",
      "train loss on epoch 186 : 0.063\n",
      "train accuracy on epoch 186: 1.000\n",
      "test loss on epoch 186: 0.307\n",
      "test accuracy on epoch 186: 0.769\n",
      "train loss on epoch 187 : 0.098\n",
      "train accuracy on epoch 187: 0.944\n",
      "test loss on epoch 187: 0.308\n",
      "test accuracy on epoch 187: 0.769\n",
      "train loss on epoch 188 : 0.191\n",
      "train accuracy on epoch 188: 0.889\n",
      "test loss on epoch 188: 0.318\n",
      "test accuracy on epoch 188: 0.769\n",
      "train loss on epoch 189 : 0.217\n",
      "train accuracy on epoch 189: 0.944\n",
      "test loss on epoch 189: 0.303\n",
      "test accuracy on epoch 189: 0.769\n",
      "train loss on epoch 190 : 0.287\n",
      "train accuracy on epoch 190: 0.889\n",
      "test loss on epoch 190: 0.309\n",
      "test accuracy on epoch 190: 0.769\n",
      "train loss on epoch 191 : 0.280\n",
      "train accuracy on epoch 191: 0.889\n",
      "test loss on epoch 191: 0.295\n",
      "test accuracy on epoch 191: 0.769\n",
      "train loss on epoch 192 : 0.103\n",
      "train accuracy on epoch 192: 1.000\n",
      "test loss on epoch 192: 0.294\n",
      "test accuracy on epoch 192: 0.769\n",
      "train loss on epoch 193 : 0.262\n",
      "train accuracy on epoch 193: 0.944\n",
      "test loss on epoch 193: 0.298\n",
      "test accuracy on epoch 193: 0.769\n",
      "train loss on epoch 194 : 0.141\n",
      "train accuracy on epoch 194: 1.000\n",
      "test loss on epoch 194: 0.298\n",
      "test accuracy on epoch 194: 0.846\n",
      "train loss on epoch 195 : 0.129\n",
      "train accuracy on epoch 195: 0.944\n",
      "test loss on epoch 195: 0.297\n",
      "test accuracy on epoch 195: 0.769\n",
      "train loss on epoch 196 : 0.159\n",
      "train accuracy on epoch 196: 0.944\n",
      "test loss on epoch 196: 0.302\n",
      "test accuracy on epoch 196: 0.769\n",
      "train loss on epoch 197 : 0.213\n",
      "train accuracy on epoch 197: 0.889\n",
      "test loss on epoch 197: 0.293\n",
      "test accuracy on epoch 197: 0.769\n",
      "train loss on epoch 198 : 0.218\n",
      "train accuracy on epoch 198: 0.944\n",
      "test loss on epoch 198: 0.295\n",
      "test accuracy on epoch 198: 0.769\n",
      "train loss on epoch 199 : 0.241\n",
      "train accuracy on epoch 199: 0.889\n",
      "test loss on epoch 199: 0.297\n",
      "test accuracy on epoch 199: 0.769\n",
      "train loss on epoch 200 : 0.346\n",
      "train accuracy on epoch 200: 0.833\n",
      "test loss on epoch 200: 0.297\n",
      "test accuracy on epoch 200: 0.769\n",
      "train loss on epoch 201 : 0.158\n",
      "train accuracy on epoch 201: 0.944\n",
      "test loss on epoch 201: 0.304\n",
      "test accuracy on epoch 201: 0.769\n",
      "train loss on epoch 202 : 0.295\n",
      "train accuracy on epoch 202: 0.889\n",
      "test loss on epoch 202: 0.306\n",
      "test accuracy on epoch 202: 0.769\n",
      "train loss on epoch 203 : 0.288\n",
      "train accuracy on epoch 203: 0.889\n",
      "test loss on epoch 203: 0.294\n",
      "test accuracy on epoch 203: 0.769\n",
      "train loss on epoch 204 : 0.055\n",
      "train accuracy on epoch 204: 1.000\n",
      "test loss on epoch 204: 0.308\n",
      "test accuracy on epoch 204: 0.769\n",
      "train loss on epoch 205 : 0.166\n",
      "train accuracy on epoch 205: 0.944\n",
      "test loss on epoch 205: 0.308\n",
      "test accuracy on epoch 205: 0.769\n",
      "train loss on epoch 206 : 0.260\n",
      "train accuracy on epoch 206: 0.889\n",
      "test loss on epoch 206: 0.301\n",
      "test accuracy on epoch 206: 0.769\n",
      "train loss on epoch 207 : 0.088\n",
      "train accuracy on epoch 207: 1.000\n",
      "test loss on epoch 207: 0.292\n",
      "test accuracy on epoch 207: 0.769\n",
      "train loss on epoch 208 : 0.234\n",
      "train accuracy on epoch 208: 0.944\n",
      "test loss on epoch 208: 0.297\n",
      "test accuracy on epoch 208: 0.769\n",
      "train loss on epoch 209 : 0.353\n",
      "train accuracy on epoch 209: 0.889\n",
      "test loss on epoch 209: 0.297\n",
      "test accuracy on epoch 209: 0.769\n",
      "train loss on epoch 210 : 0.511\n",
      "train accuracy on epoch 210: 0.833\n",
      "test loss on epoch 210: 0.302\n",
      "test accuracy on epoch 210: 0.769\n",
      "train loss on epoch 211 : 0.217\n",
      "train accuracy on epoch 211: 0.889\n",
      "test loss on epoch 211: 0.299\n",
      "test accuracy on epoch 211: 0.846\n",
      "train loss on epoch 212 : 0.141\n",
      "train accuracy on epoch 212: 0.944\n",
      "test loss on epoch 212: 0.302\n",
      "test accuracy on epoch 212: 0.769\n",
      "train loss on epoch 213 : 0.181\n",
      "train accuracy on epoch 213: 0.889\n",
      "test loss on epoch 213: 0.307\n",
      "test accuracy on epoch 213: 0.769\n",
      "train loss on epoch 214 : 0.438\n",
      "train accuracy on epoch 214: 0.833\n",
      "test loss on epoch 214: 0.298\n",
      "test accuracy on epoch 214: 0.769\n",
      "train loss on epoch 215 : 0.169\n",
      "train accuracy on epoch 215: 0.944\n",
      "test loss on epoch 215: 0.306\n",
      "test accuracy on epoch 215: 0.769\n",
      "train loss on epoch 216 : 0.206\n",
      "train accuracy on epoch 216: 0.889\n",
      "test loss on epoch 216: 0.308\n",
      "test accuracy on epoch 216: 0.769\n",
      "train loss on epoch 217 : 0.195\n",
      "train accuracy on epoch 217: 0.944\n",
      "test loss on epoch 217: 0.314\n",
      "test accuracy on epoch 217: 0.769\n",
      "train loss on epoch 218 : 0.076\n",
      "train accuracy on epoch 218: 0.944\n",
      "test loss on epoch 218: 0.297\n",
      "test accuracy on epoch 218: 0.769\n",
      "train loss on epoch 219 : 0.170\n",
      "train accuracy on epoch 219: 0.889\n",
      "test loss on epoch 219: 0.298\n",
      "test accuracy on epoch 219: 0.769\n",
      "train loss on epoch 220 : 0.154\n",
      "train accuracy on epoch 220: 0.944\n",
      "test loss on epoch 220: 0.311\n",
      "test accuracy on epoch 220: 0.769\n",
      "train loss on epoch 221 : 0.152\n",
      "train accuracy on epoch 221: 0.889\n",
      "test loss on epoch 221: 0.303\n",
      "test accuracy on epoch 221: 0.769\n",
      "train loss on epoch 222 : 0.186\n",
      "train accuracy on epoch 222: 0.944\n",
      "test loss on epoch 222: 0.309\n",
      "test accuracy on epoch 222: 0.769\n",
      "train loss on epoch 223 : 0.076\n",
      "train accuracy on epoch 223: 1.000\n",
      "test loss on epoch 223: 0.304\n",
      "test accuracy on epoch 223: 0.769\n",
      "train loss on epoch 224 : 0.203\n",
      "train accuracy on epoch 224: 0.889\n",
      "test loss on epoch 224: 0.312\n",
      "test accuracy on epoch 224: 0.769\n",
      "train loss on epoch 225 : 0.509\n",
      "train accuracy on epoch 225: 0.889\n",
      "test loss on epoch 225: 0.306\n",
      "test accuracy on epoch 225: 0.769\n",
      "train loss on epoch 226 : 0.379\n",
      "train accuracy on epoch 226: 0.889\n",
      "test loss on epoch 226: 0.304\n",
      "test accuracy on epoch 226: 0.769\n",
      "train loss on epoch 227 : 0.103\n",
      "train accuracy on epoch 227: 1.000\n",
      "test loss on epoch 227: 0.297\n",
      "test accuracy on epoch 227: 0.769\n",
      "train loss on epoch 228 : 0.201\n",
      "train accuracy on epoch 228: 0.889\n",
      "test loss on epoch 228: 0.298\n",
      "test accuracy on epoch 228: 0.769\n",
      "train loss on epoch 229 : 0.167\n",
      "train accuracy on epoch 229: 0.944\n",
      "test loss on epoch 229: 0.313\n",
      "test accuracy on epoch 229: 0.769\n",
      "train loss on epoch 230 : 0.104\n",
      "train accuracy on epoch 230: 0.944\n",
      "test loss on epoch 230: 0.308\n",
      "test accuracy on epoch 230: 0.769\n",
      "train loss on epoch 231 : 0.246\n",
      "train accuracy on epoch 231: 0.889\n",
      "test loss on epoch 231: 0.308\n",
      "test accuracy on epoch 231: 0.769\n",
      "train loss on epoch 232 : 0.284\n",
      "train accuracy on epoch 232: 0.833\n",
      "test loss on epoch 232: 0.301\n",
      "test accuracy on epoch 232: 0.769\n",
      "train loss on epoch 233 : 0.089\n",
      "train accuracy on epoch 233: 0.944\n",
      "test loss on epoch 233: 0.312\n",
      "test accuracy on epoch 233: 0.769\n",
      "train loss on epoch 234 : 0.173\n",
      "train accuracy on epoch 234: 0.889\n",
      "test loss on epoch 234: 0.313\n",
      "test accuracy on epoch 234: 0.769\n",
      "train loss on epoch 235 : 0.203\n",
      "train accuracy on epoch 235: 0.944\n",
      "test loss on epoch 235: 0.302\n",
      "test accuracy on epoch 235: 0.769\n",
      "train loss on epoch 236 : 0.221\n",
      "train accuracy on epoch 236: 0.944\n",
      "test loss on epoch 236: 0.315\n",
      "test accuracy on epoch 236: 0.769\n",
      "train loss on epoch 237 : 0.278\n",
      "train accuracy on epoch 237: 0.944\n",
      "test loss on epoch 237: 0.296\n",
      "test accuracy on epoch 237: 0.769\n",
      "train loss on epoch 238 : 0.237\n",
      "train accuracy on epoch 238: 0.889\n",
      "test loss on epoch 238: 0.314\n",
      "test accuracy on epoch 238: 0.769\n",
      "train loss on epoch 239 : 0.104\n",
      "train accuracy on epoch 239: 0.944\n",
      "test loss on epoch 239: 0.301\n",
      "test accuracy on epoch 239: 0.769\n",
      "train loss on epoch 240 : 0.187\n",
      "train accuracy on epoch 240: 0.944\n",
      "test loss on epoch 240: 0.318\n",
      "test accuracy on epoch 240: 0.769\n",
      "train loss on epoch 241 : 0.304\n",
      "train accuracy on epoch 241: 0.778\n",
      "test loss on epoch 241: 0.297\n",
      "test accuracy on epoch 241: 0.769\n",
      "train loss on epoch 242 : 0.144\n",
      "train accuracy on epoch 242: 0.944\n",
      "test loss on epoch 242: 0.313\n",
      "test accuracy on epoch 242: 0.769\n",
      "train loss on epoch 243 : 0.065\n",
      "train accuracy on epoch 243: 1.000\n",
      "test loss on epoch 243: 0.300\n",
      "test accuracy on epoch 243: 0.769\n",
      "train loss on epoch 244 : 0.254\n",
      "train accuracy on epoch 244: 0.944\n",
      "test loss on epoch 244: 0.296\n",
      "test accuracy on epoch 244: 0.769\n",
      "train loss on epoch 245 : 0.149\n",
      "train accuracy on epoch 245: 1.000\n",
      "test loss on epoch 245: 0.313\n",
      "test accuracy on epoch 245: 0.769\n",
      "train loss on epoch 246 : 0.119\n",
      "train accuracy on epoch 246: 0.944\n",
      "test loss on epoch 246: 0.299\n",
      "test accuracy on epoch 246: 0.769\n",
      "train loss on epoch 247 : 0.205\n",
      "train accuracy on epoch 247: 0.889\n",
      "test loss on epoch 247: 0.302\n",
      "test accuracy on epoch 247: 0.769\n",
      "train loss on epoch 248 : 0.150\n",
      "train accuracy on epoch 248: 0.889\n",
      "test loss on epoch 248: 0.311\n",
      "test accuracy on epoch 248: 0.769\n",
      "train loss on epoch 249 : 0.132\n",
      "train accuracy on epoch 249: 0.944\n",
      "test loss on epoch 249: 0.298\n",
      "test accuracy on epoch 249: 0.769\n",
      "train loss on epoch 250 : 0.198\n",
      "train accuracy on epoch 250: 0.944\n",
      "test loss on epoch 250: 0.297\n",
      "test accuracy on epoch 250: 0.769\n",
      "train loss on epoch 251 : 0.080\n",
      "train accuracy on epoch 251: 0.944\n",
      "test loss on epoch 251: 0.314\n",
      "test accuracy on epoch 251: 0.769\n",
      "train loss on epoch 252 : 0.059\n",
      "train accuracy on epoch 252: 1.000\n",
      "test loss on epoch 252: 0.311\n",
      "test accuracy on epoch 252: 0.769\n",
      "train loss on epoch 253 : 0.068\n",
      "train accuracy on epoch 253: 1.000\n",
      "test loss on epoch 253: 0.314\n",
      "test accuracy on epoch 253: 0.769\n",
      "train loss on epoch 254 : 0.303\n",
      "train accuracy on epoch 254: 0.889\n",
      "test loss on epoch 254: 0.313\n",
      "test accuracy on epoch 254: 0.769\n",
      "train loss on epoch 255 : 0.244\n",
      "train accuracy on epoch 255: 0.833\n",
      "test loss on epoch 255: 0.304\n",
      "test accuracy on epoch 255: 0.769\n",
      "train loss on epoch 256 : 0.149\n",
      "train accuracy on epoch 256: 0.944\n",
      "test loss on epoch 256: 0.308\n",
      "test accuracy on epoch 256: 0.769\n",
      "train loss on epoch 257 : 0.383\n",
      "train accuracy on epoch 257: 0.889\n",
      "test loss on epoch 257: 0.303\n",
      "test accuracy on epoch 257: 0.846\n",
      "train loss on epoch 258 : 0.215\n",
      "train accuracy on epoch 258: 0.944\n",
      "test loss on epoch 258: 0.308\n",
      "test accuracy on epoch 258: 0.692\n",
      "train loss on epoch 259 : 0.261\n",
      "train accuracy on epoch 259: 0.944\n",
      "test loss on epoch 259: 0.299\n",
      "test accuracy on epoch 259: 0.769\n",
      "train loss on epoch 260 : 0.114\n",
      "train accuracy on epoch 260: 1.000\n",
      "test loss on epoch 260: 0.299\n",
      "test accuracy on epoch 260: 0.769\n",
      "train loss on epoch 261 : 0.150\n",
      "train accuracy on epoch 261: 0.944\n",
      "test loss on epoch 261: 0.302\n",
      "test accuracy on epoch 261: 0.769\n",
      "train loss on epoch 262 : 0.330\n",
      "train accuracy on epoch 262: 0.833\n",
      "test loss on epoch 262: 0.294\n",
      "test accuracy on epoch 262: 0.769\n",
      "train loss on epoch 263 : 0.334\n",
      "train accuracy on epoch 263: 0.889\n",
      "test loss on epoch 263: 0.314\n",
      "test accuracy on epoch 263: 0.769\n",
      "train loss on epoch 264 : 0.190\n",
      "train accuracy on epoch 264: 0.889\n",
      "test loss on epoch 264: 0.305\n",
      "test accuracy on epoch 264: 0.769\n",
      "train loss on epoch 265 : 0.162\n",
      "train accuracy on epoch 265: 0.889\n",
      "test loss on epoch 265: 0.306\n",
      "test accuracy on epoch 265: 0.769\n",
      "train loss on epoch 266 : 0.176\n",
      "train accuracy on epoch 266: 0.944\n",
      "test loss on epoch 266: 0.321\n",
      "test accuracy on epoch 266: 0.769\n",
      "train loss on epoch 267 : 0.135\n",
      "train accuracy on epoch 267: 0.944\n",
      "test loss on epoch 267: 0.320\n",
      "test accuracy on epoch 267: 0.769\n",
      "train loss on epoch 268 : 0.244\n",
      "train accuracy on epoch 268: 0.944\n",
      "test loss on epoch 268: 0.304\n",
      "test accuracy on epoch 268: 0.769\n",
      "train loss on epoch 269 : 0.210\n",
      "train accuracy on epoch 269: 0.889\n",
      "test loss on epoch 269: 0.298\n",
      "test accuracy on epoch 269: 0.769\n",
      "train loss on epoch 270 : 0.120\n",
      "train accuracy on epoch 270: 0.944\n",
      "test loss on epoch 270: 0.316\n",
      "test accuracy on epoch 270: 0.769\n",
      "train loss on epoch 271 : 0.047\n",
      "train accuracy on epoch 271: 1.000\n",
      "test loss on epoch 271: 0.302\n",
      "test accuracy on epoch 271: 0.769\n",
      "train loss on epoch 272 : 0.205\n",
      "train accuracy on epoch 272: 0.944\n",
      "test loss on epoch 272: 0.302\n",
      "test accuracy on epoch 272: 0.769\n",
      "train loss on epoch 273 : 0.188\n",
      "train accuracy on epoch 273: 0.889\n",
      "test loss on epoch 273: 0.296\n",
      "test accuracy on epoch 273: 0.769\n",
      "train loss on epoch 274 : 0.096\n",
      "train accuracy on epoch 274: 1.000\n",
      "test loss on epoch 274: 0.304\n",
      "test accuracy on epoch 274: 0.769\n",
      "train loss on epoch 275 : 0.194\n",
      "train accuracy on epoch 275: 0.944\n",
      "test loss on epoch 275: 0.315\n",
      "test accuracy on epoch 275: 0.769\n",
      "train loss on epoch 276 : 0.194\n",
      "train accuracy on epoch 276: 0.889\n",
      "test loss on epoch 276: 0.299\n",
      "test accuracy on epoch 276: 0.769\n",
      "train loss on epoch 277 : 0.356\n",
      "train accuracy on epoch 277: 0.833\n",
      "test loss on epoch 277: 0.318\n",
      "test accuracy on epoch 277: 0.769\n",
      "train loss on epoch 278 : 0.142\n",
      "train accuracy on epoch 278: 0.944\n",
      "test loss on epoch 278: 0.301\n",
      "test accuracy on epoch 278: 0.769\n",
      "train loss on epoch 279 : 0.127\n",
      "train accuracy on epoch 279: 1.000\n",
      "test loss on epoch 279: 0.305\n",
      "test accuracy on epoch 279: 0.769\n",
      "train loss on epoch 280 : 0.458\n",
      "train accuracy on epoch 280: 0.778\n",
      "test loss on epoch 280: 0.317\n",
      "test accuracy on epoch 280: 0.769\n",
      "train loss on epoch 281 : 0.052\n",
      "train accuracy on epoch 281: 1.000\n",
      "test loss on epoch 281: 0.305\n",
      "test accuracy on epoch 281: 0.769\n",
      "train loss on epoch 282 : 0.143\n",
      "train accuracy on epoch 282: 0.889\n",
      "test loss on epoch 282: 0.316\n",
      "test accuracy on epoch 282: 0.769\n",
      "train loss on epoch 283 : 0.244\n",
      "train accuracy on epoch 283: 0.889\n",
      "test loss on epoch 283: 0.302\n",
      "test accuracy on epoch 283: 0.769\n",
      "train loss on epoch 284 : 0.152\n",
      "train accuracy on epoch 284: 0.889\n",
      "test loss on epoch 284: 0.324\n",
      "test accuracy on epoch 284: 0.769\n",
      "train loss on epoch 285 : 0.098\n",
      "train accuracy on epoch 285: 0.944\n",
      "test loss on epoch 285: 0.308\n",
      "test accuracy on epoch 285: 0.769\n",
      "train loss on epoch 286 : 0.110\n",
      "train accuracy on epoch 286: 0.944\n",
      "test loss on epoch 286: 0.307\n",
      "test accuracy on epoch 286: 0.769\n",
      "train loss on epoch 287 : 0.254\n",
      "train accuracy on epoch 287: 0.889\n",
      "test loss on epoch 287: 0.305\n",
      "test accuracy on epoch 287: 0.769\n",
      "train loss on epoch 288 : 0.244\n",
      "train accuracy on epoch 288: 0.833\n",
      "test loss on epoch 288: 0.307\n",
      "test accuracy on epoch 288: 0.769\n",
      "train loss on epoch 289 : 0.138\n",
      "train accuracy on epoch 289: 0.889\n",
      "test loss on epoch 289: 0.317\n",
      "test accuracy on epoch 289: 0.769\n",
      "train loss on epoch 290 : 0.490\n",
      "train accuracy on epoch 290: 0.833\n",
      "test loss on epoch 290: 0.305\n",
      "test accuracy on epoch 290: 0.769\n",
      "train loss on epoch 291 : 0.167\n",
      "train accuracy on epoch 291: 0.944\n",
      "test loss on epoch 291: 0.316\n",
      "test accuracy on epoch 291: 0.769\n",
      "train loss on epoch 292 : 0.422\n",
      "train accuracy on epoch 292: 0.889\n",
      "test loss on epoch 292: 0.318\n",
      "test accuracy on epoch 292: 0.769\n",
      "train loss on epoch 293 : 0.356\n",
      "train accuracy on epoch 293: 0.889\n",
      "test loss on epoch 293: 0.309\n",
      "test accuracy on epoch 293: 0.769\n",
      "train loss on epoch 294 : 0.132\n",
      "train accuracy on epoch 294: 0.944\n",
      "test loss on epoch 294: 0.312\n",
      "test accuracy on epoch 294: 0.769\n",
      "train loss on epoch 295 : 0.244\n",
      "train accuracy on epoch 295: 0.944\n",
      "test loss on epoch 295: 0.314\n",
      "test accuracy on epoch 295: 0.769\n",
      "train loss on epoch 296 : 0.162\n",
      "train accuracy on epoch 296: 0.889\n",
      "test loss on epoch 296: 0.324\n",
      "test accuracy on epoch 296: 0.769\n",
      "train loss on epoch 297 : 0.111\n",
      "train accuracy on epoch 297: 0.944\n",
      "test loss on epoch 297: 0.328\n",
      "test accuracy on epoch 297: 0.769\n",
      "train loss on epoch 298 : 0.111\n",
      "train accuracy on epoch 298: 0.889\n",
      "test loss on epoch 298: 0.330\n",
      "test accuracy on epoch 298: 0.769\n",
      "train loss on epoch 299 : 0.254\n",
      "train accuracy on epoch 299: 0.944\n",
      "test loss on epoch 299: 0.313\n",
      "test accuracy on epoch 299: 0.769\n",
      "train loss on epoch 300 : 0.187\n",
      "train accuracy on epoch 300: 0.944\n",
      "test loss on epoch 300: 0.324\n",
      "test accuracy on epoch 300: 0.769\n",
      "train loss on epoch 301 : 0.132\n",
      "train accuracy on epoch 301: 0.944\n",
      "test loss on epoch 301: 0.324\n",
      "test accuracy on epoch 301: 0.769\n",
      "train loss on epoch 302 : 0.266\n",
      "train accuracy on epoch 302: 0.833\n",
      "test loss on epoch 302: 0.307\n",
      "test accuracy on epoch 302: 0.769\n",
      "train loss on epoch 303 : 0.192\n",
      "train accuracy on epoch 303: 0.944\n",
      "test loss on epoch 303: 0.306\n",
      "test accuracy on epoch 303: 0.769\n",
      "train loss on epoch 304 : 0.108\n",
      "train accuracy on epoch 304: 1.000\n",
      "test loss on epoch 304: 0.327\n",
      "test accuracy on epoch 304: 0.769\n",
      "train loss on epoch 305 : 0.159\n",
      "train accuracy on epoch 305: 0.889\n",
      "test loss on epoch 305: 0.334\n",
      "test accuracy on epoch 305: 0.769\n",
      "train loss on epoch 306 : 0.223\n",
      "train accuracy on epoch 306: 0.944\n",
      "test loss on epoch 306: 0.334\n",
      "test accuracy on epoch 306: 0.769\n",
      "train loss on epoch 307 : 0.082\n",
      "train accuracy on epoch 307: 1.000\n",
      "test loss on epoch 307: 0.313\n",
      "test accuracy on epoch 307: 0.769\n",
      "train loss on epoch 308 : 0.312\n",
      "train accuracy on epoch 308: 0.889\n",
      "test loss on epoch 308: 0.328\n",
      "test accuracy on epoch 308: 0.769\n",
      "train loss on epoch 309 : 0.123\n",
      "train accuracy on epoch 309: 0.944\n",
      "test loss on epoch 309: 0.330\n",
      "test accuracy on epoch 309: 0.769\n",
      "train loss on epoch 310 : 0.081\n",
      "train accuracy on epoch 310: 1.000\n",
      "test loss on epoch 310: 0.308\n",
      "test accuracy on epoch 310: 0.769\n",
      "train loss on epoch 311 : 0.151\n",
      "train accuracy on epoch 311: 0.889\n",
      "test loss on epoch 311: 0.307\n",
      "test accuracy on epoch 311: 0.769\n",
      "train loss on epoch 312 : 0.197\n",
      "train accuracy on epoch 312: 0.944\n",
      "test loss on epoch 312: 0.324\n",
      "test accuracy on epoch 312: 0.769\n",
      "train loss on epoch 313 : 0.101\n",
      "train accuracy on epoch 313: 0.944\n",
      "test loss on epoch 313: 0.329\n",
      "test accuracy on epoch 313: 0.769\n",
      "train loss on epoch 314 : 0.217\n",
      "train accuracy on epoch 314: 0.889\n",
      "test loss on epoch 314: 0.333\n",
      "test accuracy on epoch 314: 0.769\n",
      "train loss on epoch 315 : 0.275\n",
      "train accuracy on epoch 315: 0.889\n",
      "test loss on epoch 315: 0.336\n",
      "test accuracy on epoch 315: 0.769\n",
      "train loss on epoch 316 : 0.040\n",
      "train accuracy on epoch 316: 1.000\n",
      "test loss on epoch 316: 0.341\n",
      "test accuracy on epoch 316: 0.769\n",
      "train loss on epoch 317 : 0.092\n",
      "train accuracy on epoch 317: 1.000\n",
      "test loss on epoch 317: 0.343\n",
      "test accuracy on epoch 317: 0.769\n",
      "train loss on epoch 318 : 0.349\n",
      "train accuracy on epoch 318: 0.833\n",
      "test loss on epoch 318: 0.321\n",
      "test accuracy on epoch 318: 0.769\n",
      "train loss on epoch 319 : 0.177\n",
      "train accuracy on epoch 319: 0.889\n",
      "test loss on epoch 319: 0.348\n",
      "test accuracy on epoch 319: 0.769\n",
      "train loss on epoch 320 : 0.125\n",
      "train accuracy on epoch 320: 0.944\n",
      "test loss on epoch 320: 0.344\n",
      "test accuracy on epoch 320: 0.769\n",
      "train loss on epoch 321 : 0.098\n",
      "train accuracy on epoch 321: 1.000\n",
      "test loss on epoch 321: 0.322\n",
      "test accuracy on epoch 321: 0.769\n",
      "train loss on epoch 322 : 0.170\n",
      "train accuracy on epoch 322: 0.944\n",
      "test loss on epoch 322: 0.346\n",
      "test accuracy on epoch 322: 0.769\n",
      "train loss on epoch 323 : 0.114\n",
      "train accuracy on epoch 323: 0.889\n",
      "test loss on epoch 323: 0.321\n",
      "test accuracy on epoch 323: 0.769\n",
      "train loss on epoch 324 : 0.414\n",
      "train accuracy on epoch 324: 0.833\n",
      "test loss on epoch 324: 0.341\n",
      "test accuracy on epoch 324: 0.769\n",
      "train loss on epoch 325 : 0.376\n",
      "train accuracy on epoch 325: 0.889\n",
      "test loss on epoch 325: 0.317\n",
      "test accuracy on epoch 325: 0.769\n",
      "train loss on epoch 326 : 0.273\n",
      "train accuracy on epoch 326: 0.833\n",
      "test loss on epoch 326: 0.320\n",
      "test accuracy on epoch 326: 0.769\n",
      "train loss on epoch 327 : 0.054\n",
      "train accuracy on epoch 327: 1.000\n",
      "test loss on epoch 327: 0.322\n",
      "test accuracy on epoch 327: 0.769\n",
      "train loss on epoch 328 : 0.152\n",
      "train accuracy on epoch 328: 0.944\n",
      "test loss on epoch 328: 0.327\n",
      "test accuracy on epoch 328: 0.769\n",
      "train loss on epoch 329 : 0.220\n",
      "train accuracy on epoch 329: 0.944\n",
      "test loss on epoch 329: 0.358\n",
      "test accuracy on epoch 329: 0.769\n",
      "train loss on epoch 330 : 0.066\n",
      "train accuracy on epoch 330: 1.000\n",
      "test loss on epoch 330: 0.357\n",
      "test accuracy on epoch 330: 0.769\n",
      "train loss on epoch 331 : 0.234\n",
      "train accuracy on epoch 331: 0.944\n",
      "test loss on epoch 331: 0.348\n",
      "test accuracy on epoch 331: 0.769\n",
      "train loss on epoch 332 : 0.142\n",
      "train accuracy on epoch 332: 0.944\n",
      "test loss on epoch 332: 0.345\n",
      "test accuracy on epoch 332: 0.769\n",
      "train loss on epoch 333 : 0.098\n",
      "train accuracy on epoch 333: 0.944\n",
      "test loss on epoch 333: 0.321\n",
      "test accuracy on epoch 333: 0.769\n",
      "train loss on epoch 334 : 0.459\n",
      "train accuracy on epoch 334: 0.889\n",
      "test loss on epoch 334: 0.314\n",
      "test accuracy on epoch 334: 0.769\n",
      "train loss on epoch 335 : 0.024\n",
      "train accuracy on epoch 335: 1.000\n",
      "test loss on epoch 335: 0.312\n",
      "test accuracy on epoch 335: 0.769\n",
      "train loss on epoch 336 : 0.139\n",
      "train accuracy on epoch 336: 0.944\n",
      "test loss on epoch 336: 0.325\n",
      "test accuracy on epoch 336: 0.769\n",
      "train loss on epoch 337 : 0.187\n",
      "train accuracy on epoch 337: 0.889\n",
      "test loss on epoch 337: 0.310\n",
      "test accuracy on epoch 337: 0.769\n",
      "train loss on epoch 338 : 0.097\n",
      "train accuracy on epoch 338: 0.944\n",
      "test loss on epoch 338: 0.303\n",
      "test accuracy on epoch 338: 0.769\n",
      "train loss on epoch 339 : 0.206\n",
      "train accuracy on epoch 339: 0.944\n",
      "test loss on epoch 339: 0.303\n",
      "test accuracy on epoch 339: 0.769\n",
      "train loss on epoch 340 : 0.306\n",
      "train accuracy on epoch 340: 0.833\n",
      "test loss on epoch 340: 0.302\n",
      "test accuracy on epoch 340: 0.769\n",
      "train loss on epoch 341 : 0.176\n",
      "train accuracy on epoch 341: 0.944\n",
      "test loss on epoch 341: 0.305\n",
      "test accuracy on epoch 341: 0.769\n",
      "train loss on epoch 342 : 0.240\n",
      "train accuracy on epoch 342: 0.944\n",
      "test loss on epoch 342: 0.317\n",
      "test accuracy on epoch 342: 0.769\n",
      "train loss on epoch 343 : 0.132\n",
      "train accuracy on epoch 343: 0.889\n",
      "test loss on epoch 343: 0.322\n",
      "test accuracy on epoch 343: 0.769\n",
      "train loss on epoch 344 : 0.208\n",
      "train accuracy on epoch 344: 0.944\n",
      "test loss on epoch 344: 0.320\n",
      "test accuracy on epoch 344: 0.769\n",
      "train loss on epoch 345 : 0.136\n",
      "train accuracy on epoch 345: 0.944\n",
      "test loss on epoch 345: 0.321\n",
      "test accuracy on epoch 345: 0.769\n",
      "train loss on epoch 346 : 0.132\n",
      "train accuracy on epoch 346: 0.944\n",
      "test loss on epoch 346: 0.316\n",
      "test accuracy on epoch 346: 0.769\n",
      "train loss on epoch 347 : 0.486\n",
      "train accuracy on epoch 347: 0.778\n",
      "test loss on epoch 347: 0.314\n",
      "test accuracy on epoch 347: 0.769\n",
      "train loss on epoch 348 : 0.168\n",
      "train accuracy on epoch 348: 0.944\n",
      "test loss on epoch 348: 0.318\n",
      "test accuracy on epoch 348: 0.769\n",
      "train loss on epoch 349 : 0.266\n",
      "train accuracy on epoch 349: 0.944\n",
      "test loss on epoch 349: 0.308\n",
      "test accuracy on epoch 349: 0.846\n",
      "train loss on epoch 350 : 0.054\n",
      "train accuracy on epoch 350: 1.000\n",
      "test loss on epoch 350: 0.312\n",
      "test accuracy on epoch 350: 0.769\n",
      "train loss on epoch 351 : 0.228\n",
      "train accuracy on epoch 351: 0.889\n",
      "test loss on epoch 351: 0.317\n",
      "test accuracy on epoch 351: 0.769\n",
      "train loss on epoch 352 : 0.393\n",
      "train accuracy on epoch 352: 0.889\n",
      "test loss on epoch 352: 0.320\n",
      "test accuracy on epoch 352: 0.769\n",
      "train loss on epoch 353 : 0.117\n",
      "train accuracy on epoch 353: 0.944\n",
      "test loss on epoch 353: 0.319\n",
      "test accuracy on epoch 353: 0.769\n",
      "train loss on epoch 354 : 0.239\n",
      "train accuracy on epoch 354: 0.889\n",
      "test loss on epoch 354: 0.322\n",
      "test accuracy on epoch 354: 0.769\n",
      "train loss on epoch 355 : 0.136\n",
      "train accuracy on epoch 355: 0.944\n",
      "test loss on epoch 355: 0.312\n",
      "test accuracy on epoch 355: 0.769\n",
      "train loss on epoch 356 : 0.075\n",
      "train accuracy on epoch 356: 1.000\n",
      "test loss on epoch 356: 0.323\n",
      "test accuracy on epoch 356: 0.769\n",
      "train loss on epoch 357 : 0.168\n",
      "train accuracy on epoch 357: 0.944\n",
      "test loss on epoch 357: 0.322\n",
      "test accuracy on epoch 357: 0.769\n",
      "train loss on epoch 358 : 0.183\n",
      "train accuracy on epoch 358: 0.889\n",
      "test loss on epoch 358: 0.315\n",
      "test accuracy on epoch 358: 0.769\n",
      "train loss on epoch 359 : 0.087\n",
      "train accuracy on epoch 359: 1.000\n",
      "test loss on epoch 359: 0.322\n",
      "test accuracy on epoch 359: 0.769\n",
      "train loss on epoch 360 : 0.090\n",
      "train accuracy on epoch 360: 1.000\n",
      "test loss on epoch 360: 0.318\n",
      "test accuracy on epoch 360: 0.769\n",
      "train loss on epoch 361 : 0.169\n",
      "train accuracy on epoch 361: 0.889\n",
      "test loss on epoch 361: 0.317\n",
      "test accuracy on epoch 361: 0.769\n",
      "train loss on epoch 362 : 0.355\n",
      "train accuracy on epoch 362: 0.889\n",
      "test loss on epoch 362: 0.329\n",
      "test accuracy on epoch 362: 0.769\n",
      "train loss on epoch 363 : 0.078\n",
      "train accuracy on epoch 363: 0.944\n",
      "test loss on epoch 363: 0.317\n",
      "test accuracy on epoch 363: 0.769\n",
      "train loss on epoch 364 : 0.079\n",
      "train accuracy on epoch 364: 1.000\n",
      "test loss on epoch 364: 0.337\n",
      "test accuracy on epoch 364: 0.769\n",
      "train loss on epoch 365 : 0.152\n",
      "train accuracy on epoch 365: 0.889\n",
      "test loss on epoch 365: 0.333\n",
      "test accuracy on epoch 365: 0.769\n",
      "train loss on epoch 366 : 0.150\n",
      "train accuracy on epoch 366: 0.944\n",
      "test loss on epoch 366: 0.319\n",
      "test accuracy on epoch 366: 0.769\n",
      "train loss on epoch 367 : 0.122\n",
      "train accuracy on epoch 367: 0.944\n",
      "test loss on epoch 367: 0.331\n",
      "test accuracy on epoch 367: 0.769\n",
      "train loss on epoch 368 : 0.054\n",
      "train accuracy on epoch 368: 1.000\n",
      "test loss on epoch 368: 0.329\n",
      "test accuracy on epoch 368: 0.769\n",
      "train loss on epoch 369 : 0.276\n",
      "train accuracy on epoch 369: 0.833\n",
      "test loss on epoch 369: 0.328\n",
      "test accuracy on epoch 369: 0.769\n",
      "train loss on epoch 370 : 0.261\n",
      "train accuracy on epoch 370: 0.778\n",
      "test loss on epoch 370: 0.326\n",
      "test accuracy on epoch 370: 0.769\n",
      "train loss on epoch 371 : 0.245\n",
      "train accuracy on epoch 371: 0.944\n",
      "test loss on epoch 371: 0.324\n",
      "test accuracy on epoch 371: 0.769\n",
      "train loss on epoch 372 : 0.332\n",
      "train accuracy on epoch 372: 0.944\n",
      "test loss on epoch 372: 0.329\n",
      "test accuracy on epoch 372: 0.769\n",
      "train loss on epoch 373 : 0.184\n",
      "train accuracy on epoch 373: 0.889\n",
      "test loss on epoch 373: 0.327\n",
      "test accuracy on epoch 373: 0.769\n",
      "train loss on epoch 374 : 0.188\n",
      "train accuracy on epoch 374: 0.944\n",
      "test loss on epoch 374: 0.331\n",
      "test accuracy on epoch 374: 0.769\n",
      "train loss on epoch 375 : 0.165\n",
      "train accuracy on epoch 375: 0.889\n",
      "test loss on epoch 375: 0.332\n",
      "test accuracy on epoch 375: 0.769\n",
      "train loss on epoch 376 : 0.193\n",
      "train accuracy on epoch 376: 0.889\n",
      "test loss on epoch 376: 0.331\n",
      "test accuracy on epoch 376: 0.769\n",
      "train loss on epoch 377 : 0.376\n",
      "train accuracy on epoch 377: 0.889\n",
      "test loss on epoch 377: 0.324\n",
      "test accuracy on epoch 377: 0.769\n",
      "train loss on epoch 378 : 0.304\n",
      "train accuracy on epoch 378: 0.833\n",
      "test loss on epoch 378: 0.331\n",
      "test accuracy on epoch 378: 0.769\n",
      "train loss on epoch 379 : 0.145\n",
      "train accuracy on epoch 379: 0.944\n",
      "test loss on epoch 379: 0.336\n",
      "test accuracy on epoch 379: 0.769\n",
      "train loss on epoch 380 : 0.291\n",
      "train accuracy on epoch 380: 0.889\n",
      "test loss on epoch 380: 0.342\n",
      "test accuracy on epoch 380: 0.769\n",
      "train loss on epoch 381 : 0.363\n",
      "train accuracy on epoch 381: 0.833\n",
      "test loss on epoch 381: 0.329\n",
      "test accuracy on epoch 381: 0.769\n",
      "train loss on epoch 382 : 0.183\n",
      "train accuracy on epoch 382: 0.889\n",
      "test loss on epoch 382: 0.354\n",
      "test accuracy on epoch 382: 0.769\n",
      "train loss on epoch 383 : 0.229\n",
      "train accuracy on epoch 383: 0.889\n",
      "test loss on epoch 383: 0.338\n",
      "test accuracy on epoch 383: 0.769\n",
      "train loss on epoch 384 : 0.085\n",
      "train accuracy on epoch 384: 1.000\n",
      "test loss on epoch 384: 0.349\n",
      "test accuracy on epoch 384: 0.769\n",
      "train loss on epoch 385 : 0.148\n",
      "train accuracy on epoch 385: 0.944\n",
      "test loss on epoch 385: 0.346\n",
      "test accuracy on epoch 385: 0.769\n",
      "train loss on epoch 386 : 0.241\n",
      "train accuracy on epoch 386: 0.889\n",
      "test loss on epoch 386: 0.341\n",
      "test accuracy on epoch 386: 0.769\n",
      "train loss on epoch 387 : 0.237\n",
      "train accuracy on epoch 387: 0.944\n",
      "test loss on epoch 387: 0.325\n",
      "test accuracy on epoch 387: 0.769\n",
      "train loss on epoch 388 : 0.112\n",
      "train accuracy on epoch 388: 0.944\n",
      "test loss on epoch 388: 0.332\n",
      "test accuracy on epoch 388: 0.769\n",
      "train loss on epoch 389 : 0.234\n",
      "train accuracy on epoch 389: 0.889\n",
      "test loss on epoch 389: 0.341\n",
      "test accuracy on epoch 389: 0.769\n",
      "train loss on epoch 390 : 0.204\n",
      "train accuracy on epoch 390: 0.889\n",
      "test loss on epoch 390: 0.342\n",
      "test accuracy on epoch 390: 0.769\n",
      "train loss on epoch 391 : 0.154\n",
      "train accuracy on epoch 391: 0.944\n",
      "test loss on epoch 391: 0.327\n",
      "test accuracy on epoch 391: 0.769\n",
      "train loss on epoch 392 : 0.171\n",
      "train accuracy on epoch 392: 0.889\n",
      "test loss on epoch 392: 0.352\n",
      "test accuracy on epoch 392: 0.769\n",
      "train loss on epoch 393 : 0.270\n",
      "train accuracy on epoch 393: 0.944\n",
      "test loss on epoch 393: 0.346\n",
      "test accuracy on epoch 393: 0.769\n",
      "train loss on epoch 394 : 0.178\n",
      "train accuracy on epoch 394: 0.944\n",
      "test loss on epoch 394: 0.327\n",
      "test accuracy on epoch 394: 0.769\n",
      "train loss on epoch 395 : 0.159\n",
      "train accuracy on epoch 395: 0.944\n",
      "test loss on epoch 395: 0.327\n",
      "test accuracy on epoch 395: 0.769\n",
      "train loss on epoch 396 : 0.169\n",
      "train accuracy on epoch 396: 0.944\n",
      "test loss on epoch 396: 0.347\n",
      "test accuracy on epoch 396: 0.769\n",
      "train loss on epoch 397 : 0.104\n",
      "train accuracy on epoch 397: 0.944\n",
      "test loss on epoch 397: 0.347\n",
      "test accuracy on epoch 397: 0.769\n",
      "train loss on epoch 398 : 0.284\n",
      "train accuracy on epoch 398: 0.889\n",
      "test loss on epoch 398: 0.334\n",
      "test accuracy on epoch 398: 0.769\n",
      "train loss on epoch 399 : 0.173\n",
      "train accuracy on epoch 399: 0.889\n",
      "test loss on epoch 399: 0.332\n",
      "test accuracy on epoch 399: 0.769\n",
      "train loss on epoch 400 : 0.165\n",
      "train accuracy on epoch 400: 0.889\n",
      "test loss on epoch 400: 0.355\n",
      "test accuracy on epoch 400: 0.769\n",
      "train loss on epoch 401 : 0.275\n",
      "train accuracy on epoch 401: 0.944\n",
      "test loss on epoch 401: 0.334\n",
      "test accuracy on epoch 401: 0.769\n",
      "train loss on epoch 402 : 0.078\n",
      "train accuracy on epoch 402: 1.000\n",
      "test loss on epoch 402: 0.333\n",
      "test accuracy on epoch 402: 0.769\n",
      "train loss on epoch 403 : 0.300\n",
      "train accuracy on epoch 403: 0.833\n",
      "test loss on epoch 403: 0.340\n",
      "test accuracy on epoch 403: 0.769\n",
      "train loss on epoch 404 : 0.185\n",
      "train accuracy on epoch 404: 0.944\n",
      "test loss on epoch 404: 0.340\n",
      "test accuracy on epoch 404: 0.769\n",
      "train loss on epoch 405 : 0.155\n",
      "train accuracy on epoch 405: 0.944\n",
      "test loss on epoch 405: 0.325\n",
      "test accuracy on epoch 405: 0.769\n",
      "train loss on epoch 406 : 0.174\n",
      "train accuracy on epoch 406: 0.944\n",
      "test loss on epoch 406: 0.338\n",
      "test accuracy on epoch 406: 0.769\n",
      "train loss on epoch 407 : 0.189\n",
      "train accuracy on epoch 407: 0.944\n",
      "test loss on epoch 407: 0.327\n",
      "test accuracy on epoch 407: 0.769\n",
      "train loss on epoch 408 : 0.107\n",
      "train accuracy on epoch 408: 0.944\n",
      "test loss on epoch 408: 0.332\n",
      "test accuracy on epoch 408: 0.769\n",
      "train loss on epoch 409 : 0.120\n",
      "train accuracy on epoch 409: 0.944\n",
      "test loss on epoch 409: 0.342\n",
      "test accuracy on epoch 409: 0.769\n",
      "train loss on epoch 410 : 0.164\n",
      "train accuracy on epoch 410: 0.889\n",
      "test loss on epoch 410: 0.333\n",
      "test accuracy on epoch 410: 0.769\n",
      "train loss on epoch 411 : 0.064\n",
      "train accuracy on epoch 411: 1.000\n",
      "test loss on epoch 411: 0.348\n",
      "test accuracy on epoch 411: 0.769\n",
      "train loss on epoch 412 : 0.449\n",
      "train accuracy on epoch 412: 0.833\n",
      "test loss on epoch 412: 0.334\n",
      "test accuracy on epoch 412: 0.769\n",
      "train loss on epoch 413 : 0.113\n",
      "train accuracy on epoch 413: 0.944\n",
      "test loss on epoch 413: 0.341\n",
      "test accuracy on epoch 413: 0.769\n",
      "train loss on epoch 414 : 0.314\n",
      "train accuracy on epoch 414: 0.889\n",
      "test loss on epoch 414: 0.334\n",
      "test accuracy on epoch 414: 0.769\n",
      "train loss on epoch 415 : 0.081\n",
      "train accuracy on epoch 415: 1.000\n",
      "test loss on epoch 415: 0.342\n",
      "test accuracy on epoch 415: 0.769\n",
      "train loss on epoch 416 : 0.304\n",
      "train accuracy on epoch 416: 0.833\n",
      "test loss on epoch 416: 0.334\n",
      "test accuracy on epoch 416: 0.769\n",
      "train loss on epoch 417 : 0.478\n",
      "train accuracy on epoch 417: 0.889\n",
      "test loss on epoch 417: 0.329\n",
      "test accuracy on epoch 417: 0.769\n",
      "train loss on epoch 418 : 0.278\n",
      "train accuracy on epoch 418: 0.889\n",
      "test loss on epoch 418: 0.350\n",
      "test accuracy on epoch 418: 0.769\n",
      "train loss on epoch 419 : 0.062\n",
      "train accuracy on epoch 419: 1.000\n",
      "test loss on epoch 419: 0.331\n",
      "test accuracy on epoch 419: 0.769\n",
      "train loss on epoch 420 : 0.188\n",
      "train accuracy on epoch 420: 0.944\n",
      "test loss on epoch 420: 0.337\n",
      "test accuracy on epoch 420: 0.769\n",
      "train loss on epoch 421 : 0.085\n",
      "train accuracy on epoch 421: 1.000\n",
      "test loss on epoch 421: 0.338\n",
      "test accuracy on epoch 421: 0.769\n",
      "train loss on epoch 422 : 0.206\n",
      "train accuracy on epoch 422: 0.944\n",
      "test loss on epoch 422: 0.352\n",
      "test accuracy on epoch 422: 0.769\n",
      "train loss on epoch 423 : 0.187\n",
      "train accuracy on epoch 423: 0.944\n",
      "test loss on epoch 423: 0.348\n",
      "test accuracy on epoch 423: 0.769\n",
      "train loss on epoch 424 : 0.320\n",
      "train accuracy on epoch 424: 0.889\n",
      "test loss on epoch 424: 0.334\n",
      "test accuracy on epoch 424: 0.769\n",
      "train loss on epoch 425 : 0.140\n",
      "train accuracy on epoch 425: 0.944\n",
      "test loss on epoch 425: 0.353\n",
      "test accuracy on epoch 425: 0.769\n",
      "train loss on epoch 426 : 0.099\n",
      "train accuracy on epoch 426: 0.944\n",
      "test loss on epoch 426: 0.335\n",
      "test accuracy on epoch 426: 0.769\n",
      "train loss on epoch 427 : 0.207\n",
      "train accuracy on epoch 427: 0.889\n",
      "test loss on epoch 427: 0.330\n",
      "test accuracy on epoch 427: 0.769\n",
      "train loss on epoch 428 : 0.271\n",
      "train accuracy on epoch 428: 0.889\n",
      "test loss on epoch 428: 0.334\n",
      "test accuracy on epoch 428: 0.769\n",
      "train loss on epoch 429 : 0.355\n",
      "train accuracy on epoch 429: 0.889\n",
      "test loss on epoch 429: 0.351\n",
      "test accuracy on epoch 429: 0.769\n",
      "train loss on epoch 430 : 0.059\n",
      "train accuracy on epoch 430: 1.000\n",
      "test loss on epoch 430: 0.337\n",
      "test accuracy on epoch 430: 0.769\n",
      "train loss on epoch 431 : 0.214\n",
      "train accuracy on epoch 431: 0.889\n",
      "test loss on epoch 431: 0.341\n",
      "test accuracy on epoch 431: 0.769\n",
      "train loss on epoch 432 : 0.268\n",
      "train accuracy on epoch 432: 0.944\n",
      "test loss on epoch 432: 0.339\n",
      "test accuracy on epoch 432: 0.769\n",
      "train loss on epoch 433 : 0.230\n",
      "train accuracy on epoch 433: 0.944\n",
      "test loss on epoch 433: 0.348\n",
      "test accuracy on epoch 433: 0.769\n",
      "train loss on epoch 434 : 0.248\n",
      "train accuracy on epoch 434: 0.889\n",
      "test loss on epoch 434: 0.350\n",
      "test accuracy on epoch 434: 0.769\n",
      "train loss on epoch 435 : 0.163\n",
      "train accuracy on epoch 435: 0.944\n",
      "test loss on epoch 435: 0.344\n",
      "test accuracy on epoch 435: 0.769\n",
      "train loss on epoch 436 : 0.195\n",
      "train accuracy on epoch 436: 0.944\n",
      "test loss on epoch 436: 0.350\n",
      "test accuracy on epoch 436: 0.769\n",
      "train loss on epoch 437 : 0.199\n",
      "train accuracy on epoch 437: 0.833\n",
      "test loss on epoch 437: 0.333\n",
      "test accuracy on epoch 437: 0.769\n",
      "train loss on epoch 438 : 0.154\n",
      "train accuracy on epoch 438: 0.944\n",
      "test loss on epoch 438: 0.334\n",
      "test accuracy on epoch 438: 0.769\n",
      "train loss on epoch 439 : 0.220\n",
      "train accuracy on epoch 439: 0.889\n",
      "test loss on epoch 439: 0.332\n",
      "test accuracy on epoch 439: 0.769\n",
      "train loss on epoch 440 : 0.183\n",
      "train accuracy on epoch 440: 0.944\n",
      "test loss on epoch 440: 0.347\n",
      "test accuracy on epoch 440: 0.769\n",
      "train loss on epoch 441 : 0.250\n",
      "train accuracy on epoch 441: 0.944\n",
      "test loss on epoch 441: 0.334\n",
      "test accuracy on epoch 441: 0.769\n",
      "train loss on epoch 442 : 0.111\n",
      "train accuracy on epoch 442: 0.944\n",
      "test loss on epoch 442: 0.332\n",
      "test accuracy on epoch 442: 0.769\n",
      "train loss on epoch 443 : 0.201\n",
      "train accuracy on epoch 443: 0.833\n",
      "test loss on epoch 443: 0.344\n",
      "test accuracy on epoch 443: 0.769\n",
      "train loss on epoch 444 : 0.554\n",
      "train accuracy on epoch 444: 0.889\n",
      "test loss on epoch 444: 0.339\n",
      "test accuracy on epoch 444: 0.769\n",
      "train loss on epoch 445 : 0.123\n",
      "train accuracy on epoch 445: 1.000\n",
      "test loss on epoch 445: 0.333\n",
      "test accuracy on epoch 445: 0.846\n",
      "train loss on epoch 446 : 0.113\n",
      "train accuracy on epoch 446: 0.944\n",
      "test loss on epoch 446: 0.342\n",
      "test accuracy on epoch 446: 0.692\n",
      "train loss on epoch 447 : 0.188\n",
      "train accuracy on epoch 447: 0.944\n",
      "test loss on epoch 447: 0.333\n",
      "test accuracy on epoch 447: 0.769\n",
      "train loss on epoch 448 : 0.054\n",
      "train accuracy on epoch 448: 1.000\n",
      "test loss on epoch 448: 0.338\n",
      "test accuracy on epoch 448: 0.769\n",
      "train loss on epoch 449 : 0.152\n",
      "train accuracy on epoch 449: 0.889\n",
      "test loss on epoch 449: 0.344\n",
      "test accuracy on epoch 449: 0.769\n",
      "train loss on epoch 450 : 0.279\n",
      "train accuracy on epoch 450: 0.833\n",
      "test loss on epoch 450: 0.341\n",
      "test accuracy on epoch 450: 0.769\n",
      "train loss on epoch 451 : 0.084\n",
      "train accuracy on epoch 451: 0.944\n",
      "test loss on epoch 451: 0.340\n",
      "test accuracy on epoch 451: 0.769\n",
      "train loss on epoch 452 : 0.324\n",
      "train accuracy on epoch 452: 0.833\n",
      "test loss on epoch 452: 0.342\n",
      "test accuracy on epoch 452: 0.769\n",
      "train loss on epoch 453 : 0.167\n",
      "train accuracy on epoch 453: 0.944\n",
      "test loss on epoch 453: 0.339\n",
      "test accuracy on epoch 453: 0.769\n",
      "train loss on epoch 454 : 0.308\n",
      "train accuracy on epoch 454: 0.833\n",
      "test loss on epoch 454: 0.342\n",
      "test accuracy on epoch 454: 0.769\n",
      "train loss on epoch 455 : 0.384\n",
      "train accuracy on epoch 455: 0.889\n",
      "test loss on epoch 455: 0.337\n",
      "test accuracy on epoch 455: 0.769\n",
      "train loss on epoch 456 : 0.249\n",
      "train accuracy on epoch 456: 0.944\n",
      "test loss on epoch 456: 0.340\n",
      "test accuracy on epoch 456: 0.769\n",
      "train loss on epoch 457 : 0.242\n",
      "train accuracy on epoch 457: 0.889\n",
      "test loss on epoch 457: 0.330\n",
      "test accuracy on epoch 457: 0.769\n",
      "train loss on epoch 458 : 0.296\n",
      "train accuracy on epoch 458: 0.944\n",
      "test loss on epoch 458: 0.333\n",
      "test accuracy on epoch 458: 0.692\n",
      "train loss on epoch 459 : 0.086\n",
      "train accuracy on epoch 459: 1.000\n",
      "test loss on epoch 459: 0.322\n",
      "test accuracy on epoch 459: 0.769\n",
      "train loss on epoch 460 : 0.126\n",
      "train accuracy on epoch 460: 0.944\n",
      "test loss on epoch 460: 0.323\n",
      "test accuracy on epoch 460: 0.769\n",
      "train loss on epoch 461 : 0.382\n",
      "train accuracy on epoch 461: 0.833\n",
      "test loss on epoch 461: 0.328\n",
      "test accuracy on epoch 461: 0.769\n",
      "train loss on epoch 462 : 0.145\n",
      "train accuracy on epoch 462: 0.944\n",
      "test loss on epoch 462: 0.325\n",
      "test accuracy on epoch 462: 0.769\n",
      "train loss on epoch 463 : 0.051\n",
      "train accuracy on epoch 463: 1.000\n",
      "test loss on epoch 463: 0.329\n",
      "test accuracy on epoch 463: 0.769\n",
      "train loss on epoch 464 : 0.087\n",
      "train accuracy on epoch 464: 1.000\n",
      "test loss on epoch 464: 0.326\n",
      "test accuracy on epoch 464: 0.769\n",
      "train loss on epoch 465 : 0.131\n",
      "train accuracy on epoch 465: 1.000\n",
      "test loss on epoch 465: 0.326\n",
      "test accuracy on epoch 465: 0.769\n",
      "train loss on epoch 466 : 0.126\n",
      "train accuracy on epoch 466: 0.889\n",
      "test loss on epoch 466: 0.324\n",
      "test accuracy on epoch 466: 0.769\n",
      "train loss on epoch 467 : 0.059\n",
      "train accuracy on epoch 467: 1.000\n",
      "test loss on epoch 467: 0.325\n",
      "test accuracy on epoch 467: 0.769\n",
      "train loss on epoch 468 : 0.236\n",
      "train accuracy on epoch 468: 0.889\n",
      "test loss on epoch 468: 0.326\n",
      "test accuracy on epoch 468: 0.769\n",
      "train loss on epoch 469 : 0.141\n",
      "train accuracy on epoch 469: 1.000\n",
      "test loss on epoch 469: 0.321\n",
      "test accuracy on epoch 469: 0.846\n",
      "train loss on epoch 470 : 0.071\n",
      "train accuracy on epoch 470: 0.944\n",
      "test loss on epoch 470: 0.321\n",
      "test accuracy on epoch 470: 0.769\n",
      "train loss on epoch 471 : 0.256\n",
      "train accuracy on epoch 471: 0.833\n",
      "test loss on epoch 471: 0.333\n",
      "test accuracy on epoch 471: 0.692\n",
      "train loss on epoch 472 : 0.159\n",
      "train accuracy on epoch 472: 0.889\n",
      "test loss on epoch 472: 0.334\n",
      "test accuracy on epoch 472: 0.769\n",
      "train loss on epoch 473 : 0.147\n",
      "train accuracy on epoch 473: 0.944\n",
      "test loss on epoch 473: 0.335\n",
      "test accuracy on epoch 473: 0.769\n",
      "train loss on epoch 474 : 0.175\n",
      "train accuracy on epoch 474: 0.944\n",
      "test loss on epoch 474: 0.321\n",
      "test accuracy on epoch 474: 0.769\n",
      "train loss on epoch 475 : 0.126\n",
      "train accuracy on epoch 475: 0.944\n",
      "test loss on epoch 475: 0.331\n",
      "test accuracy on epoch 475: 0.769\n",
      "train loss on epoch 476 : 0.141\n",
      "train accuracy on epoch 476: 0.944\n",
      "test loss on epoch 476: 0.335\n",
      "test accuracy on epoch 476: 0.769\n",
      "train loss on epoch 477 : 0.042\n",
      "train accuracy on epoch 477: 1.000\n",
      "test loss on epoch 477: 0.322\n",
      "test accuracy on epoch 477: 0.769\n",
      "train loss on epoch 478 : 0.156\n",
      "train accuracy on epoch 478: 0.944\n",
      "test loss on epoch 478: 0.329\n",
      "test accuracy on epoch 478: 0.769\n",
      "train loss on epoch 479 : 0.153\n",
      "train accuracy on epoch 479: 0.944\n",
      "test loss on epoch 479: 0.334\n",
      "test accuracy on epoch 479: 0.769\n",
      "train loss on epoch 480 : 0.270\n",
      "train accuracy on epoch 480: 0.833\n",
      "test loss on epoch 480: 0.333\n",
      "test accuracy on epoch 480: 0.769\n",
      "train loss on epoch 481 : 0.316\n",
      "train accuracy on epoch 481: 0.833\n",
      "test loss on epoch 481: 0.323\n",
      "test accuracy on epoch 481: 0.769\n",
      "train loss on epoch 482 : 0.194\n",
      "train accuracy on epoch 482: 0.889\n",
      "test loss on epoch 482: 0.330\n",
      "test accuracy on epoch 482: 0.692\n",
      "train loss on epoch 483 : 0.070\n",
      "train accuracy on epoch 483: 1.000\n",
      "test loss on epoch 483: 0.329\n",
      "test accuracy on epoch 483: 0.769\n",
      "train loss on epoch 484 : 0.291\n",
      "train accuracy on epoch 484: 0.889\n",
      "test loss on epoch 484: 0.333\n",
      "test accuracy on epoch 484: 0.769\n",
      "train loss on epoch 485 : 0.187\n",
      "train accuracy on epoch 485: 0.889\n",
      "test loss on epoch 485: 0.329\n",
      "test accuracy on epoch 485: 0.769\n",
      "train loss on epoch 486 : 0.144\n",
      "train accuracy on epoch 486: 0.889\n",
      "test loss on epoch 486: 0.331\n",
      "test accuracy on epoch 486: 0.769\n",
      "train loss on epoch 487 : 0.232\n",
      "train accuracy on epoch 487: 0.889\n",
      "test loss on epoch 487: 0.333\n",
      "test accuracy on epoch 487: 0.769\n",
      "train loss on epoch 488 : 0.224\n",
      "train accuracy on epoch 488: 0.889\n",
      "test loss on epoch 488: 0.333\n",
      "test accuracy on epoch 488: 0.769\n",
      "train loss on epoch 489 : 0.105\n",
      "train accuracy on epoch 489: 0.944\n",
      "test loss on epoch 489: 0.326\n",
      "test accuracy on epoch 489: 0.769\n",
      "train loss on epoch 490 : 0.247\n",
      "train accuracy on epoch 490: 0.944\n",
      "test loss on epoch 490: 0.326\n",
      "test accuracy on epoch 490: 0.769\n",
      "train loss on epoch 491 : 0.132\n",
      "train accuracy on epoch 491: 0.944\n",
      "test loss on epoch 491: 0.323\n",
      "test accuracy on epoch 491: 0.769\n",
      "train loss on epoch 492 : 0.212\n",
      "train accuracy on epoch 492: 0.944\n",
      "test loss on epoch 492: 0.322\n",
      "test accuracy on epoch 492: 0.769\n",
      "train loss on epoch 493 : 0.143\n",
      "train accuracy on epoch 493: 0.944\n",
      "test loss on epoch 493: 0.321\n",
      "test accuracy on epoch 493: 0.769\n",
      "train loss on epoch 494 : 0.207\n",
      "train accuracy on epoch 494: 0.944\n",
      "test loss on epoch 494: 0.332\n",
      "test accuracy on epoch 494: 0.769\n",
      "train loss on epoch 495 : 0.063\n",
      "train accuracy on epoch 495: 1.000\n",
      "test loss on epoch 495: 0.328\n",
      "test accuracy on epoch 495: 0.769\n",
      "train loss on epoch 496 : 0.044\n",
      "train accuracy on epoch 496: 1.000\n",
      "test loss on epoch 496: 0.329\n",
      "test accuracy on epoch 496: 0.769\n",
      "train loss on epoch 497 : 0.151\n",
      "train accuracy on epoch 497: 0.944\n",
      "test loss on epoch 497: 0.323\n",
      "test accuracy on epoch 497: 0.769\n",
      "train loss on epoch 498 : 0.193\n",
      "train accuracy on epoch 498: 0.889\n",
      "test loss on epoch 498: 0.328\n",
      "test accuracy on epoch 498: 0.769\n",
      "train loss on epoch 499 : 0.212\n",
      "train accuracy on epoch 499: 0.944\n",
      "test loss on epoch 499: 0.329\n",
      "test accuracy on epoch 499: 0.769\n",
      "train loss on epoch 500 : 0.165\n",
      "train accuracy on epoch 500: 0.944\n",
      "test loss on epoch 500: 0.329\n",
      "test accuracy on epoch 500: 0.769\n",
      "train loss on epoch 501 : 0.090\n",
      "train accuracy on epoch 501: 0.944\n",
      "test loss on epoch 501: 0.326\n",
      "test accuracy on epoch 501: 0.692\n",
      "train loss on epoch 502 : 0.064\n",
      "train accuracy on epoch 502: 1.000\n",
      "test loss on epoch 502: 0.316\n",
      "test accuracy on epoch 502: 0.846\n",
      "train loss on epoch 503 : 0.071\n",
      "train accuracy on epoch 503: 0.944\n",
      "test loss on epoch 503: 0.318\n",
      "test accuracy on epoch 503: 0.846\n",
      "train loss on epoch 504 : 0.231\n",
      "train accuracy on epoch 504: 0.944\n",
      "test loss on epoch 504: 0.320\n",
      "test accuracy on epoch 504: 0.769\n",
      "train loss on epoch 505 : 0.557\n",
      "train accuracy on epoch 505: 0.833\n",
      "test loss on epoch 505: 0.324\n",
      "test accuracy on epoch 505: 0.769\n",
      "train loss on epoch 506 : 0.065\n",
      "train accuracy on epoch 506: 1.000\n",
      "test loss on epoch 506: 0.323\n",
      "test accuracy on epoch 506: 0.769\n",
      "train loss on epoch 507 : 0.074\n",
      "train accuracy on epoch 507: 1.000\n",
      "test loss on epoch 507: 0.322\n",
      "test accuracy on epoch 507: 0.769\n",
      "train loss on epoch 508 : 0.139\n",
      "train accuracy on epoch 508: 0.889\n",
      "test loss on epoch 508: 0.323\n",
      "test accuracy on epoch 508: 0.769\n",
      "train loss on epoch 509 : 0.061\n",
      "train accuracy on epoch 509: 1.000\n",
      "test loss on epoch 509: 0.322\n",
      "test accuracy on epoch 509: 0.769\n",
      "train loss on epoch 510 : 0.374\n",
      "train accuracy on epoch 510: 0.889\n",
      "test loss on epoch 510: 0.322\n",
      "test accuracy on epoch 510: 0.769\n",
      "train loss on epoch 511 : 0.253\n",
      "train accuracy on epoch 511: 0.889\n",
      "test loss on epoch 511: 0.323\n",
      "test accuracy on epoch 511: 0.769\n",
      "train loss on epoch 512 : 0.244\n",
      "train accuracy on epoch 512: 0.944\n",
      "test loss on epoch 512: 0.317\n",
      "test accuracy on epoch 512: 0.769\n",
      "train loss on epoch 513 : 0.078\n",
      "train accuracy on epoch 513: 1.000\n",
      "test loss on epoch 513: 0.315\n",
      "test accuracy on epoch 513: 0.769\n",
      "train loss on epoch 514 : 0.034\n",
      "train accuracy on epoch 514: 1.000\n",
      "test loss on epoch 514: 0.322\n",
      "test accuracy on epoch 514: 0.769\n",
      "train loss on epoch 515 : 0.115\n",
      "train accuracy on epoch 515: 1.000\n",
      "test loss on epoch 515: 0.322\n",
      "test accuracy on epoch 515: 0.692\n",
      "train loss on epoch 516 : 0.239\n",
      "train accuracy on epoch 516: 0.889\n",
      "test loss on epoch 516: 0.321\n",
      "test accuracy on epoch 516: 0.769\n",
      "train loss on epoch 517 : 0.432\n",
      "train accuracy on epoch 517: 0.889\n",
      "test loss on epoch 517: 0.318\n",
      "test accuracy on epoch 517: 0.769\n",
      "train loss on epoch 518 : 0.110\n",
      "train accuracy on epoch 518: 0.944\n",
      "test loss on epoch 518: 0.322\n",
      "test accuracy on epoch 518: 0.769\n",
      "train loss on epoch 519 : 0.205\n",
      "train accuracy on epoch 519: 0.889\n",
      "test loss on epoch 519: 0.322\n",
      "test accuracy on epoch 519: 0.769\n",
      "train loss on epoch 520 : 0.244\n",
      "train accuracy on epoch 520: 0.889\n",
      "test loss on epoch 520: 0.324\n",
      "test accuracy on epoch 520: 0.769\n",
      "train loss on epoch 521 : 0.121\n",
      "train accuracy on epoch 521: 0.944\n",
      "test loss on epoch 521: 0.332\n",
      "test accuracy on epoch 521: 0.769\n",
      "train loss on epoch 522 : 0.067\n",
      "train accuracy on epoch 522: 0.944\n",
      "test loss on epoch 522: 0.332\n",
      "test accuracy on epoch 522: 0.769\n",
      "train loss on epoch 523 : 0.260\n",
      "train accuracy on epoch 523: 0.944\n",
      "test loss on epoch 523: 0.325\n",
      "test accuracy on epoch 523: 0.769\n",
      "train loss on epoch 524 : 0.065\n",
      "train accuracy on epoch 524: 1.000\n",
      "test loss on epoch 524: 0.322\n",
      "test accuracy on epoch 524: 0.769\n",
      "train loss on epoch 525 : 0.232\n",
      "train accuracy on epoch 525: 0.944\n",
      "test loss on epoch 525: 0.319\n",
      "test accuracy on epoch 525: 0.769\n",
      "train loss on epoch 526 : 0.138\n",
      "train accuracy on epoch 526: 0.889\n",
      "test loss on epoch 526: 0.314\n",
      "test accuracy on epoch 526: 0.846\n",
      "train loss on epoch 527 : 0.115\n",
      "train accuracy on epoch 527: 0.944\n",
      "test loss on epoch 527: 0.317\n",
      "test accuracy on epoch 527: 0.769\n",
      "train loss on epoch 528 : 0.232\n",
      "train accuracy on epoch 528: 0.889\n",
      "test loss on epoch 528: 0.311\n",
      "test accuracy on epoch 528: 0.769\n",
      "train loss on epoch 529 : 0.084\n",
      "train accuracy on epoch 529: 0.944\n",
      "test loss on epoch 529: 0.317\n",
      "test accuracy on epoch 529: 0.769\n",
      "train loss on epoch 530 : 0.178\n",
      "train accuracy on epoch 530: 0.833\n",
      "test loss on epoch 530: 0.315\n",
      "test accuracy on epoch 530: 0.769\n",
      "train loss on epoch 531 : 0.205\n",
      "train accuracy on epoch 531: 0.889\n",
      "test loss on epoch 531: 0.331\n",
      "test accuracy on epoch 531: 0.769\n",
      "train loss on epoch 532 : 0.165\n",
      "train accuracy on epoch 532: 0.944\n",
      "test loss on epoch 532: 0.322\n",
      "test accuracy on epoch 532: 0.769\n",
      "train loss on epoch 533 : 0.151\n",
      "train accuracy on epoch 533: 0.889\n",
      "test loss on epoch 533: 0.323\n",
      "test accuracy on epoch 533: 0.769\n",
      "train loss on epoch 534 : 0.136\n",
      "train accuracy on epoch 534: 0.944\n",
      "test loss on epoch 534: 0.330\n",
      "test accuracy on epoch 534: 0.769\n",
      "train loss on epoch 535 : 0.265\n",
      "train accuracy on epoch 535: 0.889\n",
      "test loss on epoch 535: 0.321\n",
      "test accuracy on epoch 535: 0.769\n",
      "train loss on epoch 536 : 0.294\n",
      "train accuracy on epoch 536: 0.889\n",
      "test loss on epoch 536: 0.314\n",
      "test accuracy on epoch 536: 0.769\n",
      "train loss on epoch 537 : 0.111\n",
      "train accuracy on epoch 537: 0.944\n",
      "test loss on epoch 537: 0.317\n",
      "test accuracy on epoch 537: 0.769\n",
      "train loss on epoch 538 : 0.282\n",
      "train accuracy on epoch 538: 0.889\n",
      "test loss on epoch 538: 0.327\n",
      "test accuracy on epoch 538: 0.769\n",
      "train loss on epoch 539 : 0.194\n",
      "train accuracy on epoch 539: 0.889\n",
      "test loss on epoch 539: 0.319\n",
      "test accuracy on epoch 539: 0.769\n",
      "train loss on epoch 540 : 0.389\n",
      "train accuracy on epoch 540: 0.889\n",
      "test loss on epoch 540: 0.327\n",
      "test accuracy on epoch 540: 0.769\n",
      "train loss on epoch 541 : 0.046\n",
      "train accuracy on epoch 541: 1.000\n",
      "test loss on epoch 541: 0.330\n",
      "test accuracy on epoch 541: 0.769\n",
      "train loss on epoch 542 : 0.108\n",
      "train accuracy on epoch 542: 1.000\n",
      "test loss on epoch 542: 0.311\n",
      "test accuracy on epoch 542: 0.769\n",
      "train loss on epoch 543 : 0.103\n",
      "train accuracy on epoch 543: 1.000\n",
      "test loss on epoch 543: 0.334\n",
      "test accuracy on epoch 543: 0.769\n",
      "train loss on epoch 544 : 0.116\n",
      "train accuracy on epoch 544: 1.000\n",
      "test loss on epoch 544: 0.336\n",
      "test accuracy on epoch 544: 0.769\n",
      "train loss on epoch 545 : 0.304\n",
      "train accuracy on epoch 545: 0.889\n",
      "test loss on epoch 545: 0.337\n",
      "test accuracy on epoch 545: 0.769\n",
      "train loss on epoch 546 : 0.205\n",
      "train accuracy on epoch 546: 0.944\n",
      "test loss on epoch 546: 0.319\n",
      "test accuracy on epoch 546: 0.769\n",
      "train loss on epoch 547 : 0.179\n",
      "train accuracy on epoch 547: 0.944\n",
      "test loss on epoch 547: 0.320\n",
      "test accuracy on epoch 547: 0.769\n",
      "train loss on epoch 548 : 0.231\n",
      "train accuracy on epoch 548: 0.889\n",
      "test loss on epoch 548: 0.353\n",
      "test accuracy on epoch 548: 0.769\n",
      "train loss on epoch 549 : 0.095\n",
      "train accuracy on epoch 549: 1.000\n",
      "test loss on epoch 549: 0.325\n",
      "test accuracy on epoch 549: 0.769\n",
      "train loss on epoch 550 : 0.330\n",
      "train accuracy on epoch 550: 0.778\n",
      "test loss on epoch 550: 0.340\n",
      "test accuracy on epoch 550: 0.769\n",
      "train loss on epoch 551 : 0.376\n",
      "train accuracy on epoch 551: 0.833\n",
      "test loss on epoch 551: 0.331\n",
      "test accuracy on epoch 551: 0.769\n",
      "train loss on epoch 552 : 0.208\n",
      "train accuracy on epoch 552: 0.944\n",
      "test loss on epoch 552: 0.325\n",
      "test accuracy on epoch 552: 0.769\n",
      "train loss on epoch 553 : 0.150\n",
      "train accuracy on epoch 553: 0.944\n",
      "test loss on epoch 553: 0.333\n",
      "test accuracy on epoch 553: 0.692\n",
      "train loss on epoch 554 : 0.240\n",
      "train accuracy on epoch 554: 0.889\n",
      "test loss on epoch 554: 0.320\n",
      "test accuracy on epoch 554: 0.769\n",
      "train loss on epoch 555 : 0.322\n",
      "train accuracy on epoch 555: 0.889\n",
      "test loss on epoch 555: 0.324\n",
      "test accuracy on epoch 555: 0.769\n",
      "train loss on epoch 556 : 0.078\n",
      "train accuracy on epoch 556: 0.944\n",
      "test loss on epoch 556: 0.314\n",
      "test accuracy on epoch 556: 0.846\n",
      "train loss on epoch 557 : 0.042\n",
      "train accuracy on epoch 557: 1.000\n",
      "test loss on epoch 557: 0.331\n",
      "test accuracy on epoch 557: 0.692\n",
      "train loss on epoch 558 : 0.178\n",
      "train accuracy on epoch 558: 0.944\n",
      "test loss on epoch 558: 0.322\n",
      "test accuracy on epoch 558: 0.769\n",
      "train loss on epoch 559 : 0.039\n",
      "train accuracy on epoch 559: 1.000\n",
      "test loss on epoch 559: 0.318\n",
      "test accuracy on epoch 559: 0.846\n",
      "train loss on epoch 560 : 0.125\n",
      "train accuracy on epoch 560: 0.944\n",
      "test loss on epoch 560: 0.320\n",
      "test accuracy on epoch 560: 0.769\n",
      "train loss on epoch 561 : 0.421\n",
      "train accuracy on epoch 561: 0.889\n",
      "test loss on epoch 561: 0.323\n",
      "test accuracy on epoch 561: 0.769\n",
      "train loss on epoch 562 : 0.347\n",
      "train accuracy on epoch 562: 0.889\n",
      "test loss on epoch 562: 0.329\n",
      "test accuracy on epoch 562: 0.692\n",
      "train loss on epoch 563 : 0.107\n",
      "train accuracy on epoch 563: 0.944\n",
      "test loss on epoch 563: 0.327\n",
      "test accuracy on epoch 563: 0.769\n",
      "train loss on epoch 564 : 0.209\n",
      "train accuracy on epoch 564: 0.889\n",
      "test loss on epoch 564: 0.330\n",
      "test accuracy on epoch 564: 0.769\n",
      "train loss on epoch 565 : 0.169\n",
      "train accuracy on epoch 565: 0.889\n",
      "test loss on epoch 565: 0.316\n",
      "test accuracy on epoch 565: 0.769\n",
      "train loss on epoch 566 : 0.216\n",
      "train accuracy on epoch 566: 0.889\n",
      "test loss on epoch 566: 0.327\n",
      "test accuracy on epoch 566: 0.769\n",
      "train loss on epoch 567 : 0.226\n",
      "train accuracy on epoch 567: 0.889\n",
      "test loss on epoch 567: 0.317\n",
      "test accuracy on epoch 567: 0.769\n",
      "train loss on epoch 568 : 0.062\n",
      "train accuracy on epoch 568: 1.000\n",
      "test loss on epoch 568: 0.321\n",
      "test accuracy on epoch 568: 0.769\n",
      "train loss on epoch 569 : 0.033\n",
      "train accuracy on epoch 569: 1.000\n",
      "test loss on epoch 569: 0.326\n",
      "test accuracy on epoch 569: 0.769\n",
      "train loss on epoch 570 : 0.218\n",
      "train accuracy on epoch 570: 0.889\n",
      "test loss on epoch 570: 0.330\n",
      "test accuracy on epoch 570: 0.769\n",
      "train loss on epoch 571 : 0.271\n",
      "train accuracy on epoch 571: 0.889\n",
      "test loss on epoch 571: 0.329\n",
      "test accuracy on epoch 571: 0.769\n",
      "train loss on epoch 572 : 0.328\n",
      "train accuracy on epoch 572: 0.889\n",
      "test loss on epoch 572: 0.325\n",
      "test accuracy on epoch 572: 0.769\n",
      "train loss on epoch 573 : 0.251\n",
      "train accuracy on epoch 573: 0.833\n",
      "test loss on epoch 573: 0.331\n",
      "test accuracy on epoch 573: 0.769\n",
      "train loss on epoch 574 : 0.239\n",
      "train accuracy on epoch 574: 0.889\n",
      "test loss on epoch 574: 0.324\n",
      "test accuracy on epoch 574: 0.769\n",
      "train loss on epoch 575 : 0.080\n",
      "train accuracy on epoch 575: 0.944\n",
      "test loss on epoch 575: 0.328\n",
      "test accuracy on epoch 575: 0.769\n",
      "train loss on epoch 576 : 0.202\n",
      "train accuracy on epoch 576: 0.889\n",
      "test loss on epoch 576: 0.331\n",
      "test accuracy on epoch 576: 0.769\n",
      "train loss on epoch 577 : 0.166\n",
      "train accuracy on epoch 577: 0.944\n",
      "test loss on epoch 577: 0.321\n",
      "test accuracy on epoch 577: 0.769\n",
      "train loss on epoch 578 : 0.122\n",
      "train accuracy on epoch 578: 0.944\n",
      "test loss on epoch 578: 0.331\n",
      "test accuracy on epoch 578: 0.769\n",
      "train loss on epoch 579 : 0.127\n",
      "train accuracy on epoch 579: 0.944\n",
      "test loss on epoch 579: 0.334\n",
      "test accuracy on epoch 579: 0.769\n",
      "train loss on epoch 580 : 0.083\n",
      "train accuracy on epoch 580: 0.944\n",
      "test loss on epoch 580: 0.325\n",
      "test accuracy on epoch 580: 0.769\n",
      "train loss on epoch 581 : 0.338\n",
      "train accuracy on epoch 581: 0.944\n",
      "test loss on epoch 581: 0.332\n",
      "test accuracy on epoch 581: 0.769\n",
      "train loss on epoch 582 : 0.095\n",
      "train accuracy on epoch 582: 0.944\n",
      "test loss on epoch 582: 0.327\n",
      "test accuracy on epoch 582: 0.769\n",
      "train loss on epoch 583 : 0.229\n",
      "train accuracy on epoch 583: 0.944\n",
      "test loss on epoch 583: 0.337\n",
      "test accuracy on epoch 583: 0.769\n",
      "train loss on epoch 584 : 0.192\n",
      "train accuracy on epoch 584: 0.944\n",
      "test loss on epoch 584: 0.335\n",
      "test accuracy on epoch 584: 0.769\n",
      "train loss on epoch 585 : 0.136\n",
      "train accuracy on epoch 585: 0.944\n",
      "test loss on epoch 585: 0.341\n",
      "test accuracy on epoch 585: 0.769\n",
      "train loss on epoch 586 : 0.276\n",
      "train accuracy on epoch 586: 0.944\n",
      "test loss on epoch 586: 0.337\n",
      "test accuracy on epoch 586: 0.769\n",
      "train loss on epoch 587 : 0.168\n",
      "train accuracy on epoch 587: 0.944\n",
      "test loss on epoch 587: 0.328\n",
      "test accuracy on epoch 587: 0.769\n",
      "train loss on epoch 588 : 0.114\n",
      "train accuracy on epoch 588: 0.944\n",
      "test loss on epoch 588: 0.328\n",
      "test accuracy on epoch 588: 0.769\n",
      "train loss on epoch 589 : 0.098\n",
      "train accuracy on epoch 589: 1.000\n",
      "test loss on epoch 589: 0.326\n",
      "test accuracy on epoch 589: 0.769\n",
      "train loss on epoch 590 : 0.173\n",
      "train accuracy on epoch 590: 0.944\n",
      "test loss on epoch 590: 0.329\n",
      "test accuracy on epoch 590: 0.769\n",
      "train loss on epoch 591 : 0.122\n",
      "train accuracy on epoch 591: 0.889\n",
      "test loss on epoch 591: 0.315\n",
      "test accuracy on epoch 591: 0.769\n",
      "train loss on epoch 592 : 0.099\n",
      "train accuracy on epoch 592: 1.000\n",
      "test loss on epoch 592: 0.322\n",
      "test accuracy on epoch 592: 0.769\n",
      "train loss on epoch 593 : 0.165\n",
      "train accuracy on epoch 593: 0.944\n",
      "test loss on epoch 593: 0.316\n",
      "test accuracy on epoch 593: 0.769\n",
      "train loss on epoch 594 : 0.133\n",
      "train accuracy on epoch 594: 0.944\n",
      "test loss on epoch 594: 0.331\n",
      "test accuracy on epoch 594: 0.769\n",
      "train loss on epoch 595 : 0.171\n",
      "train accuracy on epoch 595: 0.889\n",
      "test loss on epoch 595: 0.332\n",
      "test accuracy on epoch 595: 0.769\n",
      "train loss on epoch 596 : 0.133\n",
      "train accuracy on epoch 596: 0.944\n",
      "test loss on epoch 596: 0.322\n",
      "test accuracy on epoch 596: 0.769\n",
      "train loss on epoch 597 : 0.057\n",
      "train accuracy on epoch 597: 1.000\n",
      "test loss on epoch 597: 0.336\n",
      "test accuracy on epoch 597: 0.769\n",
      "train loss on epoch 598 : 0.416\n",
      "train accuracy on epoch 598: 0.778\n",
      "test loss on epoch 598: 0.333\n",
      "test accuracy on epoch 598: 0.769\n",
      "train loss on epoch 599 : 0.102\n",
      "train accuracy on epoch 599: 1.000\n",
      "test loss on epoch 599: 0.335\n",
      "test accuracy on epoch 599: 0.769\n",
      "train loss on epoch 600 : 0.304\n",
      "train accuracy on epoch 600: 0.889\n",
      "test loss on epoch 600: 0.340\n",
      "test accuracy on epoch 600: 0.769\n",
      "train loss on epoch 601 : 0.091\n",
      "train accuracy on epoch 601: 1.000\n",
      "test loss on epoch 601: 0.345\n",
      "test accuracy on epoch 601: 0.769\n",
      "train loss on epoch 602 : 0.131\n",
      "train accuracy on epoch 602: 0.944\n",
      "test loss on epoch 602: 0.322\n",
      "test accuracy on epoch 602: 0.769\n",
      "train loss on epoch 603 : 0.269\n",
      "train accuracy on epoch 603: 0.833\n",
      "test loss on epoch 603: 0.328\n",
      "test accuracy on epoch 603: 0.769\n",
      "train loss on epoch 604 : 0.170\n",
      "train accuracy on epoch 604: 0.944\n",
      "test loss on epoch 604: 0.350\n",
      "test accuracy on epoch 604: 0.769\n",
      "train loss on epoch 605 : 0.108\n",
      "train accuracy on epoch 605: 0.944\n",
      "test loss on epoch 605: 0.338\n",
      "test accuracy on epoch 605: 0.769\n",
      "train loss on epoch 606 : 0.537\n",
      "train accuracy on epoch 606: 0.889\n",
      "test loss on epoch 606: 0.339\n",
      "test accuracy on epoch 606: 0.769\n",
      "train loss on epoch 607 : 0.358\n",
      "train accuracy on epoch 607: 0.889\n",
      "test loss on epoch 607: 0.354\n",
      "test accuracy on epoch 607: 0.769\n",
      "train loss on epoch 608 : 0.039\n",
      "train accuracy on epoch 608: 1.000\n",
      "test loss on epoch 608: 0.347\n",
      "test accuracy on epoch 608: 0.769\n",
      "train loss on epoch 609 : 0.234\n",
      "train accuracy on epoch 609: 0.889\n",
      "test loss on epoch 609: 0.318\n",
      "test accuracy on epoch 609: 0.769\n",
      "train loss on epoch 610 : 0.106\n",
      "train accuracy on epoch 610: 1.000\n",
      "test loss on epoch 610: 0.340\n",
      "test accuracy on epoch 610: 0.769\n",
      "train loss on epoch 611 : 0.486\n",
      "train accuracy on epoch 611: 0.833\n",
      "test loss on epoch 611: 0.327\n",
      "test accuracy on epoch 611: 0.769\n",
      "train loss on epoch 612 : 0.234\n",
      "train accuracy on epoch 612: 0.889\n",
      "test loss on epoch 612: 0.338\n",
      "test accuracy on epoch 612: 0.692\n",
      "train loss on epoch 613 : 0.121\n",
      "train accuracy on epoch 613: 0.944\n",
      "test loss on epoch 613: 0.323\n",
      "test accuracy on epoch 613: 0.846\n",
      "train loss on epoch 614 : 0.079\n",
      "train accuracy on epoch 614: 1.000\n",
      "test loss on epoch 614: 0.326\n",
      "test accuracy on epoch 614: 0.846\n",
      "train loss on epoch 615 : 0.212\n",
      "train accuracy on epoch 615: 0.889\n",
      "test loss on epoch 615: 0.339\n",
      "test accuracy on epoch 615: 0.769\n",
      "train loss on epoch 616 : 0.229\n",
      "train accuracy on epoch 616: 0.944\n",
      "test loss on epoch 616: 0.320\n",
      "test accuracy on epoch 616: 0.769\n",
      "train loss on epoch 617 : 0.065\n",
      "train accuracy on epoch 617: 1.000\n",
      "test loss on epoch 617: 0.320\n",
      "test accuracy on epoch 617: 0.769\n",
      "train loss on epoch 618 : 0.378\n",
      "train accuracy on epoch 618: 0.833\n",
      "test loss on epoch 618: 0.324\n",
      "test accuracy on epoch 618: 0.769\n",
      "train loss on epoch 619 : 0.161\n",
      "train accuracy on epoch 619: 0.889\n",
      "test loss on epoch 619: 0.320\n",
      "test accuracy on epoch 619: 0.769\n",
      "train loss on epoch 620 : 0.176\n",
      "train accuracy on epoch 620: 0.889\n",
      "test loss on epoch 620: 0.329\n",
      "test accuracy on epoch 620: 0.769\n",
      "train loss on epoch 621 : 0.168\n",
      "train accuracy on epoch 621: 0.944\n",
      "test loss on epoch 621: 0.340\n",
      "test accuracy on epoch 621: 0.769\n",
      "train loss on epoch 622 : 0.120\n",
      "train accuracy on epoch 622: 0.944\n",
      "test loss on epoch 622: 0.334\n",
      "test accuracy on epoch 622: 0.769\n",
      "train loss on epoch 623 : 0.120\n",
      "train accuracy on epoch 623: 1.000\n",
      "test loss on epoch 623: 0.336\n",
      "test accuracy on epoch 623: 0.769\n",
      "train loss on epoch 624 : 0.221\n",
      "train accuracy on epoch 624: 0.889\n",
      "test loss on epoch 624: 0.319\n",
      "test accuracy on epoch 624: 0.769\n",
      "train loss on epoch 625 : 0.165\n",
      "train accuracy on epoch 625: 0.889\n",
      "test loss on epoch 625: 0.316\n",
      "test accuracy on epoch 625: 0.769\n",
      "train loss on epoch 626 : 0.188\n",
      "train accuracy on epoch 626: 0.944\n",
      "test loss on epoch 626: 0.330\n",
      "test accuracy on epoch 626: 0.769\n",
      "train loss on epoch 627 : 0.101\n",
      "train accuracy on epoch 627: 0.944\n",
      "test loss on epoch 627: 0.336\n",
      "test accuracy on epoch 627: 0.769\n",
      "train loss on epoch 628 : 0.214\n",
      "train accuracy on epoch 628: 0.889\n",
      "test loss on epoch 628: 0.332\n",
      "test accuracy on epoch 628: 0.769\n",
      "train loss on epoch 629 : 0.353\n",
      "train accuracy on epoch 629: 0.889\n",
      "test loss on epoch 629: 0.329\n",
      "test accuracy on epoch 629: 0.769\n",
      "train loss on epoch 630 : 0.372\n",
      "train accuracy on epoch 630: 0.833\n",
      "test loss on epoch 630: 0.321\n",
      "test accuracy on epoch 630: 0.769\n",
      "train loss on epoch 631 : 0.105\n",
      "train accuracy on epoch 631: 0.944\n",
      "test loss on epoch 631: 0.329\n",
      "test accuracy on epoch 631: 0.769\n",
      "train loss on epoch 632 : 0.347\n",
      "train accuracy on epoch 632: 0.833\n",
      "test loss on epoch 632: 0.330\n",
      "test accuracy on epoch 632: 0.769\n",
      "train loss on epoch 633 : 0.219\n",
      "train accuracy on epoch 633: 0.944\n",
      "test loss on epoch 633: 0.325\n",
      "test accuracy on epoch 633: 0.769\n",
      "train loss on epoch 634 : 0.135\n",
      "train accuracy on epoch 634: 0.944\n",
      "test loss on epoch 634: 0.310\n",
      "test accuracy on epoch 634: 0.769\n",
      "train loss on epoch 635 : 0.145\n",
      "train accuracy on epoch 635: 0.889\n",
      "test loss on epoch 635: 0.323\n",
      "test accuracy on epoch 635: 0.769\n",
      "train loss on epoch 636 : 0.187\n",
      "train accuracy on epoch 636: 0.833\n",
      "test loss on epoch 636: 0.329\n",
      "test accuracy on epoch 636: 0.769\n",
      "train loss on epoch 637 : 0.120\n",
      "train accuracy on epoch 637: 0.944\n",
      "test loss on epoch 637: 0.330\n",
      "test accuracy on epoch 637: 0.692\n",
      "train loss on epoch 638 : 0.304\n",
      "train accuracy on epoch 638: 0.889\n",
      "test loss on epoch 638: 0.328\n",
      "test accuracy on epoch 638: 0.692\n",
      "train loss on epoch 639 : 0.256\n",
      "train accuracy on epoch 639: 0.889\n",
      "test loss on epoch 639: 0.314\n",
      "test accuracy on epoch 639: 0.846\n",
      "train loss on epoch 640 : 0.091\n",
      "train accuracy on epoch 640: 1.000\n",
      "test loss on epoch 640: 0.319\n",
      "test accuracy on epoch 640: 0.769\n",
      "train loss on epoch 641 : 0.282\n",
      "train accuracy on epoch 641: 0.944\n",
      "test loss on epoch 641: 0.324\n",
      "test accuracy on epoch 641: 0.769\n",
      "train loss on epoch 642 : 0.231\n",
      "train accuracy on epoch 642: 0.889\n",
      "test loss on epoch 642: 0.331\n",
      "test accuracy on epoch 642: 0.769\n",
      "train loss on epoch 643 : 0.065\n",
      "train accuracy on epoch 643: 1.000\n",
      "test loss on epoch 643: 0.314\n",
      "test accuracy on epoch 643: 0.846\n",
      "train loss on epoch 644 : 0.096\n",
      "train accuracy on epoch 644: 1.000\n",
      "test loss on epoch 644: 0.331\n",
      "test accuracy on epoch 644: 0.692\n",
      "train loss on epoch 645 : 0.360\n",
      "train accuracy on epoch 645: 0.889\n",
      "test loss on epoch 645: 0.331\n",
      "test accuracy on epoch 645: 0.692\n",
      "train loss on epoch 646 : 0.053\n",
      "train accuracy on epoch 646: 1.000\n",
      "test loss on epoch 646: 0.322\n",
      "test accuracy on epoch 646: 0.769\n",
      "train loss on epoch 647 : 0.190\n",
      "train accuracy on epoch 647: 0.889\n",
      "test loss on epoch 647: 0.330\n",
      "test accuracy on epoch 647: 0.692\n",
      "train loss on epoch 648 : 0.075\n",
      "train accuracy on epoch 648: 1.000\n",
      "test loss on epoch 648: 0.328\n",
      "test accuracy on epoch 648: 0.692\n",
      "train loss on epoch 649 : 0.266\n",
      "train accuracy on epoch 649: 0.889\n",
      "test loss on epoch 649: 0.316\n",
      "test accuracy on epoch 649: 0.769\n",
      "train loss on epoch 650 : 0.066\n",
      "train accuracy on epoch 650: 1.000\n",
      "test loss on epoch 650: 0.325\n",
      "test accuracy on epoch 650: 0.769\n",
      "train loss on epoch 651 : 0.111\n",
      "train accuracy on epoch 651: 0.944\n",
      "test loss on epoch 651: 0.323\n",
      "test accuracy on epoch 651: 0.769\n",
      "train loss on epoch 652 : 0.080\n",
      "train accuracy on epoch 652: 1.000\n",
      "test loss on epoch 652: 0.322\n",
      "test accuracy on epoch 652: 0.769\n",
      "train loss on epoch 653 : 0.124\n",
      "train accuracy on epoch 653: 0.944\n",
      "test loss on epoch 653: 0.325\n",
      "test accuracy on epoch 653: 0.769\n",
      "train loss on epoch 654 : 0.360\n",
      "train accuracy on epoch 654: 0.889\n",
      "test loss on epoch 654: 0.331\n",
      "test accuracy on epoch 654: 0.692\n",
      "train loss on epoch 655 : 0.096\n",
      "train accuracy on epoch 655: 1.000\n",
      "test loss on epoch 655: 0.324\n",
      "test accuracy on epoch 655: 0.769\n",
      "train loss on epoch 656 : 0.097\n",
      "train accuracy on epoch 656: 0.944\n",
      "test loss on epoch 656: 0.315\n",
      "test accuracy on epoch 656: 0.846\n",
      "train loss on epoch 657 : 0.376\n",
      "train accuracy on epoch 657: 0.778\n",
      "test loss on epoch 657: 0.325\n",
      "test accuracy on epoch 657: 0.769\n",
      "train loss on epoch 658 : 0.223\n",
      "train accuracy on epoch 658: 0.944\n",
      "test loss on epoch 658: 0.322\n",
      "test accuracy on epoch 658: 0.769\n",
      "train loss on epoch 659 : 0.217\n",
      "train accuracy on epoch 659: 0.944\n",
      "test loss on epoch 659: 0.330\n",
      "test accuracy on epoch 659: 0.769\n",
      "train loss on epoch 660 : 0.067\n",
      "train accuracy on epoch 660: 1.000\n",
      "test loss on epoch 660: 0.318\n",
      "test accuracy on epoch 660: 0.769\n",
      "train loss on epoch 661 : 0.206\n",
      "train accuracy on epoch 661: 0.944\n",
      "test loss on epoch 661: 0.315\n",
      "test accuracy on epoch 661: 0.769\n",
      "train loss on epoch 662 : 0.101\n",
      "train accuracy on epoch 662: 0.944\n",
      "test loss on epoch 662: 0.314\n",
      "test accuracy on epoch 662: 0.769\n",
      "train loss on epoch 663 : 0.160\n",
      "train accuracy on epoch 663: 0.944\n",
      "test loss on epoch 663: 0.318\n",
      "test accuracy on epoch 663: 0.769\n",
      "train loss on epoch 664 : 0.126\n",
      "train accuracy on epoch 664: 0.944\n",
      "test loss on epoch 664: 0.326\n",
      "test accuracy on epoch 664: 0.769\n",
      "train loss on epoch 665 : 0.086\n",
      "train accuracy on epoch 665: 1.000\n",
      "test loss on epoch 665: 0.322\n",
      "test accuracy on epoch 665: 0.769\n",
      "train loss on epoch 666 : 0.101\n",
      "train accuracy on epoch 666: 0.944\n",
      "test loss on epoch 666: 0.308\n",
      "test accuracy on epoch 666: 0.769\n",
      "train loss on epoch 667 : 0.186\n",
      "train accuracy on epoch 667: 0.889\n",
      "test loss on epoch 667: 0.316\n",
      "test accuracy on epoch 667: 0.769\n",
      "train loss on epoch 668 : 0.047\n",
      "train accuracy on epoch 668: 1.000\n",
      "test loss on epoch 668: 0.326\n",
      "test accuracy on epoch 668: 0.769\n",
      "train loss on epoch 669 : 0.067\n",
      "train accuracy on epoch 669: 1.000\n",
      "test loss on epoch 669: 0.315\n",
      "test accuracy on epoch 669: 0.769\n",
      "train loss on epoch 670 : 0.081\n",
      "train accuracy on epoch 670: 0.944\n",
      "test loss on epoch 670: 0.315\n",
      "test accuracy on epoch 670: 0.769\n",
      "train loss on epoch 671 : 0.104\n",
      "train accuracy on epoch 671: 1.000\n",
      "test loss on epoch 671: 0.311\n",
      "test accuracy on epoch 671: 0.769\n",
      "train loss on epoch 672 : 0.330\n",
      "train accuracy on epoch 672: 0.889\n",
      "test loss on epoch 672: 0.315\n",
      "test accuracy on epoch 672: 0.769\n",
      "train loss on epoch 673 : 0.142\n",
      "train accuracy on epoch 673: 0.889\n",
      "test loss on epoch 673: 0.320\n",
      "test accuracy on epoch 673: 0.769\n",
      "train loss on epoch 674 : 0.313\n",
      "train accuracy on epoch 674: 0.889\n",
      "test loss on epoch 674: 0.313\n",
      "test accuracy on epoch 674: 0.846\n",
      "train loss on epoch 675 : 0.075\n",
      "train accuracy on epoch 675: 1.000\n",
      "test loss on epoch 675: 0.315\n",
      "test accuracy on epoch 675: 0.846\n",
      "train loss on epoch 676 : 0.099\n",
      "train accuracy on epoch 676: 1.000\n",
      "test loss on epoch 676: 0.314\n",
      "test accuracy on epoch 676: 0.769\n",
      "train loss on epoch 677 : 0.093\n",
      "train accuracy on epoch 677: 1.000\n",
      "test loss on epoch 677: 0.313\n",
      "test accuracy on epoch 677: 0.769\n",
      "train loss on epoch 678 : 0.163\n",
      "train accuracy on epoch 678: 0.944\n",
      "test loss on epoch 678: 0.320\n",
      "test accuracy on epoch 678: 0.769\n",
      "train loss on epoch 679 : 0.273\n",
      "train accuracy on epoch 679: 0.889\n",
      "test loss on epoch 679: 0.312\n",
      "test accuracy on epoch 679: 0.769\n",
      "train loss on epoch 680 : 0.165\n",
      "train accuracy on epoch 680: 0.944\n",
      "test loss on epoch 680: 0.311\n",
      "test accuracy on epoch 680: 0.769\n",
      "train loss on epoch 681 : 0.255\n",
      "train accuracy on epoch 681: 0.944\n",
      "test loss on epoch 681: 0.315\n",
      "test accuracy on epoch 681: 0.769\n",
      "train loss on epoch 682 : 0.321\n",
      "train accuracy on epoch 682: 0.889\n",
      "test loss on epoch 682: 0.315\n",
      "test accuracy on epoch 682: 0.769\n",
      "train loss on epoch 683 : 0.114\n",
      "train accuracy on epoch 683: 0.944\n",
      "test loss on epoch 683: 0.326\n",
      "test accuracy on epoch 683: 0.769\n",
      "train loss on epoch 684 : 0.050\n",
      "train accuracy on epoch 684: 1.000\n",
      "test loss on epoch 684: 0.312\n",
      "test accuracy on epoch 684: 0.846\n",
      "train loss on epoch 685 : 0.342\n",
      "train accuracy on epoch 685: 0.889\n",
      "test loss on epoch 685: 0.324\n",
      "test accuracy on epoch 685: 0.692\n",
      "train loss on epoch 686 : 0.233\n",
      "train accuracy on epoch 686: 0.889\n",
      "test loss on epoch 686: 0.317\n",
      "test accuracy on epoch 686: 0.769\n",
      "train loss on epoch 687 : 0.221\n",
      "train accuracy on epoch 687: 0.889\n",
      "test loss on epoch 687: 0.322\n",
      "test accuracy on epoch 687: 0.769\n",
      "train loss on epoch 688 : 0.125\n",
      "train accuracy on epoch 688: 0.889\n",
      "test loss on epoch 688: 0.321\n",
      "test accuracy on epoch 688: 0.769\n",
      "train loss on epoch 689 : 0.095\n",
      "train accuracy on epoch 689: 0.944\n",
      "test loss on epoch 689: 0.327\n",
      "test accuracy on epoch 689: 0.769\n",
      "train loss on epoch 690 : 0.181\n",
      "train accuracy on epoch 690: 0.944\n",
      "test loss on epoch 690: 0.335\n",
      "test accuracy on epoch 690: 0.769\n",
      "train loss on epoch 691 : 0.097\n",
      "train accuracy on epoch 691: 1.000\n",
      "test loss on epoch 691: 0.331\n",
      "test accuracy on epoch 691: 0.769\n",
      "train loss on epoch 692 : 0.356\n",
      "train accuracy on epoch 692: 0.889\n",
      "test loss on epoch 692: 0.334\n",
      "test accuracy on epoch 692: 0.769\n",
      "train loss on epoch 693 : 0.228\n",
      "train accuracy on epoch 693: 0.889\n",
      "test loss on epoch 693: 0.323\n",
      "test accuracy on epoch 693: 0.769\n",
      "train loss on epoch 694 : 0.162\n",
      "train accuracy on epoch 694: 0.889\n",
      "test loss on epoch 694: 0.331\n",
      "test accuracy on epoch 694: 0.769\n",
      "train loss on epoch 695 : 0.027\n",
      "train accuracy on epoch 695: 1.000\n",
      "test loss on epoch 695: 0.338\n",
      "test accuracy on epoch 695: 0.769\n",
      "train loss on epoch 696 : 0.101\n",
      "train accuracy on epoch 696: 0.944\n",
      "test loss on epoch 696: 0.336\n",
      "test accuracy on epoch 696: 0.769\n",
      "train loss on epoch 697 : 0.175\n",
      "train accuracy on epoch 697: 0.944\n",
      "test loss on epoch 697: 0.342\n",
      "test accuracy on epoch 697: 0.769\n",
      "train loss on epoch 698 : 0.108\n",
      "train accuracy on epoch 698: 0.944\n",
      "test loss on epoch 698: 0.340\n",
      "test accuracy on epoch 698: 0.769\n",
      "train loss on epoch 699 : 0.285\n",
      "train accuracy on epoch 699: 0.833\n",
      "test loss on epoch 699: 0.342\n",
      "test accuracy on epoch 699: 0.769\n",
      "train loss on epoch 700 : 0.101\n",
      "train accuracy on epoch 700: 1.000\n",
      "test loss on epoch 700: 0.329\n",
      "test accuracy on epoch 700: 0.769\n",
      "train loss on epoch 701 : 0.161\n",
      "train accuracy on epoch 701: 0.944\n",
      "test loss on epoch 701: 0.324\n",
      "test accuracy on epoch 701: 0.769\n",
      "train loss on epoch 702 : 0.041\n",
      "train accuracy on epoch 702: 1.000\n",
      "test loss on epoch 702: 0.327\n",
      "test accuracy on epoch 702: 0.769\n",
      "train loss on epoch 703 : 0.225\n",
      "train accuracy on epoch 703: 0.889\n",
      "test loss on epoch 703: 0.313\n",
      "test accuracy on epoch 703: 0.769\n",
      "train loss on epoch 704 : 0.149\n",
      "train accuracy on epoch 704: 0.944\n",
      "test loss on epoch 704: 0.324\n",
      "test accuracy on epoch 704: 0.692\n",
      "train loss on epoch 705 : 0.139\n",
      "train accuracy on epoch 705: 0.944\n",
      "test loss on epoch 705: 0.312\n",
      "test accuracy on epoch 705: 0.846\n",
      "train loss on epoch 706 : 0.330\n",
      "train accuracy on epoch 706: 0.833\n",
      "test loss on epoch 706: 0.320\n",
      "test accuracy on epoch 706: 0.769\n",
      "train loss on epoch 707 : 0.032\n",
      "train accuracy on epoch 707: 1.000\n",
      "test loss on epoch 707: 0.313\n",
      "test accuracy on epoch 707: 0.846\n",
      "train loss on epoch 708 : 0.082\n",
      "train accuracy on epoch 708: 1.000\n",
      "test loss on epoch 708: 0.311\n",
      "test accuracy on epoch 708: 0.846\n",
      "train loss on epoch 709 : 0.211\n",
      "train accuracy on epoch 709: 0.889\n",
      "test loss on epoch 709: 0.329\n",
      "test accuracy on epoch 709: 0.769\n",
      "train loss on epoch 710 : 0.096\n",
      "train accuracy on epoch 710: 1.000\n",
      "test loss on epoch 710: 0.330\n",
      "test accuracy on epoch 710: 0.769\n",
      "train loss on epoch 711 : 0.101\n",
      "train accuracy on epoch 711: 0.944\n",
      "test loss on epoch 711: 0.332\n",
      "test accuracy on epoch 711: 0.692\n",
      "train loss on epoch 712 : 0.172\n",
      "train accuracy on epoch 712: 0.944\n",
      "test loss on epoch 712: 0.330\n",
      "test accuracy on epoch 712: 0.692\n",
      "train loss on epoch 713 : 0.094\n",
      "train accuracy on epoch 713: 0.944\n",
      "test loss on epoch 713: 0.317\n",
      "test accuracy on epoch 713: 0.769\n",
      "train loss on epoch 714 : 0.077\n",
      "train accuracy on epoch 714: 1.000\n",
      "test loss on epoch 714: 0.334\n",
      "test accuracy on epoch 714: 0.769\n",
      "train loss on epoch 715 : 0.200\n",
      "train accuracy on epoch 715: 0.944\n",
      "test loss on epoch 715: 0.333\n",
      "test accuracy on epoch 715: 0.769\n",
      "train loss on epoch 716 : 0.293\n",
      "train accuracy on epoch 716: 0.889\n",
      "test loss on epoch 716: 0.334\n",
      "test accuracy on epoch 716: 0.769\n",
      "train loss on epoch 717 : 0.233\n",
      "train accuracy on epoch 717: 0.833\n",
      "test loss on epoch 717: 0.340\n",
      "test accuracy on epoch 717: 0.769\n",
      "train loss on epoch 718 : 0.323\n",
      "train accuracy on epoch 718: 0.833\n",
      "test loss on epoch 718: 0.348\n",
      "test accuracy on epoch 718: 0.769\n",
      "train loss on epoch 719 : 0.290\n",
      "train accuracy on epoch 719: 0.833\n",
      "test loss on epoch 719: 0.335\n",
      "test accuracy on epoch 719: 0.769\n",
      "train loss on epoch 720 : 0.283\n",
      "train accuracy on epoch 720: 0.889\n",
      "test loss on epoch 720: 0.332\n",
      "test accuracy on epoch 720: 0.769\n",
      "train loss on epoch 721 : 0.090\n",
      "train accuracy on epoch 721: 0.944\n",
      "test loss on epoch 721: 0.331\n",
      "test accuracy on epoch 721: 0.769\n",
      "train loss on epoch 722 : 0.307\n",
      "train accuracy on epoch 722: 0.889\n",
      "test loss on epoch 722: 0.325\n",
      "test accuracy on epoch 722: 0.769\n",
      "train loss on epoch 723 : 0.123\n",
      "train accuracy on epoch 723: 0.944\n",
      "test loss on epoch 723: 0.330\n",
      "test accuracy on epoch 723: 0.769\n",
      "train loss on epoch 724 : 0.288\n",
      "train accuracy on epoch 724: 0.889\n",
      "test loss on epoch 724: 0.333\n",
      "test accuracy on epoch 724: 0.769\n",
      "train loss on epoch 725 : 0.396\n",
      "train accuracy on epoch 725: 0.833\n",
      "test loss on epoch 725: 0.330\n",
      "test accuracy on epoch 725: 0.769\n",
      "train loss on epoch 726 : 0.398\n",
      "train accuracy on epoch 726: 0.778\n",
      "test loss on epoch 726: 0.339\n",
      "test accuracy on epoch 726: 0.769\n",
      "train loss on epoch 727 : 0.246\n",
      "train accuracy on epoch 727: 0.944\n",
      "test loss on epoch 727: 0.335\n",
      "test accuracy on epoch 727: 0.769\n",
      "train loss on epoch 728 : 0.122\n",
      "train accuracy on epoch 728: 0.944\n",
      "test loss on epoch 728: 0.330\n",
      "test accuracy on epoch 728: 0.769\n",
      "train loss on epoch 729 : 0.355\n",
      "train accuracy on epoch 729: 0.944\n",
      "test loss on epoch 729: 0.322\n",
      "test accuracy on epoch 729: 0.846\n",
      "train loss on epoch 730 : 0.159\n",
      "train accuracy on epoch 730: 0.944\n",
      "test loss on epoch 730: 0.318\n",
      "test accuracy on epoch 730: 0.769\n",
      "train loss on epoch 731 : 0.338\n",
      "train accuracy on epoch 731: 0.944\n",
      "test loss on epoch 731: 0.320\n",
      "test accuracy on epoch 731: 0.846\n",
      "train loss on epoch 732 : 0.133\n",
      "train accuracy on epoch 732: 0.944\n",
      "test loss on epoch 732: 0.320\n",
      "test accuracy on epoch 732: 0.846\n",
      "train loss on epoch 733 : 0.154\n",
      "train accuracy on epoch 733: 0.889\n",
      "test loss on epoch 733: 0.328\n",
      "test accuracy on epoch 733: 0.769\n",
      "train loss on epoch 734 : 0.125\n",
      "train accuracy on epoch 734: 0.944\n",
      "test loss on epoch 734: 0.329\n",
      "test accuracy on epoch 734: 0.769\n",
      "train loss on epoch 735 : 0.282\n",
      "train accuracy on epoch 735: 0.889\n",
      "test loss on epoch 735: 0.320\n",
      "test accuracy on epoch 735: 0.846\n",
      "train loss on epoch 736 : 0.180\n",
      "train accuracy on epoch 736: 0.889\n",
      "test loss on epoch 736: 0.342\n",
      "test accuracy on epoch 736: 0.692\n",
      "train loss on epoch 737 : 0.149\n",
      "train accuracy on epoch 737: 0.944\n",
      "test loss on epoch 737: 0.331\n",
      "test accuracy on epoch 737: 0.769\n",
      "train loss on epoch 738 : 0.126\n",
      "train accuracy on epoch 738: 0.944\n",
      "test loss on epoch 738: 0.332\n",
      "test accuracy on epoch 738: 0.769\n",
      "train loss on epoch 739 : 0.172\n",
      "train accuracy on epoch 739: 0.944\n",
      "test loss on epoch 739: 0.340\n",
      "test accuracy on epoch 739: 0.692\n",
      "train loss on epoch 740 : 0.158\n",
      "train accuracy on epoch 740: 0.944\n",
      "test loss on epoch 740: 0.323\n",
      "test accuracy on epoch 740: 0.846\n",
      "train loss on epoch 741 : 0.154\n",
      "train accuracy on epoch 741: 0.944\n",
      "test loss on epoch 741: 0.333\n",
      "test accuracy on epoch 741: 0.769\n",
      "train loss on epoch 742 : 0.249\n",
      "train accuracy on epoch 742: 0.944\n",
      "test loss on epoch 742: 0.334\n",
      "test accuracy on epoch 742: 0.769\n",
      "train loss on epoch 743 : 0.177\n",
      "train accuracy on epoch 743: 0.944\n",
      "test loss on epoch 743: 0.323\n",
      "test accuracy on epoch 743: 0.846\n",
      "train loss on epoch 744 : 0.124\n",
      "train accuracy on epoch 744: 1.000\n",
      "test loss on epoch 744: 0.334\n",
      "test accuracy on epoch 744: 0.769\n",
      "train loss on epoch 745 : 0.186\n",
      "train accuracy on epoch 745: 0.944\n",
      "test loss on epoch 745: 0.343\n",
      "test accuracy on epoch 745: 0.769\n",
      "train loss on epoch 746 : 0.206\n",
      "train accuracy on epoch 746: 0.889\n",
      "test loss on epoch 746: 0.346\n",
      "test accuracy on epoch 746: 0.769\n",
      "train loss on epoch 747 : 0.120\n",
      "train accuracy on epoch 747: 0.944\n",
      "test loss on epoch 747: 0.340\n",
      "test accuracy on epoch 747: 0.769\n",
      "train loss on epoch 748 : 0.290\n",
      "train accuracy on epoch 748: 0.944\n",
      "test loss on epoch 748: 0.340\n",
      "test accuracy on epoch 748: 0.769\n",
      "train loss on epoch 749 : 0.205\n",
      "train accuracy on epoch 749: 0.889\n",
      "test loss on epoch 749: 0.346\n",
      "test accuracy on epoch 749: 0.769\n",
      "train loss on epoch 750 : 0.028\n",
      "train accuracy on epoch 750: 1.000\n",
      "test loss on epoch 750: 0.347\n",
      "test accuracy on epoch 750: 0.769\n",
      "train loss on epoch 751 : 0.168\n",
      "train accuracy on epoch 751: 0.889\n",
      "test loss on epoch 751: 0.349\n",
      "test accuracy on epoch 751: 0.769\n",
      "train loss on epoch 752 : 0.184\n",
      "train accuracy on epoch 752: 0.889\n",
      "test loss on epoch 752: 0.341\n",
      "test accuracy on epoch 752: 0.769\n",
      "train loss on epoch 753 : 0.228\n",
      "train accuracy on epoch 753: 0.833\n",
      "test loss on epoch 753: 0.344\n",
      "test accuracy on epoch 753: 0.769\n",
      "train loss on epoch 754 : 0.080\n",
      "train accuracy on epoch 754: 1.000\n",
      "test loss on epoch 754: 0.337\n",
      "test accuracy on epoch 754: 0.769\n",
      "train loss on epoch 755 : 0.101\n",
      "train accuracy on epoch 755: 0.944\n",
      "test loss on epoch 755: 0.341\n",
      "test accuracy on epoch 755: 0.769\n",
      "train loss on epoch 756 : 0.079\n",
      "train accuracy on epoch 756: 1.000\n",
      "test loss on epoch 756: 0.338\n",
      "test accuracy on epoch 756: 0.769\n",
      "train loss on epoch 757 : 0.266\n",
      "train accuracy on epoch 757: 0.889\n",
      "test loss on epoch 757: 0.339\n",
      "test accuracy on epoch 757: 0.769\n",
      "train loss on epoch 758 : 0.066\n",
      "train accuracy on epoch 758: 0.944\n",
      "test loss on epoch 758: 0.332\n",
      "test accuracy on epoch 758: 0.769\n",
      "train loss on epoch 759 : 0.290\n",
      "train accuracy on epoch 759: 0.833\n",
      "test loss on epoch 759: 0.324\n",
      "test accuracy on epoch 759: 0.769\n",
      "train loss on epoch 760 : 0.206\n",
      "train accuracy on epoch 760: 0.889\n",
      "test loss on epoch 760: 0.330\n",
      "test accuracy on epoch 760: 0.769\n",
      "train loss on epoch 761 : 0.092\n",
      "train accuracy on epoch 761: 0.944\n",
      "test loss on epoch 761: 0.331\n",
      "test accuracy on epoch 761: 0.769\n",
      "train loss on epoch 762 : 0.354\n",
      "train accuracy on epoch 762: 0.944\n",
      "test loss on epoch 762: 0.330\n",
      "test accuracy on epoch 762: 0.769\n",
      "train loss on epoch 763 : 0.336\n",
      "train accuracy on epoch 763: 0.944\n",
      "test loss on epoch 763: 0.326\n",
      "test accuracy on epoch 763: 0.769\n",
      "train loss on epoch 764 : 0.083\n",
      "train accuracy on epoch 764: 0.944\n",
      "test loss on epoch 764: 0.321\n",
      "test accuracy on epoch 764: 0.846\n",
      "train loss on epoch 765 : 0.170\n",
      "train accuracy on epoch 765: 0.889\n",
      "test loss on epoch 765: 0.329\n",
      "test accuracy on epoch 765: 0.769\n",
      "train loss on epoch 766 : 0.244\n",
      "train accuracy on epoch 766: 0.889\n",
      "test loss on epoch 766: 0.337\n",
      "test accuracy on epoch 766: 0.692\n",
      "train loss on epoch 767 : 0.261\n",
      "train accuracy on epoch 767: 0.889\n",
      "test loss on epoch 767: 0.336\n",
      "test accuracy on epoch 767: 0.769\n",
      "train loss on epoch 768 : 0.309\n",
      "train accuracy on epoch 768: 0.889\n",
      "test loss on epoch 768: 0.334\n",
      "test accuracy on epoch 768: 0.769\n",
      "train loss on epoch 769 : 0.325\n",
      "train accuracy on epoch 769: 0.944\n",
      "test loss on epoch 769: 0.339\n",
      "test accuracy on epoch 769: 0.769\n",
      "train loss on epoch 770 : 0.090\n",
      "train accuracy on epoch 770: 0.944\n",
      "test loss on epoch 770: 0.351\n",
      "test accuracy on epoch 770: 0.769\n",
      "train loss on epoch 771 : 0.174\n",
      "train accuracy on epoch 771: 0.889\n",
      "test loss on epoch 771: 0.356\n",
      "test accuracy on epoch 771: 0.769\n",
      "train loss on epoch 772 : 0.235\n",
      "train accuracy on epoch 772: 0.944\n",
      "test loss on epoch 772: 0.355\n",
      "test accuracy on epoch 772: 0.769\n",
      "train loss on epoch 773 : 0.160\n",
      "train accuracy on epoch 773: 0.944\n",
      "test loss on epoch 773: 0.352\n",
      "test accuracy on epoch 773: 0.769\n",
      "train loss on epoch 774 : 0.205\n",
      "train accuracy on epoch 774: 0.944\n",
      "test loss on epoch 774: 0.347\n",
      "test accuracy on epoch 774: 0.769\n",
      "train loss on epoch 775 : 0.093\n",
      "train accuracy on epoch 775: 0.944\n",
      "test loss on epoch 775: 0.345\n",
      "test accuracy on epoch 775: 0.769\n",
      "train loss on epoch 776 : 0.144\n",
      "train accuracy on epoch 776: 0.944\n",
      "test loss on epoch 776: 0.335\n",
      "test accuracy on epoch 776: 0.769\n",
      "train loss on epoch 777 : 0.133\n",
      "train accuracy on epoch 777: 0.889\n",
      "test loss on epoch 777: 0.335\n",
      "test accuracy on epoch 777: 0.769\n",
      "train loss on epoch 778 : 0.267\n",
      "train accuracy on epoch 778: 0.889\n",
      "test loss on epoch 778: 0.347\n",
      "test accuracy on epoch 778: 0.769\n",
      "train loss on epoch 779 : 0.213\n",
      "train accuracy on epoch 779: 0.889\n",
      "test loss on epoch 779: 0.354\n",
      "test accuracy on epoch 779: 0.769\n",
      "train loss on epoch 780 : 0.252\n",
      "train accuracy on epoch 780: 0.833\n",
      "test loss on epoch 780: 0.357\n",
      "test accuracy on epoch 780: 0.769\n",
      "train loss on epoch 781 : 0.357\n",
      "train accuracy on epoch 781: 0.889\n",
      "test loss on epoch 781: 0.349\n",
      "test accuracy on epoch 781: 0.769\n",
      "train loss on epoch 782 : 0.387\n",
      "train accuracy on epoch 782: 0.889\n",
      "test loss on epoch 782: 0.351\n",
      "test accuracy on epoch 782: 0.769\n",
      "train loss on epoch 783 : 0.324\n",
      "train accuracy on epoch 783: 0.889\n",
      "test loss on epoch 783: 0.346\n",
      "test accuracy on epoch 783: 0.769\n",
      "train loss on epoch 784 : 0.136\n",
      "train accuracy on epoch 784: 0.944\n",
      "test loss on epoch 784: 0.339\n",
      "test accuracy on epoch 784: 0.769\n",
      "train loss on epoch 785 : 0.343\n",
      "train accuracy on epoch 785: 0.889\n",
      "test loss on epoch 785: 0.342\n",
      "test accuracy on epoch 785: 0.769\n",
      "train loss on epoch 786 : 0.095\n",
      "train accuracy on epoch 786: 0.944\n",
      "test loss on epoch 786: 0.341\n",
      "test accuracy on epoch 786: 0.769\n",
      "train loss on epoch 787 : 0.125\n",
      "train accuracy on epoch 787: 0.944\n",
      "test loss on epoch 787: 0.331\n",
      "test accuracy on epoch 787: 0.769\n",
      "train loss on epoch 788 : 0.240\n",
      "train accuracy on epoch 788: 0.833\n",
      "test loss on epoch 788: 0.333\n",
      "test accuracy on epoch 788: 0.769\n",
      "train loss on epoch 789 : 0.182\n",
      "train accuracy on epoch 789: 0.889\n",
      "test loss on epoch 789: 0.330\n",
      "test accuracy on epoch 789: 0.769\n",
      "train loss on epoch 790 : 0.217\n",
      "train accuracy on epoch 790: 0.944\n",
      "test loss on epoch 790: 0.329\n",
      "test accuracy on epoch 790: 0.769\n",
      "train loss on epoch 791 : 0.052\n",
      "train accuracy on epoch 791: 1.000\n",
      "test loss on epoch 791: 0.335\n",
      "test accuracy on epoch 791: 0.769\n",
      "train loss on epoch 792 : 0.174\n",
      "train accuracy on epoch 792: 0.944\n",
      "test loss on epoch 792: 0.331\n",
      "test accuracy on epoch 792: 0.769\n",
      "train loss on epoch 793 : 0.294\n",
      "train accuracy on epoch 793: 0.833\n",
      "test loss on epoch 793: 0.333\n",
      "test accuracy on epoch 793: 0.769\n",
      "train loss on epoch 794 : 0.206\n",
      "train accuracy on epoch 794: 0.889\n",
      "test loss on epoch 794: 0.331\n",
      "test accuracy on epoch 794: 0.769\n",
      "train loss on epoch 795 : 0.133\n",
      "train accuracy on epoch 795: 0.944\n",
      "test loss on epoch 795: 0.330\n",
      "test accuracy on epoch 795: 0.769\n",
      "train loss on epoch 796 : 0.183\n",
      "train accuracy on epoch 796: 0.944\n",
      "test loss on epoch 796: 0.333\n",
      "test accuracy on epoch 796: 0.769\n",
      "train loss on epoch 797 : 0.172\n",
      "train accuracy on epoch 797: 0.944\n",
      "test loss on epoch 797: 0.340\n",
      "test accuracy on epoch 797: 0.769\n",
      "train loss on epoch 798 : 0.105\n",
      "train accuracy on epoch 798: 1.000\n",
      "test loss on epoch 798: 0.337\n",
      "test accuracy on epoch 798: 0.769\n",
      "train loss on epoch 799 : 0.511\n",
      "train accuracy on epoch 799: 0.833\n",
      "test loss on epoch 799: 0.333\n",
      "test accuracy on epoch 799: 0.769\n",
      "train loss on epoch 800 : 0.152\n",
      "train accuracy on epoch 800: 0.944\n",
      "test loss on epoch 800: 0.333\n",
      "test accuracy on epoch 800: 0.769\n",
      "train loss on epoch 801 : 0.158\n",
      "train accuracy on epoch 801: 0.944\n",
      "test loss on epoch 801: 0.330\n",
      "test accuracy on epoch 801: 0.846\n",
      "train loss on epoch 802 : 0.350\n",
      "train accuracy on epoch 802: 0.944\n",
      "test loss on epoch 802: 0.333\n",
      "test accuracy on epoch 802: 0.769\n",
      "train loss on epoch 803 : 0.075\n",
      "train accuracy on epoch 803: 1.000\n",
      "test loss on epoch 803: 0.333\n",
      "test accuracy on epoch 803: 0.769\n",
      "train loss on epoch 804 : 0.222\n",
      "train accuracy on epoch 804: 0.944\n",
      "test loss on epoch 804: 0.345\n",
      "test accuracy on epoch 804: 0.769\n",
      "train loss on epoch 805 : 0.247\n",
      "train accuracy on epoch 805: 0.944\n",
      "test loss on epoch 805: 0.342\n",
      "test accuracy on epoch 805: 0.769\n",
      "train loss on epoch 806 : 0.221\n",
      "train accuracy on epoch 806: 0.889\n",
      "test loss on epoch 806: 0.341\n",
      "test accuracy on epoch 806: 0.769\n",
      "train loss on epoch 807 : 0.201\n",
      "train accuracy on epoch 807: 0.889\n",
      "test loss on epoch 807: 0.344\n",
      "test accuracy on epoch 807: 0.769\n",
      "train loss on epoch 808 : 0.333\n",
      "train accuracy on epoch 808: 0.889\n",
      "test loss on epoch 808: 0.344\n",
      "test accuracy on epoch 808: 0.769\n",
      "train loss on epoch 809 : 0.068\n",
      "train accuracy on epoch 809: 1.000\n",
      "test loss on epoch 809: 0.344\n",
      "test accuracy on epoch 809: 0.769\n",
      "train loss on epoch 810 : 0.157\n",
      "train accuracy on epoch 810: 0.944\n",
      "test loss on epoch 810: 0.341\n",
      "test accuracy on epoch 810: 0.769\n",
      "train loss on epoch 811 : 0.267\n",
      "train accuracy on epoch 811: 0.889\n",
      "test loss on epoch 811: 0.344\n",
      "test accuracy on epoch 811: 0.769\n",
      "train loss on epoch 812 : 0.075\n",
      "train accuracy on epoch 812: 1.000\n",
      "test loss on epoch 812: 0.341\n",
      "test accuracy on epoch 812: 0.769\n",
      "train loss on epoch 813 : 0.202\n",
      "train accuracy on epoch 813: 0.944\n",
      "test loss on epoch 813: 0.339\n",
      "test accuracy on epoch 813: 0.769\n",
      "train loss on epoch 814 : 0.382\n",
      "train accuracy on epoch 814: 0.889\n",
      "test loss on epoch 814: 0.338\n",
      "test accuracy on epoch 814: 0.769\n",
      "train loss on epoch 815 : 0.115\n",
      "train accuracy on epoch 815: 0.944\n",
      "test loss on epoch 815: 0.332\n",
      "test accuracy on epoch 815: 0.769\n",
      "train loss on epoch 816 : 0.132\n",
      "train accuracy on epoch 816: 0.944\n",
      "test loss on epoch 816: 0.332\n",
      "test accuracy on epoch 816: 0.769\n",
      "train loss on epoch 817 : 0.186\n",
      "train accuracy on epoch 817: 0.944\n",
      "test loss on epoch 817: 0.328\n",
      "test accuracy on epoch 817: 0.769\n",
      "train loss on epoch 818 : 0.148\n",
      "train accuracy on epoch 818: 0.944\n",
      "test loss on epoch 818: 0.334\n",
      "test accuracy on epoch 818: 0.769\n",
      "train loss on epoch 819 : 0.260\n",
      "train accuracy on epoch 819: 0.944\n",
      "test loss on epoch 819: 0.342\n",
      "test accuracy on epoch 819: 0.769\n",
      "train loss on epoch 820 : 0.215\n",
      "train accuracy on epoch 820: 0.889\n",
      "test loss on epoch 820: 0.345\n",
      "test accuracy on epoch 820: 0.769\n",
      "train loss on epoch 821 : 0.376\n",
      "train accuracy on epoch 821: 0.889\n",
      "test loss on epoch 821: 0.347\n",
      "test accuracy on epoch 821: 0.769\n",
      "train loss on epoch 822 : 0.074\n",
      "train accuracy on epoch 822: 1.000\n",
      "test loss on epoch 822: 0.354\n",
      "test accuracy on epoch 822: 0.769\n",
      "train loss on epoch 823 : 0.179\n",
      "train accuracy on epoch 823: 0.944\n",
      "test loss on epoch 823: 0.354\n",
      "test accuracy on epoch 823: 0.769\n",
      "train loss on epoch 824 : 0.322\n",
      "train accuracy on epoch 824: 0.889\n",
      "test loss on epoch 824: 0.351\n",
      "test accuracy on epoch 824: 0.769\n",
      "train loss on epoch 825 : 0.359\n",
      "train accuracy on epoch 825: 0.833\n",
      "test loss on epoch 825: 0.350\n",
      "test accuracy on epoch 825: 0.769\n",
      "train loss on epoch 826 : 0.275\n",
      "train accuracy on epoch 826: 0.944\n",
      "test loss on epoch 826: 0.342\n",
      "test accuracy on epoch 826: 0.769\n",
      "train loss on epoch 827 : 0.050\n",
      "train accuracy on epoch 827: 1.000\n",
      "test loss on epoch 827: 0.338\n",
      "test accuracy on epoch 827: 0.769\n",
      "train loss on epoch 828 : 0.029\n",
      "train accuracy on epoch 828: 1.000\n",
      "test loss on epoch 828: 0.337\n",
      "test accuracy on epoch 828: 0.769\n",
      "train loss on epoch 829 : 0.213\n",
      "train accuracy on epoch 829: 0.889\n",
      "test loss on epoch 829: 0.335\n",
      "test accuracy on epoch 829: 0.769\n",
      "train loss on epoch 830 : 0.135\n",
      "train accuracy on epoch 830: 0.944\n",
      "test loss on epoch 830: 0.336\n",
      "test accuracy on epoch 830: 0.769\n",
      "train loss on epoch 831 : 0.131\n",
      "train accuracy on epoch 831: 0.944\n",
      "test loss on epoch 831: 0.332\n",
      "test accuracy on epoch 831: 0.769\n",
      "train loss on epoch 832 : 0.226\n",
      "train accuracy on epoch 832: 0.889\n",
      "test loss on epoch 832: 0.328\n",
      "test accuracy on epoch 832: 0.769\n",
      "train loss on epoch 833 : 0.094\n",
      "train accuracy on epoch 833: 1.000\n",
      "test loss on epoch 833: 0.332\n",
      "test accuracy on epoch 833: 0.769\n",
      "train loss on epoch 834 : 0.073\n",
      "train accuracy on epoch 834: 1.000\n",
      "test loss on epoch 834: 0.332\n",
      "test accuracy on epoch 834: 0.692\n",
      "train loss on epoch 835 : 0.206\n",
      "train accuracy on epoch 835: 0.944\n",
      "test loss on epoch 835: 0.324\n",
      "test accuracy on epoch 835: 0.846\n",
      "train loss on epoch 836 : 0.042\n",
      "train accuracy on epoch 836: 1.000\n",
      "test loss on epoch 836: 0.327\n",
      "test accuracy on epoch 836: 0.769\n",
      "train loss on epoch 837 : 0.225\n",
      "train accuracy on epoch 837: 0.944\n",
      "test loss on epoch 837: 0.327\n",
      "test accuracy on epoch 837: 0.769\n",
      "train loss on epoch 838 : 0.053\n",
      "train accuracy on epoch 838: 1.000\n",
      "test loss on epoch 838: 0.319\n",
      "test accuracy on epoch 838: 0.769\n",
      "train loss on epoch 839 : 0.435\n",
      "train accuracy on epoch 839: 0.833\n",
      "test loss on epoch 839: 0.321\n",
      "test accuracy on epoch 839: 0.769\n",
      "train loss on epoch 840 : 0.134\n",
      "train accuracy on epoch 840: 0.944\n",
      "test loss on epoch 840: 0.325\n",
      "test accuracy on epoch 840: 0.769\n",
      "train loss on epoch 841 : 0.337\n",
      "train accuracy on epoch 841: 0.833\n",
      "test loss on epoch 841: 0.328\n",
      "test accuracy on epoch 841: 0.846\n",
      "train loss on epoch 842 : 0.418\n",
      "train accuracy on epoch 842: 0.778\n",
      "test loss on epoch 842: 0.336\n",
      "test accuracy on epoch 842: 0.692\n",
      "train loss on epoch 843 : 0.212\n",
      "train accuracy on epoch 843: 0.889\n",
      "test loss on epoch 843: 0.337\n",
      "test accuracy on epoch 843: 0.692\n",
      "train loss on epoch 844 : 0.078\n",
      "train accuracy on epoch 844: 1.000\n",
      "test loss on epoch 844: 0.341\n",
      "test accuracy on epoch 844: 0.692\n",
      "train loss on epoch 845 : 0.249\n",
      "train accuracy on epoch 845: 0.944\n",
      "test loss on epoch 845: 0.332\n",
      "test accuracy on epoch 845: 0.769\n",
      "train loss on epoch 846 : 0.242\n",
      "train accuracy on epoch 846: 0.889\n",
      "test loss on epoch 846: 0.337\n",
      "test accuracy on epoch 846: 0.769\n",
      "train loss on epoch 847 : 0.282\n",
      "train accuracy on epoch 847: 0.833\n",
      "test loss on epoch 847: 0.336\n",
      "test accuracy on epoch 847: 0.692\n",
      "train loss on epoch 848 : 0.117\n",
      "train accuracy on epoch 848: 0.944\n",
      "test loss on epoch 848: 0.336\n",
      "test accuracy on epoch 848: 0.769\n",
      "train loss on epoch 849 : 0.058\n",
      "train accuracy on epoch 849: 1.000\n",
      "test loss on epoch 849: 0.327\n",
      "test accuracy on epoch 849: 0.769\n",
      "train loss on epoch 850 : 0.057\n",
      "train accuracy on epoch 850: 1.000\n",
      "test loss on epoch 850: 0.339\n",
      "test accuracy on epoch 850: 0.769\n",
      "train loss on epoch 851 : 0.204\n",
      "train accuracy on epoch 851: 0.944\n",
      "test loss on epoch 851: 0.333\n",
      "test accuracy on epoch 851: 0.769\n",
      "train loss on epoch 852 : 0.139\n",
      "train accuracy on epoch 852: 0.944\n",
      "test loss on epoch 852: 0.333\n",
      "test accuracy on epoch 852: 0.846\n",
      "train loss on epoch 853 : 0.173\n",
      "train accuracy on epoch 853: 0.944\n",
      "test loss on epoch 853: 0.336\n",
      "test accuracy on epoch 853: 0.769\n",
      "train loss on epoch 854 : 0.369\n",
      "train accuracy on epoch 854: 0.833\n",
      "test loss on epoch 854: 0.330\n",
      "test accuracy on epoch 854: 0.769\n",
      "train loss on epoch 855 : 0.063\n",
      "train accuracy on epoch 855: 1.000\n",
      "test loss on epoch 855: 0.340\n",
      "test accuracy on epoch 855: 0.769\n",
      "train loss on epoch 856 : 0.222\n",
      "train accuracy on epoch 856: 0.833\n",
      "test loss on epoch 856: 0.329\n",
      "test accuracy on epoch 856: 0.769\n",
      "train loss on epoch 857 : 0.056\n",
      "train accuracy on epoch 857: 1.000\n",
      "test loss on epoch 857: 0.336\n",
      "test accuracy on epoch 857: 0.769\n",
      "train loss on epoch 858 : 0.236\n",
      "train accuracy on epoch 858: 0.889\n",
      "test loss on epoch 858: 0.327\n",
      "test accuracy on epoch 858: 0.846\n",
      "train loss on epoch 859 : 0.177\n",
      "train accuracy on epoch 859: 0.889\n",
      "test loss on epoch 859: 0.329\n",
      "test accuracy on epoch 859: 0.846\n",
      "train loss on epoch 860 : 0.147\n",
      "train accuracy on epoch 860: 0.944\n",
      "test loss on epoch 860: 0.331\n",
      "test accuracy on epoch 860: 0.769\n",
      "train loss on epoch 861 : 0.037\n",
      "train accuracy on epoch 861: 1.000\n",
      "test loss on epoch 861: 0.334\n",
      "test accuracy on epoch 861: 0.769\n",
      "train loss on epoch 862 : 0.368\n",
      "train accuracy on epoch 862: 0.889\n",
      "test loss on epoch 862: 0.336\n",
      "test accuracy on epoch 862: 0.692\n",
      "train loss on epoch 863 : 0.184\n",
      "train accuracy on epoch 863: 0.944\n",
      "test loss on epoch 863: 0.330\n",
      "test accuracy on epoch 863: 0.769\n",
      "train loss on epoch 864 : 0.334\n",
      "train accuracy on epoch 864: 0.833\n",
      "test loss on epoch 864: 0.339\n",
      "test accuracy on epoch 864: 0.692\n",
      "train loss on epoch 865 : 0.242\n",
      "train accuracy on epoch 865: 0.889\n",
      "test loss on epoch 865: 0.335\n",
      "test accuracy on epoch 865: 0.769\n",
      "train loss on epoch 866 : 0.168\n",
      "train accuracy on epoch 866: 0.889\n",
      "test loss on epoch 866: 0.344\n",
      "test accuracy on epoch 866: 0.769\n",
      "train loss on epoch 867 : 0.147\n",
      "train accuracy on epoch 867: 0.944\n",
      "test loss on epoch 867: 0.346\n",
      "test accuracy on epoch 867: 0.769\n",
      "train loss on epoch 868 : 0.078\n",
      "train accuracy on epoch 868: 1.000\n",
      "test loss on epoch 868: 0.348\n",
      "test accuracy on epoch 868: 0.769\n",
      "train loss on epoch 869 : 0.213\n",
      "train accuracy on epoch 869: 0.944\n",
      "test loss on epoch 869: 0.339\n",
      "test accuracy on epoch 869: 0.769\n",
      "train loss on epoch 870 : 0.216\n",
      "train accuracy on epoch 870: 0.944\n",
      "test loss on epoch 870: 0.320\n",
      "test accuracy on epoch 870: 0.769\n",
      "train loss on epoch 871 : 0.359\n",
      "train accuracy on epoch 871: 0.944\n",
      "test loss on epoch 871: 0.336\n",
      "test accuracy on epoch 871: 0.769\n",
      "train loss on epoch 872 : 0.107\n",
      "train accuracy on epoch 872: 0.944\n",
      "test loss on epoch 872: 0.329\n",
      "test accuracy on epoch 872: 0.769\n",
      "train loss on epoch 873 : 0.078\n",
      "train accuracy on epoch 873: 1.000\n",
      "test loss on epoch 873: 0.326\n",
      "test accuracy on epoch 873: 0.769\n",
      "train loss on epoch 874 : 0.102\n",
      "train accuracy on epoch 874: 0.944\n",
      "test loss on epoch 874: 0.332\n",
      "test accuracy on epoch 874: 0.769\n",
      "train loss on epoch 875 : 0.152\n",
      "train accuracy on epoch 875: 0.889\n",
      "test loss on epoch 875: 0.337\n",
      "test accuracy on epoch 875: 0.769\n",
      "train loss on epoch 876 : 0.040\n",
      "train accuracy on epoch 876: 1.000\n",
      "test loss on epoch 876: 0.331\n",
      "test accuracy on epoch 876: 0.769\n",
      "train loss on epoch 877 : 0.135\n",
      "train accuracy on epoch 877: 0.944\n",
      "test loss on epoch 877: 0.317\n",
      "test accuracy on epoch 877: 0.769\n",
      "train loss on epoch 878 : 0.189\n",
      "train accuracy on epoch 878: 0.889\n",
      "test loss on epoch 878: 0.334\n",
      "test accuracy on epoch 878: 0.769\n",
      "train loss on epoch 879 : 0.097\n",
      "train accuracy on epoch 879: 0.944\n",
      "test loss on epoch 879: 0.334\n",
      "test accuracy on epoch 879: 0.769\n",
      "train loss on epoch 880 : 0.119\n",
      "train accuracy on epoch 880: 0.944\n",
      "test loss on epoch 880: 0.325\n",
      "test accuracy on epoch 880: 0.769\n",
      "train loss on epoch 881 : 0.188\n",
      "train accuracy on epoch 881: 0.889\n",
      "test loss on epoch 881: 0.331\n",
      "test accuracy on epoch 881: 0.769\n",
      "train loss on epoch 882 : 0.117\n",
      "train accuracy on epoch 882: 1.000\n",
      "test loss on epoch 882: 0.318\n",
      "test accuracy on epoch 882: 0.769\n",
      "train loss on epoch 883 : 0.155\n",
      "train accuracy on epoch 883: 0.944\n",
      "test loss on epoch 883: 0.330\n",
      "test accuracy on epoch 883: 0.769\n",
      "train loss on epoch 884 : 0.326\n",
      "train accuracy on epoch 884: 0.889\n",
      "test loss on epoch 884: 0.336\n",
      "test accuracy on epoch 884: 0.692\n",
      "train loss on epoch 885 : 0.410\n",
      "train accuracy on epoch 885: 0.889\n",
      "test loss on epoch 885: 0.332\n",
      "test accuracy on epoch 885: 0.769\n",
      "train loss on epoch 886 : 0.146\n",
      "train accuracy on epoch 886: 0.889\n",
      "test loss on epoch 886: 0.324\n",
      "test accuracy on epoch 886: 0.769\n",
      "train loss on epoch 887 : 0.070\n",
      "train accuracy on epoch 887: 1.000\n",
      "test loss on epoch 887: 0.329\n",
      "test accuracy on epoch 887: 0.769\n",
      "train loss on epoch 888 : 0.195\n",
      "train accuracy on epoch 888: 0.833\n",
      "test loss on epoch 888: 0.338\n",
      "test accuracy on epoch 888: 0.692\n",
      "train loss on epoch 889 : 0.205\n",
      "train accuracy on epoch 889: 0.889\n",
      "test loss on epoch 889: 0.338\n",
      "test accuracy on epoch 889: 0.692\n",
      "train loss on epoch 890 : 0.298\n",
      "train accuracy on epoch 890: 0.889\n",
      "test loss on epoch 890: 0.329\n",
      "test accuracy on epoch 890: 0.846\n",
      "train loss on epoch 891 : 0.185\n",
      "train accuracy on epoch 891: 0.889\n",
      "test loss on epoch 891: 0.337\n",
      "test accuracy on epoch 891: 0.769\n",
      "train loss on epoch 892 : 0.380\n",
      "train accuracy on epoch 892: 0.889\n",
      "test loss on epoch 892: 0.329\n",
      "test accuracy on epoch 892: 0.769\n",
      "train loss on epoch 893 : 0.295\n",
      "train accuracy on epoch 893: 0.833\n",
      "test loss on epoch 893: 0.327\n",
      "test accuracy on epoch 893: 0.769\n",
      "train loss on epoch 894 : 0.290\n",
      "train accuracy on epoch 894: 0.889\n",
      "test loss on epoch 894: 0.326\n",
      "test accuracy on epoch 894: 0.769\n",
      "train loss on epoch 895 : 0.107\n",
      "train accuracy on epoch 895: 0.944\n",
      "test loss on epoch 895: 0.331\n",
      "test accuracy on epoch 895: 0.769\n",
      "train loss on epoch 896 : 0.347\n",
      "train accuracy on epoch 896: 0.889\n",
      "test loss on epoch 896: 0.325\n",
      "test accuracy on epoch 896: 0.846\n",
      "train loss on epoch 897 : 0.238\n",
      "train accuracy on epoch 897: 0.944\n",
      "test loss on epoch 897: 0.338\n",
      "test accuracy on epoch 897: 0.692\n",
      "train loss on epoch 898 : 0.088\n",
      "train accuracy on epoch 898: 1.000\n",
      "test loss on epoch 898: 0.332\n",
      "test accuracy on epoch 898: 0.769\n",
      "train loss on epoch 899 : 0.137\n",
      "train accuracy on epoch 899: 0.944\n",
      "test loss on epoch 899: 0.325\n",
      "test accuracy on epoch 899: 0.769\n",
      "train loss on epoch 900 : 0.136\n",
      "train accuracy on epoch 900: 0.944\n",
      "test loss on epoch 900: 0.336\n",
      "test accuracy on epoch 900: 0.769\n",
      "train loss on epoch 901 : 0.040\n",
      "train accuracy on epoch 901: 1.000\n",
      "test loss on epoch 901: 0.335\n",
      "test accuracy on epoch 901: 0.769\n",
      "train loss on epoch 902 : 0.158\n",
      "train accuracy on epoch 902: 0.944\n",
      "test loss on epoch 902: 0.334\n",
      "test accuracy on epoch 902: 0.769\n",
      "train loss on epoch 903 : 0.231\n",
      "train accuracy on epoch 903: 0.944\n",
      "test loss on epoch 903: 0.334\n",
      "test accuracy on epoch 903: 0.769\n",
      "train loss on epoch 904 : 0.128\n",
      "train accuracy on epoch 904: 0.944\n",
      "test loss on epoch 904: 0.322\n",
      "test accuracy on epoch 904: 0.769\n",
      "train loss on epoch 905 : 0.287\n",
      "train accuracy on epoch 905: 0.889\n",
      "test loss on epoch 905: 0.326\n",
      "test accuracy on epoch 905: 0.769\n",
      "train loss on epoch 906 : 0.259\n",
      "train accuracy on epoch 906: 0.889\n",
      "test loss on epoch 906: 0.327\n",
      "test accuracy on epoch 906: 0.769\n",
      "train loss on epoch 907 : 0.247\n",
      "train accuracy on epoch 907: 0.944\n",
      "test loss on epoch 907: 0.321\n",
      "test accuracy on epoch 907: 0.769\n",
      "train loss on epoch 908 : 0.311\n",
      "train accuracy on epoch 908: 0.944\n",
      "test loss on epoch 908: 0.325\n",
      "test accuracy on epoch 908: 0.769\n",
      "train loss on epoch 909 : 0.269\n",
      "train accuracy on epoch 909: 0.944\n",
      "test loss on epoch 909: 0.323\n",
      "test accuracy on epoch 909: 0.769\n",
      "train loss on epoch 910 : 0.418\n",
      "train accuracy on epoch 910: 0.889\n",
      "test loss on epoch 910: 0.321\n",
      "test accuracy on epoch 910: 0.769\n",
      "train loss on epoch 911 : 0.098\n",
      "train accuracy on epoch 911: 1.000\n",
      "test loss on epoch 911: 0.340\n",
      "test accuracy on epoch 911: 0.769\n",
      "train loss on epoch 912 : 0.095\n",
      "train accuracy on epoch 912: 1.000\n",
      "test loss on epoch 912: 0.344\n",
      "test accuracy on epoch 912: 0.769\n",
      "train loss on epoch 913 : 0.222\n",
      "train accuracy on epoch 913: 0.944\n",
      "test loss on epoch 913: 0.339\n",
      "test accuracy on epoch 913: 0.769\n",
      "train loss on epoch 914 : 0.148\n",
      "train accuracy on epoch 914: 0.944\n",
      "test loss on epoch 914: 0.330\n",
      "test accuracy on epoch 914: 0.769\n",
      "train loss on epoch 915 : 0.108\n",
      "train accuracy on epoch 915: 1.000\n",
      "test loss on epoch 915: 0.328\n",
      "test accuracy on epoch 915: 0.846\n",
      "train loss on epoch 916 : 0.179\n",
      "train accuracy on epoch 916: 0.944\n",
      "test loss on epoch 916: 0.338\n",
      "test accuracy on epoch 916: 0.692\n",
      "train loss on epoch 917 : 0.075\n",
      "train accuracy on epoch 917: 1.000\n",
      "test loss on epoch 917: 0.330\n",
      "test accuracy on epoch 917: 0.769\n",
      "train loss on epoch 918 : 0.130\n",
      "train accuracy on epoch 918: 0.889\n",
      "test loss on epoch 918: 0.339\n",
      "test accuracy on epoch 918: 0.769\n",
      "train loss on epoch 919 : 0.144\n",
      "train accuracy on epoch 919: 0.889\n",
      "test loss on epoch 919: 0.321\n",
      "test accuracy on epoch 919: 0.846\n",
      "train loss on epoch 920 : 0.080\n",
      "train accuracy on epoch 920: 0.944\n",
      "test loss on epoch 920: 0.338\n",
      "test accuracy on epoch 920: 0.692\n",
      "train loss on epoch 921 : 0.188\n",
      "train accuracy on epoch 921: 0.889\n",
      "test loss on epoch 921: 0.330\n",
      "test accuracy on epoch 921: 0.769\n",
      "train loss on epoch 922 : 0.133\n",
      "train accuracy on epoch 922: 0.889\n",
      "test loss on epoch 922: 0.324\n",
      "test accuracy on epoch 922: 0.846\n",
      "train loss on epoch 923 : 0.298\n",
      "train accuracy on epoch 923: 0.889\n",
      "test loss on epoch 923: 0.326\n",
      "test accuracy on epoch 923: 0.769\n",
      "train loss on epoch 924 : 0.222\n",
      "train accuracy on epoch 924: 0.944\n",
      "test loss on epoch 924: 0.344\n",
      "test accuracy on epoch 924: 0.769\n",
      "train loss on epoch 925 : 0.187\n",
      "train accuracy on epoch 925: 0.889\n",
      "test loss on epoch 925: 0.344\n",
      "test accuracy on epoch 925: 0.769\n",
      "train loss on epoch 926 : 0.098\n",
      "train accuracy on epoch 926: 0.944\n",
      "test loss on epoch 926: 0.343\n",
      "test accuracy on epoch 926: 0.769\n",
      "train loss on epoch 927 : 0.200\n",
      "train accuracy on epoch 927: 0.944\n",
      "test loss on epoch 927: 0.339\n",
      "test accuracy on epoch 927: 0.769\n",
      "train loss on epoch 928 : 0.292\n",
      "train accuracy on epoch 928: 0.833\n",
      "test loss on epoch 928: 0.337\n",
      "test accuracy on epoch 928: 0.769\n",
      "train loss on epoch 929 : 0.157\n",
      "train accuracy on epoch 929: 0.889\n",
      "test loss on epoch 929: 0.337\n",
      "test accuracy on epoch 929: 0.769\n",
      "train loss on epoch 930 : 0.192\n",
      "train accuracy on epoch 930: 0.889\n",
      "test loss on epoch 930: 0.333\n",
      "test accuracy on epoch 930: 0.769\n",
      "train loss on epoch 931 : 0.244\n",
      "train accuracy on epoch 931: 0.889\n",
      "test loss on epoch 931: 0.334\n",
      "test accuracy on epoch 931: 0.769\n",
      "train loss on epoch 932 : 0.176\n",
      "train accuracy on epoch 932: 0.944\n",
      "test loss on epoch 932: 0.328\n",
      "test accuracy on epoch 932: 0.769\n",
      "train loss on epoch 933 : 0.081\n",
      "train accuracy on epoch 933: 1.000\n",
      "test loss on epoch 933: 0.326\n",
      "test accuracy on epoch 933: 0.769\n",
      "train loss on epoch 934 : 0.352\n",
      "train accuracy on epoch 934: 0.889\n",
      "test loss on epoch 934: 0.333\n",
      "test accuracy on epoch 934: 0.769\n",
      "train loss on epoch 935 : 0.093\n",
      "train accuracy on epoch 935: 0.944\n",
      "test loss on epoch 935: 0.329\n",
      "test accuracy on epoch 935: 0.769\n",
      "train loss on epoch 936 : 0.315\n",
      "train accuracy on epoch 936: 0.889\n",
      "test loss on epoch 936: 0.334\n",
      "test accuracy on epoch 936: 0.769\n",
      "train loss on epoch 937 : 0.236\n",
      "train accuracy on epoch 937: 0.944\n",
      "test loss on epoch 937: 0.336\n",
      "test accuracy on epoch 937: 0.769\n",
      "train loss on epoch 938 : 0.164\n",
      "train accuracy on epoch 938: 0.944\n",
      "test loss on epoch 938: 0.335\n",
      "test accuracy on epoch 938: 0.769\n",
      "train loss on epoch 939 : 0.415\n",
      "train accuracy on epoch 939: 0.833\n",
      "test loss on epoch 939: 0.342\n",
      "test accuracy on epoch 939: 0.769\n",
      "train loss on epoch 940 : 0.155\n",
      "train accuracy on epoch 940: 0.944\n",
      "test loss on epoch 940: 0.342\n",
      "test accuracy on epoch 940: 0.769\n",
      "train loss on epoch 941 : 0.247\n",
      "train accuracy on epoch 941: 0.889\n",
      "test loss on epoch 941: 0.337\n",
      "test accuracy on epoch 941: 0.769\n",
      "train loss on epoch 942 : 0.167\n",
      "train accuracy on epoch 942: 0.889\n",
      "test loss on epoch 942: 0.338\n",
      "test accuracy on epoch 942: 0.769\n",
      "train loss on epoch 943 : 0.230\n",
      "train accuracy on epoch 943: 0.944\n",
      "test loss on epoch 943: 0.325\n",
      "test accuracy on epoch 943: 0.769\n",
      "train loss on epoch 944 : 0.132\n",
      "train accuracy on epoch 944: 1.000\n",
      "test loss on epoch 944: 0.321\n",
      "test accuracy on epoch 944: 0.846\n",
      "train loss on epoch 945 : 0.145\n",
      "train accuracy on epoch 945: 0.944\n",
      "test loss on epoch 945: 0.322\n",
      "test accuracy on epoch 945: 0.769\n",
      "train loss on epoch 946 : 0.288\n",
      "train accuracy on epoch 946: 0.889\n",
      "test loss on epoch 946: 0.330\n",
      "test accuracy on epoch 946: 0.769\n",
      "train loss on epoch 947 : 0.123\n",
      "train accuracy on epoch 947: 0.944\n",
      "test loss on epoch 947: 0.337\n",
      "test accuracy on epoch 947: 0.769\n",
      "train loss on epoch 948 : 0.135\n",
      "train accuracy on epoch 948: 0.944\n",
      "test loss on epoch 948: 0.340\n",
      "test accuracy on epoch 948: 0.769\n",
      "train loss on epoch 949 : 0.265\n",
      "train accuracy on epoch 949: 0.889\n",
      "test loss on epoch 949: 0.329\n",
      "test accuracy on epoch 949: 0.769\n",
      "train loss on epoch 950 : 0.153\n",
      "train accuracy on epoch 950: 0.944\n",
      "test loss on epoch 950: 0.330\n",
      "test accuracy on epoch 950: 0.769\n",
      "train loss on epoch 951 : 0.222\n",
      "train accuracy on epoch 951: 0.889\n",
      "test loss on epoch 951: 0.332\n",
      "test accuracy on epoch 951: 0.769\n",
      "train loss on epoch 952 : 0.281\n",
      "train accuracy on epoch 952: 0.833\n",
      "test loss on epoch 952: 0.342\n",
      "test accuracy on epoch 952: 0.692\n",
      "train loss on epoch 953 : 0.315\n",
      "train accuracy on epoch 953: 0.889\n",
      "test loss on epoch 953: 0.321\n",
      "test accuracy on epoch 953: 0.769\n",
      "train loss on epoch 954 : 0.375\n",
      "train accuracy on epoch 954: 0.944\n",
      "test loss on epoch 954: 0.342\n",
      "test accuracy on epoch 954: 0.769\n",
      "train loss on epoch 955 : 0.095\n",
      "train accuracy on epoch 955: 1.000\n",
      "test loss on epoch 955: 0.336\n",
      "test accuracy on epoch 955: 0.769\n",
      "train loss on epoch 956 : 0.378\n",
      "train accuracy on epoch 956: 0.889\n",
      "test loss on epoch 956: 0.322\n",
      "test accuracy on epoch 956: 0.769\n",
      "train loss on epoch 957 : 0.135\n",
      "train accuracy on epoch 957: 0.944\n",
      "test loss on epoch 957: 0.340\n",
      "test accuracy on epoch 957: 0.769\n",
      "train loss on epoch 958 : 0.168\n",
      "train accuracy on epoch 958: 0.889\n",
      "test loss on epoch 958: 0.334\n",
      "test accuracy on epoch 958: 0.769\n",
      "train loss on epoch 959 : 0.232\n",
      "train accuracy on epoch 959: 0.944\n",
      "test loss on epoch 959: 0.338\n",
      "test accuracy on epoch 959: 0.692\n",
      "train loss on epoch 960 : 0.055\n",
      "train accuracy on epoch 960: 1.000\n",
      "test loss on epoch 960: 0.329\n",
      "test accuracy on epoch 960: 0.769\n",
      "train loss on epoch 961 : 0.070\n",
      "train accuracy on epoch 961: 1.000\n",
      "test loss on epoch 961: 0.340\n",
      "test accuracy on epoch 961: 0.692\n",
      "train loss on epoch 962 : 0.106\n",
      "train accuracy on epoch 962: 1.000\n",
      "test loss on epoch 962: 0.331\n",
      "test accuracy on epoch 962: 0.769\n",
      "train loss on epoch 963 : 0.075\n",
      "train accuracy on epoch 963: 0.944\n",
      "test loss on epoch 963: 0.332\n",
      "test accuracy on epoch 963: 0.769\n",
      "train loss on epoch 964 : 0.068\n",
      "train accuracy on epoch 964: 1.000\n",
      "test loss on epoch 964: 0.325\n",
      "test accuracy on epoch 964: 0.846\n",
      "train loss on epoch 965 : 0.061\n",
      "train accuracy on epoch 965: 1.000\n",
      "test loss on epoch 965: 0.323\n",
      "test accuracy on epoch 965: 0.846\n",
      "train loss on epoch 966 : 0.059\n",
      "train accuracy on epoch 966: 1.000\n",
      "test loss on epoch 966: 0.322\n",
      "test accuracy on epoch 966: 0.846\n",
      "train loss on epoch 967 : 0.056\n",
      "train accuracy on epoch 967: 1.000\n",
      "test loss on epoch 967: 0.336\n",
      "test accuracy on epoch 967: 0.692\n",
      "train loss on epoch 968 : 0.327\n",
      "train accuracy on epoch 968: 0.889\n",
      "test loss on epoch 968: 0.336\n",
      "test accuracy on epoch 968: 0.692\n",
      "train loss on epoch 969 : 0.250\n",
      "train accuracy on epoch 969: 0.833\n",
      "test loss on epoch 969: 0.335\n",
      "test accuracy on epoch 969: 0.769\n",
      "train loss on epoch 970 : 0.119\n",
      "train accuracy on epoch 970: 0.944\n",
      "test loss on epoch 970: 0.337\n",
      "test accuracy on epoch 970: 0.769\n",
      "train loss on epoch 971 : 0.312\n",
      "train accuracy on epoch 971: 0.889\n",
      "test loss on epoch 971: 0.340\n",
      "test accuracy on epoch 971: 0.769\n",
      "train loss on epoch 972 : 0.289\n",
      "train accuracy on epoch 972: 0.833\n",
      "test loss on epoch 972: 0.334\n",
      "test accuracy on epoch 972: 0.769\n",
      "train loss on epoch 973 : 0.034\n",
      "train accuracy on epoch 973: 1.000\n",
      "test loss on epoch 973: 0.338\n",
      "test accuracy on epoch 973: 0.769\n",
      "train loss on epoch 974 : 0.076\n",
      "train accuracy on epoch 974: 0.944\n",
      "test loss on epoch 974: 0.333\n",
      "test accuracy on epoch 974: 0.769\n",
      "train loss on epoch 975 : 0.210\n",
      "train accuracy on epoch 975: 0.889\n",
      "test loss on epoch 975: 0.329\n",
      "test accuracy on epoch 975: 0.769\n",
      "train loss on epoch 976 : 0.164\n",
      "train accuracy on epoch 976: 0.944\n",
      "test loss on epoch 976: 0.336\n",
      "test accuracy on epoch 976: 0.769\n",
      "train loss on epoch 977 : 0.300\n",
      "train accuracy on epoch 977: 0.833\n",
      "test loss on epoch 977: 0.333\n",
      "test accuracy on epoch 977: 0.692\n",
      "train loss on epoch 978 : 0.191\n",
      "train accuracy on epoch 978: 0.889\n",
      "test loss on epoch 978: 0.317\n",
      "test accuracy on epoch 978: 0.846\n",
      "train loss on epoch 979 : 0.325\n",
      "train accuracy on epoch 979: 0.889\n",
      "test loss on epoch 979: 0.332\n",
      "test accuracy on epoch 979: 0.692\n",
      "train loss on epoch 980 : 0.364\n",
      "train accuracy on epoch 980: 0.889\n",
      "test loss on epoch 980: 0.322\n",
      "test accuracy on epoch 980: 0.846\n",
      "train loss on epoch 981 : 0.275\n",
      "train accuracy on epoch 981: 0.889\n",
      "test loss on epoch 981: 0.334\n",
      "test accuracy on epoch 981: 0.692\n",
      "train loss on epoch 982 : 0.233\n",
      "train accuracy on epoch 982: 0.833\n",
      "test loss on epoch 982: 0.333\n",
      "test accuracy on epoch 982: 0.769\n",
      "train loss on epoch 983 : 0.113\n",
      "train accuracy on epoch 983: 0.944\n",
      "test loss on epoch 983: 0.337\n",
      "test accuracy on epoch 983: 0.769\n",
      "train loss on epoch 984 : 0.274\n",
      "train accuracy on epoch 984: 0.889\n",
      "test loss on epoch 984: 0.339\n",
      "test accuracy on epoch 984: 0.769\n",
      "train loss on epoch 985 : 0.067\n",
      "train accuracy on epoch 985: 1.000\n",
      "test loss on epoch 985: 0.336\n",
      "test accuracy on epoch 985: 0.769\n",
      "train loss on epoch 986 : 0.105\n",
      "train accuracy on epoch 986: 0.944\n",
      "test loss on epoch 986: 0.347\n",
      "test accuracy on epoch 986: 0.769\n",
      "train loss on epoch 987 : 0.100\n",
      "train accuracy on epoch 987: 0.944\n",
      "test loss on epoch 987: 0.345\n",
      "test accuracy on epoch 987: 0.769\n",
      "train loss on epoch 988 : 0.208\n",
      "train accuracy on epoch 988: 0.889\n",
      "test loss on epoch 988: 0.347\n",
      "test accuracy on epoch 988: 0.769\n",
      "train loss on epoch 989 : 0.133\n",
      "train accuracy on epoch 989: 0.944\n",
      "test loss on epoch 989: 0.344\n",
      "test accuracy on epoch 989: 0.769\n",
      "train loss on epoch 990 : 0.176\n",
      "train accuracy on epoch 990: 0.889\n",
      "test loss on epoch 990: 0.329\n",
      "test accuracy on epoch 990: 0.769\n",
      "train loss on epoch 991 : 0.251\n",
      "train accuracy on epoch 991: 0.833\n",
      "test loss on epoch 991: 0.339\n",
      "test accuracy on epoch 991: 0.692\n",
      "train loss on epoch 992 : 0.383\n",
      "train accuracy on epoch 992: 0.833\n",
      "test loss on epoch 992: 0.322\n",
      "test accuracy on epoch 992: 0.769\n",
      "train loss on epoch 993 : 0.289\n",
      "train accuracy on epoch 993: 0.889\n",
      "test loss on epoch 993: 0.322\n",
      "test accuracy on epoch 993: 0.769\n",
      "train loss on epoch 994 : 0.060\n",
      "train accuracy on epoch 994: 1.000\n",
      "test loss on epoch 994: 0.342\n",
      "test accuracy on epoch 994: 0.769\n",
      "train loss on epoch 995 : 0.081\n",
      "train accuracy on epoch 995: 0.944\n",
      "test loss on epoch 995: 0.337\n",
      "test accuracy on epoch 995: 0.769\n",
      "train loss on epoch 996 : 0.174\n",
      "train accuracy on epoch 996: 0.889\n",
      "test loss on epoch 996: 0.322\n",
      "test accuracy on epoch 996: 0.769\n",
      "train loss on epoch 997 : 0.077\n",
      "train accuracy on epoch 997: 0.944\n",
      "test loss on epoch 997: 0.326\n",
      "test accuracy on epoch 997: 0.769\n",
      "train loss on epoch 998 : 0.163\n",
      "train accuracy on epoch 998: 0.944\n",
      "test loss on epoch 998: 0.331\n",
      "test accuracy on epoch 998: 0.769\n",
      "train loss on epoch 999 : 0.278\n",
      "train accuracy on epoch 999: 0.944\n",
      "test loss on epoch 999: 0.339\n",
      "test accuracy on epoch 999: 0.692\n",
      "train loss on epoch 1000 : 0.240\n",
      "train accuracy on epoch 1000: 0.944\n",
      "test loss on epoch 1000: 0.343\n",
      "test accuracy on epoch 1000: 0.692\n",
      "train loss on epoch 1001 : 0.088\n",
      "train accuracy on epoch 1001: 0.944\n",
      "test loss on epoch 1001: 0.344\n",
      "test accuracy on epoch 1001: 0.769\n",
      "train loss on epoch 1002 : 0.277\n",
      "train accuracy on epoch 1002: 0.889\n",
      "test loss on epoch 1002: 0.330\n",
      "test accuracy on epoch 1002: 0.769\n",
      "train loss on epoch 1003 : 0.152\n",
      "train accuracy on epoch 1003: 0.889\n",
      "test loss on epoch 1003: 0.342\n",
      "test accuracy on epoch 1003: 0.769\n",
      "train loss on epoch 1004 : 0.126\n",
      "train accuracy on epoch 1004: 0.944\n",
      "test loss on epoch 1004: 0.323\n",
      "test accuracy on epoch 1004: 0.769\n",
      "train loss on epoch 1005 : 0.066\n",
      "train accuracy on epoch 1005: 1.000\n",
      "test loss on epoch 1005: 0.329\n",
      "test accuracy on epoch 1005: 0.769\n",
      "train loss on epoch 1006 : 0.131\n",
      "train accuracy on epoch 1006: 0.944\n",
      "test loss on epoch 1006: 0.333\n",
      "test accuracy on epoch 1006: 0.769\n",
      "train loss on epoch 1007 : 0.115\n",
      "train accuracy on epoch 1007: 0.944\n",
      "test loss on epoch 1007: 0.322\n",
      "test accuracy on epoch 1007: 0.846\n",
      "train loss on epoch 1008 : 0.079\n",
      "train accuracy on epoch 1008: 1.000\n",
      "test loss on epoch 1008: 0.331\n",
      "test accuracy on epoch 1008: 0.769\n",
      "train loss on epoch 1009 : 0.219\n",
      "train accuracy on epoch 1009: 0.833\n",
      "test loss on epoch 1009: 0.342\n",
      "test accuracy on epoch 1009: 0.769\n",
      "train loss on epoch 1010 : 0.040\n",
      "train accuracy on epoch 1010: 1.000\n",
      "test loss on epoch 1010: 0.336\n",
      "test accuracy on epoch 1010: 0.769\n",
      "train loss on epoch 1011 : 0.163\n",
      "train accuracy on epoch 1011: 0.944\n",
      "test loss on epoch 1011: 0.332\n",
      "test accuracy on epoch 1011: 0.769\n",
      "train loss on epoch 1012 : 0.161\n",
      "train accuracy on epoch 1012: 0.889\n",
      "test loss on epoch 1012: 0.341\n",
      "test accuracy on epoch 1012: 0.692\n",
      "train loss on epoch 1013 : 0.263\n",
      "train accuracy on epoch 1013: 0.944\n",
      "test loss on epoch 1013: 0.336\n",
      "test accuracy on epoch 1013: 0.769\n",
      "train loss on epoch 1014 : 0.119\n",
      "train accuracy on epoch 1014: 0.944\n",
      "test loss on epoch 1014: 0.344\n",
      "test accuracy on epoch 1014: 0.769\n",
      "train loss on epoch 1015 : 0.186\n",
      "train accuracy on epoch 1015: 0.889\n",
      "test loss on epoch 1015: 0.341\n",
      "test accuracy on epoch 1015: 0.769\n",
      "train loss on epoch 1016 : 0.216\n",
      "train accuracy on epoch 1016: 0.833\n",
      "test loss on epoch 1016: 0.334\n",
      "test accuracy on epoch 1016: 0.769\n",
      "train loss on epoch 1017 : 0.544\n",
      "train accuracy on epoch 1017: 0.833\n",
      "test loss on epoch 1017: 0.334\n",
      "test accuracy on epoch 1017: 0.769\n",
      "train loss on epoch 1018 : 0.435\n",
      "train accuracy on epoch 1018: 0.889\n",
      "test loss on epoch 1018: 0.333\n",
      "test accuracy on epoch 1018: 0.769\n",
      "train loss on epoch 1019 : 0.082\n",
      "train accuracy on epoch 1019: 0.944\n",
      "test loss on epoch 1019: 0.329\n",
      "test accuracy on epoch 1019: 0.846\n",
      "train loss on epoch 1020 : 0.195\n",
      "train accuracy on epoch 1020: 0.944\n",
      "test loss on epoch 1020: 0.327\n",
      "test accuracy on epoch 1020: 0.846\n",
      "train loss on epoch 1021 : 0.129\n",
      "train accuracy on epoch 1021: 0.889\n",
      "test loss on epoch 1021: 0.336\n",
      "test accuracy on epoch 1021: 0.769\n",
      "train loss on epoch 1022 : 0.069\n",
      "train accuracy on epoch 1022: 1.000\n",
      "test loss on epoch 1022: 0.332\n",
      "test accuracy on epoch 1022: 0.769\n",
      "train loss on epoch 1023 : 0.067\n",
      "train accuracy on epoch 1023: 1.000\n",
      "test loss on epoch 1023: 0.323\n",
      "test accuracy on epoch 1023: 0.846\n",
      "train loss on epoch 1024 : 0.085\n",
      "train accuracy on epoch 1024: 1.000\n",
      "test loss on epoch 1024: 0.344\n",
      "test accuracy on epoch 1024: 0.692\n",
      "train loss on epoch 1025 : 0.156\n",
      "train accuracy on epoch 1025: 0.889\n",
      "test loss on epoch 1025: 0.343\n",
      "test accuracy on epoch 1025: 0.692\n",
      "train loss on epoch 1026 : 0.127\n",
      "train accuracy on epoch 1026: 0.944\n",
      "test loss on epoch 1026: 0.332\n",
      "test accuracy on epoch 1026: 0.769\n",
      "train loss on epoch 1027 : 0.057\n",
      "train accuracy on epoch 1027: 1.000\n",
      "test loss on epoch 1027: 0.322\n",
      "test accuracy on epoch 1027: 0.846\n",
      "train loss on epoch 1028 : 0.140\n",
      "train accuracy on epoch 1028: 0.944\n",
      "test loss on epoch 1028: 0.335\n",
      "test accuracy on epoch 1028: 0.769\n",
      "train loss on epoch 1029 : 0.133\n",
      "train accuracy on epoch 1029: 0.944\n",
      "test loss on epoch 1029: 0.324\n",
      "test accuracy on epoch 1029: 0.769\n",
      "train loss on epoch 1030 : 0.096\n",
      "train accuracy on epoch 1030: 1.000\n",
      "test loss on epoch 1030: 0.343\n",
      "test accuracy on epoch 1030: 0.769\n",
      "train loss on epoch 1031 : 0.427\n",
      "train accuracy on epoch 1031: 0.889\n",
      "test loss on epoch 1031: 0.323\n",
      "test accuracy on epoch 1031: 0.769\n",
      "train loss on epoch 1032 : 0.267\n",
      "train accuracy on epoch 1032: 0.889\n",
      "test loss on epoch 1032: 0.342\n",
      "test accuracy on epoch 1032: 0.769\n",
      "train loss on epoch 1033 : 0.216\n",
      "train accuracy on epoch 1033: 0.944\n",
      "test loss on epoch 1033: 0.341\n",
      "test accuracy on epoch 1033: 0.769\n",
      "train loss on epoch 1034 : 0.171\n",
      "train accuracy on epoch 1034: 0.944\n",
      "test loss on epoch 1034: 0.331\n",
      "test accuracy on epoch 1034: 0.769\n",
      "train loss on epoch 1035 : 0.067\n",
      "train accuracy on epoch 1035: 1.000\n",
      "test loss on epoch 1035: 0.341\n",
      "test accuracy on epoch 1035: 0.692\n",
      "train loss on epoch 1036 : 0.245\n",
      "train accuracy on epoch 1036: 0.889\n",
      "test loss on epoch 1036: 0.339\n",
      "test accuracy on epoch 1036: 0.692\n",
      "train loss on epoch 1037 : 0.109\n",
      "train accuracy on epoch 1037: 0.944\n",
      "test loss on epoch 1037: 0.327\n",
      "test accuracy on epoch 1037: 0.846\n",
      "train loss on epoch 1038 : 0.194\n",
      "train accuracy on epoch 1038: 0.944\n",
      "test loss on epoch 1038: 0.322\n",
      "test accuracy on epoch 1038: 0.846\n",
      "train loss on epoch 1039 : 0.357\n",
      "train accuracy on epoch 1039: 0.889\n",
      "test loss on epoch 1039: 0.341\n",
      "test accuracy on epoch 1039: 0.769\n",
      "train loss on epoch 1040 : 0.165\n",
      "train accuracy on epoch 1040: 0.944\n",
      "test loss on epoch 1040: 0.345\n",
      "test accuracy on epoch 1040: 0.692\n",
      "train loss on epoch 1041 : 0.166\n",
      "train accuracy on epoch 1041: 0.889\n",
      "test loss on epoch 1041: 0.329\n",
      "test accuracy on epoch 1041: 0.769\n",
      "train loss on epoch 1042 : 0.060\n",
      "train accuracy on epoch 1042: 1.000\n",
      "test loss on epoch 1042: 0.341\n",
      "test accuracy on epoch 1042: 0.769\n",
      "train loss on epoch 1043 : 0.208\n",
      "train accuracy on epoch 1043: 0.944\n",
      "test loss on epoch 1043: 0.347\n",
      "test accuracy on epoch 1043: 0.769\n",
      "train loss on epoch 1044 : 0.124\n",
      "train accuracy on epoch 1044: 0.944\n",
      "test loss on epoch 1044: 0.336\n",
      "test accuracy on epoch 1044: 0.769\n",
      "train loss on epoch 1045 : 0.288\n",
      "train accuracy on epoch 1045: 0.944\n",
      "test loss on epoch 1045: 0.339\n",
      "test accuracy on epoch 1045: 0.769\n",
      "train loss on epoch 1046 : 0.361\n",
      "train accuracy on epoch 1046: 0.833\n",
      "test loss on epoch 1046: 0.342\n",
      "test accuracy on epoch 1046: 0.769\n",
      "train loss on epoch 1047 : 0.137\n",
      "train accuracy on epoch 1047: 0.944\n",
      "test loss on epoch 1047: 0.349\n",
      "test accuracy on epoch 1047: 0.769\n",
      "train loss on epoch 1048 : 0.237\n",
      "train accuracy on epoch 1048: 0.944\n",
      "test loss on epoch 1048: 0.343\n",
      "test accuracy on epoch 1048: 0.769\n",
      "train loss on epoch 1049 : 0.153\n",
      "train accuracy on epoch 1049: 0.889\n",
      "test loss on epoch 1049: 0.328\n",
      "test accuracy on epoch 1049: 0.769\n",
      "train loss on epoch 1050 : 0.141\n",
      "train accuracy on epoch 1050: 0.944\n",
      "test loss on epoch 1050: 0.336\n",
      "test accuracy on epoch 1050: 0.769\n",
      "train loss on epoch 1051 : 0.329\n",
      "train accuracy on epoch 1051: 0.889\n",
      "test loss on epoch 1051: 0.341\n",
      "test accuracy on epoch 1051: 0.769\n",
      "train loss on epoch 1052 : 0.111\n",
      "train accuracy on epoch 1052: 0.944\n",
      "test loss on epoch 1052: 0.325\n",
      "test accuracy on epoch 1052: 0.769\n",
      "train loss on epoch 1053 : 0.247\n",
      "train accuracy on epoch 1053: 0.833\n",
      "test loss on epoch 1053: 0.338\n",
      "test accuracy on epoch 1053: 0.692\n",
      "train loss on epoch 1054 : 0.113\n",
      "train accuracy on epoch 1054: 0.889\n",
      "test loss on epoch 1054: 0.331\n",
      "test accuracy on epoch 1054: 0.769\n",
      "train loss on epoch 1055 : 0.355\n",
      "train accuracy on epoch 1055: 0.889\n",
      "test loss on epoch 1055: 0.338\n",
      "test accuracy on epoch 1055: 0.769\n",
      "train loss on epoch 1056 : 0.156\n",
      "train accuracy on epoch 1056: 0.944\n",
      "test loss on epoch 1056: 0.338\n",
      "test accuracy on epoch 1056: 0.692\n",
      "train loss on epoch 1057 : 0.176\n",
      "train accuracy on epoch 1057: 0.944\n",
      "test loss on epoch 1057: 0.336\n",
      "test accuracy on epoch 1057: 0.769\n",
      "train loss on epoch 1058 : 0.092\n",
      "train accuracy on epoch 1058: 0.944\n",
      "test loss on epoch 1058: 0.344\n",
      "test accuracy on epoch 1058: 0.769\n",
      "train loss on epoch 1059 : 0.153\n",
      "train accuracy on epoch 1059: 0.944\n",
      "test loss on epoch 1059: 0.337\n",
      "test accuracy on epoch 1059: 0.769\n",
      "train loss on epoch 1060 : 0.081\n",
      "train accuracy on epoch 1060: 0.944\n",
      "test loss on epoch 1060: 0.344\n",
      "test accuracy on epoch 1060: 0.692\n",
      "train loss on epoch 1061 : 0.228\n",
      "train accuracy on epoch 1061: 0.944\n",
      "test loss on epoch 1061: 0.335\n",
      "test accuracy on epoch 1061: 0.769\n",
      "train loss on epoch 1062 : 0.094\n",
      "train accuracy on epoch 1062: 1.000\n",
      "test loss on epoch 1062: 0.342\n",
      "test accuracy on epoch 1062: 0.769\n",
      "train loss on epoch 1063 : 0.165\n",
      "train accuracy on epoch 1063: 0.889\n",
      "test loss on epoch 1063: 0.335\n",
      "test accuracy on epoch 1063: 0.769\n",
      "train loss on epoch 1064 : 0.233\n",
      "train accuracy on epoch 1064: 0.833\n",
      "test loss on epoch 1064: 0.340\n",
      "test accuracy on epoch 1064: 0.769\n",
      "train loss on epoch 1065 : 0.216\n",
      "train accuracy on epoch 1065: 0.889\n",
      "test loss on epoch 1065: 0.343\n",
      "test accuracy on epoch 1065: 0.769\n",
      "train loss on epoch 1066 : 0.184\n",
      "train accuracy on epoch 1066: 0.889\n",
      "test loss on epoch 1066: 0.332\n",
      "test accuracy on epoch 1066: 0.769\n",
      "train loss on epoch 1067 : 0.111\n",
      "train accuracy on epoch 1067: 0.944\n",
      "test loss on epoch 1067: 0.347\n",
      "test accuracy on epoch 1067: 0.769\n",
      "train loss on epoch 1068 : 0.258\n",
      "train accuracy on epoch 1068: 0.833\n",
      "test loss on epoch 1068: 0.330\n",
      "test accuracy on epoch 1068: 0.769\n",
      "train loss on epoch 1069 : 0.264\n",
      "train accuracy on epoch 1069: 0.944\n",
      "test loss on epoch 1069: 0.331\n",
      "test accuracy on epoch 1069: 0.769\n",
      "train loss on epoch 1070 : 0.081\n",
      "train accuracy on epoch 1070: 1.000\n",
      "test loss on epoch 1070: 0.323\n",
      "test accuracy on epoch 1070: 0.769\n",
      "train loss on epoch 1071 : 0.256\n",
      "train accuracy on epoch 1071: 0.944\n",
      "test loss on epoch 1071: 0.349\n",
      "test accuracy on epoch 1071: 0.769\n",
      "train loss on epoch 1072 : 0.190\n",
      "train accuracy on epoch 1072: 0.889\n",
      "test loss on epoch 1072: 0.326\n",
      "test accuracy on epoch 1072: 0.769\n",
      "train loss on epoch 1073 : 0.067\n",
      "train accuracy on epoch 1073: 1.000\n",
      "test loss on epoch 1073: 0.327\n",
      "test accuracy on epoch 1073: 0.769\n",
      "train loss on epoch 1074 : 0.198\n",
      "train accuracy on epoch 1074: 0.889\n",
      "test loss on epoch 1074: 0.349\n",
      "test accuracy on epoch 1074: 0.769\n",
      "train loss on epoch 1075 : 0.455\n",
      "train accuracy on epoch 1075: 0.833\n",
      "test loss on epoch 1075: 0.346\n",
      "test accuracy on epoch 1075: 0.769\n",
      "train loss on epoch 1076 : 0.135\n",
      "train accuracy on epoch 1076: 0.889\n",
      "test loss on epoch 1076: 0.325\n",
      "test accuracy on epoch 1076: 0.769\n",
      "train loss on epoch 1077 : 0.209\n",
      "train accuracy on epoch 1077: 0.889\n",
      "test loss on epoch 1077: 0.340\n",
      "test accuracy on epoch 1077: 0.769\n",
      "train loss on epoch 1078 : 0.084\n",
      "train accuracy on epoch 1078: 0.944\n",
      "test loss on epoch 1078: 0.321\n",
      "test accuracy on epoch 1078: 0.769\n",
      "train loss on epoch 1079 : 0.152\n",
      "train accuracy on epoch 1079: 0.944\n",
      "test loss on epoch 1079: 0.321\n",
      "test accuracy on epoch 1079: 0.769\n",
      "train loss on epoch 1080 : 0.121\n",
      "train accuracy on epoch 1080: 0.944\n",
      "test loss on epoch 1080: 0.322\n",
      "test accuracy on epoch 1080: 0.769\n",
      "train loss on epoch 1081 : 0.162\n",
      "train accuracy on epoch 1081: 0.944\n",
      "test loss on epoch 1081: 0.330\n",
      "test accuracy on epoch 1081: 0.769\n",
      "train loss on epoch 1082 : 0.051\n",
      "train accuracy on epoch 1082: 1.000\n",
      "test loss on epoch 1082: 0.321\n",
      "test accuracy on epoch 1082: 0.769\n",
      "train loss on epoch 1083 : 0.299\n",
      "train accuracy on epoch 1083: 0.889\n",
      "test loss on epoch 1083: 0.323\n",
      "test accuracy on epoch 1083: 0.769\n",
      "train loss on epoch 1084 : 0.245\n",
      "train accuracy on epoch 1084: 0.889\n",
      "test loss on epoch 1084: 0.327\n",
      "test accuracy on epoch 1084: 0.769\n",
      "train loss on epoch 1085 : 0.140\n",
      "train accuracy on epoch 1085: 0.944\n",
      "test loss on epoch 1085: 0.357\n",
      "test accuracy on epoch 1085: 0.769\n",
      "train loss on epoch 1086 : 0.085\n",
      "train accuracy on epoch 1086: 0.944\n",
      "test loss on epoch 1086: 0.340\n",
      "test accuracy on epoch 1086: 0.769\n",
      "train loss on epoch 1087 : 0.265\n",
      "train accuracy on epoch 1087: 0.944\n",
      "test loss on epoch 1087: 0.325\n",
      "test accuracy on epoch 1087: 0.769\n",
      "train loss on epoch 1088 : 0.187\n",
      "train accuracy on epoch 1088: 0.944\n",
      "test loss on epoch 1088: 0.322\n",
      "test accuracy on epoch 1088: 0.769\n",
      "train loss on epoch 1089 : 0.266\n",
      "train accuracy on epoch 1089: 0.889\n",
      "test loss on epoch 1089: 0.319\n",
      "test accuracy on epoch 1089: 0.769\n",
      "train loss on epoch 1090 : 0.165\n",
      "train accuracy on epoch 1090: 0.889\n",
      "test loss on epoch 1090: 0.321\n",
      "test accuracy on epoch 1090: 0.769\n",
      "train loss on epoch 1091 : 0.286\n",
      "train accuracy on epoch 1091: 0.889\n",
      "test loss on epoch 1091: 0.341\n",
      "test accuracy on epoch 1091: 0.769\n",
      "train loss on epoch 1092 : 0.238\n",
      "train accuracy on epoch 1092: 0.889\n",
      "test loss on epoch 1092: 0.318\n",
      "test accuracy on epoch 1092: 0.769\n",
      "train loss on epoch 1093 : 0.141\n",
      "train accuracy on epoch 1093: 0.944\n",
      "test loss on epoch 1093: 0.327\n",
      "test accuracy on epoch 1093: 0.769\n",
      "train loss on epoch 1094 : 0.130\n",
      "train accuracy on epoch 1094: 0.944\n",
      "test loss on epoch 1094: 0.327\n",
      "test accuracy on epoch 1094: 0.769\n",
      "train loss on epoch 1095 : 0.190\n",
      "train accuracy on epoch 1095: 0.944\n",
      "test loss on epoch 1095: 0.337\n",
      "test accuracy on epoch 1095: 0.692\n",
      "train loss on epoch 1096 : 0.318\n",
      "train accuracy on epoch 1096: 0.833\n",
      "test loss on epoch 1096: 0.337\n",
      "test accuracy on epoch 1096: 0.692\n",
      "train loss on epoch 1097 : 0.055\n",
      "train accuracy on epoch 1097: 1.000\n",
      "test loss on epoch 1097: 0.320\n",
      "test accuracy on epoch 1097: 0.846\n",
      "train loss on epoch 1098 : 0.053\n",
      "train accuracy on epoch 1098: 1.000\n",
      "test loss on epoch 1098: 0.317\n",
      "test accuracy on epoch 1098: 0.846\n",
      "train loss on epoch 1099 : 0.074\n",
      "train accuracy on epoch 1099: 1.000\n",
      "test loss on epoch 1099: 0.331\n",
      "test accuracy on epoch 1099: 0.692\n",
      "train loss on epoch 1100 : 0.160\n",
      "train accuracy on epoch 1100: 0.944\n",
      "test loss on epoch 1100: 0.332\n",
      "test accuracy on epoch 1100: 0.692\n",
      "train loss on epoch 1101 : 0.162\n",
      "train accuracy on epoch 1101: 0.944\n",
      "test loss on epoch 1101: 0.323\n",
      "test accuracy on epoch 1101: 0.769\n",
      "train loss on epoch 1102 : 0.202\n",
      "train accuracy on epoch 1102: 0.889\n",
      "test loss on epoch 1102: 0.319\n",
      "test accuracy on epoch 1102: 0.846\n",
      "train loss on epoch 1103 : 0.326\n",
      "train accuracy on epoch 1103: 0.889\n",
      "test loss on epoch 1103: 0.319\n",
      "test accuracy on epoch 1103: 0.769\n",
      "train loss on epoch 1104 : 0.232\n",
      "train accuracy on epoch 1104: 0.944\n",
      "test loss on epoch 1104: 0.337\n",
      "test accuracy on epoch 1104: 0.769\n",
      "train loss on epoch 1105 : 0.497\n",
      "train accuracy on epoch 1105: 0.889\n",
      "test loss on epoch 1105: 0.332\n",
      "test accuracy on epoch 1105: 0.769\n",
      "train loss on epoch 1106 : 0.304\n",
      "train accuracy on epoch 1106: 0.833\n",
      "test loss on epoch 1106: 0.338\n",
      "test accuracy on epoch 1106: 0.769\n",
      "train loss on epoch 1107 : 0.262\n",
      "train accuracy on epoch 1107: 0.889\n",
      "test loss on epoch 1107: 0.341\n",
      "test accuracy on epoch 1107: 0.769\n",
      "train loss on epoch 1108 : 0.331\n",
      "train accuracy on epoch 1108: 0.889\n",
      "test loss on epoch 1108: 0.335\n",
      "test accuracy on epoch 1108: 0.769\n",
      "train loss on epoch 1109 : 0.040\n",
      "train accuracy on epoch 1109: 1.000\n",
      "test loss on epoch 1109: 0.336\n",
      "test accuracy on epoch 1109: 0.769\n",
      "train loss on epoch 1110 : 0.178\n",
      "train accuracy on epoch 1110: 0.889\n",
      "test loss on epoch 1110: 0.333\n",
      "test accuracy on epoch 1110: 0.769\n",
      "train loss on epoch 1111 : 0.061\n",
      "train accuracy on epoch 1111: 0.944\n",
      "test loss on epoch 1111: 0.337\n",
      "test accuracy on epoch 1111: 0.692\n",
      "train loss on epoch 1112 : 0.159\n",
      "train accuracy on epoch 1112: 0.944\n",
      "test loss on epoch 1112: 0.322\n",
      "test accuracy on epoch 1112: 0.846\n",
      "train loss on epoch 1113 : 0.104\n",
      "train accuracy on epoch 1113: 1.000\n",
      "test loss on epoch 1113: 0.326\n",
      "test accuracy on epoch 1113: 0.846\n",
      "train loss on epoch 1114 : 0.106\n",
      "train accuracy on epoch 1114: 1.000\n",
      "test loss on epoch 1114: 0.331\n",
      "test accuracy on epoch 1114: 0.769\n",
      "train loss on epoch 1115 : 0.131\n",
      "train accuracy on epoch 1115: 0.889\n",
      "test loss on epoch 1115: 0.324\n",
      "test accuracy on epoch 1115: 0.846\n",
      "train loss on epoch 1116 : 0.150\n",
      "train accuracy on epoch 1116: 0.944\n",
      "test loss on epoch 1116: 0.328\n",
      "test accuracy on epoch 1116: 0.769\n",
      "train loss on epoch 1117 : 0.292\n",
      "train accuracy on epoch 1117: 0.833\n",
      "test loss on epoch 1117: 0.317\n",
      "test accuracy on epoch 1117: 0.769\n",
      "train loss on epoch 1118 : 0.278\n",
      "train accuracy on epoch 1118: 0.889\n",
      "test loss on epoch 1118: 0.319\n",
      "test accuracy on epoch 1118: 0.769\n",
      "train loss on epoch 1119 : 0.256\n",
      "train accuracy on epoch 1119: 0.889\n",
      "test loss on epoch 1119: 0.322\n",
      "test accuracy on epoch 1119: 0.769\n",
      "train loss on epoch 1120 : 0.155\n",
      "train accuracy on epoch 1120: 0.944\n",
      "test loss on epoch 1120: 0.328\n",
      "test accuracy on epoch 1120: 0.769\n",
      "train loss on epoch 1121 : 0.123\n",
      "train accuracy on epoch 1121: 0.944\n",
      "test loss on epoch 1121: 0.326\n",
      "test accuracy on epoch 1121: 0.769\n",
      "train loss on epoch 1122 : 0.120\n",
      "train accuracy on epoch 1122: 0.944\n",
      "test loss on epoch 1122: 0.323\n",
      "test accuracy on epoch 1122: 0.769\n",
      "train loss on epoch 1123 : 0.358\n",
      "train accuracy on epoch 1123: 0.889\n",
      "test loss on epoch 1123: 0.340\n",
      "test accuracy on epoch 1123: 0.769\n",
      "train loss on epoch 1124 : 0.058\n",
      "train accuracy on epoch 1124: 1.000\n",
      "test loss on epoch 1124: 0.331\n",
      "test accuracy on epoch 1124: 0.769\n",
      "train loss on epoch 1125 : 0.059\n",
      "train accuracy on epoch 1125: 1.000\n",
      "test loss on epoch 1125: 0.343\n",
      "test accuracy on epoch 1125: 0.769\n",
      "train loss on epoch 1126 : 0.166\n",
      "train accuracy on epoch 1126: 0.889\n",
      "test loss on epoch 1126: 0.345\n",
      "test accuracy on epoch 1126: 0.769\n",
      "train loss on epoch 1127 : 0.164\n",
      "train accuracy on epoch 1127: 0.944\n",
      "test loss on epoch 1127: 0.338\n",
      "test accuracy on epoch 1127: 0.769\n",
      "train loss on epoch 1128 : 0.238\n",
      "train accuracy on epoch 1128: 0.833\n",
      "test loss on epoch 1128: 0.345\n",
      "test accuracy on epoch 1128: 0.769\n",
      "train loss on epoch 1129 : 0.393\n",
      "train accuracy on epoch 1129: 0.889\n",
      "test loss on epoch 1129: 0.323\n",
      "test accuracy on epoch 1129: 0.769\n",
      "train loss on epoch 1130 : 0.296\n",
      "train accuracy on epoch 1130: 0.944\n",
      "test loss on epoch 1130: 0.337\n",
      "test accuracy on epoch 1130: 0.769\n",
      "train loss on epoch 1131 : 0.180\n",
      "train accuracy on epoch 1131: 0.944\n",
      "test loss on epoch 1131: 0.321\n",
      "test accuracy on epoch 1131: 0.769\n",
      "train loss on epoch 1132 : 0.254\n",
      "train accuracy on epoch 1132: 0.944\n",
      "test loss on epoch 1132: 0.325\n",
      "test accuracy on epoch 1132: 0.769\n",
      "train loss on epoch 1133 : 0.284\n",
      "train accuracy on epoch 1133: 0.889\n",
      "test loss on epoch 1133: 0.321\n",
      "test accuracy on epoch 1133: 0.846\n",
      "train loss on epoch 1134 : 0.197\n",
      "train accuracy on epoch 1134: 0.833\n",
      "test loss on epoch 1134: 0.338\n",
      "test accuracy on epoch 1134: 0.692\n",
      "train loss on epoch 1135 : 0.128\n",
      "train accuracy on epoch 1135: 0.889\n",
      "test loss on epoch 1135: 0.335\n",
      "test accuracy on epoch 1135: 0.769\n",
      "train loss on epoch 1136 : 0.048\n",
      "train accuracy on epoch 1136: 1.000\n",
      "test loss on epoch 1136: 0.331\n",
      "test accuracy on epoch 1136: 0.769\n",
      "train loss on epoch 1137 : 0.290\n",
      "train accuracy on epoch 1137: 0.833\n",
      "test loss on epoch 1137: 0.326\n",
      "test accuracy on epoch 1137: 0.769\n",
      "train loss on epoch 1138 : 0.179\n",
      "train accuracy on epoch 1138: 0.889\n",
      "test loss on epoch 1138: 0.333\n",
      "test accuracy on epoch 1138: 0.769\n",
      "train loss on epoch 1139 : 0.205\n",
      "train accuracy on epoch 1139: 0.833\n",
      "test loss on epoch 1139: 0.335\n",
      "test accuracy on epoch 1139: 0.769\n",
      "train loss on epoch 1140 : 0.256\n",
      "train accuracy on epoch 1140: 0.889\n",
      "test loss on epoch 1140: 0.335\n",
      "test accuracy on epoch 1140: 0.769\n",
      "train loss on epoch 1141 : 0.068\n",
      "train accuracy on epoch 1141: 1.000\n",
      "test loss on epoch 1141: 0.338\n",
      "test accuracy on epoch 1141: 0.769\n",
      "train loss on epoch 1142 : 0.164\n",
      "train accuracy on epoch 1142: 0.944\n",
      "test loss on epoch 1142: 0.346\n",
      "test accuracy on epoch 1142: 0.769\n",
      "train loss on epoch 1143 : 0.307\n",
      "train accuracy on epoch 1143: 0.889\n",
      "test loss on epoch 1143: 0.343\n",
      "test accuracy on epoch 1143: 0.769\n",
      "train loss on epoch 1144 : 0.065\n",
      "train accuracy on epoch 1144: 0.944\n",
      "test loss on epoch 1144: 0.339\n",
      "test accuracy on epoch 1144: 0.769\n",
      "train loss on epoch 1145 : 0.039\n",
      "train accuracy on epoch 1145: 1.000\n",
      "test loss on epoch 1145: 0.335\n",
      "test accuracy on epoch 1145: 0.769\n",
      "train loss on epoch 1146 : 0.171\n",
      "train accuracy on epoch 1146: 1.000\n",
      "test loss on epoch 1146: 0.318\n",
      "test accuracy on epoch 1146: 0.769\n",
      "train loss on epoch 1147 : 0.114\n",
      "train accuracy on epoch 1147: 0.944\n",
      "test loss on epoch 1147: 0.328\n",
      "test accuracy on epoch 1147: 0.769\n",
      "train loss on epoch 1148 : 0.299\n",
      "train accuracy on epoch 1148: 0.944\n",
      "test loss on epoch 1148: 0.324\n",
      "test accuracy on epoch 1148: 0.769\n",
      "train loss on epoch 1149 : 0.206\n",
      "train accuracy on epoch 1149: 0.889\n",
      "test loss on epoch 1149: 0.340\n",
      "test accuracy on epoch 1149: 0.769\n",
      "train loss on epoch 1150 : 0.150\n",
      "train accuracy on epoch 1150: 0.944\n",
      "test loss on epoch 1150: 0.339\n",
      "test accuracy on epoch 1150: 0.769\n",
      "train loss on epoch 1151 : 0.124\n",
      "train accuracy on epoch 1151: 0.944\n",
      "test loss on epoch 1151: 0.345\n",
      "test accuracy on epoch 1151: 0.769\n",
      "train loss on epoch 1152 : 0.261\n",
      "train accuracy on epoch 1152: 0.944\n",
      "test loss on epoch 1152: 0.353\n",
      "test accuracy on epoch 1152: 0.769\n",
      "train loss on epoch 1153 : 0.103\n",
      "train accuracy on epoch 1153: 1.000\n",
      "test loss on epoch 1153: 0.348\n",
      "test accuracy on epoch 1153: 0.769\n",
      "train loss on epoch 1154 : 0.054\n",
      "train accuracy on epoch 1154: 1.000\n",
      "test loss on epoch 1154: 0.360\n",
      "test accuracy on epoch 1154: 0.769\n",
      "train loss on epoch 1155 : 0.032\n",
      "train accuracy on epoch 1155: 1.000\n",
      "test loss on epoch 1155: 0.362\n",
      "test accuracy on epoch 1155: 0.769\n",
      "train loss on epoch 1156 : 0.119\n",
      "train accuracy on epoch 1156: 0.944\n",
      "test loss on epoch 1156: 0.355\n",
      "test accuracy on epoch 1156: 0.769\n",
      "train loss on epoch 1157 : 0.221\n",
      "train accuracy on epoch 1157: 0.889\n",
      "test loss on epoch 1157: 0.361\n",
      "test accuracy on epoch 1157: 0.769\n",
      "train loss on epoch 1158 : 0.153\n",
      "train accuracy on epoch 1158: 0.889\n",
      "test loss on epoch 1158: 0.357\n",
      "test accuracy on epoch 1158: 0.769\n",
      "train loss on epoch 1159 : 0.081\n",
      "train accuracy on epoch 1159: 1.000\n",
      "test loss on epoch 1159: 0.356\n",
      "test accuracy on epoch 1159: 0.769\n",
      "train loss on epoch 1160 : 0.071\n",
      "train accuracy on epoch 1160: 1.000\n",
      "test loss on epoch 1160: 0.359\n",
      "test accuracy on epoch 1160: 0.769\n",
      "train loss on epoch 1161 : 0.463\n",
      "train accuracy on epoch 1161: 0.778\n",
      "test loss on epoch 1161: 0.354\n",
      "test accuracy on epoch 1161: 0.769\n",
      "train loss on epoch 1162 : 0.227\n",
      "train accuracy on epoch 1162: 0.944\n",
      "test loss on epoch 1162: 0.347\n",
      "test accuracy on epoch 1162: 0.769\n",
      "train loss on epoch 1163 : 0.270\n",
      "train accuracy on epoch 1163: 0.833\n",
      "test loss on epoch 1163: 0.340\n",
      "test accuracy on epoch 1163: 0.769\n",
      "train loss on epoch 1164 : 0.171\n",
      "train accuracy on epoch 1164: 0.833\n",
      "test loss on epoch 1164: 0.341\n",
      "test accuracy on epoch 1164: 0.769\n",
      "train loss on epoch 1165 : 0.378\n",
      "train accuracy on epoch 1165: 0.889\n",
      "test loss on epoch 1165: 0.329\n",
      "test accuracy on epoch 1165: 0.769\n",
      "train loss on epoch 1166 : 0.159\n",
      "train accuracy on epoch 1166: 0.944\n",
      "test loss on epoch 1166: 0.323\n",
      "test accuracy on epoch 1166: 0.769\n",
      "train loss on epoch 1167 : 0.071\n",
      "train accuracy on epoch 1167: 1.000\n",
      "test loss on epoch 1167: 0.332\n",
      "test accuracy on epoch 1167: 0.769\n",
      "train loss on epoch 1168 : 0.134\n",
      "train accuracy on epoch 1168: 0.944\n",
      "test loss on epoch 1168: 0.333\n",
      "test accuracy on epoch 1168: 0.769\n",
      "train loss on epoch 1169 : 0.302\n",
      "train accuracy on epoch 1169: 0.889\n",
      "test loss on epoch 1169: 0.325\n",
      "test accuracy on epoch 1169: 0.769\n",
      "train loss on epoch 1170 : 0.110\n",
      "train accuracy on epoch 1170: 1.000\n",
      "test loss on epoch 1170: 0.318\n",
      "test accuracy on epoch 1170: 0.769\n",
      "train loss on epoch 1171 : 0.085\n",
      "train accuracy on epoch 1171: 1.000\n",
      "test loss on epoch 1171: 0.325\n",
      "test accuracy on epoch 1171: 0.769\n",
      "train loss on epoch 1172 : 0.199\n",
      "train accuracy on epoch 1172: 0.944\n",
      "test loss on epoch 1172: 0.323\n",
      "test accuracy on epoch 1172: 0.769\n",
      "train loss on epoch 1173 : 0.336\n",
      "train accuracy on epoch 1173: 0.833\n",
      "test loss on epoch 1173: 0.339\n",
      "test accuracy on epoch 1173: 0.769\n",
      "train loss on epoch 1174 : 0.078\n",
      "train accuracy on epoch 1174: 1.000\n",
      "test loss on epoch 1174: 0.342\n",
      "test accuracy on epoch 1174: 0.769\n",
      "train loss on epoch 1175 : 0.242\n",
      "train accuracy on epoch 1175: 0.889\n",
      "test loss on epoch 1175: 0.348\n",
      "test accuracy on epoch 1175: 0.769\n",
      "train loss on epoch 1176 : 0.125\n",
      "train accuracy on epoch 1176: 0.944\n",
      "test loss on epoch 1176: 0.344\n",
      "test accuracy on epoch 1176: 0.769\n",
      "train loss on epoch 1177 : 0.380\n",
      "train accuracy on epoch 1177: 0.889\n",
      "test loss on epoch 1177: 0.347\n",
      "test accuracy on epoch 1177: 0.769\n",
      "train loss on epoch 1178 : 0.082\n",
      "train accuracy on epoch 1178: 0.944\n",
      "test loss on epoch 1178: 0.353\n",
      "test accuracy on epoch 1178: 0.769\n",
      "train loss on epoch 1179 : 0.075\n",
      "train accuracy on epoch 1179: 1.000\n",
      "test loss on epoch 1179: 0.354\n",
      "test accuracy on epoch 1179: 0.769\n",
      "train loss on epoch 1180 : 0.151\n",
      "train accuracy on epoch 1180: 0.944\n",
      "test loss on epoch 1180: 0.346\n",
      "test accuracy on epoch 1180: 0.769\n",
      "train loss on epoch 1181 : 0.258\n",
      "train accuracy on epoch 1181: 0.833\n",
      "test loss on epoch 1181: 0.347\n",
      "test accuracy on epoch 1181: 0.769\n",
      "train loss on epoch 1182 : 0.121\n",
      "train accuracy on epoch 1182: 0.889\n",
      "test loss on epoch 1182: 0.341\n",
      "test accuracy on epoch 1182: 0.769\n",
      "train loss on epoch 1183 : 0.146\n",
      "train accuracy on epoch 1183: 0.944\n",
      "test loss on epoch 1183: 0.338\n",
      "test accuracy on epoch 1183: 0.769\n",
      "train loss on epoch 1184 : 0.426\n",
      "train accuracy on epoch 1184: 0.833\n",
      "test loss on epoch 1184: 0.345\n",
      "test accuracy on epoch 1184: 0.769\n",
      "train loss on epoch 1185 : 0.106\n",
      "train accuracy on epoch 1185: 1.000\n",
      "test loss on epoch 1185: 0.336\n",
      "test accuracy on epoch 1185: 0.769\n",
      "train loss on epoch 1186 : 0.164\n",
      "train accuracy on epoch 1186: 0.889\n",
      "test loss on epoch 1186: 0.338\n",
      "test accuracy on epoch 1186: 0.769\n",
      "train loss on epoch 1187 : 0.300\n",
      "train accuracy on epoch 1187: 0.944\n",
      "test loss on epoch 1187: 0.342\n",
      "test accuracy on epoch 1187: 0.769\n",
      "train loss on epoch 1188 : 0.491\n",
      "train accuracy on epoch 1188: 0.833\n",
      "test loss on epoch 1188: 0.343\n",
      "test accuracy on epoch 1188: 0.769\n",
      "train loss on epoch 1189 : 0.083\n",
      "train accuracy on epoch 1189: 1.000\n",
      "test loss on epoch 1189: 0.344\n",
      "test accuracy on epoch 1189: 0.769\n",
      "train loss on epoch 1190 : 0.233\n",
      "train accuracy on epoch 1190: 0.889\n",
      "test loss on epoch 1190: 0.340\n",
      "test accuracy on epoch 1190: 0.769\n",
      "train loss on epoch 1191 : 0.123\n",
      "train accuracy on epoch 1191: 0.944\n",
      "test loss on epoch 1191: 0.348\n",
      "test accuracy on epoch 1191: 0.769\n",
      "train loss on epoch 1192 : 0.292\n",
      "train accuracy on epoch 1192: 0.944\n",
      "test loss on epoch 1192: 0.357\n",
      "test accuracy on epoch 1192: 0.769\n",
      "train loss on epoch 1193 : 0.050\n",
      "train accuracy on epoch 1193: 1.000\n",
      "test loss on epoch 1193: 0.364\n",
      "test accuracy on epoch 1193: 0.769\n",
      "train loss on epoch 1194 : 0.182\n",
      "train accuracy on epoch 1194: 0.944\n",
      "test loss on epoch 1194: 0.362\n",
      "test accuracy on epoch 1194: 0.769\n",
      "train loss on epoch 1195 : 0.135\n",
      "train accuracy on epoch 1195: 0.944\n",
      "test loss on epoch 1195: 0.359\n",
      "test accuracy on epoch 1195: 0.769\n",
      "train loss on epoch 1196 : 0.111\n",
      "train accuracy on epoch 1196: 0.944\n",
      "test loss on epoch 1196: 0.354\n",
      "test accuracy on epoch 1196: 0.769\n",
      "train loss on epoch 1197 : 0.085\n",
      "train accuracy on epoch 1197: 1.000\n",
      "test loss on epoch 1197: 0.342\n",
      "test accuracy on epoch 1197: 0.769\n",
      "train loss on epoch 1198 : 0.162\n",
      "train accuracy on epoch 1198: 0.944\n",
      "test loss on epoch 1198: 0.345\n",
      "test accuracy on epoch 1198: 0.769\n",
      "train loss on epoch 1199 : 0.197\n",
      "train accuracy on epoch 1199: 0.944\n",
      "test loss on epoch 1199: 0.349\n",
      "test accuracy on epoch 1199: 0.769\n",
      "train loss on epoch 1200 : 0.184\n",
      "train accuracy on epoch 1200: 0.944\n",
      "test loss on epoch 1200: 0.346\n",
      "test accuracy on epoch 1200: 0.769\n",
      "train loss on epoch 1201 : 0.101\n",
      "train accuracy on epoch 1201: 0.944\n",
      "test loss on epoch 1201: 0.345\n",
      "test accuracy on epoch 1201: 0.769\n",
      "train loss on epoch 1202 : 0.418\n",
      "train accuracy on epoch 1202: 0.889\n",
      "test loss on epoch 1202: 0.344\n",
      "test accuracy on epoch 1202: 0.769\n",
      "train loss on epoch 1203 : 0.391\n",
      "train accuracy on epoch 1203: 0.889\n",
      "test loss on epoch 1203: 0.342\n",
      "test accuracy on epoch 1203: 0.769\n",
      "train loss on epoch 1204 : 0.102\n",
      "train accuracy on epoch 1204: 0.944\n",
      "test loss on epoch 1204: 0.349\n",
      "test accuracy on epoch 1204: 0.769\n",
      "train loss on epoch 1205 : 0.080\n",
      "train accuracy on epoch 1205: 0.944\n",
      "test loss on epoch 1205: 0.348\n",
      "test accuracy on epoch 1205: 0.769\n",
      "train loss on epoch 1206 : 0.168\n",
      "train accuracy on epoch 1206: 0.944\n",
      "test loss on epoch 1206: 0.348\n",
      "test accuracy on epoch 1206: 0.769\n",
      "train loss on epoch 1207 : 0.166\n",
      "train accuracy on epoch 1207: 0.889\n",
      "test loss on epoch 1207: 0.349\n",
      "test accuracy on epoch 1207: 0.769\n",
      "train loss on epoch 1208 : 0.151\n",
      "train accuracy on epoch 1208: 0.889\n",
      "test loss on epoch 1208: 0.351\n",
      "test accuracy on epoch 1208: 0.769\n",
      "train loss on epoch 1209 : 0.131\n",
      "train accuracy on epoch 1209: 0.944\n",
      "test loss on epoch 1209: 0.354\n",
      "test accuracy on epoch 1209: 0.769\n",
      "train loss on epoch 1210 : 0.174\n",
      "train accuracy on epoch 1210: 0.944\n",
      "test loss on epoch 1210: 0.349\n",
      "test accuracy on epoch 1210: 0.769\n",
      "train loss on epoch 1211 : 0.185\n",
      "train accuracy on epoch 1211: 0.889\n",
      "test loss on epoch 1211: 0.347\n",
      "test accuracy on epoch 1211: 0.769\n",
      "train loss on epoch 1212 : 0.152\n",
      "train accuracy on epoch 1212: 0.944\n",
      "test loss on epoch 1212: 0.347\n",
      "test accuracy on epoch 1212: 0.769\n",
      "train loss on epoch 1213 : 0.226\n",
      "train accuracy on epoch 1213: 0.889\n",
      "test loss on epoch 1213: 0.347\n",
      "test accuracy on epoch 1213: 0.769\n",
      "train loss on epoch 1214 : 0.168\n",
      "train accuracy on epoch 1214: 0.889\n",
      "test loss on epoch 1214: 0.349\n",
      "test accuracy on epoch 1214: 0.769\n",
      "train loss on epoch 1215 : 0.168\n",
      "train accuracy on epoch 1215: 0.889\n",
      "test loss on epoch 1215: 0.344\n",
      "test accuracy on epoch 1215: 0.769\n",
      "train loss on epoch 1216 : 0.313\n",
      "train accuracy on epoch 1216: 0.889\n",
      "test loss on epoch 1216: 0.344\n",
      "test accuracy on epoch 1216: 0.769\n",
      "train loss on epoch 1217 : 0.237\n",
      "train accuracy on epoch 1217: 0.833\n",
      "test loss on epoch 1217: 0.345\n",
      "test accuracy on epoch 1217: 0.769\n",
      "train loss on epoch 1218 : 0.306\n",
      "train accuracy on epoch 1218: 0.833\n",
      "test loss on epoch 1218: 0.343\n",
      "test accuracy on epoch 1218: 0.769\n",
      "train loss on epoch 1219 : 0.131\n",
      "train accuracy on epoch 1219: 0.944\n",
      "test loss on epoch 1219: 0.341\n",
      "test accuracy on epoch 1219: 0.769\n",
      "train loss on epoch 1220 : 0.063\n",
      "train accuracy on epoch 1220: 1.000\n",
      "test loss on epoch 1220: 0.345\n",
      "test accuracy on epoch 1220: 0.769\n",
      "train loss on epoch 1221 : 0.124\n",
      "train accuracy on epoch 1221: 0.944\n",
      "test loss on epoch 1221: 0.341\n",
      "test accuracy on epoch 1221: 0.769\n",
      "train loss on epoch 1222 : 0.061\n",
      "train accuracy on epoch 1222: 1.000\n",
      "test loss on epoch 1222: 0.341\n",
      "test accuracy on epoch 1222: 0.769\n",
      "train loss on epoch 1223 : 0.021\n",
      "train accuracy on epoch 1223: 1.000\n",
      "test loss on epoch 1223: 0.338\n",
      "test accuracy on epoch 1223: 0.769\n",
      "train loss on epoch 1224 : 0.255\n",
      "train accuracy on epoch 1224: 0.889\n",
      "test loss on epoch 1224: 0.344\n",
      "test accuracy on epoch 1224: 0.769\n",
      "train loss on epoch 1225 : 0.094\n",
      "train accuracy on epoch 1225: 1.000\n",
      "test loss on epoch 1225: 0.352\n",
      "test accuracy on epoch 1225: 0.769\n",
      "train loss on epoch 1226 : 0.119\n",
      "train accuracy on epoch 1226: 0.944\n",
      "test loss on epoch 1226: 0.354\n",
      "test accuracy on epoch 1226: 0.769\n",
      "train loss on epoch 1227 : 0.361\n",
      "train accuracy on epoch 1227: 0.889\n",
      "test loss on epoch 1227: 0.350\n",
      "test accuracy on epoch 1227: 0.769\n",
      "train loss on epoch 1228 : 0.151\n",
      "train accuracy on epoch 1228: 0.944\n",
      "test loss on epoch 1228: 0.339\n",
      "test accuracy on epoch 1228: 0.769\n",
      "train loss on epoch 1229 : 0.070\n",
      "train accuracy on epoch 1229: 1.000\n",
      "test loss on epoch 1229: 0.334\n",
      "test accuracy on epoch 1229: 0.769\n",
      "train loss on epoch 1230 : 0.069\n",
      "train accuracy on epoch 1230: 1.000\n",
      "test loss on epoch 1230: 0.326\n",
      "test accuracy on epoch 1230: 0.769\n",
      "train loss on epoch 1231 : 0.279\n",
      "train accuracy on epoch 1231: 0.833\n",
      "test loss on epoch 1231: 0.327\n",
      "test accuracy on epoch 1231: 0.769\n",
      "train loss on epoch 1232 : 0.157\n",
      "train accuracy on epoch 1232: 0.889\n",
      "test loss on epoch 1232: 0.329\n",
      "test accuracy on epoch 1232: 0.769\n",
      "train loss on epoch 1233 : 0.243\n",
      "train accuracy on epoch 1233: 0.889\n",
      "test loss on epoch 1233: 0.336\n",
      "test accuracy on epoch 1233: 0.769\n",
      "train loss on epoch 1234 : 0.302\n",
      "train accuracy on epoch 1234: 0.833\n",
      "test loss on epoch 1234: 0.344\n",
      "test accuracy on epoch 1234: 0.769\n",
      "train loss on epoch 1235 : 0.236\n",
      "train accuracy on epoch 1235: 0.944\n",
      "test loss on epoch 1235: 0.343\n",
      "test accuracy on epoch 1235: 0.769\n",
      "train loss on epoch 1236 : 0.171\n",
      "train accuracy on epoch 1236: 0.889\n",
      "test loss on epoch 1236: 0.348\n",
      "test accuracy on epoch 1236: 0.769\n",
      "train loss on epoch 1237 : 0.270\n",
      "train accuracy on epoch 1237: 0.889\n",
      "test loss on epoch 1237: 0.354\n",
      "test accuracy on epoch 1237: 0.769\n",
      "train loss on epoch 1238 : 0.170\n",
      "train accuracy on epoch 1238: 0.889\n",
      "test loss on epoch 1238: 0.355\n",
      "test accuracy on epoch 1238: 0.769\n",
      "train loss on epoch 1239 : 0.148\n",
      "train accuracy on epoch 1239: 0.889\n",
      "test loss on epoch 1239: 0.364\n",
      "test accuracy on epoch 1239: 0.769\n",
      "train loss on epoch 1240 : 0.132\n",
      "train accuracy on epoch 1240: 0.889\n",
      "test loss on epoch 1240: 0.364\n",
      "test accuracy on epoch 1240: 0.769\n",
      "train loss on epoch 1241 : 0.138\n",
      "train accuracy on epoch 1241: 1.000\n",
      "test loss on epoch 1241: 0.368\n",
      "test accuracy on epoch 1241: 0.769\n",
      "train loss on epoch 1242 : 0.079\n",
      "train accuracy on epoch 1242: 0.944\n",
      "test loss on epoch 1242: 0.356\n",
      "test accuracy on epoch 1242: 0.769\n",
      "train loss on epoch 1243 : 0.265\n",
      "train accuracy on epoch 1243: 0.944\n",
      "test loss on epoch 1243: 0.361\n",
      "test accuracy on epoch 1243: 0.769\n",
      "train loss on epoch 1244 : 0.138\n",
      "train accuracy on epoch 1244: 0.889\n",
      "test loss on epoch 1244: 0.352\n",
      "test accuracy on epoch 1244: 0.769\n",
      "train loss on epoch 1245 : 0.256\n",
      "train accuracy on epoch 1245: 0.833\n",
      "test loss on epoch 1245: 0.353\n",
      "test accuracy on epoch 1245: 0.769\n",
      "train loss on epoch 1246 : 0.147\n",
      "train accuracy on epoch 1246: 0.944\n",
      "test loss on epoch 1246: 0.359\n",
      "test accuracy on epoch 1246: 0.769\n",
      "train loss on epoch 1247 : 0.156\n",
      "train accuracy on epoch 1247: 0.944\n",
      "test loss on epoch 1247: 0.358\n",
      "test accuracy on epoch 1247: 0.769\n",
      "train loss on epoch 1248 : 0.072\n",
      "train accuracy on epoch 1248: 0.944\n",
      "test loss on epoch 1248: 0.354\n",
      "test accuracy on epoch 1248: 0.769\n",
      "train loss on epoch 1249 : 0.183\n",
      "train accuracy on epoch 1249: 0.944\n",
      "test loss on epoch 1249: 0.350\n",
      "test accuracy on epoch 1249: 0.769\n",
      "train loss on epoch 1250 : 0.269\n",
      "train accuracy on epoch 1250: 0.833\n",
      "test loss on epoch 1250: 0.354\n",
      "test accuracy on epoch 1250: 0.769\n",
      "train loss on epoch 1251 : 0.113\n",
      "train accuracy on epoch 1251: 0.944\n",
      "test loss on epoch 1251: 0.356\n",
      "test accuracy on epoch 1251: 0.769\n",
      "train loss on epoch 1252 : 0.384\n",
      "train accuracy on epoch 1252: 0.833\n",
      "test loss on epoch 1252: 0.363\n",
      "test accuracy on epoch 1252: 0.769\n",
      "train loss on epoch 1253 : 0.087\n",
      "train accuracy on epoch 1253: 0.944\n",
      "test loss on epoch 1253: 0.357\n",
      "test accuracy on epoch 1253: 0.769\n",
      "train loss on epoch 1254 : 0.319\n",
      "train accuracy on epoch 1254: 0.889\n",
      "test loss on epoch 1254: 0.370\n",
      "test accuracy on epoch 1254: 0.769\n",
      "train loss on epoch 1255 : 0.101\n",
      "train accuracy on epoch 1255: 0.944\n",
      "test loss on epoch 1255: 0.366\n",
      "test accuracy on epoch 1255: 0.769\n",
      "train loss on epoch 1256 : 0.190\n",
      "train accuracy on epoch 1256: 0.944\n",
      "test loss on epoch 1256: 0.363\n",
      "test accuracy on epoch 1256: 0.769\n",
      "train loss on epoch 1257 : 0.179\n",
      "train accuracy on epoch 1257: 0.889\n",
      "test loss on epoch 1257: 0.369\n",
      "test accuracy on epoch 1257: 0.769\n",
      "train loss on epoch 1258 : 0.369\n",
      "train accuracy on epoch 1258: 0.833\n",
      "test loss on epoch 1258: 0.369\n",
      "test accuracy on epoch 1258: 0.769\n",
      "train loss on epoch 1259 : 0.063\n",
      "train accuracy on epoch 1259: 1.000\n",
      "test loss on epoch 1259: 0.359\n",
      "test accuracy on epoch 1259: 0.769\n",
      "train loss on epoch 1260 : 0.296\n",
      "train accuracy on epoch 1260: 0.889\n",
      "test loss on epoch 1260: 0.359\n",
      "test accuracy on epoch 1260: 0.769\n",
      "train loss on epoch 1261 : 0.298\n",
      "train accuracy on epoch 1261: 0.944\n",
      "test loss on epoch 1261: 0.343\n",
      "test accuracy on epoch 1261: 0.769\n",
      "train loss on epoch 1262 : 0.305\n",
      "train accuracy on epoch 1262: 0.833\n",
      "test loss on epoch 1262: 0.336\n",
      "test accuracy on epoch 1262: 0.769\n",
      "train loss on epoch 1263 : 0.071\n",
      "train accuracy on epoch 1263: 0.944\n",
      "test loss on epoch 1263: 0.333\n",
      "test accuracy on epoch 1263: 0.769\n",
      "train loss on epoch 1264 : 0.325\n",
      "train accuracy on epoch 1264: 0.833\n",
      "test loss on epoch 1264: 0.329\n",
      "test accuracy on epoch 1264: 0.769\n",
      "train loss on epoch 1265 : 0.321\n",
      "train accuracy on epoch 1265: 0.944\n",
      "test loss on epoch 1265: 0.331\n",
      "test accuracy on epoch 1265: 0.769\n",
      "train loss on epoch 1266 : 0.325\n",
      "train accuracy on epoch 1266: 0.833\n",
      "test loss on epoch 1266: 0.328\n",
      "test accuracy on epoch 1266: 0.769\n",
      "train loss on epoch 1267 : 0.291\n",
      "train accuracy on epoch 1267: 0.944\n",
      "test loss on epoch 1267: 0.328\n",
      "test accuracy on epoch 1267: 0.769\n",
      "train loss on epoch 1268 : 0.204\n",
      "train accuracy on epoch 1268: 0.944\n",
      "test loss on epoch 1268: 0.325\n",
      "test accuracy on epoch 1268: 0.769\n",
      "train loss on epoch 1269 : 0.384\n",
      "train accuracy on epoch 1269: 0.833\n",
      "test loss on epoch 1269: 0.323\n",
      "test accuracy on epoch 1269: 0.769\n",
      "train loss on epoch 1270 : 0.292\n",
      "train accuracy on epoch 1270: 0.833\n",
      "test loss on epoch 1270: 0.327\n",
      "test accuracy on epoch 1270: 0.769\n",
      "train loss on epoch 1271 : 0.218\n",
      "train accuracy on epoch 1271: 0.889\n",
      "test loss on epoch 1271: 0.325\n",
      "test accuracy on epoch 1271: 0.769\n",
      "train loss on epoch 1272 : 0.362\n",
      "train accuracy on epoch 1272: 0.889\n",
      "test loss on epoch 1272: 0.330\n",
      "test accuracy on epoch 1272: 0.769\n",
      "train loss on epoch 1273 : 0.273\n",
      "train accuracy on epoch 1273: 0.889\n",
      "test loss on epoch 1273: 0.327\n",
      "test accuracy on epoch 1273: 0.769\n",
      "train loss on epoch 1274 : 0.036\n",
      "train accuracy on epoch 1274: 1.000\n",
      "test loss on epoch 1274: 0.323\n",
      "test accuracy on epoch 1274: 0.769\n",
      "train loss on epoch 1275 : 0.264\n",
      "train accuracy on epoch 1275: 0.889\n",
      "test loss on epoch 1275: 0.322\n",
      "test accuracy on epoch 1275: 0.769\n",
      "train loss on epoch 1276 : 0.201\n",
      "train accuracy on epoch 1276: 0.944\n",
      "test loss on epoch 1276: 0.330\n",
      "test accuracy on epoch 1276: 0.769\n",
      "train loss on epoch 1277 : 0.100\n",
      "train accuracy on epoch 1277: 0.944\n",
      "test loss on epoch 1277: 0.348\n",
      "test accuracy on epoch 1277: 0.769\n",
      "train loss on epoch 1278 : 0.234\n",
      "train accuracy on epoch 1278: 0.889\n",
      "test loss on epoch 1278: 0.341\n",
      "test accuracy on epoch 1278: 0.769\n",
      "train loss on epoch 1279 : 0.100\n",
      "train accuracy on epoch 1279: 1.000\n",
      "test loss on epoch 1279: 0.340\n",
      "test accuracy on epoch 1279: 0.769\n",
      "train loss on epoch 1280 : 0.271\n",
      "train accuracy on epoch 1280: 0.833\n",
      "test loss on epoch 1280: 0.345\n",
      "test accuracy on epoch 1280: 0.769\n",
      "train loss on epoch 1281 : 0.119\n",
      "train accuracy on epoch 1281: 0.944\n",
      "test loss on epoch 1281: 0.338\n",
      "test accuracy on epoch 1281: 0.769\n",
      "train loss on epoch 1282 : 0.236\n",
      "train accuracy on epoch 1282: 0.944\n",
      "test loss on epoch 1282: 0.340\n",
      "test accuracy on epoch 1282: 0.769\n",
      "train loss on epoch 1283 : 0.105\n",
      "train accuracy on epoch 1283: 0.944\n",
      "test loss on epoch 1283: 0.329\n",
      "test accuracy on epoch 1283: 0.769\n",
      "train loss on epoch 1284 : 0.191\n",
      "train accuracy on epoch 1284: 0.889\n",
      "test loss on epoch 1284: 0.323\n",
      "test accuracy on epoch 1284: 0.769\n",
      "train loss on epoch 1285 : 0.346\n",
      "train accuracy on epoch 1285: 0.833\n",
      "test loss on epoch 1285: 0.330\n",
      "test accuracy on epoch 1285: 0.769\n",
      "train loss on epoch 1286 : 0.138\n",
      "train accuracy on epoch 1286: 0.889\n",
      "test loss on epoch 1286: 0.324\n",
      "test accuracy on epoch 1286: 0.769\n",
      "train loss on epoch 1287 : 0.166\n",
      "train accuracy on epoch 1287: 0.889\n",
      "test loss on epoch 1287: 0.324\n",
      "test accuracy on epoch 1287: 0.769\n",
      "train loss on epoch 1288 : 0.107\n",
      "train accuracy on epoch 1288: 1.000\n",
      "test loss on epoch 1288: 0.333\n",
      "test accuracy on epoch 1288: 0.769\n",
      "train loss on epoch 1289 : 0.047\n",
      "train accuracy on epoch 1289: 1.000\n",
      "test loss on epoch 1289: 0.333\n",
      "test accuracy on epoch 1289: 0.769\n",
      "train loss on epoch 1290 : 0.078\n",
      "train accuracy on epoch 1290: 1.000\n",
      "test loss on epoch 1290: 0.337\n",
      "test accuracy on epoch 1290: 0.769\n",
      "train loss on epoch 1291 : 0.115\n",
      "train accuracy on epoch 1291: 0.944\n",
      "test loss on epoch 1291: 0.340\n",
      "test accuracy on epoch 1291: 0.769\n",
      "train loss on epoch 1292 : 0.251\n",
      "train accuracy on epoch 1292: 0.889\n",
      "test loss on epoch 1292: 0.338\n",
      "test accuracy on epoch 1292: 0.769\n",
      "train loss on epoch 1293 : 0.402\n",
      "train accuracy on epoch 1293: 0.889\n",
      "test loss on epoch 1293: 0.345\n",
      "test accuracy on epoch 1293: 0.769\n",
      "train loss on epoch 1294 : 0.239\n",
      "train accuracy on epoch 1294: 0.889\n",
      "test loss on epoch 1294: 0.353\n",
      "test accuracy on epoch 1294: 0.769\n",
      "train loss on epoch 1295 : 0.249\n",
      "train accuracy on epoch 1295: 0.944\n",
      "test loss on epoch 1295: 0.340\n",
      "test accuracy on epoch 1295: 0.769\n",
      "train loss on epoch 1296 : 0.147\n",
      "train accuracy on epoch 1296: 0.944\n",
      "test loss on epoch 1296: 0.354\n",
      "test accuracy on epoch 1296: 0.769\n",
      "train loss on epoch 1297 : 0.170\n",
      "train accuracy on epoch 1297: 0.889\n",
      "test loss on epoch 1297: 0.346\n",
      "test accuracy on epoch 1297: 0.769\n",
      "train loss on epoch 1298 : 0.315\n",
      "train accuracy on epoch 1298: 0.889\n",
      "test loss on epoch 1298: 0.359\n",
      "test accuracy on epoch 1298: 0.769\n",
      "train loss on epoch 1299 : 0.149\n",
      "train accuracy on epoch 1299: 0.889\n",
      "test loss on epoch 1299: 0.348\n",
      "test accuracy on epoch 1299: 0.769\n",
      "train loss on epoch 1300 : 0.262\n",
      "train accuracy on epoch 1300: 0.944\n",
      "test loss on epoch 1300: 0.354\n",
      "test accuracy on epoch 1300: 0.769\n",
      "train loss on epoch 1301 : 0.303\n",
      "train accuracy on epoch 1301: 0.833\n",
      "test loss on epoch 1301: 0.342\n",
      "test accuracy on epoch 1301: 0.769\n",
      "train loss on epoch 1302 : 0.192\n",
      "train accuracy on epoch 1302: 0.889\n",
      "test loss on epoch 1302: 0.331\n",
      "test accuracy on epoch 1302: 0.769\n",
      "train loss on epoch 1303 : 0.283\n",
      "train accuracy on epoch 1303: 0.833\n",
      "test loss on epoch 1303: 0.330\n",
      "test accuracy on epoch 1303: 0.769\n",
      "train loss on epoch 1304 : 0.262\n",
      "train accuracy on epoch 1304: 0.889\n",
      "test loss on epoch 1304: 0.326\n",
      "test accuracy on epoch 1304: 0.769\n",
      "train loss on epoch 1305 : 0.402\n",
      "train accuracy on epoch 1305: 0.889\n",
      "test loss on epoch 1305: 0.327\n",
      "test accuracy on epoch 1305: 0.769\n",
      "train loss on epoch 1306 : 0.301\n",
      "train accuracy on epoch 1306: 0.889\n",
      "test loss on epoch 1306: 0.326\n",
      "test accuracy on epoch 1306: 0.769\n",
      "train loss on epoch 1307 : 0.330\n",
      "train accuracy on epoch 1307: 0.889\n",
      "test loss on epoch 1307: 0.333\n",
      "test accuracy on epoch 1307: 0.769\n",
      "train loss on epoch 1308 : 0.126\n",
      "train accuracy on epoch 1308: 0.944\n",
      "test loss on epoch 1308: 0.327\n",
      "test accuracy on epoch 1308: 0.769\n",
      "train loss on epoch 1309 : 0.074\n",
      "train accuracy on epoch 1309: 1.000\n",
      "test loss on epoch 1309: 0.322\n",
      "test accuracy on epoch 1309: 0.769\n",
      "train loss on epoch 1310 : 0.075\n",
      "train accuracy on epoch 1310: 1.000\n",
      "test loss on epoch 1310: 0.321\n",
      "test accuracy on epoch 1310: 0.769\n",
      "train loss on epoch 1311 : 0.126\n",
      "train accuracy on epoch 1311: 0.944\n",
      "test loss on epoch 1311: 0.321\n",
      "test accuracy on epoch 1311: 0.769\n",
      "train loss on epoch 1312 : 0.209\n",
      "train accuracy on epoch 1312: 0.944\n",
      "test loss on epoch 1312: 0.326\n",
      "test accuracy on epoch 1312: 0.769\n",
      "train loss on epoch 1313 : 0.093\n",
      "train accuracy on epoch 1313: 0.944\n",
      "test loss on epoch 1313: 0.328\n",
      "test accuracy on epoch 1313: 0.769\n",
      "train loss on epoch 1314 : 0.390\n",
      "train accuracy on epoch 1314: 0.889\n",
      "test loss on epoch 1314: 0.329\n",
      "test accuracy on epoch 1314: 0.769\n",
      "train loss on epoch 1315 : 0.087\n",
      "train accuracy on epoch 1315: 0.944\n",
      "test loss on epoch 1315: 0.322\n",
      "test accuracy on epoch 1315: 0.769\n",
      "train loss on epoch 1316 : 0.221\n",
      "train accuracy on epoch 1316: 0.833\n",
      "test loss on epoch 1316: 0.320\n",
      "test accuracy on epoch 1316: 0.769\n",
      "train loss on epoch 1317 : 0.147\n",
      "train accuracy on epoch 1317: 0.889\n",
      "test loss on epoch 1317: 0.320\n",
      "test accuracy on epoch 1317: 0.769\n",
      "train loss on epoch 1318 : 0.276\n",
      "train accuracy on epoch 1318: 0.889\n",
      "test loss on epoch 1318: 0.321\n",
      "test accuracy on epoch 1318: 0.769\n",
      "train loss on epoch 1319 : 0.262\n",
      "train accuracy on epoch 1319: 0.889\n",
      "test loss on epoch 1319: 0.324\n",
      "test accuracy on epoch 1319: 0.769\n",
      "train loss on epoch 1320 : 0.101\n",
      "train accuracy on epoch 1320: 0.944\n",
      "test loss on epoch 1320: 0.326\n",
      "test accuracy on epoch 1320: 0.769\n",
      "train loss on epoch 1321 : 0.410\n",
      "train accuracy on epoch 1321: 0.889\n",
      "test loss on epoch 1321: 0.322\n",
      "test accuracy on epoch 1321: 0.769\n",
      "train loss on epoch 1322 : 0.161\n",
      "train accuracy on epoch 1322: 0.944\n",
      "test loss on epoch 1322: 0.326\n",
      "test accuracy on epoch 1322: 0.769\n",
      "train loss on epoch 1323 : 0.151\n",
      "train accuracy on epoch 1323: 0.944\n",
      "test loss on epoch 1323: 0.325\n",
      "test accuracy on epoch 1323: 0.769\n",
      "train loss on epoch 1324 : 0.288\n",
      "train accuracy on epoch 1324: 0.889\n",
      "test loss on epoch 1324: 0.323\n",
      "test accuracy on epoch 1324: 0.769\n",
      "train loss on epoch 1325 : 0.269\n",
      "train accuracy on epoch 1325: 0.833\n",
      "test loss on epoch 1325: 0.322\n",
      "test accuracy on epoch 1325: 0.769\n",
      "train loss on epoch 1326 : 0.132\n",
      "train accuracy on epoch 1326: 1.000\n",
      "test loss on epoch 1326: 0.327\n",
      "test accuracy on epoch 1326: 0.769\n",
      "train loss on epoch 1327 : 0.135\n",
      "train accuracy on epoch 1327: 1.000\n",
      "test loss on epoch 1327: 0.334\n",
      "test accuracy on epoch 1327: 0.769\n",
      "train loss on epoch 1328 : 0.063\n",
      "train accuracy on epoch 1328: 1.000\n",
      "test loss on epoch 1328: 0.342\n",
      "test accuracy on epoch 1328: 0.769\n",
      "train loss on epoch 1329 : 0.277\n",
      "train accuracy on epoch 1329: 0.889\n",
      "test loss on epoch 1329: 0.345\n",
      "test accuracy on epoch 1329: 0.769\n",
      "train loss on epoch 1330 : 0.160\n",
      "train accuracy on epoch 1330: 0.889\n",
      "test loss on epoch 1330: 0.346\n",
      "test accuracy on epoch 1330: 0.769\n",
      "train loss on epoch 1331 : 0.266\n",
      "train accuracy on epoch 1331: 0.833\n",
      "test loss on epoch 1331: 0.346\n",
      "test accuracy on epoch 1331: 0.769\n",
      "train loss on epoch 1332 : 0.264\n",
      "train accuracy on epoch 1332: 0.833\n",
      "test loss on epoch 1332: 0.353\n",
      "test accuracy on epoch 1332: 0.769\n",
      "train loss on epoch 1333 : 0.151\n",
      "train accuracy on epoch 1333: 0.889\n",
      "test loss on epoch 1333: 0.357\n",
      "test accuracy on epoch 1333: 0.769\n",
      "train loss on epoch 1334 : 0.124\n",
      "train accuracy on epoch 1334: 0.889\n",
      "test loss on epoch 1334: 0.347\n",
      "test accuracy on epoch 1334: 0.769\n",
      "train loss on epoch 1335 : 0.134\n",
      "train accuracy on epoch 1335: 0.944\n",
      "test loss on epoch 1335: 0.339\n",
      "test accuracy on epoch 1335: 0.769\n",
      "train loss on epoch 1336 : 0.200\n",
      "train accuracy on epoch 1336: 0.889\n",
      "test loss on epoch 1336: 0.339\n",
      "test accuracy on epoch 1336: 0.769\n",
      "train loss on epoch 1337 : 0.241\n",
      "train accuracy on epoch 1337: 0.944\n",
      "test loss on epoch 1337: 0.336\n",
      "test accuracy on epoch 1337: 0.769\n",
      "train loss on epoch 1338 : 0.053\n",
      "train accuracy on epoch 1338: 1.000\n",
      "test loss on epoch 1338: 0.329\n",
      "test accuracy on epoch 1338: 0.769\n",
      "train loss on epoch 1339 : 0.340\n",
      "train accuracy on epoch 1339: 0.944\n",
      "test loss on epoch 1339: 0.325\n",
      "test accuracy on epoch 1339: 0.769\n",
      "train loss on epoch 1340 : 0.247\n",
      "train accuracy on epoch 1340: 0.833\n",
      "test loss on epoch 1340: 0.325\n",
      "test accuracy on epoch 1340: 0.769\n",
      "train loss on epoch 1341 : 0.209\n",
      "train accuracy on epoch 1341: 0.944\n",
      "test loss on epoch 1341: 0.325\n",
      "test accuracy on epoch 1341: 0.769\n",
      "train loss on epoch 1342 : 0.132\n",
      "train accuracy on epoch 1342: 0.944\n",
      "test loss on epoch 1342: 0.326\n",
      "test accuracy on epoch 1342: 0.769\n",
      "train loss on epoch 1343 : 0.157\n",
      "train accuracy on epoch 1343: 0.889\n",
      "test loss on epoch 1343: 0.325\n",
      "test accuracy on epoch 1343: 0.769\n",
      "train loss on epoch 1344 : 0.170\n",
      "train accuracy on epoch 1344: 0.889\n",
      "test loss on epoch 1344: 0.333\n",
      "test accuracy on epoch 1344: 0.769\n",
      "train loss on epoch 1345 : 0.093\n",
      "train accuracy on epoch 1345: 1.000\n",
      "test loss on epoch 1345: 0.336\n",
      "test accuracy on epoch 1345: 0.769\n",
      "train loss on epoch 1346 : 0.201\n",
      "train accuracy on epoch 1346: 0.944\n",
      "test loss on epoch 1346: 0.335\n",
      "test accuracy on epoch 1346: 0.769\n",
      "train loss on epoch 1347 : 0.150\n",
      "train accuracy on epoch 1347: 0.944\n",
      "test loss on epoch 1347: 0.331\n",
      "test accuracy on epoch 1347: 0.769\n",
      "train loss on epoch 1348 : 0.259\n",
      "train accuracy on epoch 1348: 0.944\n",
      "test loss on epoch 1348: 0.343\n",
      "test accuracy on epoch 1348: 0.769\n",
      "train loss on epoch 1349 : 0.188\n",
      "train accuracy on epoch 1349: 0.889\n",
      "test loss on epoch 1349: 0.342\n",
      "test accuracy on epoch 1349: 0.769\n",
      "train loss on epoch 1350 : 0.080\n",
      "train accuracy on epoch 1350: 0.944\n",
      "test loss on epoch 1350: 0.345\n",
      "test accuracy on epoch 1350: 0.769\n",
      "train loss on epoch 1351 : 0.179\n",
      "train accuracy on epoch 1351: 0.944\n",
      "test loss on epoch 1351: 0.345\n",
      "test accuracy on epoch 1351: 0.769\n",
      "train loss on epoch 1352 : 0.068\n",
      "train accuracy on epoch 1352: 1.000\n",
      "test loss on epoch 1352: 0.336\n",
      "test accuracy on epoch 1352: 0.769\n",
      "train loss on epoch 1353 : 0.108\n",
      "train accuracy on epoch 1353: 0.944\n",
      "test loss on epoch 1353: 0.334\n",
      "test accuracy on epoch 1353: 0.769\n",
      "train loss on epoch 1354 : 0.087\n",
      "train accuracy on epoch 1354: 1.000\n",
      "test loss on epoch 1354: 0.334\n",
      "test accuracy on epoch 1354: 0.769\n",
      "train loss on epoch 1355 : 0.023\n",
      "train accuracy on epoch 1355: 1.000\n",
      "test loss on epoch 1355: 0.335\n",
      "test accuracy on epoch 1355: 0.769\n",
      "train loss on epoch 1356 : 0.216\n",
      "train accuracy on epoch 1356: 0.889\n",
      "test loss on epoch 1356: 0.328\n",
      "test accuracy on epoch 1356: 0.769\n",
      "train loss on epoch 1357 : 0.086\n",
      "train accuracy on epoch 1357: 1.000\n",
      "test loss on epoch 1357: 0.326\n",
      "test accuracy on epoch 1357: 0.769\n",
      "train loss on epoch 1358 : 0.350\n",
      "train accuracy on epoch 1358: 0.889\n",
      "test loss on epoch 1358: 0.331\n",
      "test accuracy on epoch 1358: 0.769\n",
      "train loss on epoch 1359 : 0.225\n",
      "train accuracy on epoch 1359: 0.889\n",
      "test loss on epoch 1359: 0.331\n",
      "test accuracy on epoch 1359: 0.769\n",
      "train loss on epoch 1360 : 0.401\n",
      "train accuracy on epoch 1360: 0.889\n",
      "test loss on epoch 1360: 0.335\n",
      "test accuracy on epoch 1360: 0.769\n",
      "train loss on epoch 1361 : 0.149\n",
      "train accuracy on epoch 1361: 0.889\n",
      "test loss on epoch 1361: 0.326\n",
      "test accuracy on epoch 1361: 0.769\n",
      "train loss on epoch 1362 : 0.127\n",
      "train accuracy on epoch 1362: 0.944\n",
      "test loss on epoch 1362: 0.321\n",
      "test accuracy on epoch 1362: 0.769\n",
      "train loss on epoch 1363 : 0.038\n",
      "train accuracy on epoch 1363: 1.000\n",
      "test loss on epoch 1363: 0.322\n",
      "test accuracy on epoch 1363: 0.769\n",
      "train loss on epoch 1364 : 0.149\n",
      "train accuracy on epoch 1364: 0.889\n",
      "test loss on epoch 1364: 0.328\n",
      "test accuracy on epoch 1364: 0.769\n",
      "train loss on epoch 1365 : 0.152\n",
      "train accuracy on epoch 1365: 0.944\n",
      "test loss on epoch 1365: 0.326\n",
      "test accuracy on epoch 1365: 0.769\n",
      "train loss on epoch 1366 : 0.129\n",
      "train accuracy on epoch 1366: 0.944\n",
      "test loss on epoch 1366: 0.324\n",
      "test accuracy on epoch 1366: 0.769\n",
      "train loss on epoch 1367 : 0.128\n",
      "train accuracy on epoch 1367: 0.944\n",
      "test loss on epoch 1367: 0.327\n",
      "test accuracy on epoch 1367: 0.769\n",
      "train loss on epoch 1368 : 0.353\n",
      "train accuracy on epoch 1368: 0.944\n",
      "test loss on epoch 1368: 0.325\n",
      "test accuracy on epoch 1368: 0.769\n",
      "train loss on epoch 1369 : 0.114\n",
      "train accuracy on epoch 1369: 1.000\n",
      "test loss on epoch 1369: 0.323\n",
      "test accuracy on epoch 1369: 0.769\n",
      "train loss on epoch 1370 : 0.047\n",
      "train accuracy on epoch 1370: 1.000\n",
      "test loss on epoch 1370: 0.323\n",
      "test accuracy on epoch 1370: 0.769\n",
      "train loss on epoch 1371 : 0.066\n",
      "train accuracy on epoch 1371: 1.000\n",
      "test loss on epoch 1371: 0.315\n",
      "test accuracy on epoch 1371: 0.769\n",
      "train loss on epoch 1372 : 0.179\n",
      "train accuracy on epoch 1372: 0.944\n",
      "test loss on epoch 1372: 0.317\n",
      "test accuracy on epoch 1372: 0.769\n",
      "train loss on epoch 1373 : 0.317\n",
      "train accuracy on epoch 1373: 0.833\n",
      "test loss on epoch 1373: 0.321\n",
      "test accuracy on epoch 1373: 0.769\n",
      "train loss on epoch 1374 : 0.333\n",
      "train accuracy on epoch 1374: 0.833\n",
      "test loss on epoch 1374: 0.340\n",
      "test accuracy on epoch 1374: 0.769\n",
      "train loss on epoch 1375 : 0.108\n",
      "train accuracy on epoch 1375: 0.944\n",
      "test loss on epoch 1375: 0.346\n",
      "test accuracy on epoch 1375: 0.769\n",
      "train loss on epoch 1376 : 0.065\n",
      "train accuracy on epoch 1376: 0.944\n",
      "test loss on epoch 1376: 0.356\n",
      "test accuracy on epoch 1376: 0.769\n",
      "train loss on epoch 1377 : 0.242\n",
      "train accuracy on epoch 1377: 0.833\n",
      "test loss on epoch 1377: 0.372\n",
      "test accuracy on epoch 1377: 0.769\n",
      "train loss on epoch 1378 : 0.057\n",
      "train accuracy on epoch 1378: 1.000\n",
      "test loss on epoch 1378: 0.358\n",
      "test accuracy on epoch 1378: 0.769\n",
      "train loss on epoch 1379 : 0.282\n",
      "train accuracy on epoch 1379: 0.889\n",
      "test loss on epoch 1379: 0.360\n",
      "test accuracy on epoch 1379: 0.769\n",
      "train loss on epoch 1380 : 0.286\n",
      "train accuracy on epoch 1380: 0.889\n",
      "test loss on epoch 1380: 0.347\n",
      "test accuracy on epoch 1380: 0.769\n",
      "train loss on epoch 1381 : 0.073\n",
      "train accuracy on epoch 1381: 1.000\n",
      "test loss on epoch 1381: 0.352\n",
      "test accuracy on epoch 1381: 0.769\n",
      "train loss on epoch 1382 : 0.105\n",
      "train accuracy on epoch 1382: 0.889\n",
      "test loss on epoch 1382: 0.358\n",
      "test accuracy on epoch 1382: 0.769\n",
      "train loss on epoch 1383 : 0.076\n",
      "train accuracy on epoch 1383: 1.000\n",
      "test loss on epoch 1383: 0.379\n",
      "test accuracy on epoch 1383: 0.769\n",
      "train loss on epoch 1384 : 0.542\n",
      "train accuracy on epoch 1384: 0.889\n",
      "test loss on epoch 1384: 0.365\n",
      "test accuracy on epoch 1384: 0.769\n",
      "train loss on epoch 1385 : 0.211\n",
      "train accuracy on epoch 1385: 0.944\n",
      "test loss on epoch 1385: 0.358\n",
      "test accuracy on epoch 1385: 0.769\n",
      "train loss on epoch 1386 : 0.276\n",
      "train accuracy on epoch 1386: 0.944\n",
      "test loss on epoch 1386: 0.356\n",
      "test accuracy on epoch 1386: 0.769\n",
      "train loss on epoch 1387 : 0.216\n",
      "train accuracy on epoch 1387: 0.944\n",
      "test loss on epoch 1387: 0.334\n",
      "test accuracy on epoch 1387: 0.769\n",
      "train loss on epoch 1388 : 0.107\n",
      "train accuracy on epoch 1388: 1.000\n",
      "test loss on epoch 1388: 0.330\n",
      "test accuracy on epoch 1388: 0.769\n",
      "train loss on epoch 1389 : 0.292\n",
      "train accuracy on epoch 1389: 0.889\n",
      "test loss on epoch 1389: 0.343\n",
      "test accuracy on epoch 1389: 0.769\n",
      "train loss on epoch 1390 : 0.064\n",
      "train accuracy on epoch 1390: 1.000\n",
      "test loss on epoch 1390: 0.341\n",
      "test accuracy on epoch 1390: 0.769\n",
      "train loss on epoch 1391 : 0.069\n",
      "train accuracy on epoch 1391: 0.944\n",
      "test loss on epoch 1391: 0.332\n",
      "test accuracy on epoch 1391: 0.769\n",
      "train loss on epoch 1392 : 0.238\n",
      "train accuracy on epoch 1392: 0.944\n",
      "test loss on epoch 1392: 0.322\n",
      "test accuracy on epoch 1392: 0.769\n",
      "train loss on epoch 1393 : 0.043\n",
      "train accuracy on epoch 1393: 1.000\n",
      "test loss on epoch 1393: 0.333\n",
      "test accuracy on epoch 1393: 0.769\n",
      "train loss on epoch 1394 : 0.280\n",
      "train accuracy on epoch 1394: 0.889\n",
      "test loss on epoch 1394: 0.331\n",
      "test accuracy on epoch 1394: 0.769\n",
      "train loss on epoch 1395 : 0.153\n",
      "train accuracy on epoch 1395: 0.944\n",
      "test loss on epoch 1395: 0.326\n",
      "test accuracy on epoch 1395: 0.769\n",
      "train loss on epoch 1396 : 0.116\n",
      "train accuracy on epoch 1396: 1.000\n",
      "test loss on epoch 1396: 0.328\n",
      "test accuracy on epoch 1396: 0.769\n",
      "train loss on epoch 1397 : 0.159\n",
      "train accuracy on epoch 1397: 0.889\n",
      "test loss on epoch 1397: 0.327\n",
      "test accuracy on epoch 1397: 0.769\n",
      "train loss on epoch 1398 : 0.134\n",
      "train accuracy on epoch 1398: 0.944\n",
      "test loss on epoch 1398: 0.327\n",
      "test accuracy on epoch 1398: 0.769\n",
      "train loss on epoch 1399 : 0.062\n",
      "train accuracy on epoch 1399: 1.000\n",
      "test loss on epoch 1399: 0.322\n",
      "test accuracy on epoch 1399: 0.769\n",
      "train loss on epoch 1400 : 0.140\n",
      "train accuracy on epoch 1400: 0.944\n",
      "test loss on epoch 1400: 0.321\n",
      "test accuracy on epoch 1400: 0.769\n",
      "train loss on epoch 1401 : 0.213\n",
      "train accuracy on epoch 1401: 0.889\n",
      "test loss on epoch 1401: 0.327\n",
      "test accuracy on epoch 1401: 0.769\n",
      "train loss on epoch 1402 : 0.045\n",
      "train accuracy on epoch 1402: 1.000\n",
      "test loss on epoch 1402: 0.327\n",
      "test accuracy on epoch 1402: 0.769\n",
      "train loss on epoch 1403 : 0.197\n",
      "train accuracy on epoch 1403: 0.833\n",
      "test loss on epoch 1403: 0.334\n",
      "test accuracy on epoch 1403: 0.769\n",
      "train loss on epoch 1404 : 0.153\n",
      "train accuracy on epoch 1404: 0.889\n",
      "test loss on epoch 1404: 0.339\n",
      "test accuracy on epoch 1404: 0.769\n",
      "train loss on epoch 1405 : 0.309\n",
      "train accuracy on epoch 1405: 0.944\n",
      "test loss on epoch 1405: 0.330\n",
      "test accuracy on epoch 1405: 0.769\n",
      "train loss on epoch 1406 : 0.167\n",
      "train accuracy on epoch 1406: 0.889\n",
      "test loss on epoch 1406: 0.322\n",
      "test accuracy on epoch 1406: 0.769\n",
      "train loss on epoch 1407 : 0.178\n",
      "train accuracy on epoch 1407: 0.889\n",
      "test loss on epoch 1407: 0.322\n",
      "test accuracy on epoch 1407: 0.769\n",
      "train loss on epoch 1408 : 0.098\n",
      "train accuracy on epoch 1408: 0.944\n",
      "test loss on epoch 1408: 0.321\n",
      "test accuracy on epoch 1408: 0.769\n",
      "train loss on epoch 1409 : 0.240\n",
      "train accuracy on epoch 1409: 0.889\n",
      "test loss on epoch 1409: 0.322\n",
      "test accuracy on epoch 1409: 0.769\n",
      "train loss on epoch 1410 : 0.232\n",
      "train accuracy on epoch 1410: 0.944\n",
      "test loss on epoch 1410: 0.327\n",
      "test accuracy on epoch 1410: 0.769\n",
      "train loss on epoch 1411 : 0.114\n",
      "train accuracy on epoch 1411: 0.944\n",
      "test loss on epoch 1411: 0.325\n",
      "test accuracy on epoch 1411: 0.769\n",
      "train loss on epoch 1412 : 0.080\n",
      "train accuracy on epoch 1412: 0.944\n",
      "test loss on epoch 1412: 0.333\n",
      "test accuracy on epoch 1412: 0.769\n",
      "train loss on epoch 1413 : 0.130\n",
      "train accuracy on epoch 1413: 0.944\n",
      "test loss on epoch 1413: 0.327\n",
      "test accuracy on epoch 1413: 0.769\n",
      "train loss on epoch 1414 : 0.183\n",
      "train accuracy on epoch 1414: 0.889\n",
      "test loss on epoch 1414: 0.335\n",
      "test accuracy on epoch 1414: 0.769\n",
      "train loss on epoch 1415 : 0.167\n",
      "train accuracy on epoch 1415: 0.944\n",
      "test loss on epoch 1415: 0.341\n",
      "test accuracy on epoch 1415: 0.769\n",
      "train loss on epoch 1416 : 0.058\n",
      "train accuracy on epoch 1416: 1.000\n",
      "test loss on epoch 1416: 0.338\n",
      "test accuracy on epoch 1416: 0.769\n",
      "train loss on epoch 1417 : 0.056\n",
      "train accuracy on epoch 1417: 1.000\n",
      "test loss on epoch 1417: 0.347\n",
      "test accuracy on epoch 1417: 0.769\n",
      "train loss on epoch 1418 : 0.512\n",
      "train accuracy on epoch 1418: 0.833\n",
      "test loss on epoch 1418: 0.343\n",
      "test accuracy on epoch 1418: 0.769\n",
      "train loss on epoch 1419 : 0.067\n",
      "train accuracy on epoch 1419: 1.000\n",
      "test loss on epoch 1419: 0.333\n",
      "test accuracy on epoch 1419: 0.769\n",
      "train loss on epoch 1420 : 0.239\n",
      "train accuracy on epoch 1420: 0.889\n",
      "test loss on epoch 1420: 0.331\n",
      "test accuracy on epoch 1420: 0.769\n",
      "train loss on epoch 1421 : 0.126\n",
      "train accuracy on epoch 1421: 0.944\n",
      "test loss on epoch 1421: 0.330\n",
      "test accuracy on epoch 1421: 0.769\n",
      "train loss on epoch 1422 : 0.167\n",
      "train accuracy on epoch 1422: 0.944\n",
      "test loss on epoch 1422: 0.336\n",
      "test accuracy on epoch 1422: 0.769\n",
      "train loss on epoch 1423 : 0.101\n",
      "train accuracy on epoch 1423: 0.944\n",
      "test loss on epoch 1423: 0.341\n",
      "test accuracy on epoch 1423: 0.769\n",
      "train loss on epoch 1424 : 0.088\n",
      "train accuracy on epoch 1424: 0.944\n",
      "test loss on epoch 1424: 0.344\n",
      "test accuracy on epoch 1424: 0.769\n",
      "train loss on epoch 1425 : 0.097\n",
      "train accuracy on epoch 1425: 0.944\n",
      "test loss on epoch 1425: 0.355\n",
      "test accuracy on epoch 1425: 0.769\n",
      "train loss on epoch 1426 : 0.068\n",
      "train accuracy on epoch 1426: 1.000\n",
      "test loss on epoch 1426: 0.362\n",
      "test accuracy on epoch 1426: 0.769\n",
      "train loss on epoch 1427 : 0.134\n",
      "train accuracy on epoch 1427: 0.889\n",
      "test loss on epoch 1427: 0.366\n",
      "test accuracy on epoch 1427: 0.769\n",
      "train loss on epoch 1428 : 0.224\n",
      "train accuracy on epoch 1428: 0.889\n",
      "test loss on epoch 1428: 0.353\n",
      "test accuracy on epoch 1428: 0.769\n",
      "train loss on epoch 1429 : 0.136\n",
      "train accuracy on epoch 1429: 0.944\n",
      "test loss on epoch 1429: 0.336\n",
      "test accuracy on epoch 1429: 0.769\n",
      "train loss on epoch 1430 : 0.077\n",
      "train accuracy on epoch 1430: 1.000\n",
      "test loss on epoch 1430: 0.335\n",
      "test accuracy on epoch 1430: 0.769\n",
      "train loss on epoch 1431 : 0.274\n",
      "train accuracy on epoch 1431: 0.944\n",
      "test loss on epoch 1431: 0.339\n",
      "test accuracy on epoch 1431: 0.769\n",
      "train loss on epoch 1432 : 0.058\n",
      "train accuracy on epoch 1432: 1.000\n",
      "test loss on epoch 1432: 0.352\n",
      "test accuracy on epoch 1432: 0.769\n",
      "train loss on epoch 1433 : 0.201\n",
      "train accuracy on epoch 1433: 0.889\n",
      "test loss on epoch 1433: 0.341\n",
      "test accuracy on epoch 1433: 0.769\n",
      "train loss on epoch 1434 : 0.368\n",
      "train accuracy on epoch 1434: 0.889\n",
      "test loss on epoch 1434: 0.337\n",
      "test accuracy on epoch 1434: 0.769\n",
      "train loss on epoch 1435 : 0.104\n",
      "train accuracy on epoch 1435: 0.944\n",
      "test loss on epoch 1435: 0.351\n",
      "test accuracy on epoch 1435: 0.769\n",
      "train loss on epoch 1436 : 0.063\n",
      "train accuracy on epoch 1436: 1.000\n",
      "test loss on epoch 1436: 0.345\n",
      "test accuracy on epoch 1436: 0.769\n",
      "train loss on epoch 1437 : 0.194\n",
      "train accuracy on epoch 1437: 0.889\n",
      "test loss on epoch 1437: 0.349\n",
      "test accuracy on epoch 1437: 0.769\n",
      "train loss on epoch 1438 : 0.132\n",
      "train accuracy on epoch 1438: 0.944\n",
      "test loss on epoch 1438: 0.357\n",
      "test accuracy on epoch 1438: 0.769\n",
      "train loss on epoch 1439 : 0.118\n",
      "train accuracy on epoch 1439: 0.944\n",
      "test loss on epoch 1439: 0.351\n",
      "test accuracy on epoch 1439: 0.769\n",
      "train loss on epoch 1440 : 0.277\n",
      "train accuracy on epoch 1440: 0.889\n",
      "test loss on epoch 1440: 0.337\n",
      "test accuracy on epoch 1440: 0.769\n",
      "train loss on epoch 1441 : 0.312\n",
      "train accuracy on epoch 1441: 0.889\n",
      "test loss on epoch 1441: 0.324\n",
      "test accuracy on epoch 1441: 0.769\n",
      "train loss on epoch 1442 : 0.017\n",
      "train accuracy on epoch 1442: 1.000\n",
      "test loss on epoch 1442: 0.321\n",
      "test accuracy on epoch 1442: 0.769\n",
      "train loss on epoch 1443 : 0.105\n",
      "train accuracy on epoch 1443: 1.000\n",
      "test loss on epoch 1443: 0.325\n",
      "test accuracy on epoch 1443: 0.769\n",
      "train loss on epoch 1444 : 0.140\n",
      "train accuracy on epoch 1444: 0.889\n",
      "test loss on epoch 1444: 0.323\n",
      "test accuracy on epoch 1444: 0.769\n",
      "train loss on epoch 1445 : 0.097\n",
      "train accuracy on epoch 1445: 0.944\n",
      "test loss on epoch 1445: 0.334\n",
      "test accuracy on epoch 1445: 0.769\n",
      "train loss on epoch 1446 : 0.064\n",
      "train accuracy on epoch 1446: 1.000\n",
      "test loss on epoch 1446: 0.334\n",
      "test accuracy on epoch 1446: 0.769\n",
      "train loss on epoch 1447 : 0.056\n",
      "train accuracy on epoch 1447: 1.000\n",
      "test loss on epoch 1447: 0.339\n",
      "test accuracy on epoch 1447: 0.769\n",
      "train loss on epoch 1448 : 0.266\n",
      "train accuracy on epoch 1448: 0.833\n",
      "test loss on epoch 1448: 0.344\n",
      "test accuracy on epoch 1448: 0.769\n",
      "train loss on epoch 1449 : 0.178\n",
      "train accuracy on epoch 1449: 0.944\n",
      "test loss on epoch 1449: 0.348\n",
      "test accuracy on epoch 1449: 0.769\n",
      "train loss on epoch 1450 : 0.082\n",
      "train accuracy on epoch 1450: 1.000\n",
      "test loss on epoch 1450: 0.336\n",
      "test accuracy on epoch 1450: 0.769\n",
      "train loss on epoch 1451 : 0.104\n",
      "train accuracy on epoch 1451: 0.944\n",
      "test loss on epoch 1451: 0.346\n",
      "test accuracy on epoch 1451: 0.769\n",
      "train loss on epoch 1452 : 0.231\n",
      "train accuracy on epoch 1452: 0.889\n",
      "test loss on epoch 1452: 0.351\n",
      "test accuracy on epoch 1452: 0.769\n",
      "train loss on epoch 1453 : 0.102\n",
      "train accuracy on epoch 1453: 1.000\n",
      "test loss on epoch 1453: 0.343\n",
      "test accuracy on epoch 1453: 0.769\n",
      "train loss on epoch 1454 : 0.112\n",
      "train accuracy on epoch 1454: 0.944\n",
      "test loss on epoch 1454: 0.345\n",
      "test accuracy on epoch 1454: 0.769\n",
      "train loss on epoch 1455 : 0.174\n",
      "train accuracy on epoch 1455: 0.944\n",
      "test loss on epoch 1455: 0.372\n",
      "test accuracy on epoch 1455: 0.769\n",
      "train loss on epoch 1456 : 0.148\n",
      "train accuracy on epoch 1456: 0.944\n",
      "test loss on epoch 1456: 0.373\n",
      "test accuracy on epoch 1456: 0.769\n",
      "train loss on epoch 1457 : 0.093\n",
      "train accuracy on epoch 1457: 0.944\n",
      "test loss on epoch 1457: 0.361\n",
      "test accuracy on epoch 1457: 0.769\n",
      "train loss on epoch 1458 : 0.143\n",
      "train accuracy on epoch 1458: 0.944\n",
      "test loss on epoch 1458: 0.337\n",
      "test accuracy on epoch 1458: 0.769\n",
      "train loss on epoch 1459 : 0.157\n",
      "train accuracy on epoch 1459: 0.889\n",
      "test loss on epoch 1459: 0.340\n",
      "test accuracy on epoch 1459: 0.769\n",
      "train loss on epoch 1460 : 0.135\n",
      "train accuracy on epoch 1460: 0.944\n",
      "test loss on epoch 1460: 0.323\n",
      "test accuracy on epoch 1460: 0.769\n",
      "train loss on epoch 1461 : 0.120\n",
      "train accuracy on epoch 1461: 0.944\n",
      "test loss on epoch 1461: 0.322\n",
      "test accuracy on epoch 1461: 0.769\n",
      "train loss on epoch 1462 : 0.153\n",
      "train accuracy on epoch 1462: 0.889\n",
      "test loss on epoch 1462: 0.328\n",
      "test accuracy on epoch 1462: 0.769\n",
      "train loss on epoch 1463 : 0.194\n",
      "train accuracy on epoch 1463: 0.889\n",
      "test loss on epoch 1463: 0.336\n",
      "test accuracy on epoch 1463: 0.769\n",
      "train loss on epoch 1464 : 0.078\n",
      "train accuracy on epoch 1464: 0.944\n",
      "test loss on epoch 1464: 0.344\n",
      "test accuracy on epoch 1464: 0.769\n",
      "train loss on epoch 1465 : 0.274\n",
      "train accuracy on epoch 1465: 0.833\n",
      "test loss on epoch 1465: 0.354\n",
      "test accuracy on epoch 1465: 0.769\n",
      "train loss on epoch 1466 : 0.116\n",
      "train accuracy on epoch 1466: 0.944\n",
      "test loss on epoch 1466: 0.361\n",
      "test accuracy on epoch 1466: 0.769\n",
      "train loss on epoch 1467 : 0.037\n",
      "train accuracy on epoch 1467: 1.000\n",
      "test loss on epoch 1467: 0.365\n",
      "test accuracy on epoch 1467: 0.769\n",
      "train loss on epoch 1468 : 0.231\n",
      "train accuracy on epoch 1468: 0.944\n",
      "test loss on epoch 1468: 0.356\n",
      "test accuracy on epoch 1468: 0.769\n",
      "train loss on epoch 1469 : 0.128\n",
      "train accuracy on epoch 1469: 0.889\n",
      "test loss on epoch 1469: 0.362\n",
      "test accuracy on epoch 1469: 0.769\n",
      "train loss on epoch 1470 : 0.200\n",
      "train accuracy on epoch 1470: 0.889\n",
      "test loss on epoch 1470: 0.345\n",
      "test accuracy on epoch 1470: 0.769\n",
      "train loss on epoch 1471 : 0.430\n",
      "train accuracy on epoch 1471: 0.833\n",
      "test loss on epoch 1471: 0.334\n",
      "test accuracy on epoch 1471: 0.769\n",
      "train loss on epoch 1472 : 0.162\n",
      "train accuracy on epoch 1472: 0.889\n",
      "test loss on epoch 1472: 0.334\n",
      "test accuracy on epoch 1472: 0.769\n",
      "train loss on epoch 1473 : 0.130\n",
      "train accuracy on epoch 1473: 0.889\n",
      "test loss on epoch 1473: 0.335\n",
      "test accuracy on epoch 1473: 0.769\n",
      "train loss on epoch 1474 : 0.300\n",
      "train accuracy on epoch 1474: 0.889\n",
      "test loss on epoch 1474: 0.360\n",
      "test accuracy on epoch 1474: 0.769\n",
      "train loss on epoch 1475 : 0.184\n",
      "train accuracy on epoch 1475: 0.889\n",
      "test loss on epoch 1475: 0.368\n",
      "test accuracy on epoch 1475: 0.769\n",
      "train loss on epoch 1476 : 0.315\n",
      "train accuracy on epoch 1476: 0.889\n",
      "test loss on epoch 1476: 0.369\n",
      "test accuracy on epoch 1476: 0.769\n",
      "train loss on epoch 1477 : 0.048\n",
      "train accuracy on epoch 1477: 1.000\n",
      "test loss on epoch 1477: 0.360\n",
      "test accuracy on epoch 1477: 0.769\n",
      "train loss on epoch 1478 : 0.193\n",
      "train accuracy on epoch 1478: 0.944\n",
      "test loss on epoch 1478: 0.362\n",
      "test accuracy on epoch 1478: 0.769\n",
      "train loss on epoch 1479 : 0.051\n",
      "train accuracy on epoch 1479: 1.000\n",
      "test loss on epoch 1479: 0.377\n",
      "test accuracy on epoch 1479: 0.769\n",
      "train loss on epoch 1480 : 0.074\n",
      "train accuracy on epoch 1480: 1.000\n",
      "test loss on epoch 1480: 0.379\n",
      "test accuracy on epoch 1480: 0.769\n",
      "train loss on epoch 1481 : 0.174\n",
      "train accuracy on epoch 1481: 0.889\n",
      "test loss on epoch 1481: 0.366\n",
      "test accuracy on epoch 1481: 0.769\n",
      "train loss on epoch 1482 : 0.132\n",
      "train accuracy on epoch 1482: 1.000\n",
      "test loss on epoch 1482: 0.369\n",
      "test accuracy on epoch 1482: 0.769\n",
      "train loss on epoch 1483 : 0.165\n",
      "train accuracy on epoch 1483: 0.944\n",
      "test loss on epoch 1483: 0.383\n",
      "test accuracy on epoch 1483: 0.769\n",
      "train loss on epoch 1484 : 0.139\n",
      "train accuracy on epoch 1484: 0.944\n",
      "test loss on epoch 1484: 0.381\n",
      "test accuracy on epoch 1484: 0.769\n",
      "train loss on epoch 1485 : 0.166\n",
      "train accuracy on epoch 1485: 0.889\n",
      "test loss on epoch 1485: 0.382\n",
      "test accuracy on epoch 1485: 0.769\n",
      "train loss on epoch 1486 : 0.341\n",
      "train accuracy on epoch 1486: 0.889\n",
      "test loss on epoch 1486: 0.380\n",
      "test accuracy on epoch 1486: 0.769\n",
      "train loss on epoch 1487 : 0.047\n",
      "train accuracy on epoch 1487: 1.000\n",
      "test loss on epoch 1487: 0.387\n",
      "test accuracy on epoch 1487: 0.769\n",
      "train loss on epoch 1488 : 0.107\n",
      "train accuracy on epoch 1488: 1.000\n",
      "test loss on epoch 1488: 0.386\n",
      "test accuracy on epoch 1488: 0.769\n",
      "train loss on epoch 1489 : 0.199\n",
      "train accuracy on epoch 1489: 0.889\n",
      "test loss on epoch 1489: 0.372\n",
      "test accuracy on epoch 1489: 0.769\n",
      "train loss on epoch 1490 : 0.085\n",
      "train accuracy on epoch 1490: 1.000\n",
      "test loss on epoch 1490: 0.365\n",
      "test accuracy on epoch 1490: 0.769\n",
      "train loss on epoch 1491 : 0.259\n",
      "train accuracy on epoch 1491: 0.889\n",
      "test loss on epoch 1491: 0.362\n",
      "test accuracy on epoch 1491: 0.769\n",
      "train loss on epoch 1492 : 0.288\n",
      "train accuracy on epoch 1492: 0.889\n",
      "test loss on epoch 1492: 0.359\n",
      "test accuracy on epoch 1492: 0.769\n",
      "train loss on epoch 1493 : 0.045\n",
      "train accuracy on epoch 1493: 1.000\n",
      "test loss on epoch 1493: 0.362\n",
      "test accuracy on epoch 1493: 0.769\n",
      "train loss on epoch 1494 : 0.085\n",
      "train accuracy on epoch 1494: 1.000\n",
      "test loss on epoch 1494: 0.354\n",
      "test accuracy on epoch 1494: 0.769\n",
      "train loss on epoch 1495 : 0.040\n",
      "train accuracy on epoch 1495: 1.000\n",
      "test loss on epoch 1495: 0.338\n",
      "test accuracy on epoch 1495: 0.769\n",
      "train loss on epoch 1496 : 0.202\n",
      "train accuracy on epoch 1496: 0.944\n",
      "test loss on epoch 1496: 0.342\n",
      "test accuracy on epoch 1496: 0.769\n",
      "train loss on epoch 1497 : 0.109\n",
      "train accuracy on epoch 1497: 0.944\n",
      "test loss on epoch 1497: 0.329\n",
      "test accuracy on epoch 1497: 0.769\n",
      "train loss on epoch 1498 : 0.012\n",
      "train accuracy on epoch 1498: 1.000\n",
      "test loss on epoch 1498: 0.327\n",
      "test accuracy on epoch 1498: 0.769\n",
      "train loss on epoch 1499 : 0.092\n",
      "train accuracy on epoch 1499: 1.000\n",
      "test loss on epoch 1499: 0.325\n",
      "test accuracy on epoch 1499: 0.769\n",
      "train loss on epoch 1500 : 0.351\n",
      "train accuracy on epoch 1500: 0.944\n",
      "test loss on epoch 1500: 0.335\n",
      "test accuracy on epoch 1500: 0.769\n",
      "train loss on epoch 1501 : 0.341\n",
      "train accuracy on epoch 1501: 0.944\n",
      "test loss on epoch 1501: 0.338\n",
      "test accuracy on epoch 1501: 0.769\n",
      "train loss on epoch 1502 : 0.052\n",
      "train accuracy on epoch 1502: 0.944\n",
      "test loss on epoch 1502: 0.350\n",
      "test accuracy on epoch 1502: 0.769\n",
      "train loss on epoch 1503 : 0.082\n",
      "train accuracy on epoch 1503: 1.000\n",
      "test loss on epoch 1503: 0.344\n",
      "test accuracy on epoch 1503: 0.769\n",
      "train loss on epoch 1504 : 0.217\n",
      "train accuracy on epoch 1504: 0.889\n",
      "test loss on epoch 1504: 0.351\n",
      "test accuracy on epoch 1504: 0.769\n",
      "train loss on epoch 1505 : 0.162\n",
      "train accuracy on epoch 1505: 0.889\n",
      "test loss on epoch 1505: 0.336\n",
      "test accuracy on epoch 1505: 0.769\n",
      "train loss on epoch 1506 : 0.035\n",
      "train accuracy on epoch 1506: 1.000\n",
      "test loss on epoch 1506: 0.330\n",
      "test accuracy on epoch 1506: 0.769\n",
      "train loss on epoch 1507 : 0.101\n",
      "train accuracy on epoch 1507: 0.944\n",
      "test loss on epoch 1507: 0.329\n",
      "test accuracy on epoch 1507: 0.769\n",
      "train loss on epoch 1508 : 0.267\n",
      "train accuracy on epoch 1508: 0.833\n",
      "test loss on epoch 1508: 0.342\n",
      "test accuracy on epoch 1508: 0.769\n",
      "train loss on epoch 1509 : 0.333\n",
      "train accuracy on epoch 1509: 0.889\n",
      "test loss on epoch 1509: 0.340\n",
      "test accuracy on epoch 1509: 0.769\n",
      "train loss on epoch 1510 : 0.216\n",
      "train accuracy on epoch 1510: 0.889\n",
      "test loss on epoch 1510: 0.342\n",
      "test accuracy on epoch 1510: 0.769\n",
      "train loss on epoch 1511 : 0.141\n",
      "train accuracy on epoch 1511: 0.889\n",
      "test loss on epoch 1511: 0.333\n",
      "test accuracy on epoch 1511: 0.769\n",
      "train loss on epoch 1512 : 0.113\n",
      "train accuracy on epoch 1512: 0.944\n",
      "test loss on epoch 1512: 0.329\n",
      "test accuracy on epoch 1512: 0.769\n",
      "train loss on epoch 1513 : 0.272\n",
      "train accuracy on epoch 1513: 0.889\n",
      "test loss on epoch 1513: 0.338\n",
      "test accuracy on epoch 1513: 0.769\n",
      "train loss on epoch 1514 : 0.140\n",
      "train accuracy on epoch 1514: 0.889\n",
      "test loss on epoch 1514: 0.340\n",
      "test accuracy on epoch 1514: 0.769\n",
      "train loss on epoch 1515 : 0.073\n",
      "train accuracy on epoch 1515: 1.000\n",
      "test loss on epoch 1515: 0.342\n",
      "test accuracy on epoch 1515: 0.769\n",
      "train loss on epoch 1516 : 0.152\n",
      "train accuracy on epoch 1516: 0.889\n",
      "test loss on epoch 1516: 0.334\n",
      "test accuracy on epoch 1516: 0.769\n",
      "train loss on epoch 1517 : 0.093\n",
      "train accuracy on epoch 1517: 0.944\n",
      "test loss on epoch 1517: 0.337\n",
      "test accuracy on epoch 1517: 0.769\n",
      "train loss on epoch 1518 : 0.112\n",
      "train accuracy on epoch 1518: 0.944\n",
      "test loss on epoch 1518: 0.343\n",
      "test accuracy on epoch 1518: 0.769\n",
      "train loss on epoch 1519 : 0.058\n",
      "train accuracy on epoch 1519: 1.000\n",
      "test loss on epoch 1519: 0.342\n",
      "test accuracy on epoch 1519: 0.769\n",
      "train loss on epoch 1520 : 0.441\n",
      "train accuracy on epoch 1520: 0.833\n",
      "test loss on epoch 1520: 0.345\n",
      "test accuracy on epoch 1520: 0.769\n",
      "train loss on epoch 1521 : 0.328\n",
      "train accuracy on epoch 1521: 0.889\n",
      "test loss on epoch 1521: 0.343\n",
      "test accuracy on epoch 1521: 0.769\n",
      "train loss on epoch 1522 : 0.265\n",
      "train accuracy on epoch 1522: 0.889\n",
      "test loss on epoch 1522: 0.343\n",
      "test accuracy on epoch 1522: 0.769\n",
      "train loss on epoch 1523 : 0.057\n",
      "train accuracy on epoch 1523: 1.000\n",
      "test loss on epoch 1523: 0.334\n",
      "test accuracy on epoch 1523: 0.769\n",
      "train loss on epoch 1524 : 0.230\n",
      "train accuracy on epoch 1524: 0.889\n",
      "test loss on epoch 1524: 0.339\n",
      "test accuracy on epoch 1524: 0.769\n",
      "train loss on epoch 1525 : 0.134\n",
      "train accuracy on epoch 1525: 0.944\n",
      "test loss on epoch 1525: 0.338\n",
      "test accuracy on epoch 1525: 0.769\n",
      "train loss on epoch 1526 : 0.276\n",
      "train accuracy on epoch 1526: 0.944\n",
      "test loss on epoch 1526: 0.331\n",
      "test accuracy on epoch 1526: 0.769\n",
      "train loss on epoch 1527 : 0.155\n",
      "train accuracy on epoch 1527: 0.944\n",
      "test loss on epoch 1527: 0.325\n",
      "test accuracy on epoch 1527: 0.769\n",
      "train loss on epoch 1528 : 0.122\n",
      "train accuracy on epoch 1528: 0.944\n",
      "test loss on epoch 1528: 0.326\n",
      "test accuracy on epoch 1528: 0.769\n",
      "train loss on epoch 1529 : 0.357\n",
      "train accuracy on epoch 1529: 0.944\n",
      "test loss on epoch 1529: 0.320\n",
      "test accuracy on epoch 1529: 0.769\n",
      "train loss on epoch 1530 : 0.065\n",
      "train accuracy on epoch 1530: 1.000\n",
      "test loss on epoch 1530: 0.328\n",
      "test accuracy on epoch 1530: 0.769\n",
      "train loss on epoch 1531 : 0.349\n",
      "train accuracy on epoch 1531: 0.833\n",
      "test loss on epoch 1531: 0.328\n",
      "test accuracy on epoch 1531: 0.769\n",
      "train loss on epoch 1532 : 0.141\n",
      "train accuracy on epoch 1532: 0.944\n",
      "test loss on epoch 1532: 0.324\n",
      "test accuracy on epoch 1532: 0.769\n",
      "train loss on epoch 1533 : 0.247\n",
      "train accuracy on epoch 1533: 0.889\n",
      "test loss on epoch 1533: 0.328\n",
      "test accuracy on epoch 1533: 0.769\n",
      "train loss on epoch 1534 : 0.117\n",
      "train accuracy on epoch 1534: 0.944\n",
      "test loss on epoch 1534: 0.327\n",
      "test accuracy on epoch 1534: 0.769\n",
      "train loss on epoch 1535 : 0.286\n",
      "train accuracy on epoch 1535: 0.889\n",
      "test loss on epoch 1535: 0.341\n",
      "test accuracy on epoch 1535: 0.769\n",
      "train loss on epoch 1536 : 0.309\n",
      "train accuracy on epoch 1536: 0.889\n",
      "test loss on epoch 1536: 0.335\n",
      "test accuracy on epoch 1536: 0.769\n",
      "train loss on epoch 1537 : 0.168\n",
      "train accuracy on epoch 1537: 0.944\n",
      "test loss on epoch 1537: 0.336\n",
      "test accuracy on epoch 1537: 0.769\n",
      "train loss on epoch 1538 : 0.053\n",
      "train accuracy on epoch 1538: 1.000\n",
      "test loss on epoch 1538: 0.337\n",
      "test accuracy on epoch 1538: 0.769\n",
      "train loss on epoch 1539 : 0.086\n",
      "train accuracy on epoch 1539: 1.000\n",
      "test loss on epoch 1539: 0.353\n",
      "test accuracy on epoch 1539: 0.769\n",
      "train loss on epoch 1540 : 0.099\n",
      "train accuracy on epoch 1540: 0.944\n",
      "test loss on epoch 1540: 0.359\n",
      "test accuracy on epoch 1540: 0.769\n",
      "train loss on epoch 1541 : 0.278\n",
      "train accuracy on epoch 1541: 0.889\n",
      "test loss on epoch 1541: 0.343\n",
      "test accuracy on epoch 1541: 0.769\n",
      "train loss on epoch 1542 : 0.284\n",
      "train accuracy on epoch 1542: 0.944\n",
      "test loss on epoch 1542: 0.348\n",
      "test accuracy on epoch 1542: 0.769\n",
      "train loss on epoch 1543 : 0.157\n",
      "train accuracy on epoch 1543: 0.889\n",
      "test loss on epoch 1543: 0.340\n",
      "test accuracy on epoch 1543: 0.769\n",
      "train loss on epoch 1544 : 0.066\n",
      "train accuracy on epoch 1544: 1.000\n",
      "test loss on epoch 1544: 0.337\n",
      "test accuracy on epoch 1544: 0.769\n",
      "train loss on epoch 1545 : 0.142\n",
      "train accuracy on epoch 1545: 0.944\n",
      "test loss on epoch 1545: 0.349\n",
      "test accuracy on epoch 1545: 0.769\n",
      "train loss on epoch 1546 : 0.110\n",
      "train accuracy on epoch 1546: 0.944\n",
      "test loss on epoch 1546: 0.344\n",
      "test accuracy on epoch 1546: 0.769\n",
      "train loss on epoch 1547 : 0.136\n",
      "train accuracy on epoch 1547: 0.889\n",
      "test loss on epoch 1547: 0.344\n",
      "test accuracy on epoch 1547: 0.769\n",
      "train loss on epoch 1548 : 0.042\n",
      "train accuracy on epoch 1548: 1.000\n",
      "test loss on epoch 1548: 0.351\n",
      "test accuracy on epoch 1548: 0.769\n",
      "train loss on epoch 1549 : 0.127\n",
      "train accuracy on epoch 1549: 0.944\n",
      "test loss on epoch 1549: 0.345\n",
      "test accuracy on epoch 1549: 0.769\n",
      "train loss on epoch 1550 : 0.101\n",
      "train accuracy on epoch 1550: 0.944\n",
      "test loss on epoch 1550: 0.342\n",
      "test accuracy on epoch 1550: 0.769\n",
      "train loss on epoch 1551 : 0.055\n",
      "train accuracy on epoch 1551: 1.000\n",
      "test loss on epoch 1551: 0.341\n",
      "test accuracy on epoch 1551: 0.769\n",
      "train loss on epoch 1552 : 0.189\n",
      "train accuracy on epoch 1552: 0.944\n",
      "test loss on epoch 1552: 0.335\n",
      "test accuracy on epoch 1552: 0.769\n",
      "train loss on epoch 1553 : 0.124\n",
      "train accuracy on epoch 1553: 0.944\n",
      "test loss on epoch 1553: 0.327\n",
      "test accuracy on epoch 1553: 0.769\n",
      "train loss on epoch 1554 : 0.077\n",
      "train accuracy on epoch 1554: 1.000\n",
      "test loss on epoch 1554: 0.327\n",
      "test accuracy on epoch 1554: 0.769\n",
      "train loss on epoch 1555 : 0.114\n",
      "train accuracy on epoch 1555: 1.000\n",
      "test loss on epoch 1555: 0.319\n",
      "test accuracy on epoch 1555: 0.769\n",
      "train loss on epoch 1556 : 0.227\n",
      "train accuracy on epoch 1556: 0.833\n",
      "test loss on epoch 1556: 0.315\n",
      "test accuracy on epoch 1556: 0.769\n",
      "train loss on epoch 1557 : 0.037\n",
      "train accuracy on epoch 1557: 1.000\n",
      "test loss on epoch 1557: 0.323\n",
      "test accuracy on epoch 1557: 0.769\n",
      "train loss on epoch 1558 : 0.201\n",
      "train accuracy on epoch 1558: 0.944\n",
      "test loss on epoch 1558: 0.322\n",
      "test accuracy on epoch 1558: 0.769\n",
      "train loss on epoch 1559 : 0.115\n",
      "train accuracy on epoch 1559: 1.000\n",
      "test loss on epoch 1559: 0.329\n",
      "test accuracy on epoch 1559: 0.769\n",
      "train loss on epoch 1560 : 0.191\n",
      "train accuracy on epoch 1560: 0.944\n",
      "test loss on epoch 1560: 0.350\n",
      "test accuracy on epoch 1560: 0.769\n",
      "train loss on epoch 1561 : 0.098\n",
      "train accuracy on epoch 1561: 0.944\n",
      "test loss on epoch 1561: 0.355\n",
      "test accuracy on epoch 1561: 0.769\n",
      "train loss on epoch 1562 : 0.077\n",
      "train accuracy on epoch 1562: 0.944\n",
      "test loss on epoch 1562: 0.336\n",
      "test accuracy on epoch 1562: 0.769\n",
      "train loss on epoch 1563 : 0.233\n",
      "train accuracy on epoch 1563: 0.944\n",
      "test loss on epoch 1563: 0.328\n",
      "test accuracy on epoch 1563: 0.769\n",
      "train loss on epoch 1564 : 0.081\n",
      "train accuracy on epoch 1564: 0.944\n",
      "test loss on epoch 1564: 0.333\n",
      "test accuracy on epoch 1564: 0.769\n",
      "train loss on epoch 1565 : 0.180\n",
      "train accuracy on epoch 1565: 0.944\n",
      "test loss on epoch 1565: 0.331\n",
      "test accuracy on epoch 1565: 0.769\n",
      "train loss on epoch 1566 : 0.117\n",
      "train accuracy on epoch 1566: 0.944\n",
      "test loss on epoch 1566: 0.337\n",
      "test accuracy on epoch 1566: 0.769\n",
      "train loss on epoch 1567 : 0.306\n",
      "train accuracy on epoch 1567: 0.889\n",
      "test loss on epoch 1567: 0.350\n",
      "test accuracy on epoch 1567: 0.769\n",
      "train loss on epoch 1568 : 0.202\n",
      "train accuracy on epoch 1568: 0.944\n",
      "test loss on epoch 1568: 0.370\n",
      "test accuracy on epoch 1568: 0.769\n",
      "train loss on epoch 1569 : 0.310\n",
      "train accuracy on epoch 1569: 0.944\n",
      "test loss on epoch 1569: 0.387\n",
      "test accuracy on epoch 1569: 0.769\n",
      "train loss on epoch 1570 : 0.244\n",
      "train accuracy on epoch 1570: 0.889\n",
      "test loss on epoch 1570: 0.366\n",
      "test accuracy on epoch 1570: 0.769\n",
      "train loss on epoch 1571 : 0.146\n",
      "train accuracy on epoch 1571: 0.944\n",
      "test loss on epoch 1571: 0.354\n",
      "test accuracy on epoch 1571: 0.769\n",
      "train loss on epoch 1572 : 0.170\n",
      "train accuracy on epoch 1572: 0.944\n",
      "test loss on epoch 1572: 0.337\n",
      "test accuracy on epoch 1572: 0.769\n",
      "train loss on epoch 1573 : 0.073\n",
      "train accuracy on epoch 1573: 1.000\n",
      "test loss on epoch 1573: 0.341\n",
      "test accuracy on epoch 1573: 0.769\n",
      "train loss on epoch 1574 : 0.087\n",
      "train accuracy on epoch 1574: 1.000\n",
      "test loss on epoch 1574: 0.333\n",
      "test accuracy on epoch 1574: 0.769\n",
      "train loss on epoch 1575 : 0.112\n",
      "train accuracy on epoch 1575: 0.889\n",
      "test loss on epoch 1575: 0.320\n",
      "test accuracy on epoch 1575: 0.769\n",
      "train loss on epoch 1576 : 0.066\n",
      "train accuracy on epoch 1576: 0.944\n",
      "test loss on epoch 1576: 0.321\n",
      "test accuracy on epoch 1576: 0.769\n",
      "train loss on epoch 1577 : 0.269\n",
      "train accuracy on epoch 1577: 0.944\n",
      "test loss on epoch 1577: 0.327\n",
      "test accuracy on epoch 1577: 0.769\n",
      "train loss on epoch 1578 : 0.064\n",
      "train accuracy on epoch 1578: 0.944\n",
      "test loss on epoch 1578: 0.324\n",
      "test accuracy on epoch 1578: 0.769\n",
      "train loss on epoch 1579 : 0.133\n",
      "train accuracy on epoch 1579: 0.944\n",
      "test loss on epoch 1579: 0.335\n",
      "test accuracy on epoch 1579: 0.769\n",
      "train loss on epoch 1580 : 0.180\n",
      "train accuracy on epoch 1580: 0.889\n",
      "test loss on epoch 1580: 0.330\n",
      "test accuracy on epoch 1580: 0.769\n",
      "train loss on epoch 1581 : 0.377\n",
      "train accuracy on epoch 1581: 0.833\n",
      "test loss on epoch 1581: 0.340\n",
      "test accuracy on epoch 1581: 0.769\n",
      "train loss on epoch 1582 : 0.076\n",
      "train accuracy on epoch 1582: 1.000\n",
      "test loss on epoch 1582: 0.343\n",
      "test accuracy on epoch 1582: 0.769\n",
      "train loss on epoch 1583 : 0.024\n",
      "train accuracy on epoch 1583: 1.000\n",
      "test loss on epoch 1583: 0.344\n",
      "test accuracy on epoch 1583: 0.769\n",
      "train loss on epoch 1584 : 0.100\n",
      "train accuracy on epoch 1584: 0.944\n",
      "test loss on epoch 1584: 0.330\n",
      "test accuracy on epoch 1584: 0.769\n",
      "train loss on epoch 1585 : 0.163\n",
      "train accuracy on epoch 1585: 0.944\n",
      "test loss on epoch 1585: 0.340\n",
      "test accuracy on epoch 1585: 0.769\n",
      "train loss on epoch 1586 : 0.484\n",
      "train accuracy on epoch 1586: 0.889\n",
      "test loss on epoch 1586: 0.338\n",
      "test accuracy on epoch 1586: 0.769\n",
      "train loss on epoch 1587 : 0.200\n",
      "train accuracy on epoch 1587: 0.889\n",
      "test loss on epoch 1587: 0.324\n",
      "test accuracy on epoch 1587: 0.769\n",
      "train loss on epoch 1588 : 0.256\n",
      "train accuracy on epoch 1588: 0.889\n",
      "test loss on epoch 1588: 0.337\n",
      "test accuracy on epoch 1588: 0.769\n",
      "train loss on epoch 1589 : 0.133\n",
      "train accuracy on epoch 1589: 0.944\n",
      "test loss on epoch 1589: 0.336\n",
      "test accuracy on epoch 1589: 0.769\n",
      "train loss on epoch 1590 : 0.220\n",
      "train accuracy on epoch 1590: 0.889\n",
      "test loss on epoch 1590: 0.326\n",
      "test accuracy on epoch 1590: 0.769\n",
      "train loss on epoch 1591 : 0.257\n",
      "train accuracy on epoch 1591: 0.889\n",
      "test loss on epoch 1591: 0.331\n",
      "test accuracy on epoch 1591: 0.769\n",
      "train loss on epoch 1592 : 0.021\n",
      "train accuracy on epoch 1592: 1.000\n",
      "test loss on epoch 1592: 0.331\n",
      "test accuracy on epoch 1592: 0.769\n",
      "train loss on epoch 1593 : 0.229\n",
      "train accuracy on epoch 1593: 0.944\n",
      "test loss on epoch 1593: 0.324\n",
      "test accuracy on epoch 1593: 0.769\n",
      "train loss on epoch 1594 : 0.088\n",
      "train accuracy on epoch 1594: 0.944\n",
      "test loss on epoch 1594: 0.329\n",
      "test accuracy on epoch 1594: 0.769\n",
      "train loss on epoch 1595 : 0.125\n",
      "train accuracy on epoch 1595: 0.889\n",
      "test loss on epoch 1595: 0.324\n",
      "test accuracy on epoch 1595: 0.769\n",
      "train loss on epoch 1596 : 0.154\n",
      "train accuracy on epoch 1596: 0.944\n",
      "test loss on epoch 1596: 0.332\n",
      "test accuracy on epoch 1596: 0.769\n",
      "train loss on epoch 1597 : 0.110\n",
      "train accuracy on epoch 1597: 0.944\n",
      "test loss on epoch 1597: 0.322\n",
      "test accuracy on epoch 1597: 0.769\n",
      "train loss on epoch 1598 : 0.073\n",
      "train accuracy on epoch 1598: 0.944\n",
      "test loss on epoch 1598: 0.338\n",
      "test accuracy on epoch 1598: 0.769\n",
      "train loss on epoch 1599 : 0.053\n",
      "train accuracy on epoch 1599: 1.000\n",
      "test loss on epoch 1599: 0.329\n",
      "test accuracy on epoch 1599: 0.769\n",
      "train loss on epoch 1600 : 0.288\n",
      "train accuracy on epoch 1600: 0.889\n",
      "test loss on epoch 1600: 0.332\n",
      "test accuracy on epoch 1600: 0.769\n",
      "train loss on epoch 1601 : 0.135\n",
      "train accuracy on epoch 1601: 0.889\n",
      "test loss on epoch 1601: 0.330\n",
      "test accuracy on epoch 1601: 0.769\n",
      "train loss on epoch 1602 : 0.156\n",
      "train accuracy on epoch 1602: 0.889\n",
      "test loss on epoch 1602: 0.335\n",
      "test accuracy on epoch 1602: 0.769\n",
      "train loss on epoch 1603 : 0.258\n",
      "train accuracy on epoch 1603: 0.889\n",
      "test loss on epoch 1603: 0.333\n",
      "test accuracy on epoch 1603: 0.769\n",
      "train loss on epoch 1604 : 0.128\n",
      "train accuracy on epoch 1604: 0.944\n",
      "test loss on epoch 1604: 0.322\n",
      "test accuracy on epoch 1604: 0.769\n",
      "train loss on epoch 1605 : 0.067\n",
      "train accuracy on epoch 1605: 1.000\n",
      "test loss on epoch 1605: 0.321\n",
      "test accuracy on epoch 1605: 0.769\n",
      "train loss on epoch 1606 : 0.351\n",
      "train accuracy on epoch 1606: 0.889\n",
      "test loss on epoch 1606: 0.316\n",
      "test accuracy on epoch 1606: 0.769\n",
      "train loss on epoch 1607 : 0.061\n",
      "train accuracy on epoch 1607: 0.944\n",
      "test loss on epoch 1607: 0.318\n",
      "test accuracy on epoch 1607: 0.769\n",
      "train loss on epoch 1608 : 0.236\n",
      "train accuracy on epoch 1608: 0.889\n",
      "test loss on epoch 1608: 0.315\n",
      "test accuracy on epoch 1608: 0.769\n",
      "train loss on epoch 1609 : 0.285\n",
      "train accuracy on epoch 1609: 0.889\n",
      "test loss on epoch 1609: 0.318\n",
      "test accuracy on epoch 1609: 0.769\n",
      "train loss on epoch 1610 : 0.163\n",
      "train accuracy on epoch 1610: 0.944\n",
      "test loss on epoch 1610: 0.313\n",
      "test accuracy on epoch 1610: 0.769\n",
      "train loss on epoch 1611 : 0.110\n",
      "train accuracy on epoch 1611: 1.000\n",
      "test loss on epoch 1611: 0.314\n",
      "test accuracy on epoch 1611: 0.769\n",
      "train loss on epoch 1612 : 0.307\n",
      "train accuracy on epoch 1612: 0.944\n",
      "test loss on epoch 1612: 0.327\n",
      "test accuracy on epoch 1612: 0.769\n",
      "train loss on epoch 1613 : 0.112\n",
      "train accuracy on epoch 1613: 0.944\n",
      "test loss on epoch 1613: 0.329\n",
      "test accuracy on epoch 1613: 0.769\n",
      "train loss on epoch 1614 : 0.248\n",
      "train accuracy on epoch 1614: 0.944\n",
      "test loss on epoch 1614: 0.333\n",
      "test accuracy on epoch 1614: 0.769\n",
      "train loss on epoch 1615 : 0.105\n",
      "train accuracy on epoch 1615: 0.944\n",
      "test loss on epoch 1615: 0.337\n",
      "test accuracy on epoch 1615: 0.769\n",
      "train loss on epoch 1616 : 0.190\n",
      "train accuracy on epoch 1616: 0.944\n",
      "test loss on epoch 1616: 0.331\n",
      "test accuracy on epoch 1616: 0.769\n",
      "train loss on epoch 1617 : 0.260\n",
      "train accuracy on epoch 1617: 0.944\n",
      "test loss on epoch 1617: 0.341\n",
      "test accuracy on epoch 1617: 0.769\n",
      "train loss on epoch 1618 : 0.304\n",
      "train accuracy on epoch 1618: 0.944\n",
      "test loss on epoch 1618: 0.347\n",
      "test accuracy on epoch 1618: 0.769\n",
      "train loss on epoch 1619 : 0.181\n",
      "train accuracy on epoch 1619: 0.944\n",
      "test loss on epoch 1619: 0.355\n",
      "test accuracy on epoch 1619: 0.769\n",
      "train loss on epoch 1620 : 0.075\n",
      "train accuracy on epoch 1620: 0.944\n",
      "test loss on epoch 1620: 0.352\n",
      "test accuracy on epoch 1620: 0.769\n",
      "train loss on epoch 1621 : 0.120\n",
      "train accuracy on epoch 1621: 0.944\n",
      "test loss on epoch 1621: 0.349\n",
      "test accuracy on epoch 1621: 0.769\n",
      "train loss on epoch 1622 : 0.160\n",
      "train accuracy on epoch 1622: 0.889\n",
      "test loss on epoch 1622: 0.333\n",
      "test accuracy on epoch 1622: 0.769\n",
      "train loss on epoch 1623 : 0.248\n",
      "train accuracy on epoch 1623: 0.889\n",
      "test loss on epoch 1623: 0.323\n",
      "test accuracy on epoch 1623: 0.769\n",
      "train loss on epoch 1624 : 0.157\n",
      "train accuracy on epoch 1624: 0.944\n",
      "test loss on epoch 1624: 0.319\n",
      "test accuracy on epoch 1624: 0.769\n",
      "train loss on epoch 1625 : 0.074\n",
      "train accuracy on epoch 1625: 0.944\n",
      "test loss on epoch 1625: 0.325\n",
      "test accuracy on epoch 1625: 0.769\n",
      "train loss on epoch 1626 : 0.164\n",
      "train accuracy on epoch 1626: 0.889\n",
      "test loss on epoch 1626: 0.325\n",
      "test accuracy on epoch 1626: 0.769\n",
      "train loss on epoch 1627 : 0.028\n",
      "train accuracy on epoch 1627: 1.000\n",
      "test loss on epoch 1627: 0.333\n",
      "test accuracy on epoch 1627: 0.769\n",
      "train loss on epoch 1628 : 0.348\n",
      "train accuracy on epoch 1628: 0.944\n",
      "test loss on epoch 1628: 0.345\n",
      "test accuracy on epoch 1628: 0.769\n",
      "train loss on epoch 1629 : 0.061\n",
      "train accuracy on epoch 1629: 1.000\n",
      "test loss on epoch 1629: 0.346\n",
      "test accuracy on epoch 1629: 0.769\n",
      "train loss on epoch 1630 : 0.236\n",
      "train accuracy on epoch 1630: 0.889\n",
      "test loss on epoch 1630: 0.343\n",
      "test accuracy on epoch 1630: 0.769\n",
      "train loss on epoch 1631 : 0.475\n",
      "train accuracy on epoch 1631: 0.889\n",
      "test loss on epoch 1631: 0.348\n",
      "test accuracy on epoch 1631: 0.769\n",
      "train loss on epoch 1632 : 0.016\n",
      "train accuracy on epoch 1632: 1.000\n",
      "test loss on epoch 1632: 0.346\n",
      "test accuracy on epoch 1632: 0.769\n",
      "train loss on epoch 1633 : 0.169\n",
      "train accuracy on epoch 1633: 0.944\n",
      "test loss on epoch 1633: 0.336\n",
      "test accuracy on epoch 1633: 0.769\n",
      "train loss on epoch 1634 : 0.106\n",
      "train accuracy on epoch 1634: 0.944\n",
      "test loss on epoch 1634: 0.336\n",
      "test accuracy on epoch 1634: 0.769\n",
      "train loss on epoch 1635 : 0.060\n",
      "train accuracy on epoch 1635: 0.944\n",
      "test loss on epoch 1635: 0.339\n",
      "test accuracy on epoch 1635: 0.769\n",
      "train loss on epoch 1636 : 0.064\n",
      "train accuracy on epoch 1636: 1.000\n",
      "test loss on epoch 1636: 0.336\n",
      "test accuracy on epoch 1636: 0.769\n",
      "train loss on epoch 1637 : 0.112\n",
      "train accuracy on epoch 1637: 0.889\n",
      "test loss on epoch 1637: 0.329\n",
      "test accuracy on epoch 1637: 0.769\n",
      "train loss on epoch 1638 : 0.238\n",
      "train accuracy on epoch 1638: 0.889\n",
      "test loss on epoch 1638: 0.326\n",
      "test accuracy on epoch 1638: 0.769\n",
      "train loss on epoch 1639 : 0.208\n",
      "train accuracy on epoch 1639: 0.889\n",
      "test loss on epoch 1639: 0.325\n",
      "test accuracy on epoch 1639: 0.769\n",
      "train loss on epoch 1640 : 0.072\n",
      "train accuracy on epoch 1640: 1.000\n",
      "test loss on epoch 1640: 0.320\n",
      "test accuracy on epoch 1640: 0.769\n",
      "train loss on epoch 1641 : 0.225\n",
      "train accuracy on epoch 1641: 0.944\n",
      "test loss on epoch 1641: 0.321\n",
      "test accuracy on epoch 1641: 0.769\n",
      "train loss on epoch 1642 : 0.311\n",
      "train accuracy on epoch 1642: 0.889\n",
      "test loss on epoch 1642: 0.326\n",
      "test accuracy on epoch 1642: 0.769\n",
      "train loss on epoch 1643 : 0.124\n",
      "train accuracy on epoch 1643: 1.000\n",
      "test loss on epoch 1643: 0.330\n",
      "test accuracy on epoch 1643: 0.769\n",
      "train loss on epoch 1644 : 0.057\n",
      "train accuracy on epoch 1644: 1.000\n",
      "test loss on epoch 1644: 0.333\n",
      "test accuracy on epoch 1644: 0.769\n",
      "train loss on epoch 1645 : 0.217\n",
      "train accuracy on epoch 1645: 0.944\n",
      "test loss on epoch 1645: 0.324\n",
      "test accuracy on epoch 1645: 0.769\n",
      "train loss on epoch 1646 : 0.177\n",
      "train accuracy on epoch 1646: 0.889\n",
      "test loss on epoch 1646: 0.324\n",
      "test accuracy on epoch 1646: 0.769\n",
      "train loss on epoch 1647 : 0.413\n",
      "train accuracy on epoch 1647: 0.889\n",
      "test loss on epoch 1647: 0.329\n",
      "test accuracy on epoch 1647: 0.769\n",
      "train loss on epoch 1648 : 0.143\n",
      "train accuracy on epoch 1648: 0.944\n",
      "test loss on epoch 1648: 0.320\n",
      "test accuracy on epoch 1648: 0.769\n",
      "train loss on epoch 1649 : 0.205\n",
      "train accuracy on epoch 1649: 0.889\n",
      "test loss on epoch 1649: 0.322\n",
      "test accuracy on epoch 1649: 0.769\n",
      "train loss on epoch 1650 : 0.094\n",
      "train accuracy on epoch 1650: 0.944\n",
      "test loss on epoch 1650: 0.322\n",
      "test accuracy on epoch 1650: 0.769\n",
      "train loss on epoch 1651 : 0.246\n",
      "train accuracy on epoch 1651: 0.889\n",
      "test loss on epoch 1651: 0.321\n",
      "test accuracy on epoch 1651: 0.769\n",
      "train loss on epoch 1652 : 0.277\n",
      "train accuracy on epoch 1652: 0.833\n",
      "test loss on epoch 1652: 0.319\n",
      "test accuracy on epoch 1652: 0.769\n",
      "train loss on epoch 1653 : 0.215\n",
      "train accuracy on epoch 1653: 0.944\n",
      "test loss on epoch 1653: 0.320\n",
      "test accuracy on epoch 1653: 0.769\n",
      "train loss on epoch 1654 : 0.131\n",
      "train accuracy on epoch 1654: 0.944\n",
      "test loss on epoch 1654: 0.326\n",
      "test accuracy on epoch 1654: 0.769\n",
      "train loss on epoch 1655 : 0.352\n",
      "train accuracy on epoch 1655: 0.889\n",
      "test loss on epoch 1655: 0.336\n",
      "test accuracy on epoch 1655: 0.769\n",
      "train loss on epoch 1656 : 0.092\n",
      "train accuracy on epoch 1656: 1.000\n",
      "test loss on epoch 1656: 0.337\n",
      "test accuracy on epoch 1656: 0.769\n",
      "train loss on epoch 1657 : 0.036\n",
      "train accuracy on epoch 1657: 1.000\n",
      "test loss on epoch 1657: 0.331\n",
      "test accuracy on epoch 1657: 0.769\n",
      "train loss on epoch 1658 : 0.529\n",
      "train accuracy on epoch 1658: 0.833\n",
      "test loss on epoch 1658: 0.335\n",
      "test accuracy on epoch 1658: 0.769\n",
      "train loss on epoch 1659 : 0.085\n",
      "train accuracy on epoch 1659: 0.944\n",
      "test loss on epoch 1659: 0.337\n",
      "test accuracy on epoch 1659: 0.769\n",
      "train loss on epoch 1660 : 0.104\n",
      "train accuracy on epoch 1660: 0.944\n",
      "test loss on epoch 1660: 0.351\n",
      "test accuracy on epoch 1660: 0.769\n",
      "train loss on epoch 1661 : 0.144\n",
      "train accuracy on epoch 1661: 0.944\n",
      "test loss on epoch 1661: 0.345\n",
      "test accuracy on epoch 1661: 0.769\n",
      "train loss on epoch 1662 : 0.366\n",
      "train accuracy on epoch 1662: 0.833\n",
      "test loss on epoch 1662: 0.338\n",
      "test accuracy on epoch 1662: 0.769\n",
      "train loss on epoch 1663 : 0.190\n",
      "train accuracy on epoch 1663: 0.944\n",
      "test loss on epoch 1663: 0.320\n",
      "test accuracy on epoch 1663: 0.769\n",
      "train loss on epoch 1664 : 0.087\n",
      "train accuracy on epoch 1664: 0.944\n",
      "test loss on epoch 1664: 0.326\n",
      "test accuracy on epoch 1664: 0.769\n",
      "train loss on epoch 1665 : 0.103\n",
      "train accuracy on epoch 1665: 0.944\n",
      "test loss on epoch 1665: 0.315\n",
      "test accuracy on epoch 1665: 0.769\n",
      "train loss on epoch 1666 : 0.088\n",
      "train accuracy on epoch 1666: 0.944\n",
      "test loss on epoch 1666: 0.316\n",
      "test accuracy on epoch 1666: 0.769\n",
      "train loss on epoch 1667 : 0.128\n",
      "train accuracy on epoch 1667: 0.944\n",
      "test loss on epoch 1667: 0.315\n",
      "test accuracy on epoch 1667: 0.769\n",
      "train loss on epoch 1668 : 0.197\n",
      "train accuracy on epoch 1668: 0.889\n",
      "test loss on epoch 1668: 0.315\n",
      "test accuracy on epoch 1668: 0.769\n",
      "train loss on epoch 1669 : 0.259\n",
      "train accuracy on epoch 1669: 0.889\n",
      "test loss on epoch 1669: 0.321\n",
      "test accuracy on epoch 1669: 0.769\n",
      "train loss on epoch 1670 : 0.223\n",
      "train accuracy on epoch 1670: 0.889\n",
      "test loss on epoch 1670: 0.326\n",
      "test accuracy on epoch 1670: 0.769\n",
      "train loss on epoch 1671 : 0.299\n",
      "train accuracy on epoch 1671: 0.889\n",
      "test loss on epoch 1671: 0.337\n",
      "test accuracy on epoch 1671: 0.769\n",
      "train loss on epoch 1672 : 0.275\n",
      "train accuracy on epoch 1672: 0.889\n",
      "test loss on epoch 1672: 0.332\n",
      "test accuracy on epoch 1672: 0.769\n",
      "train loss on epoch 1673 : 0.204\n",
      "train accuracy on epoch 1673: 0.889\n",
      "test loss on epoch 1673: 0.333\n",
      "test accuracy on epoch 1673: 0.769\n",
      "train loss on epoch 1674 : 0.094\n",
      "train accuracy on epoch 1674: 1.000\n",
      "test loss on epoch 1674: 0.327\n",
      "test accuracy on epoch 1674: 0.769\n",
      "train loss on epoch 1675 : 0.304\n",
      "train accuracy on epoch 1675: 0.889\n",
      "test loss on epoch 1675: 0.330\n",
      "test accuracy on epoch 1675: 0.769\n",
      "train loss on epoch 1676 : 0.159\n",
      "train accuracy on epoch 1676: 0.944\n",
      "test loss on epoch 1676: 0.345\n",
      "test accuracy on epoch 1676: 0.769\n",
      "train loss on epoch 1677 : 0.487\n",
      "train accuracy on epoch 1677: 0.889\n",
      "test loss on epoch 1677: 0.347\n",
      "test accuracy on epoch 1677: 0.769\n",
      "train loss on epoch 1678 : 0.170\n",
      "train accuracy on epoch 1678: 0.889\n",
      "test loss on epoch 1678: 0.354\n",
      "test accuracy on epoch 1678: 0.769\n",
      "train loss on epoch 1679 : 0.126\n",
      "train accuracy on epoch 1679: 0.944\n",
      "test loss on epoch 1679: 0.366\n",
      "test accuracy on epoch 1679: 0.769\n",
      "train loss on epoch 1680 : 0.189\n",
      "train accuracy on epoch 1680: 0.944\n",
      "test loss on epoch 1680: 0.376\n",
      "test accuracy on epoch 1680: 0.769\n",
      "train loss on epoch 1681 : 0.165\n",
      "train accuracy on epoch 1681: 0.944\n",
      "test loss on epoch 1681: 0.371\n",
      "test accuracy on epoch 1681: 0.769\n",
      "train loss on epoch 1682 : 0.139\n",
      "train accuracy on epoch 1682: 0.944\n",
      "test loss on epoch 1682: 0.361\n",
      "test accuracy on epoch 1682: 0.769\n",
      "train loss on epoch 1683 : 0.235\n",
      "train accuracy on epoch 1683: 0.889\n",
      "test loss on epoch 1683: 0.351\n",
      "test accuracy on epoch 1683: 0.769\n",
      "train loss on epoch 1684 : 0.267\n",
      "train accuracy on epoch 1684: 0.944\n",
      "test loss on epoch 1684: 0.343\n",
      "test accuracy on epoch 1684: 0.769\n",
      "train loss on epoch 1685 : 0.408\n",
      "train accuracy on epoch 1685: 0.889\n",
      "test loss on epoch 1685: 0.334\n",
      "test accuracy on epoch 1685: 0.769\n",
      "train loss on epoch 1686 : 0.154\n",
      "train accuracy on epoch 1686: 0.944\n",
      "test loss on epoch 1686: 0.323\n",
      "test accuracy on epoch 1686: 0.769\n",
      "train loss on epoch 1687 : 0.044\n",
      "train accuracy on epoch 1687: 1.000\n",
      "test loss on epoch 1687: 0.325\n",
      "test accuracy on epoch 1687: 0.769\n",
      "train loss on epoch 1688 : 0.285\n",
      "train accuracy on epoch 1688: 0.833\n",
      "test loss on epoch 1688: 0.317\n",
      "test accuracy on epoch 1688: 0.769\n",
      "train loss on epoch 1689 : 0.060\n",
      "train accuracy on epoch 1689: 1.000\n",
      "test loss on epoch 1689: 0.320\n",
      "test accuracy on epoch 1689: 0.769\n",
      "train loss on epoch 1690 : 0.328\n",
      "train accuracy on epoch 1690: 0.889\n",
      "test loss on epoch 1690: 0.326\n",
      "test accuracy on epoch 1690: 0.769\n",
      "train loss on epoch 1691 : 0.199\n",
      "train accuracy on epoch 1691: 0.944\n",
      "test loss on epoch 1691: 0.336\n",
      "test accuracy on epoch 1691: 0.769\n",
      "train loss on epoch 1692 : 0.125\n",
      "train accuracy on epoch 1692: 0.944\n",
      "test loss on epoch 1692: 0.334\n",
      "test accuracy on epoch 1692: 0.769\n",
      "train loss on epoch 1693 : 0.182\n",
      "train accuracy on epoch 1693: 0.944\n",
      "test loss on epoch 1693: 0.331\n",
      "test accuracy on epoch 1693: 0.769\n",
      "train loss on epoch 1694 : 0.287\n",
      "train accuracy on epoch 1694: 0.889\n",
      "test loss on epoch 1694: 0.339\n",
      "test accuracy on epoch 1694: 0.769\n",
      "train loss on epoch 1695 : 0.055\n",
      "train accuracy on epoch 1695: 1.000\n",
      "test loss on epoch 1695: 0.332\n",
      "test accuracy on epoch 1695: 0.769\n",
      "train loss on epoch 1696 : 0.204\n",
      "train accuracy on epoch 1696: 0.889\n",
      "test loss on epoch 1696: 0.337\n",
      "test accuracy on epoch 1696: 0.769\n",
      "train loss on epoch 1697 : 0.199\n",
      "train accuracy on epoch 1697: 0.889\n",
      "test loss on epoch 1697: 0.331\n",
      "test accuracy on epoch 1697: 0.769\n",
      "train loss on epoch 1698 : 0.276\n",
      "train accuracy on epoch 1698: 0.889\n",
      "test loss on epoch 1698: 0.327\n",
      "test accuracy on epoch 1698: 0.769\n",
      "train loss on epoch 1699 : 0.269\n",
      "train accuracy on epoch 1699: 0.833\n",
      "test loss on epoch 1699: 0.327\n",
      "test accuracy on epoch 1699: 0.769\n",
      "train loss on epoch 1700 : 0.251\n",
      "train accuracy on epoch 1700: 0.833\n",
      "test loss on epoch 1700: 0.337\n",
      "test accuracy on epoch 1700: 0.769\n",
      "train loss on epoch 1701 : 0.107\n",
      "train accuracy on epoch 1701: 0.944\n",
      "test loss on epoch 1701: 0.340\n",
      "test accuracy on epoch 1701: 0.769\n",
      "train loss on epoch 1702 : 0.124\n",
      "train accuracy on epoch 1702: 0.944\n",
      "test loss on epoch 1702: 0.353\n",
      "test accuracy on epoch 1702: 0.769\n",
      "train loss on epoch 1703 : 0.241\n",
      "train accuracy on epoch 1703: 0.889\n",
      "test loss on epoch 1703: 0.346\n",
      "test accuracy on epoch 1703: 0.769\n",
      "train loss on epoch 1704 : 0.075\n",
      "train accuracy on epoch 1704: 0.944\n",
      "test loss on epoch 1704: 0.347\n",
      "test accuracy on epoch 1704: 0.769\n",
      "train loss on epoch 1705 : 0.315\n",
      "train accuracy on epoch 1705: 0.778\n",
      "test loss on epoch 1705: 0.334\n",
      "test accuracy on epoch 1705: 0.769\n",
      "train loss on epoch 1706 : 0.257\n",
      "train accuracy on epoch 1706: 0.889\n",
      "test loss on epoch 1706: 0.327\n",
      "test accuracy on epoch 1706: 0.769\n",
      "train loss on epoch 1707 : 0.095\n",
      "train accuracy on epoch 1707: 0.944\n",
      "test loss on epoch 1707: 0.322\n",
      "test accuracy on epoch 1707: 0.769\n",
      "train loss on epoch 1708 : 0.422\n",
      "train accuracy on epoch 1708: 0.889\n",
      "test loss on epoch 1708: 0.318\n",
      "test accuracy on epoch 1708: 0.769\n",
      "train loss on epoch 1709 : 0.221\n",
      "train accuracy on epoch 1709: 0.889\n",
      "test loss on epoch 1709: 0.315\n",
      "test accuracy on epoch 1709: 0.769\n",
      "train loss on epoch 1710 : 0.242\n",
      "train accuracy on epoch 1710: 0.889\n",
      "test loss on epoch 1710: 0.315\n",
      "test accuracy on epoch 1710: 0.769\n",
      "train loss on epoch 1711 : 0.234\n",
      "train accuracy on epoch 1711: 0.889\n",
      "test loss on epoch 1711: 0.317\n",
      "test accuracy on epoch 1711: 0.769\n",
      "train loss on epoch 1712 : 0.370\n",
      "train accuracy on epoch 1712: 0.889\n",
      "test loss on epoch 1712: 0.319\n",
      "test accuracy on epoch 1712: 0.769\n",
      "train loss on epoch 1713 : 0.115\n",
      "train accuracy on epoch 1713: 0.944\n",
      "test loss on epoch 1713: 0.315\n",
      "test accuracy on epoch 1713: 0.769\n",
      "train loss on epoch 1714 : 0.258\n",
      "train accuracy on epoch 1714: 0.944\n",
      "test loss on epoch 1714: 0.311\n",
      "test accuracy on epoch 1714: 0.769\n",
      "train loss on epoch 1715 : 0.162\n",
      "train accuracy on epoch 1715: 0.944\n",
      "test loss on epoch 1715: 0.309\n",
      "test accuracy on epoch 1715: 0.769\n",
      "train loss on epoch 1716 : 0.156\n",
      "train accuracy on epoch 1716: 0.944\n",
      "test loss on epoch 1716: 0.303\n",
      "test accuracy on epoch 1716: 0.769\n",
      "train loss on epoch 1717 : 0.226\n",
      "train accuracy on epoch 1717: 0.889\n",
      "test loss on epoch 1717: 0.301\n",
      "test accuracy on epoch 1717: 0.769\n",
      "train loss on epoch 1718 : 0.071\n",
      "train accuracy on epoch 1718: 1.000\n",
      "test loss on epoch 1718: 0.303\n",
      "test accuracy on epoch 1718: 0.769\n",
      "train loss on epoch 1719 : 0.152\n",
      "train accuracy on epoch 1719: 1.000\n",
      "test loss on epoch 1719: 0.309\n",
      "test accuracy on epoch 1719: 0.692\n",
      "train loss on epoch 1720 : 0.115\n",
      "train accuracy on epoch 1720: 0.944\n",
      "test loss on epoch 1720: 0.308\n",
      "test accuracy on epoch 1720: 0.692\n",
      "train loss on epoch 1721 : 0.140\n",
      "train accuracy on epoch 1721: 0.889\n",
      "test loss on epoch 1721: 0.308\n",
      "test accuracy on epoch 1721: 0.769\n",
      "train loss on epoch 1722 : 0.300\n",
      "train accuracy on epoch 1722: 0.944\n",
      "test loss on epoch 1722: 0.313\n",
      "test accuracy on epoch 1722: 0.769\n",
      "train loss on epoch 1723 : 0.034\n",
      "train accuracy on epoch 1723: 1.000\n",
      "test loss on epoch 1723: 0.303\n",
      "test accuracy on epoch 1723: 0.769\n",
      "train loss on epoch 1724 : 0.266\n",
      "train accuracy on epoch 1724: 0.944\n",
      "test loss on epoch 1724: 0.307\n",
      "test accuracy on epoch 1724: 0.769\n",
      "train loss on epoch 1725 : 0.338\n",
      "train accuracy on epoch 1725: 0.889\n",
      "test loss on epoch 1725: 0.307\n",
      "test accuracy on epoch 1725: 0.769\n",
      "train loss on epoch 1726 : 0.525\n",
      "train accuracy on epoch 1726: 0.833\n",
      "test loss on epoch 1726: 0.309\n",
      "test accuracy on epoch 1726: 0.769\n",
      "train loss on epoch 1727 : 0.218\n",
      "train accuracy on epoch 1727: 0.889\n",
      "test loss on epoch 1727: 0.311\n",
      "test accuracy on epoch 1727: 0.769\n",
      "train loss on epoch 1728 : 0.122\n",
      "train accuracy on epoch 1728: 0.944\n",
      "test loss on epoch 1728: 0.312\n",
      "test accuracy on epoch 1728: 0.769\n",
      "train loss on epoch 1729 : 0.128\n",
      "train accuracy on epoch 1729: 0.944\n",
      "test loss on epoch 1729: 0.320\n",
      "test accuracy on epoch 1729: 0.769\n",
      "train loss on epoch 1730 : 0.043\n",
      "train accuracy on epoch 1730: 1.000\n",
      "test loss on epoch 1730: 0.324\n",
      "test accuracy on epoch 1730: 0.769\n",
      "train loss on epoch 1731 : 0.136\n",
      "train accuracy on epoch 1731: 0.889\n",
      "test loss on epoch 1731: 0.323\n",
      "test accuracy on epoch 1731: 0.769\n",
      "train loss on epoch 1732 : 0.189\n",
      "train accuracy on epoch 1732: 0.944\n",
      "test loss on epoch 1732: 0.321\n",
      "test accuracy on epoch 1732: 0.769\n",
      "train loss on epoch 1733 : 0.433\n",
      "train accuracy on epoch 1733: 0.833\n",
      "test loss on epoch 1733: 0.324\n",
      "test accuracy on epoch 1733: 0.769\n",
      "train loss on epoch 1734 : 0.263\n",
      "train accuracy on epoch 1734: 0.889\n",
      "test loss on epoch 1734: 0.319\n",
      "test accuracy on epoch 1734: 0.769\n",
      "train loss on epoch 1735 : 0.085\n",
      "train accuracy on epoch 1735: 1.000\n",
      "test loss on epoch 1735: 0.318\n",
      "test accuracy on epoch 1735: 0.769\n",
      "train loss on epoch 1736 : 0.100\n",
      "train accuracy on epoch 1736: 0.944\n",
      "test loss on epoch 1736: 0.321\n",
      "test accuracy on epoch 1736: 0.769\n",
      "train loss on epoch 1737 : 0.375\n",
      "train accuracy on epoch 1737: 0.889\n",
      "test loss on epoch 1737: 0.323\n",
      "test accuracy on epoch 1737: 0.769\n",
      "train loss on epoch 1738 : 0.106\n",
      "train accuracy on epoch 1738: 1.000\n",
      "test loss on epoch 1738: 0.324\n",
      "test accuracy on epoch 1738: 0.769\n",
      "train loss on epoch 1739 : 0.090\n",
      "train accuracy on epoch 1739: 1.000\n",
      "test loss on epoch 1739: 0.324\n",
      "test accuracy on epoch 1739: 0.769\n",
      "train loss on epoch 1740 : 0.253\n",
      "train accuracy on epoch 1740: 0.889\n",
      "test loss on epoch 1740: 0.326\n",
      "test accuracy on epoch 1740: 0.769\n",
      "train loss on epoch 1741 : 0.188\n",
      "train accuracy on epoch 1741: 0.944\n",
      "test loss on epoch 1741: 0.320\n",
      "test accuracy on epoch 1741: 0.769\n",
      "train loss on epoch 1742 : 0.181\n",
      "train accuracy on epoch 1742: 0.944\n",
      "test loss on epoch 1742: 0.316\n",
      "test accuracy on epoch 1742: 0.769\n",
      "train loss on epoch 1743 : 0.206\n",
      "train accuracy on epoch 1743: 0.889\n",
      "test loss on epoch 1743: 0.315\n",
      "test accuracy on epoch 1743: 0.769\n",
      "train loss on epoch 1744 : 0.196\n",
      "train accuracy on epoch 1744: 0.944\n",
      "test loss on epoch 1744: 0.320\n",
      "test accuracy on epoch 1744: 0.769\n",
      "train loss on epoch 1745 : 0.230\n",
      "train accuracy on epoch 1745: 0.944\n",
      "test loss on epoch 1745: 0.323\n",
      "test accuracy on epoch 1745: 0.769\n",
      "train loss on epoch 1746 : 0.073\n",
      "train accuracy on epoch 1746: 1.000\n",
      "test loss on epoch 1746: 0.325\n",
      "test accuracy on epoch 1746: 0.769\n",
      "train loss on epoch 1747 : 0.508\n",
      "train accuracy on epoch 1747: 0.722\n",
      "test loss on epoch 1747: 0.327\n",
      "test accuracy on epoch 1747: 0.769\n",
      "train loss on epoch 1748 : 0.130\n",
      "train accuracy on epoch 1748: 0.944\n",
      "test loss on epoch 1748: 0.324\n",
      "test accuracy on epoch 1748: 0.769\n",
      "train loss on epoch 1749 : 0.298\n",
      "train accuracy on epoch 1749: 0.889\n",
      "test loss on epoch 1749: 0.325\n",
      "test accuracy on epoch 1749: 0.769\n",
      "train loss on epoch 1750 : 0.264\n",
      "train accuracy on epoch 1750: 0.889\n",
      "test loss on epoch 1750: 0.322\n",
      "test accuracy on epoch 1750: 0.769\n",
      "train loss on epoch 1751 : 0.182\n",
      "train accuracy on epoch 1751: 0.944\n",
      "test loss on epoch 1751: 0.321\n",
      "test accuracy on epoch 1751: 0.769\n",
      "train loss on epoch 1752 : 0.184\n",
      "train accuracy on epoch 1752: 0.944\n",
      "test loss on epoch 1752: 0.322\n",
      "test accuracy on epoch 1752: 0.769\n",
      "train loss on epoch 1753 : 0.072\n",
      "train accuracy on epoch 1753: 1.000\n",
      "test loss on epoch 1753: 0.323\n",
      "test accuracy on epoch 1753: 0.769\n",
      "train loss on epoch 1754 : 0.168\n",
      "train accuracy on epoch 1754: 0.944\n",
      "test loss on epoch 1754: 0.323\n",
      "test accuracy on epoch 1754: 0.769\n",
      "train loss on epoch 1755 : 0.048\n",
      "train accuracy on epoch 1755: 1.000\n",
      "test loss on epoch 1755: 0.323\n",
      "test accuracy on epoch 1755: 0.769\n",
      "train loss on epoch 1756 : 0.254\n",
      "train accuracy on epoch 1756: 0.889\n",
      "test loss on epoch 1756: 0.329\n",
      "test accuracy on epoch 1756: 0.769\n",
      "train loss on epoch 1757 : 0.209\n",
      "train accuracy on epoch 1757: 0.944\n",
      "test loss on epoch 1757: 0.323\n",
      "test accuracy on epoch 1757: 0.769\n",
      "train loss on epoch 1758 : 0.120\n",
      "train accuracy on epoch 1758: 0.944\n",
      "test loss on epoch 1758: 0.321\n",
      "test accuracy on epoch 1758: 0.769\n",
      "train loss on epoch 1759 : 0.147\n",
      "train accuracy on epoch 1759: 0.944\n",
      "test loss on epoch 1759: 0.313\n",
      "test accuracy on epoch 1759: 0.769\n",
      "train loss on epoch 1760 : 0.060\n",
      "train accuracy on epoch 1760: 1.000\n",
      "test loss on epoch 1760: 0.312\n",
      "test accuracy on epoch 1760: 0.769\n",
      "train loss on epoch 1761 : 0.119\n",
      "train accuracy on epoch 1761: 1.000\n",
      "test loss on epoch 1761: 0.315\n",
      "test accuracy on epoch 1761: 0.769\n",
      "train loss on epoch 1762 : 0.220\n",
      "train accuracy on epoch 1762: 0.944\n",
      "test loss on epoch 1762: 0.310\n",
      "test accuracy on epoch 1762: 0.769\n",
      "train loss on epoch 1763 : 0.108\n",
      "train accuracy on epoch 1763: 0.889\n",
      "test loss on epoch 1763: 0.315\n",
      "test accuracy on epoch 1763: 0.769\n",
      "train loss on epoch 1764 : 0.114\n",
      "train accuracy on epoch 1764: 0.944\n",
      "test loss on epoch 1764: 0.316\n",
      "test accuracy on epoch 1764: 0.769\n",
      "train loss on epoch 1765 : 0.239\n",
      "train accuracy on epoch 1765: 0.944\n",
      "test loss on epoch 1765: 0.309\n",
      "test accuracy on epoch 1765: 0.769\n",
      "train loss on epoch 1766 : 0.048\n",
      "train accuracy on epoch 1766: 1.000\n",
      "test loss on epoch 1766: 0.315\n",
      "test accuracy on epoch 1766: 0.769\n",
      "train loss on epoch 1767 : 0.230\n",
      "train accuracy on epoch 1767: 0.889\n",
      "test loss on epoch 1767: 0.315\n",
      "test accuracy on epoch 1767: 0.769\n",
      "train loss on epoch 1768 : 0.111\n",
      "train accuracy on epoch 1768: 0.944\n",
      "test loss on epoch 1768: 0.311\n",
      "test accuracy on epoch 1768: 0.769\n",
      "train loss on epoch 1769 : 0.329\n",
      "train accuracy on epoch 1769: 0.944\n",
      "test loss on epoch 1769: 0.308\n",
      "test accuracy on epoch 1769: 0.846\n",
      "train loss on epoch 1770 : 0.248\n",
      "train accuracy on epoch 1770: 0.944\n",
      "test loss on epoch 1770: 0.309\n",
      "test accuracy on epoch 1770: 0.769\n",
      "train loss on epoch 1771 : 0.044\n",
      "train accuracy on epoch 1771: 1.000\n",
      "test loss on epoch 1771: 0.314\n",
      "test accuracy on epoch 1771: 0.692\n",
      "train loss on epoch 1772 : 0.291\n",
      "train accuracy on epoch 1772: 0.944\n",
      "test loss on epoch 1772: 0.315\n",
      "test accuracy on epoch 1772: 0.769\n",
      "train loss on epoch 1773 : 0.189\n",
      "train accuracy on epoch 1773: 0.889\n",
      "test loss on epoch 1773: 0.316\n",
      "test accuracy on epoch 1773: 0.769\n",
      "train loss on epoch 1774 : 0.310\n",
      "train accuracy on epoch 1774: 0.889\n",
      "test loss on epoch 1774: 0.322\n",
      "test accuracy on epoch 1774: 0.769\n",
      "train loss on epoch 1775 : 0.362\n",
      "train accuracy on epoch 1775: 0.889\n",
      "test loss on epoch 1775: 0.321\n",
      "test accuracy on epoch 1775: 0.769\n",
      "train loss on epoch 1776 : 0.081\n",
      "train accuracy on epoch 1776: 1.000\n",
      "test loss on epoch 1776: 0.324\n",
      "test accuracy on epoch 1776: 0.769\n",
      "train loss on epoch 1777 : 0.108\n",
      "train accuracy on epoch 1777: 0.944\n",
      "test loss on epoch 1777: 0.320\n",
      "test accuracy on epoch 1777: 0.769\n",
      "train loss on epoch 1778 : 0.229\n",
      "train accuracy on epoch 1778: 0.889\n",
      "test loss on epoch 1778: 0.319\n",
      "test accuracy on epoch 1778: 0.769\n",
      "train loss on epoch 1779 : 0.178\n",
      "train accuracy on epoch 1779: 0.889\n",
      "test loss on epoch 1779: 0.320\n",
      "test accuracy on epoch 1779: 0.769\n",
      "train loss on epoch 1780 : 0.255\n",
      "train accuracy on epoch 1780: 0.889\n",
      "test loss on epoch 1780: 0.316\n",
      "test accuracy on epoch 1780: 0.769\n",
      "train loss on epoch 1781 : 0.128\n",
      "train accuracy on epoch 1781: 0.889\n",
      "test loss on epoch 1781: 0.312\n",
      "test accuracy on epoch 1781: 0.769\n",
      "train loss on epoch 1782 : 0.250\n",
      "train accuracy on epoch 1782: 0.889\n",
      "test loss on epoch 1782: 0.317\n",
      "test accuracy on epoch 1782: 0.769\n",
      "train loss on epoch 1783 : 0.054\n",
      "train accuracy on epoch 1783: 1.000\n",
      "test loss on epoch 1783: 0.318\n",
      "test accuracy on epoch 1783: 0.769\n",
      "train loss on epoch 1784 : 0.098\n",
      "train accuracy on epoch 1784: 0.944\n",
      "test loss on epoch 1784: 0.314\n",
      "test accuracy on epoch 1784: 0.769\n",
      "train loss on epoch 1785 : 0.083\n",
      "train accuracy on epoch 1785: 1.000\n",
      "test loss on epoch 1785: 0.320\n",
      "test accuracy on epoch 1785: 0.769\n",
      "train loss on epoch 1786 : 0.291\n",
      "train accuracy on epoch 1786: 0.889\n",
      "test loss on epoch 1786: 0.319\n",
      "test accuracy on epoch 1786: 0.769\n",
      "train loss on epoch 1787 : 0.113\n",
      "train accuracy on epoch 1787: 0.944\n",
      "test loss on epoch 1787: 0.319\n",
      "test accuracy on epoch 1787: 0.769\n",
      "train loss on epoch 1788 : 0.083\n",
      "train accuracy on epoch 1788: 1.000\n",
      "test loss on epoch 1788: 0.321\n",
      "test accuracy on epoch 1788: 0.769\n",
      "train loss on epoch 1789 : 0.128\n",
      "train accuracy on epoch 1789: 0.944\n",
      "test loss on epoch 1789: 0.316\n",
      "test accuracy on epoch 1789: 0.769\n",
      "train loss on epoch 1790 : 0.618\n",
      "train accuracy on epoch 1790: 0.833\n",
      "test loss on epoch 1790: 0.312\n",
      "test accuracy on epoch 1790: 0.769\n",
      "train loss on epoch 1791 : 0.068\n",
      "train accuracy on epoch 1791: 1.000\n",
      "test loss on epoch 1791: 0.320\n",
      "test accuracy on epoch 1791: 0.769\n",
      "train loss on epoch 1792 : 0.097\n",
      "train accuracy on epoch 1792: 0.944\n",
      "test loss on epoch 1792: 0.320\n",
      "test accuracy on epoch 1792: 0.769\n",
      "train loss on epoch 1793 : 0.242\n",
      "train accuracy on epoch 1793: 0.889\n",
      "test loss on epoch 1793: 0.321\n",
      "test accuracy on epoch 1793: 0.769\n",
      "train loss on epoch 1794 : 0.314\n",
      "train accuracy on epoch 1794: 0.944\n",
      "test loss on epoch 1794: 0.315\n",
      "test accuracy on epoch 1794: 0.769\n",
      "train loss on epoch 1795 : 0.190\n",
      "train accuracy on epoch 1795: 0.944\n",
      "test loss on epoch 1795: 0.323\n",
      "test accuracy on epoch 1795: 0.769\n",
      "train loss on epoch 1796 : 0.177\n",
      "train accuracy on epoch 1796: 0.889\n",
      "test loss on epoch 1796: 0.317\n",
      "test accuracy on epoch 1796: 0.769\n",
      "train loss on epoch 1797 : 0.160\n",
      "train accuracy on epoch 1797: 0.889\n",
      "test loss on epoch 1797: 0.322\n",
      "test accuracy on epoch 1797: 0.769\n",
      "train loss on epoch 1798 : 0.253\n",
      "train accuracy on epoch 1798: 0.889\n",
      "test loss on epoch 1798: 0.311\n",
      "test accuracy on epoch 1798: 0.769\n",
      "train loss on epoch 1799 : 0.283\n",
      "train accuracy on epoch 1799: 0.833\n",
      "test loss on epoch 1799: 0.313\n",
      "test accuracy on epoch 1799: 0.769\n",
      "train loss on epoch 1800 : 0.131\n",
      "train accuracy on epoch 1800: 0.944\n",
      "test loss on epoch 1800: 0.326\n",
      "test accuracy on epoch 1800: 0.769\n",
      "train loss on epoch 1801 : 0.117\n",
      "train accuracy on epoch 1801: 0.944\n",
      "test loss on epoch 1801: 0.331\n",
      "test accuracy on epoch 1801: 0.769\n",
      "train loss on epoch 1802 : 0.099\n",
      "train accuracy on epoch 1802: 0.944\n",
      "test loss on epoch 1802: 0.331\n",
      "test accuracy on epoch 1802: 0.769\n",
      "train loss on epoch 1803 : 0.193\n",
      "train accuracy on epoch 1803: 0.889\n",
      "test loss on epoch 1803: 0.328\n",
      "test accuracy on epoch 1803: 0.769\n",
      "train loss on epoch 1804 : 0.111\n",
      "train accuracy on epoch 1804: 0.944\n",
      "test loss on epoch 1804: 0.330\n",
      "test accuracy on epoch 1804: 0.769\n",
      "train loss on epoch 1805 : 0.229\n",
      "train accuracy on epoch 1805: 0.944\n",
      "test loss on epoch 1805: 0.328\n",
      "test accuracy on epoch 1805: 0.769\n",
      "train loss on epoch 1806 : 0.198\n",
      "train accuracy on epoch 1806: 0.889\n",
      "test loss on epoch 1806: 0.315\n",
      "test accuracy on epoch 1806: 0.769\n",
      "train loss on epoch 1807 : 0.343\n",
      "train accuracy on epoch 1807: 0.778\n",
      "test loss on epoch 1807: 0.313\n",
      "test accuracy on epoch 1807: 0.769\n",
      "train loss on epoch 1808 : 0.083\n",
      "train accuracy on epoch 1808: 0.944\n",
      "test loss on epoch 1808: 0.316\n",
      "test accuracy on epoch 1808: 0.769\n",
      "train loss on epoch 1809 : 0.079\n",
      "train accuracy on epoch 1809: 1.000\n",
      "test loss on epoch 1809: 0.310\n",
      "test accuracy on epoch 1809: 0.769\n",
      "train loss on epoch 1810 : 0.537\n",
      "train accuracy on epoch 1810: 0.889\n",
      "test loss on epoch 1810: 0.309\n",
      "test accuracy on epoch 1810: 0.769\n",
      "train loss on epoch 1811 : 0.199\n",
      "train accuracy on epoch 1811: 0.944\n",
      "test loss on epoch 1811: 0.309\n",
      "test accuracy on epoch 1811: 0.769\n",
      "train loss on epoch 1812 : 0.312\n",
      "train accuracy on epoch 1812: 0.833\n",
      "test loss on epoch 1812: 0.311\n",
      "test accuracy on epoch 1812: 0.769\n",
      "train loss on epoch 1813 : 0.207\n",
      "train accuracy on epoch 1813: 0.889\n",
      "test loss on epoch 1813: 0.312\n",
      "test accuracy on epoch 1813: 0.769\n",
      "train loss on epoch 1814 : 0.239\n",
      "train accuracy on epoch 1814: 0.889\n",
      "test loss on epoch 1814: 0.308\n",
      "test accuracy on epoch 1814: 0.769\n",
      "train loss on epoch 1815 : 0.313\n",
      "train accuracy on epoch 1815: 0.889\n",
      "test loss on epoch 1815: 0.316\n",
      "test accuracy on epoch 1815: 0.769\n",
      "train loss on epoch 1816 : 0.098\n",
      "train accuracy on epoch 1816: 0.944\n",
      "test loss on epoch 1816: 0.315\n",
      "test accuracy on epoch 1816: 0.769\n",
      "train loss on epoch 1817 : 0.144\n",
      "train accuracy on epoch 1817: 0.889\n",
      "test loss on epoch 1817: 0.308\n",
      "test accuracy on epoch 1817: 0.769\n",
      "train loss on epoch 1818 : 0.278\n",
      "train accuracy on epoch 1818: 0.889\n",
      "test loss on epoch 1818: 0.310\n",
      "test accuracy on epoch 1818: 0.769\n",
      "train loss on epoch 1819 : 0.177\n",
      "train accuracy on epoch 1819: 0.833\n",
      "test loss on epoch 1819: 0.308\n",
      "test accuracy on epoch 1819: 0.769\n",
      "train loss on epoch 1820 : 0.141\n",
      "train accuracy on epoch 1820: 0.944\n",
      "test loss on epoch 1820: 0.314\n",
      "test accuracy on epoch 1820: 0.769\n",
      "train loss on epoch 1821 : 0.121\n",
      "train accuracy on epoch 1821: 0.944\n",
      "test loss on epoch 1821: 0.312\n",
      "test accuracy on epoch 1821: 0.769\n",
      "train loss on epoch 1822 : 0.202\n",
      "train accuracy on epoch 1822: 0.889\n",
      "test loss on epoch 1822: 0.315\n",
      "test accuracy on epoch 1822: 0.769\n",
      "train loss on epoch 1823 : 0.201\n",
      "train accuracy on epoch 1823: 0.889\n",
      "test loss on epoch 1823: 0.313\n",
      "test accuracy on epoch 1823: 0.769\n",
      "train loss on epoch 1824 : 0.191\n",
      "train accuracy on epoch 1824: 0.889\n",
      "test loss on epoch 1824: 0.322\n",
      "test accuracy on epoch 1824: 0.769\n",
      "train loss on epoch 1825 : 0.127\n",
      "train accuracy on epoch 1825: 0.944\n",
      "test loss on epoch 1825: 0.315\n",
      "test accuracy on epoch 1825: 0.769\n",
      "train loss on epoch 1826 : 0.252\n",
      "train accuracy on epoch 1826: 0.889\n",
      "test loss on epoch 1826: 0.310\n",
      "test accuracy on epoch 1826: 0.769\n",
      "train loss on epoch 1827 : 0.286\n",
      "train accuracy on epoch 1827: 0.833\n",
      "test loss on epoch 1827: 0.318\n",
      "test accuracy on epoch 1827: 0.769\n",
      "train loss on epoch 1828 : 0.070\n",
      "train accuracy on epoch 1828: 1.000\n",
      "test loss on epoch 1828: 0.319\n",
      "test accuracy on epoch 1828: 0.769\n",
      "train loss on epoch 1829 : 0.308\n",
      "train accuracy on epoch 1829: 0.889\n",
      "test loss on epoch 1829: 0.314\n",
      "test accuracy on epoch 1829: 0.769\n",
      "train loss on epoch 1830 : 0.118\n",
      "train accuracy on epoch 1830: 0.944\n",
      "test loss on epoch 1830: 0.325\n",
      "test accuracy on epoch 1830: 0.769\n",
      "train loss on epoch 1831 : 0.166\n",
      "train accuracy on epoch 1831: 0.889\n",
      "test loss on epoch 1831: 0.319\n",
      "test accuracy on epoch 1831: 0.769\n",
      "train loss on epoch 1832 : 0.122\n",
      "train accuracy on epoch 1832: 0.944\n",
      "test loss on epoch 1832: 0.320\n",
      "test accuracy on epoch 1832: 0.769\n",
      "train loss on epoch 1833 : 0.081\n",
      "train accuracy on epoch 1833: 0.944\n",
      "test loss on epoch 1833: 0.313\n",
      "test accuracy on epoch 1833: 0.769\n",
      "train loss on epoch 1834 : 0.167\n",
      "train accuracy on epoch 1834: 0.889\n",
      "test loss on epoch 1834: 0.311\n",
      "test accuracy on epoch 1834: 0.769\n",
      "train loss on epoch 1835 : 0.490\n",
      "train accuracy on epoch 1835: 0.889\n",
      "test loss on epoch 1835: 0.309\n",
      "test accuracy on epoch 1835: 0.769\n",
      "train loss on epoch 1836 : 0.269\n",
      "train accuracy on epoch 1836: 0.889\n",
      "test loss on epoch 1836: 0.307\n",
      "test accuracy on epoch 1836: 0.769\n",
      "train loss on epoch 1837 : 0.198\n",
      "train accuracy on epoch 1837: 0.889\n",
      "test loss on epoch 1837: 0.311\n",
      "test accuracy on epoch 1837: 0.769\n",
      "train loss on epoch 1838 : 0.082\n",
      "train accuracy on epoch 1838: 0.944\n",
      "test loss on epoch 1838: 0.311\n",
      "test accuracy on epoch 1838: 0.769\n",
      "train loss on epoch 1839 : 0.161\n",
      "train accuracy on epoch 1839: 0.889\n",
      "test loss on epoch 1839: 0.307\n",
      "test accuracy on epoch 1839: 0.769\n",
      "train loss on epoch 1840 : 0.099\n",
      "train accuracy on epoch 1840: 0.944\n",
      "test loss on epoch 1840: 0.314\n",
      "test accuracy on epoch 1840: 0.769\n",
      "train loss on epoch 1841 : 0.182\n",
      "train accuracy on epoch 1841: 0.833\n",
      "test loss on epoch 1841: 0.310\n",
      "test accuracy on epoch 1841: 0.769\n",
      "train loss on epoch 1842 : 0.126\n",
      "train accuracy on epoch 1842: 0.944\n",
      "test loss on epoch 1842: 0.309\n",
      "test accuracy on epoch 1842: 0.769\n",
      "train loss on epoch 1843 : 0.300\n",
      "train accuracy on epoch 1843: 0.833\n",
      "test loss on epoch 1843: 0.305\n",
      "test accuracy on epoch 1843: 0.769\n",
      "train loss on epoch 1844 : 0.088\n",
      "train accuracy on epoch 1844: 1.000\n",
      "test loss on epoch 1844: 0.300\n",
      "test accuracy on epoch 1844: 0.769\n",
      "train loss on epoch 1845 : 0.096\n",
      "train accuracy on epoch 1845: 1.000\n",
      "test loss on epoch 1845: 0.309\n",
      "test accuracy on epoch 1845: 0.769\n",
      "train loss on epoch 1846 : 0.068\n",
      "train accuracy on epoch 1846: 1.000\n",
      "test loss on epoch 1846: 0.306\n",
      "test accuracy on epoch 1846: 0.769\n",
      "train loss on epoch 1847 : 0.094\n",
      "train accuracy on epoch 1847: 0.944\n",
      "test loss on epoch 1847: 0.305\n",
      "test accuracy on epoch 1847: 0.769\n",
      "train loss on epoch 1848 : 0.480\n",
      "train accuracy on epoch 1848: 0.833\n",
      "test loss on epoch 1848: 0.307\n",
      "test accuracy on epoch 1848: 0.769\n",
      "train loss on epoch 1849 : 0.250\n",
      "train accuracy on epoch 1849: 0.833\n",
      "test loss on epoch 1849: 0.306\n",
      "test accuracy on epoch 1849: 0.769\n",
      "train loss on epoch 1850 : 0.093\n",
      "train accuracy on epoch 1850: 0.944\n",
      "test loss on epoch 1850: 0.303\n",
      "test accuracy on epoch 1850: 0.769\n",
      "train loss on epoch 1851 : 0.197\n",
      "train accuracy on epoch 1851: 0.889\n",
      "test loss on epoch 1851: 0.305\n",
      "test accuracy on epoch 1851: 0.769\n",
      "train loss on epoch 1852 : 0.188\n",
      "train accuracy on epoch 1852: 0.944\n",
      "test loss on epoch 1852: 0.306\n",
      "test accuracy on epoch 1852: 0.769\n",
      "train loss on epoch 1853 : 0.272\n",
      "train accuracy on epoch 1853: 0.833\n",
      "test loss on epoch 1853: 0.311\n",
      "test accuracy on epoch 1853: 0.769\n",
      "train loss on epoch 1854 : 0.034\n",
      "train accuracy on epoch 1854: 1.000\n",
      "test loss on epoch 1854: 0.310\n",
      "test accuracy on epoch 1854: 0.769\n",
      "train loss on epoch 1855 : 0.173\n",
      "train accuracy on epoch 1855: 0.889\n",
      "test loss on epoch 1855: 0.308\n",
      "test accuracy on epoch 1855: 0.769\n",
      "train loss on epoch 1856 : 0.145\n",
      "train accuracy on epoch 1856: 0.944\n",
      "test loss on epoch 1856: 0.306\n",
      "test accuracy on epoch 1856: 0.692\n",
      "train loss on epoch 1857 : 0.137\n",
      "train accuracy on epoch 1857: 0.944\n",
      "test loss on epoch 1857: 0.301\n",
      "test accuracy on epoch 1857: 0.769\n",
      "train loss on epoch 1858 : 0.128\n",
      "train accuracy on epoch 1858: 0.944\n",
      "test loss on epoch 1858: 0.306\n",
      "test accuracy on epoch 1858: 0.769\n",
      "train loss on epoch 1859 : 0.313\n",
      "train accuracy on epoch 1859: 0.833\n",
      "test loss on epoch 1859: 0.307\n",
      "test accuracy on epoch 1859: 0.769\n",
      "train loss on epoch 1860 : 0.320\n",
      "train accuracy on epoch 1860: 0.833\n",
      "test loss on epoch 1860: 0.305\n",
      "test accuracy on epoch 1860: 0.769\n",
      "train loss on epoch 1861 : 0.175\n",
      "train accuracy on epoch 1861: 0.889\n",
      "test loss on epoch 1861: 0.308\n",
      "test accuracy on epoch 1861: 0.769\n",
      "train loss on epoch 1862 : 0.102\n",
      "train accuracy on epoch 1862: 0.944\n",
      "test loss on epoch 1862: 0.307\n",
      "test accuracy on epoch 1862: 0.769\n",
      "train loss on epoch 1863 : 0.162\n",
      "train accuracy on epoch 1863: 0.889\n",
      "test loss on epoch 1863: 0.308\n",
      "test accuracy on epoch 1863: 0.769\n",
      "train loss on epoch 1864 : 0.104\n",
      "train accuracy on epoch 1864: 0.944\n",
      "test loss on epoch 1864: 0.310\n",
      "test accuracy on epoch 1864: 0.769\n",
      "train loss on epoch 1865 : 0.089\n",
      "train accuracy on epoch 1865: 1.000\n",
      "test loss on epoch 1865: 0.312\n",
      "test accuracy on epoch 1865: 0.769\n",
      "train loss on epoch 1866 : 0.308\n",
      "train accuracy on epoch 1866: 0.833\n",
      "test loss on epoch 1866: 0.314\n",
      "test accuracy on epoch 1866: 0.769\n",
      "train loss on epoch 1867 : 0.186\n",
      "train accuracy on epoch 1867: 0.944\n",
      "test loss on epoch 1867: 0.315\n",
      "test accuracy on epoch 1867: 0.769\n",
      "train loss on epoch 1868 : 0.068\n",
      "train accuracy on epoch 1868: 1.000\n",
      "test loss on epoch 1868: 0.316\n",
      "test accuracy on epoch 1868: 0.769\n",
      "train loss on epoch 1869 : 0.090\n",
      "train accuracy on epoch 1869: 1.000\n",
      "test loss on epoch 1869: 0.308\n",
      "test accuracy on epoch 1869: 0.769\n",
      "train loss on epoch 1870 : 0.125\n",
      "train accuracy on epoch 1870: 0.944\n",
      "test loss on epoch 1870: 0.306\n",
      "test accuracy on epoch 1870: 0.769\n",
      "train loss on epoch 1871 : 0.203\n",
      "train accuracy on epoch 1871: 0.889\n",
      "test loss on epoch 1871: 0.309\n",
      "test accuracy on epoch 1871: 0.769\n",
      "train loss on epoch 1872 : 0.297\n",
      "train accuracy on epoch 1872: 0.944\n",
      "test loss on epoch 1872: 0.303\n",
      "test accuracy on epoch 1872: 0.769\n",
      "train loss on epoch 1873 : 0.173\n",
      "train accuracy on epoch 1873: 0.944\n",
      "test loss on epoch 1873: 0.303\n",
      "test accuracy on epoch 1873: 0.769\n",
      "train loss on epoch 1874 : 0.172\n",
      "train accuracy on epoch 1874: 0.889\n",
      "test loss on epoch 1874: 0.304\n",
      "test accuracy on epoch 1874: 0.769\n",
      "train loss on epoch 1875 : 0.127\n",
      "train accuracy on epoch 1875: 0.944\n",
      "test loss on epoch 1875: 0.306\n",
      "test accuracy on epoch 1875: 0.769\n",
      "train loss on epoch 1876 : 0.349\n",
      "train accuracy on epoch 1876: 0.833\n",
      "test loss on epoch 1876: 0.301\n",
      "test accuracy on epoch 1876: 0.769\n",
      "train loss on epoch 1877 : 0.113\n",
      "train accuracy on epoch 1877: 0.944\n",
      "test loss on epoch 1877: 0.303\n",
      "test accuracy on epoch 1877: 0.769\n",
      "train loss on epoch 1878 : 0.068\n",
      "train accuracy on epoch 1878: 1.000\n",
      "test loss on epoch 1878: 0.306\n",
      "test accuracy on epoch 1878: 0.769\n",
      "train loss on epoch 1879 : 0.041\n",
      "train accuracy on epoch 1879: 1.000\n",
      "test loss on epoch 1879: 0.302\n",
      "test accuracy on epoch 1879: 0.769\n",
      "train loss on epoch 1880 : 0.184\n",
      "train accuracy on epoch 1880: 0.944\n",
      "test loss on epoch 1880: 0.304\n",
      "test accuracy on epoch 1880: 0.769\n",
      "train loss on epoch 1881 : 0.082\n",
      "train accuracy on epoch 1881: 0.944\n",
      "test loss on epoch 1881: 0.303\n",
      "test accuracy on epoch 1881: 0.769\n",
      "train loss on epoch 1882 : 0.307\n",
      "train accuracy on epoch 1882: 0.944\n",
      "test loss on epoch 1882: 0.307\n",
      "test accuracy on epoch 1882: 0.769\n",
      "train loss on epoch 1883 : 0.390\n",
      "train accuracy on epoch 1883: 0.889\n",
      "test loss on epoch 1883: 0.302\n",
      "test accuracy on epoch 1883: 0.769\n",
      "train loss on epoch 1884 : 0.115\n",
      "train accuracy on epoch 1884: 0.944\n",
      "test loss on epoch 1884: 0.304\n",
      "test accuracy on epoch 1884: 0.769\n",
      "train loss on epoch 1885 : 0.186\n",
      "train accuracy on epoch 1885: 0.944\n",
      "test loss on epoch 1885: 0.307\n",
      "test accuracy on epoch 1885: 0.769\n",
      "train loss on epoch 1886 : 0.258\n",
      "train accuracy on epoch 1886: 0.833\n",
      "test loss on epoch 1886: 0.307\n",
      "test accuracy on epoch 1886: 0.769\n",
      "train loss on epoch 1887 : 0.364\n",
      "train accuracy on epoch 1887: 0.889\n",
      "test loss on epoch 1887: 0.306\n",
      "test accuracy on epoch 1887: 0.769\n",
      "train loss on epoch 1888 : 0.362\n",
      "train accuracy on epoch 1888: 0.889\n",
      "test loss on epoch 1888: 0.306\n",
      "test accuracy on epoch 1888: 0.769\n",
      "train loss on epoch 1889 : 0.188\n",
      "train accuracy on epoch 1889: 0.889\n",
      "test loss on epoch 1889: 0.310\n",
      "test accuracy on epoch 1889: 0.769\n",
      "train loss on epoch 1890 : 0.237\n",
      "train accuracy on epoch 1890: 0.889\n",
      "test loss on epoch 1890: 0.308\n",
      "test accuracy on epoch 1890: 0.769\n",
      "train loss on epoch 1891 : 0.356\n",
      "train accuracy on epoch 1891: 0.889\n",
      "test loss on epoch 1891: 0.308\n",
      "test accuracy on epoch 1891: 0.769\n",
      "train loss on epoch 1892 : 0.075\n",
      "train accuracy on epoch 1892: 1.000\n",
      "test loss on epoch 1892: 0.307\n",
      "test accuracy on epoch 1892: 0.769\n",
      "train loss on epoch 1893 : 0.152\n",
      "train accuracy on epoch 1893: 0.944\n",
      "test loss on epoch 1893: 0.307\n",
      "test accuracy on epoch 1893: 0.769\n",
      "train loss on epoch 1894 : 0.051\n",
      "train accuracy on epoch 1894: 1.000\n",
      "test loss on epoch 1894: 0.309\n",
      "test accuracy on epoch 1894: 0.769\n",
      "train loss on epoch 1895 : 0.201\n",
      "train accuracy on epoch 1895: 0.944\n",
      "test loss on epoch 1895: 0.304\n",
      "test accuracy on epoch 1895: 0.769\n",
      "train loss on epoch 1896 : 0.314\n",
      "train accuracy on epoch 1896: 0.833\n",
      "test loss on epoch 1896: 0.305\n",
      "test accuracy on epoch 1896: 0.769\n",
      "train loss on epoch 1897 : 0.286\n",
      "train accuracy on epoch 1897: 0.944\n",
      "test loss on epoch 1897: 0.305\n",
      "test accuracy on epoch 1897: 0.769\n",
      "train loss on epoch 1898 : 0.150\n",
      "train accuracy on epoch 1898: 0.889\n",
      "test loss on epoch 1898: 0.310\n",
      "test accuracy on epoch 1898: 0.769\n",
      "train loss on epoch 1899 : 0.162\n",
      "train accuracy on epoch 1899: 0.944\n",
      "test loss on epoch 1899: 0.308\n",
      "test accuracy on epoch 1899: 0.769\n",
      "train loss on epoch 1900 : 0.104\n",
      "train accuracy on epoch 1900: 0.944\n",
      "test loss on epoch 1900: 0.314\n",
      "test accuracy on epoch 1900: 0.769\n",
      "train loss on epoch 1901 : 0.075\n",
      "train accuracy on epoch 1901: 1.000\n",
      "test loss on epoch 1901: 0.308\n",
      "test accuracy on epoch 1901: 0.769\n",
      "train loss on epoch 1902 : 0.068\n",
      "train accuracy on epoch 1902: 1.000\n",
      "test loss on epoch 1902: 0.311\n",
      "test accuracy on epoch 1902: 0.769\n",
      "train loss on epoch 1903 : 0.286\n",
      "train accuracy on epoch 1903: 0.889\n",
      "test loss on epoch 1903: 0.312\n",
      "test accuracy on epoch 1903: 0.769\n",
      "train loss on epoch 1904 : 0.175\n",
      "train accuracy on epoch 1904: 0.889\n",
      "test loss on epoch 1904: 0.305\n",
      "test accuracy on epoch 1904: 0.769\n",
      "train loss on epoch 1905 : 0.114\n",
      "train accuracy on epoch 1905: 0.944\n",
      "test loss on epoch 1905: 0.307\n",
      "test accuracy on epoch 1905: 0.769\n",
      "train loss on epoch 1906 : 0.152\n",
      "train accuracy on epoch 1906: 0.944\n",
      "test loss on epoch 1906: 0.310\n",
      "test accuracy on epoch 1906: 0.769\n",
      "train loss on epoch 1907 : 0.079\n",
      "train accuracy on epoch 1907: 1.000\n",
      "test loss on epoch 1907: 0.313\n",
      "test accuracy on epoch 1907: 0.769\n",
      "train loss on epoch 1908 : 0.089\n",
      "train accuracy on epoch 1908: 1.000\n",
      "test loss on epoch 1908: 0.313\n",
      "test accuracy on epoch 1908: 0.769\n",
      "train loss on epoch 1909 : 0.176\n",
      "train accuracy on epoch 1909: 0.944\n",
      "test loss on epoch 1909: 0.312\n",
      "test accuracy on epoch 1909: 0.769\n",
      "train loss on epoch 1910 : 0.140\n",
      "train accuracy on epoch 1910: 0.889\n",
      "test loss on epoch 1910: 0.306\n",
      "test accuracy on epoch 1910: 0.769\n",
      "train loss on epoch 1911 : 0.290\n",
      "train accuracy on epoch 1911: 0.944\n",
      "test loss on epoch 1911: 0.305\n",
      "test accuracy on epoch 1911: 0.769\n",
      "train loss on epoch 1912 : 0.070\n",
      "train accuracy on epoch 1912: 0.944\n",
      "test loss on epoch 1912: 0.312\n",
      "test accuracy on epoch 1912: 0.769\n",
      "train loss on epoch 1913 : 0.139\n",
      "train accuracy on epoch 1913: 0.944\n",
      "test loss on epoch 1913: 0.314\n",
      "test accuracy on epoch 1913: 0.769\n",
      "train loss on epoch 1914 : 0.160\n",
      "train accuracy on epoch 1914: 0.944\n",
      "test loss on epoch 1914: 0.319\n",
      "test accuracy on epoch 1914: 0.769\n",
      "train loss on epoch 1915 : 0.219\n",
      "train accuracy on epoch 1915: 0.944\n",
      "test loss on epoch 1915: 0.317\n",
      "test accuracy on epoch 1915: 0.769\n",
      "train loss on epoch 1916 : 0.212\n",
      "train accuracy on epoch 1916: 0.889\n",
      "test loss on epoch 1916: 0.330\n",
      "test accuracy on epoch 1916: 0.769\n",
      "train loss on epoch 1917 : 0.262\n",
      "train accuracy on epoch 1917: 0.889\n",
      "test loss on epoch 1917: 0.322\n",
      "test accuracy on epoch 1917: 0.769\n",
      "train loss on epoch 1918 : 0.232\n",
      "train accuracy on epoch 1918: 0.944\n",
      "test loss on epoch 1918: 0.318\n",
      "test accuracy on epoch 1918: 0.769\n",
      "train loss on epoch 1919 : 0.118\n",
      "train accuracy on epoch 1919: 0.944\n",
      "test loss on epoch 1919: 0.324\n",
      "test accuracy on epoch 1919: 0.769\n",
      "train loss on epoch 1920 : 0.169\n",
      "train accuracy on epoch 1920: 0.944\n",
      "test loss on epoch 1920: 0.318\n",
      "test accuracy on epoch 1920: 0.769\n",
      "train loss on epoch 1921 : 0.116\n",
      "train accuracy on epoch 1921: 0.944\n",
      "test loss on epoch 1921: 0.319\n",
      "test accuracy on epoch 1921: 0.769\n",
      "train loss on epoch 1922 : 0.328\n",
      "train accuracy on epoch 1922: 0.944\n",
      "test loss on epoch 1922: 0.318\n",
      "test accuracy on epoch 1922: 0.769\n",
      "train loss on epoch 1923 : 0.157\n",
      "train accuracy on epoch 1923: 0.944\n",
      "test loss on epoch 1923: 0.312\n",
      "test accuracy on epoch 1923: 0.769\n",
      "train loss on epoch 1924 : 0.225\n",
      "train accuracy on epoch 1924: 0.889\n",
      "test loss on epoch 1924: 0.305\n",
      "test accuracy on epoch 1924: 0.769\n",
      "train loss on epoch 1925 : 0.108\n",
      "train accuracy on epoch 1925: 1.000\n",
      "test loss on epoch 1925: 0.307\n",
      "test accuracy on epoch 1925: 0.769\n",
      "train loss on epoch 1926 : 0.369\n",
      "train accuracy on epoch 1926: 0.889\n",
      "test loss on epoch 1926: 0.304\n",
      "test accuracy on epoch 1926: 0.769\n",
      "train loss on epoch 1927 : 0.320\n",
      "train accuracy on epoch 1927: 0.889\n",
      "test loss on epoch 1927: 0.311\n",
      "test accuracy on epoch 1927: 0.769\n",
      "train loss on epoch 1928 : 0.190\n",
      "train accuracy on epoch 1928: 0.889\n",
      "test loss on epoch 1928: 0.312\n",
      "test accuracy on epoch 1928: 0.769\n",
      "train loss on epoch 1929 : 0.080\n",
      "train accuracy on epoch 1929: 0.944\n",
      "test loss on epoch 1929: 0.323\n",
      "test accuracy on epoch 1929: 0.769\n",
      "train loss on epoch 1930 : 0.128\n",
      "train accuracy on epoch 1930: 0.944\n",
      "test loss on epoch 1930: 0.321\n",
      "test accuracy on epoch 1930: 0.769\n",
      "train loss on epoch 1931 : 0.221\n",
      "train accuracy on epoch 1931: 0.944\n",
      "test loss on epoch 1931: 0.323\n",
      "test accuracy on epoch 1931: 0.769\n",
      "train loss on epoch 1932 : 0.451\n",
      "train accuracy on epoch 1932: 0.889\n",
      "test loss on epoch 1932: 0.330\n",
      "test accuracy on epoch 1932: 0.769\n",
      "train loss on epoch 1933 : 0.226\n",
      "train accuracy on epoch 1933: 0.889\n",
      "test loss on epoch 1933: 0.322\n",
      "test accuracy on epoch 1933: 0.769\n",
      "train loss on epoch 1934 : 0.265\n",
      "train accuracy on epoch 1934: 0.889\n",
      "test loss on epoch 1934: 0.321\n",
      "test accuracy on epoch 1934: 0.769\n",
      "train loss on epoch 1935 : 0.256\n",
      "train accuracy on epoch 1935: 0.889\n",
      "test loss on epoch 1935: 0.314\n",
      "test accuracy on epoch 1935: 0.769\n",
      "train loss on epoch 1936 : 0.130\n",
      "train accuracy on epoch 1936: 0.944\n",
      "test loss on epoch 1936: 0.314\n",
      "test accuracy on epoch 1936: 0.769\n",
      "train loss on epoch 1937 : 0.103\n",
      "train accuracy on epoch 1937: 0.944\n",
      "test loss on epoch 1937: 0.308\n",
      "test accuracy on epoch 1937: 0.769\n",
      "train loss on epoch 1938 : 0.075\n",
      "train accuracy on epoch 1938: 1.000\n",
      "test loss on epoch 1938: 0.303\n",
      "test accuracy on epoch 1938: 0.769\n",
      "train loss on epoch 1939 : 0.265\n",
      "train accuracy on epoch 1939: 0.944\n",
      "test loss on epoch 1939: 0.318\n",
      "test accuracy on epoch 1939: 0.769\n",
      "train loss on epoch 1940 : 0.076\n",
      "train accuracy on epoch 1940: 1.000\n",
      "test loss on epoch 1940: 0.319\n",
      "test accuracy on epoch 1940: 0.769\n",
      "train loss on epoch 1941 : 0.244\n",
      "train accuracy on epoch 1941: 0.944\n",
      "test loss on epoch 1941: 0.304\n",
      "test accuracy on epoch 1941: 0.769\n",
      "train loss on epoch 1942 : 0.174\n",
      "train accuracy on epoch 1942: 0.944\n",
      "test loss on epoch 1942: 0.317\n",
      "test accuracy on epoch 1942: 0.769\n",
      "train loss on epoch 1943 : 0.155\n",
      "train accuracy on epoch 1943: 1.000\n",
      "test loss on epoch 1943: 0.317\n",
      "test accuracy on epoch 1943: 0.769\n",
      "train loss on epoch 1944 : 0.129\n",
      "train accuracy on epoch 1944: 0.889\n",
      "test loss on epoch 1944: 0.321\n",
      "test accuracy on epoch 1944: 0.769\n",
      "train loss on epoch 1945 : 0.152\n",
      "train accuracy on epoch 1945: 0.889\n",
      "test loss on epoch 1945: 0.317\n",
      "test accuracy on epoch 1945: 0.769\n",
      "train loss on epoch 1946 : 0.338\n",
      "train accuracy on epoch 1946: 0.833\n",
      "test loss on epoch 1946: 0.327\n",
      "test accuracy on epoch 1946: 0.769\n",
      "train loss on epoch 1947 : 0.201\n",
      "train accuracy on epoch 1947: 0.889\n",
      "test loss on epoch 1947: 0.307\n",
      "test accuracy on epoch 1947: 0.769\n",
      "train loss on epoch 1948 : 0.125\n",
      "train accuracy on epoch 1948: 0.944\n",
      "test loss on epoch 1948: 0.318\n",
      "test accuracy on epoch 1948: 0.769\n",
      "train loss on epoch 1949 : 0.152\n",
      "train accuracy on epoch 1949: 0.889\n",
      "test loss on epoch 1949: 0.306\n",
      "test accuracy on epoch 1949: 0.769\n",
      "train loss on epoch 1950 : 0.145\n",
      "train accuracy on epoch 1950: 0.944\n",
      "test loss on epoch 1950: 0.321\n",
      "test accuracy on epoch 1950: 0.769\n",
      "train loss on epoch 1951 : 0.174\n",
      "train accuracy on epoch 1951: 0.944\n",
      "test loss on epoch 1951: 0.314\n",
      "test accuracy on epoch 1951: 0.769\n",
      "train loss on epoch 1952 : 0.430\n",
      "train accuracy on epoch 1952: 0.889\n",
      "test loss on epoch 1952: 0.308\n",
      "test accuracy on epoch 1952: 0.769\n",
      "train loss on epoch 1953 : 0.193\n",
      "train accuracy on epoch 1953: 0.944\n",
      "test loss on epoch 1953: 0.307\n",
      "test accuracy on epoch 1953: 0.769\n",
      "train loss on epoch 1954 : 0.087\n",
      "train accuracy on epoch 1954: 0.944\n",
      "test loss on epoch 1954: 0.316\n",
      "test accuracy on epoch 1954: 0.769\n",
      "train loss on epoch 1955 : 0.209\n",
      "train accuracy on epoch 1955: 0.889\n",
      "test loss on epoch 1955: 0.320\n",
      "test accuracy on epoch 1955: 0.769\n",
      "train loss on epoch 1956 : 0.114\n",
      "train accuracy on epoch 1956: 0.944\n",
      "test loss on epoch 1956: 0.316\n",
      "test accuracy on epoch 1956: 0.769\n",
      "train loss on epoch 1957 : 0.089\n",
      "train accuracy on epoch 1957: 0.944\n",
      "test loss on epoch 1957: 0.317\n",
      "test accuracy on epoch 1957: 0.769\n",
      "train loss on epoch 1958 : 0.172\n",
      "train accuracy on epoch 1958: 0.889\n",
      "test loss on epoch 1958: 0.311\n",
      "test accuracy on epoch 1958: 0.769\n",
      "train loss on epoch 1959 : 0.441\n",
      "train accuracy on epoch 1959: 0.833\n",
      "test loss on epoch 1959: 0.313\n",
      "test accuracy on epoch 1959: 0.769\n",
      "train loss on epoch 1960 : 0.137\n",
      "train accuracy on epoch 1960: 0.944\n",
      "test loss on epoch 1960: 0.317\n",
      "test accuracy on epoch 1960: 0.769\n",
      "train loss on epoch 1961 : 0.226\n",
      "train accuracy on epoch 1961: 0.889\n",
      "test loss on epoch 1961: 0.314\n",
      "test accuracy on epoch 1961: 0.769\n",
      "train loss on epoch 1962 : 0.151\n",
      "train accuracy on epoch 1962: 0.944\n",
      "test loss on epoch 1962: 0.311\n",
      "test accuracy on epoch 1962: 0.769\n",
      "train loss on epoch 1963 : 0.132\n",
      "train accuracy on epoch 1963: 0.889\n",
      "test loss on epoch 1963: 0.316\n",
      "test accuracy on epoch 1963: 0.769\n",
      "train loss on epoch 1964 : 0.066\n",
      "train accuracy on epoch 1964: 0.944\n",
      "test loss on epoch 1964: 0.317\n",
      "test accuracy on epoch 1964: 0.769\n",
      "train loss on epoch 1965 : 0.299\n",
      "train accuracy on epoch 1965: 0.889\n",
      "test loss on epoch 1965: 0.316\n",
      "test accuracy on epoch 1965: 0.769\n",
      "train loss on epoch 1966 : 0.245\n",
      "train accuracy on epoch 1966: 0.944\n",
      "test loss on epoch 1966: 0.322\n",
      "test accuracy on epoch 1966: 0.769\n",
      "train loss on epoch 1967 : 0.065\n",
      "train accuracy on epoch 1967: 1.000\n",
      "test loss on epoch 1967: 0.326\n",
      "test accuracy on epoch 1967: 0.769\n",
      "train loss on epoch 1968 : 0.242\n",
      "train accuracy on epoch 1968: 0.944\n",
      "test loss on epoch 1968: 0.320\n",
      "test accuracy on epoch 1968: 0.769\n",
      "train loss on epoch 1969 : 0.053\n",
      "train accuracy on epoch 1969: 1.000\n",
      "test loss on epoch 1969: 0.320\n",
      "test accuracy on epoch 1969: 0.769\n",
      "train loss on epoch 1970 : 0.375\n",
      "train accuracy on epoch 1970: 0.889\n",
      "test loss on epoch 1970: 0.313\n",
      "test accuracy on epoch 1970: 0.769\n",
      "train loss on epoch 1971 : 0.129\n",
      "train accuracy on epoch 1971: 0.944\n",
      "test loss on epoch 1971: 0.315\n",
      "test accuracy on epoch 1971: 0.769\n",
      "train loss on epoch 1972 : 0.131\n",
      "train accuracy on epoch 1972: 0.944\n",
      "test loss on epoch 1972: 0.319\n",
      "test accuracy on epoch 1972: 0.769\n",
      "train loss on epoch 1973 : 0.092\n",
      "train accuracy on epoch 1973: 0.944\n",
      "test loss on epoch 1973: 0.314\n",
      "test accuracy on epoch 1973: 0.769\n",
      "train loss on epoch 1974 : 0.079\n",
      "train accuracy on epoch 1974: 0.944\n",
      "test loss on epoch 1974: 0.310\n",
      "test accuracy on epoch 1974: 0.769\n",
      "train loss on epoch 1975 : 0.184\n",
      "train accuracy on epoch 1975: 0.944\n",
      "test loss on epoch 1975: 0.312\n",
      "test accuracy on epoch 1975: 0.769\n",
      "train loss on epoch 1976 : 0.130\n",
      "train accuracy on epoch 1976: 0.889\n",
      "test loss on epoch 1976: 0.313\n",
      "test accuracy on epoch 1976: 0.769\n",
      "train loss on epoch 1977 : 0.231\n",
      "train accuracy on epoch 1977: 0.944\n",
      "test loss on epoch 1977: 0.311\n",
      "test accuracy on epoch 1977: 0.769\n",
      "train loss on epoch 1978 : 0.363\n",
      "train accuracy on epoch 1978: 0.833\n",
      "test loss on epoch 1978: 0.309\n",
      "test accuracy on epoch 1978: 0.769\n",
      "train loss on epoch 1979 : 0.069\n",
      "train accuracy on epoch 1979: 0.944\n",
      "test loss on epoch 1979: 0.313\n",
      "test accuracy on epoch 1979: 0.769\n",
      "train loss on epoch 1980 : 0.064\n",
      "train accuracy on epoch 1980: 1.000\n",
      "test loss on epoch 1980: 0.315\n",
      "test accuracy on epoch 1980: 0.769\n",
      "train loss on epoch 1981 : 0.090\n",
      "train accuracy on epoch 1981: 1.000\n",
      "test loss on epoch 1981: 0.318\n",
      "test accuracy on epoch 1981: 0.769\n",
      "train loss on epoch 1982 : 0.213\n",
      "train accuracy on epoch 1982: 0.889\n",
      "test loss on epoch 1982: 0.315\n",
      "test accuracy on epoch 1982: 0.769\n",
      "train loss on epoch 1983 : 0.298\n",
      "train accuracy on epoch 1983: 0.944\n",
      "test loss on epoch 1983: 0.317\n",
      "test accuracy on epoch 1983: 0.769\n",
      "train loss on epoch 1984 : 0.132\n",
      "train accuracy on epoch 1984: 0.944\n",
      "test loss on epoch 1984: 0.315\n",
      "test accuracy on epoch 1984: 0.769\n",
      "train loss on epoch 1985 : 0.195\n",
      "train accuracy on epoch 1985: 0.889\n",
      "test loss on epoch 1985: 0.317\n",
      "test accuracy on epoch 1985: 0.769\n",
      "train loss on epoch 1986 : 0.107\n",
      "train accuracy on epoch 1986: 0.944\n",
      "test loss on epoch 1986: 0.317\n",
      "test accuracy on epoch 1986: 0.692\n",
      "train loss on epoch 1987 : 0.334\n",
      "train accuracy on epoch 1987: 0.889\n",
      "test loss on epoch 1987: 0.311\n",
      "test accuracy on epoch 1987: 0.769\n",
      "train loss on epoch 1988 : 0.177\n",
      "train accuracy on epoch 1988: 0.944\n",
      "test loss on epoch 1988: 0.318\n",
      "test accuracy on epoch 1988: 0.769\n",
      "train loss on epoch 1989 : 0.110\n",
      "train accuracy on epoch 1989: 0.944\n",
      "test loss on epoch 1989: 0.308\n",
      "test accuracy on epoch 1989: 0.769\n",
      "train loss on epoch 1990 : 0.230\n",
      "train accuracy on epoch 1990: 0.833\n",
      "test loss on epoch 1990: 0.317\n",
      "test accuracy on epoch 1990: 0.769\n",
      "train loss on epoch 1991 : 0.325\n",
      "train accuracy on epoch 1991: 0.833\n",
      "test loss on epoch 1991: 0.326\n",
      "test accuracy on epoch 1991: 0.769\n",
      "train loss on epoch 1992 : 0.157\n",
      "train accuracy on epoch 1992: 0.944\n",
      "test loss on epoch 1992: 0.313\n",
      "test accuracy on epoch 1992: 0.769\n",
      "train loss on epoch 1993 : 0.162\n",
      "train accuracy on epoch 1993: 0.889\n",
      "test loss on epoch 1993: 0.314\n",
      "test accuracy on epoch 1993: 0.769\n",
      "train loss on epoch 1994 : 0.130\n",
      "train accuracy on epoch 1994: 0.944\n",
      "test loss on epoch 1994: 0.332\n",
      "test accuracy on epoch 1994: 0.769\n",
      "train loss on epoch 1995 : 0.208\n",
      "train accuracy on epoch 1995: 0.833\n",
      "test loss on epoch 1995: 0.320\n",
      "test accuracy on epoch 1995: 0.769\n",
      "train loss on epoch 1996 : 0.311\n",
      "train accuracy on epoch 1996: 0.889\n",
      "test loss on epoch 1996: 0.334\n",
      "test accuracy on epoch 1996: 0.769\n",
      "train loss on epoch 1997 : 0.146\n",
      "train accuracy on epoch 1997: 0.944\n",
      "test loss on epoch 1997: 0.316\n",
      "test accuracy on epoch 1997: 0.769\n",
      "train loss on epoch 1998 : 0.094\n",
      "train accuracy on epoch 1998: 0.944\n",
      "test loss on epoch 1998: 0.336\n",
      "test accuracy on epoch 1998: 0.769\n",
      "train loss on epoch 1999 : 0.266\n",
      "train accuracy on epoch 1999: 0.944\n",
      "test loss on epoch 1999: 0.321\n",
      "test accuracy on epoch 1999: 0.769\n",
      "train loss on epoch 2000 : 0.254\n",
      "train accuracy on epoch 2000: 0.889\n",
      "test loss on epoch 2000: 0.337\n",
      "test accuracy on epoch 2000: 0.769\n",
      "train loss on epoch 2001 : 0.025\n",
      "train accuracy on epoch 2001: 1.000\n",
      "test loss on epoch 2001: 0.317\n",
      "test accuracy on epoch 2001: 0.769\n",
      "train loss on epoch 2002 : 0.092\n",
      "train accuracy on epoch 2002: 0.944\n",
      "test loss on epoch 2002: 0.330\n",
      "test accuracy on epoch 2002: 0.769\n",
      "train loss on epoch 2003 : 0.122\n",
      "train accuracy on epoch 2003: 0.944\n",
      "test loss on epoch 2003: 0.328\n",
      "test accuracy on epoch 2003: 0.769\n",
      "train loss on epoch 2004 : 0.122\n",
      "train accuracy on epoch 2004: 0.944\n",
      "test loss on epoch 2004: 0.316\n",
      "test accuracy on epoch 2004: 0.769\n",
      "train loss on epoch 2005 : 0.261\n",
      "train accuracy on epoch 2005: 0.944\n",
      "test loss on epoch 2005: 0.318\n",
      "test accuracy on epoch 2005: 0.769\n",
      "train loss on epoch 2006 : 0.255\n",
      "train accuracy on epoch 2006: 0.944\n",
      "test loss on epoch 2006: 0.323\n",
      "test accuracy on epoch 2006: 0.769\n",
      "train loss on epoch 2007 : 0.218\n",
      "train accuracy on epoch 2007: 0.889\n",
      "test loss on epoch 2007: 0.332\n",
      "test accuracy on epoch 2007: 0.769\n",
      "train loss on epoch 2008 : 0.084\n",
      "train accuracy on epoch 2008: 1.000\n",
      "test loss on epoch 2008: 0.325\n",
      "test accuracy on epoch 2008: 0.769\n",
      "train loss on epoch 2009 : 0.056\n",
      "train accuracy on epoch 2009: 1.000\n",
      "test loss on epoch 2009: 0.326\n",
      "test accuracy on epoch 2009: 0.769\n",
      "train loss on epoch 2010 : 0.475\n",
      "train accuracy on epoch 2010: 0.833\n",
      "test loss on epoch 2010: 0.334\n",
      "test accuracy on epoch 2010: 0.769\n",
      "train loss on epoch 2011 : 0.358\n",
      "train accuracy on epoch 2011: 0.889\n",
      "test loss on epoch 2011: 0.325\n",
      "test accuracy on epoch 2011: 0.769\n",
      "train loss on epoch 2012 : 0.172\n",
      "train accuracy on epoch 2012: 0.889\n",
      "test loss on epoch 2012: 0.333\n",
      "test accuracy on epoch 2012: 0.769\n",
      "train loss on epoch 2013 : 0.200\n",
      "train accuracy on epoch 2013: 0.889\n",
      "test loss on epoch 2013: 0.321\n",
      "test accuracy on epoch 2013: 0.769\n",
      "train loss on epoch 2014 : 0.145\n",
      "train accuracy on epoch 2014: 0.889\n",
      "test loss on epoch 2014: 0.335\n",
      "test accuracy on epoch 2014: 0.769\n",
      "train loss on epoch 2015 : 0.277\n",
      "train accuracy on epoch 2015: 0.889\n",
      "test loss on epoch 2015: 0.337\n",
      "test accuracy on epoch 2015: 0.769\n",
      "train loss on epoch 2016 : 0.439\n",
      "train accuracy on epoch 2016: 0.889\n",
      "test loss on epoch 2016: 0.335\n",
      "test accuracy on epoch 2016: 0.769\n",
      "train loss on epoch 2017 : 0.259\n",
      "train accuracy on epoch 2017: 0.833\n",
      "test loss on epoch 2017: 0.321\n",
      "test accuracy on epoch 2017: 0.769\n",
      "train loss on epoch 2018 : 0.179\n",
      "train accuracy on epoch 2018: 0.944\n",
      "test loss on epoch 2018: 0.318\n",
      "test accuracy on epoch 2018: 0.769\n",
      "train loss on epoch 2019 : 0.172\n",
      "train accuracy on epoch 2019: 0.889\n",
      "test loss on epoch 2019: 0.330\n",
      "test accuracy on epoch 2019: 0.769\n",
      "train loss on epoch 2020 : 0.187\n",
      "train accuracy on epoch 2020: 0.944\n",
      "test loss on epoch 2020: 0.328\n",
      "test accuracy on epoch 2020: 0.769\n",
      "train loss on epoch 2021 : 0.145\n",
      "train accuracy on epoch 2021: 0.944\n",
      "test loss on epoch 2021: 0.327\n",
      "test accuracy on epoch 2021: 0.769\n",
      "train loss on epoch 2022 : 0.076\n",
      "train accuracy on epoch 2022: 1.000\n",
      "test loss on epoch 2022: 0.330\n",
      "test accuracy on epoch 2022: 0.769\n",
      "train loss on epoch 2023 : 0.166\n",
      "train accuracy on epoch 2023: 0.944\n",
      "test loss on epoch 2023: 0.319\n",
      "test accuracy on epoch 2023: 0.769\n",
      "train loss on epoch 2024 : 0.420\n",
      "train accuracy on epoch 2024: 0.889\n",
      "test loss on epoch 2024: 0.327\n",
      "test accuracy on epoch 2024: 0.769\n",
      "train loss on epoch 2025 : 0.265\n",
      "train accuracy on epoch 2025: 0.833\n",
      "test loss on epoch 2025: 0.326\n",
      "test accuracy on epoch 2025: 0.769\n",
      "train loss on epoch 2026 : 0.367\n",
      "train accuracy on epoch 2026: 0.889\n",
      "test loss on epoch 2026: 0.324\n",
      "test accuracy on epoch 2026: 0.769\n",
      "train loss on epoch 2027 : 0.216\n",
      "train accuracy on epoch 2027: 0.889\n",
      "test loss on epoch 2027: 0.317\n",
      "test accuracy on epoch 2027: 0.769\n",
      "train loss on epoch 2028 : 0.141\n",
      "train accuracy on epoch 2028: 0.944\n",
      "test loss on epoch 2028: 0.322\n",
      "test accuracy on epoch 2028: 0.769\n",
      "train loss on epoch 2029 : 0.339\n",
      "train accuracy on epoch 2029: 0.944\n",
      "test loss on epoch 2029: 0.330\n",
      "test accuracy on epoch 2029: 0.769\n",
      "train loss on epoch 2030 : 0.158\n",
      "train accuracy on epoch 2030: 0.944\n",
      "test loss on epoch 2030: 0.331\n",
      "test accuracy on epoch 2030: 0.769\n",
      "train loss on epoch 2031 : 0.096\n",
      "train accuracy on epoch 2031: 0.944\n",
      "test loss on epoch 2031: 0.331\n",
      "test accuracy on epoch 2031: 0.769\n",
      "train loss on epoch 2032 : 0.116\n",
      "train accuracy on epoch 2032: 0.889\n",
      "test loss on epoch 2032: 0.327\n",
      "test accuracy on epoch 2032: 0.769\n",
      "train loss on epoch 2033 : 0.250\n",
      "train accuracy on epoch 2033: 0.889\n",
      "test loss on epoch 2033: 0.325\n",
      "test accuracy on epoch 2033: 0.769\n",
      "train loss on epoch 2034 : 0.116\n",
      "train accuracy on epoch 2034: 0.944\n",
      "test loss on epoch 2034: 0.319\n",
      "test accuracy on epoch 2034: 0.769\n",
      "train loss on epoch 2035 : 0.109\n",
      "train accuracy on epoch 2035: 0.944\n",
      "test loss on epoch 2035: 0.323\n",
      "test accuracy on epoch 2035: 0.769\n",
      "train loss on epoch 2036 : 0.189\n",
      "train accuracy on epoch 2036: 0.833\n",
      "test loss on epoch 2036: 0.327\n",
      "test accuracy on epoch 2036: 0.769\n",
      "train loss on epoch 2037 : 0.087\n",
      "train accuracy on epoch 2037: 0.944\n",
      "test loss on epoch 2037: 0.337\n",
      "test accuracy on epoch 2037: 0.769\n",
      "train loss on epoch 2038 : 0.197\n",
      "train accuracy on epoch 2038: 0.944\n",
      "test loss on epoch 2038: 0.341\n",
      "test accuracy on epoch 2038: 0.769\n",
      "train loss on epoch 2039 : 0.133\n",
      "train accuracy on epoch 2039: 0.889\n",
      "test loss on epoch 2039: 0.327\n",
      "test accuracy on epoch 2039: 0.769\n",
      "train loss on epoch 2040 : 0.246\n",
      "train accuracy on epoch 2040: 0.889\n",
      "test loss on epoch 2040: 0.327\n",
      "test accuracy on epoch 2040: 0.769\n",
      "train loss on epoch 2041 : 0.300\n",
      "train accuracy on epoch 2041: 0.889\n",
      "test loss on epoch 2041: 0.326\n",
      "test accuracy on epoch 2041: 0.769\n",
      "train loss on epoch 2042 : 0.157\n",
      "train accuracy on epoch 2042: 0.944\n",
      "test loss on epoch 2042: 0.336\n",
      "test accuracy on epoch 2042: 0.769\n",
      "train loss on epoch 2043 : 0.113\n",
      "train accuracy on epoch 2043: 0.944\n",
      "test loss on epoch 2043: 0.334\n",
      "test accuracy on epoch 2043: 0.769\n",
      "train loss on epoch 2044 : 0.248\n",
      "train accuracy on epoch 2044: 0.833\n",
      "test loss on epoch 2044: 0.331\n",
      "test accuracy on epoch 2044: 0.846\n",
      "train loss on epoch 2045 : 0.083\n",
      "train accuracy on epoch 2045: 1.000\n",
      "test loss on epoch 2045: 0.326\n",
      "test accuracy on epoch 2045: 0.769\n",
      "train loss on epoch 2046 : 0.041\n",
      "train accuracy on epoch 2046: 1.000\n",
      "test loss on epoch 2046: 0.337\n",
      "test accuracy on epoch 2046: 0.769\n",
      "train loss on epoch 2047 : 0.142\n",
      "train accuracy on epoch 2047: 0.889\n",
      "test loss on epoch 2047: 0.328\n",
      "test accuracy on epoch 2047: 0.769\n",
      "train loss on epoch 2048 : 0.107\n",
      "train accuracy on epoch 2048: 0.944\n",
      "test loss on epoch 2048: 0.334\n",
      "test accuracy on epoch 2048: 0.769\n",
      "train loss on epoch 2049 : 0.177\n",
      "train accuracy on epoch 2049: 0.944\n",
      "test loss on epoch 2049: 0.331\n",
      "test accuracy on epoch 2049: 0.769\n",
      "train loss on epoch 2050 : 0.196\n",
      "train accuracy on epoch 2050: 0.833\n",
      "test loss on epoch 2050: 0.334\n",
      "test accuracy on epoch 2050: 0.769\n",
      "train loss on epoch 2051 : 0.333\n",
      "train accuracy on epoch 2051: 0.889\n",
      "test loss on epoch 2051: 0.333\n",
      "test accuracy on epoch 2051: 0.769\n",
      "train loss on epoch 2052 : 0.115\n",
      "train accuracy on epoch 2052: 0.944\n",
      "test loss on epoch 2052: 0.336\n",
      "test accuracy on epoch 2052: 0.769\n",
      "train loss on epoch 2053 : 0.117\n",
      "train accuracy on epoch 2053: 0.944\n",
      "test loss on epoch 2053: 0.333\n",
      "test accuracy on epoch 2053: 0.769\n",
      "train loss on epoch 2054 : 0.112\n",
      "train accuracy on epoch 2054: 0.944\n",
      "test loss on epoch 2054: 0.336\n",
      "test accuracy on epoch 2054: 0.769\n",
      "train loss on epoch 2055 : 0.263\n",
      "train accuracy on epoch 2055: 0.889\n",
      "test loss on epoch 2055: 0.330\n",
      "test accuracy on epoch 2055: 0.769\n",
      "train loss on epoch 2056 : 0.369\n",
      "train accuracy on epoch 2056: 0.889\n",
      "test loss on epoch 2056: 0.331\n",
      "test accuracy on epoch 2056: 0.769\n",
      "train loss on epoch 2057 : 0.067\n",
      "train accuracy on epoch 2057: 1.000\n",
      "test loss on epoch 2057: 0.331\n",
      "test accuracy on epoch 2057: 0.769\n",
      "train loss on epoch 2058 : 0.154\n",
      "train accuracy on epoch 2058: 0.944\n",
      "test loss on epoch 2058: 0.328\n",
      "test accuracy on epoch 2058: 0.769\n",
      "train loss on epoch 2059 : 0.303\n",
      "train accuracy on epoch 2059: 0.944\n",
      "test loss on epoch 2059: 0.334\n",
      "test accuracy on epoch 2059: 0.769\n",
      "train loss on epoch 2060 : 0.321\n",
      "train accuracy on epoch 2060: 0.889\n",
      "test loss on epoch 2060: 0.334\n",
      "test accuracy on epoch 2060: 0.769\n",
      "train loss on epoch 2061 : 0.113\n",
      "train accuracy on epoch 2061: 0.944\n",
      "test loss on epoch 2061: 0.338\n",
      "test accuracy on epoch 2061: 0.769\n",
      "train loss on epoch 2062 : 0.089\n",
      "train accuracy on epoch 2062: 0.944\n",
      "test loss on epoch 2062: 0.329\n",
      "test accuracy on epoch 2062: 0.769\n",
      "train loss on epoch 2063 : 0.293\n",
      "train accuracy on epoch 2063: 0.944\n",
      "test loss on epoch 2063: 0.326\n",
      "test accuracy on epoch 2063: 0.769\n",
      "train loss on epoch 2064 : 0.377\n",
      "train accuracy on epoch 2064: 0.889\n",
      "test loss on epoch 2064: 0.329\n",
      "test accuracy on epoch 2064: 0.769\n",
      "train loss on epoch 2065 : 0.094\n",
      "train accuracy on epoch 2065: 1.000\n",
      "test loss on epoch 2065: 0.341\n",
      "test accuracy on epoch 2065: 0.769\n",
      "train loss on epoch 2066 : 0.070\n",
      "train accuracy on epoch 2066: 0.944\n",
      "test loss on epoch 2066: 0.342\n",
      "test accuracy on epoch 2066: 0.769\n",
      "train loss on epoch 2067 : 0.117\n",
      "train accuracy on epoch 2067: 1.000\n",
      "test loss on epoch 2067: 0.329\n",
      "test accuracy on epoch 2067: 0.769\n",
      "train loss on epoch 2068 : 0.266\n",
      "train accuracy on epoch 2068: 0.889\n",
      "test loss on epoch 2068: 0.336\n",
      "test accuracy on epoch 2068: 0.769\n",
      "train loss on epoch 2069 : 0.122\n",
      "train accuracy on epoch 2069: 1.000\n",
      "test loss on epoch 2069: 0.336\n",
      "test accuracy on epoch 2069: 0.769\n",
      "train loss on epoch 2070 : 0.152\n",
      "train accuracy on epoch 2070: 0.944\n",
      "test loss on epoch 2070: 0.340\n",
      "test accuracy on epoch 2070: 0.692\n",
      "train loss on epoch 2071 : 0.076\n",
      "train accuracy on epoch 2071: 1.000\n",
      "test loss on epoch 2071: 0.333\n",
      "test accuracy on epoch 2071: 0.769\n",
      "train loss on epoch 2072 : 0.182\n",
      "train accuracy on epoch 2072: 0.833\n",
      "test loss on epoch 2072: 0.341\n",
      "test accuracy on epoch 2072: 0.769\n",
      "train loss on epoch 2073 : 0.121\n",
      "train accuracy on epoch 2073: 0.944\n",
      "test loss on epoch 2073: 0.334\n",
      "test accuracy on epoch 2073: 0.769\n",
      "train loss on epoch 2074 : 0.149\n",
      "train accuracy on epoch 2074: 0.889\n",
      "test loss on epoch 2074: 0.344\n",
      "test accuracy on epoch 2074: 0.692\n",
      "train loss on epoch 2075 : 0.232\n",
      "train accuracy on epoch 2075: 0.944\n",
      "test loss on epoch 2075: 0.334\n",
      "test accuracy on epoch 2075: 0.846\n",
      "train loss on epoch 2076 : 0.192\n",
      "train accuracy on epoch 2076: 0.889\n",
      "test loss on epoch 2076: 0.340\n",
      "test accuracy on epoch 2076: 0.769\n",
      "train loss on epoch 2077 : 0.122\n",
      "train accuracy on epoch 2077: 0.944\n",
      "test loss on epoch 2077: 0.338\n",
      "test accuracy on epoch 2077: 0.769\n",
      "train loss on epoch 2078 : 0.211\n",
      "train accuracy on epoch 2078: 0.889\n",
      "test loss on epoch 2078: 0.342\n",
      "test accuracy on epoch 2078: 0.769\n",
      "train loss on epoch 2079 : 0.312\n",
      "train accuracy on epoch 2079: 0.833\n",
      "test loss on epoch 2079: 0.339\n",
      "test accuracy on epoch 2079: 0.769\n",
      "train loss on epoch 2080 : 0.260\n",
      "train accuracy on epoch 2080: 0.889\n",
      "test loss on epoch 2080: 0.342\n",
      "test accuracy on epoch 2080: 0.692\n",
      "train loss on epoch 2081 : 0.284\n",
      "train accuracy on epoch 2081: 0.889\n",
      "test loss on epoch 2081: 0.330\n",
      "test accuracy on epoch 2081: 0.846\n",
      "train loss on epoch 2082 : 0.203\n",
      "train accuracy on epoch 2082: 0.944\n",
      "test loss on epoch 2082: 0.333\n",
      "test accuracy on epoch 2082: 0.769\n",
      "train loss on epoch 2083 : 0.232\n",
      "train accuracy on epoch 2083: 0.944\n",
      "test loss on epoch 2083: 0.331\n",
      "test accuracy on epoch 2083: 0.769\n",
      "train loss on epoch 2084 : 0.394\n",
      "train accuracy on epoch 2084: 0.833\n",
      "test loss on epoch 2084: 0.339\n",
      "test accuracy on epoch 2084: 0.769\n",
      "train loss on epoch 2085 : 0.036\n",
      "train accuracy on epoch 2085: 1.000\n",
      "test loss on epoch 2085: 0.354\n",
      "test accuracy on epoch 2085: 0.769\n",
      "train loss on epoch 2086 : 0.238\n",
      "train accuracy on epoch 2086: 0.833\n",
      "test loss on epoch 2086: 0.346\n",
      "test accuracy on epoch 2086: 0.769\n",
      "train loss on epoch 2087 : 0.123\n",
      "train accuracy on epoch 2087: 0.944\n",
      "test loss on epoch 2087: 0.342\n",
      "test accuracy on epoch 2087: 0.769\n",
      "train loss on epoch 2088 : 0.198\n",
      "train accuracy on epoch 2088: 0.944\n",
      "test loss on epoch 2088: 0.346\n",
      "test accuracy on epoch 2088: 0.769\n",
      "train loss on epoch 2089 : 0.133\n",
      "train accuracy on epoch 2089: 0.889\n",
      "test loss on epoch 2089: 0.348\n",
      "test accuracy on epoch 2089: 0.769\n",
      "train loss on epoch 2090 : 0.264\n",
      "train accuracy on epoch 2090: 0.889\n",
      "test loss on epoch 2090: 0.349\n",
      "test accuracy on epoch 2090: 0.769\n",
      "train loss on epoch 2091 : 0.230\n",
      "train accuracy on epoch 2091: 0.889\n",
      "test loss on epoch 2091: 0.345\n",
      "test accuracy on epoch 2091: 0.769\n",
      "train loss on epoch 2092 : 0.247\n",
      "train accuracy on epoch 2092: 0.889\n",
      "test loss on epoch 2092: 0.335\n",
      "test accuracy on epoch 2092: 0.769\n",
      "train loss on epoch 2093 : 0.338\n",
      "train accuracy on epoch 2093: 0.889\n",
      "test loss on epoch 2093: 0.329\n",
      "test accuracy on epoch 2093: 0.769\n",
      "train loss on epoch 2094 : 0.186\n",
      "train accuracy on epoch 2094: 0.889\n",
      "test loss on epoch 2094: 0.332\n",
      "test accuracy on epoch 2094: 0.769\n",
      "train loss on epoch 2095 : 0.139\n",
      "train accuracy on epoch 2095: 0.944\n",
      "test loss on epoch 2095: 0.331\n",
      "test accuracy on epoch 2095: 0.769\n",
      "train loss on epoch 2096 : 0.137\n",
      "train accuracy on epoch 2096: 0.944\n",
      "test loss on epoch 2096: 0.323\n",
      "test accuracy on epoch 2096: 0.846\n",
      "train loss on epoch 2097 : 0.124\n",
      "train accuracy on epoch 2097: 0.944\n",
      "test loss on epoch 2097: 0.327\n",
      "test accuracy on epoch 2097: 0.769\n",
      "train loss on epoch 2098 : 0.087\n",
      "train accuracy on epoch 2098: 0.944\n",
      "test loss on epoch 2098: 0.327\n",
      "test accuracy on epoch 2098: 0.769\n",
      "train loss on epoch 2099 : 0.131\n",
      "train accuracy on epoch 2099: 0.944\n",
      "test loss on epoch 2099: 0.337\n",
      "test accuracy on epoch 2099: 0.692\n",
      "train loss on epoch 2100 : 0.086\n",
      "train accuracy on epoch 2100: 1.000\n",
      "test loss on epoch 2100: 0.322\n",
      "test accuracy on epoch 2100: 0.846\n",
      "train loss on epoch 2101 : 0.069\n",
      "train accuracy on epoch 2101: 1.000\n",
      "test loss on epoch 2101: 0.330\n",
      "test accuracy on epoch 2101: 0.769\n",
      "train loss on epoch 2102 : 0.048\n",
      "train accuracy on epoch 2102: 1.000\n",
      "test loss on epoch 2102: 0.339\n",
      "test accuracy on epoch 2102: 0.769\n",
      "train loss on epoch 2103 : 0.086\n",
      "train accuracy on epoch 2103: 0.944\n",
      "test loss on epoch 2103: 0.328\n",
      "test accuracy on epoch 2103: 0.769\n",
      "train loss on epoch 2104 : 0.021\n",
      "train accuracy on epoch 2104: 1.000\n",
      "test loss on epoch 2104: 0.332\n",
      "test accuracy on epoch 2104: 0.769\n",
      "train loss on epoch 2105 : 0.096\n",
      "train accuracy on epoch 2105: 0.944\n",
      "test loss on epoch 2105: 0.332\n",
      "test accuracy on epoch 2105: 0.769\n",
      "train loss on epoch 2106 : 0.394\n",
      "train accuracy on epoch 2106: 0.833\n",
      "test loss on epoch 2106: 0.334\n",
      "test accuracy on epoch 2106: 0.769\n",
      "train loss on epoch 2107 : 0.318\n",
      "train accuracy on epoch 2107: 0.889\n",
      "test loss on epoch 2107: 0.335\n",
      "test accuracy on epoch 2107: 0.692\n",
      "train loss on epoch 2108 : 0.223\n",
      "train accuracy on epoch 2108: 0.944\n",
      "test loss on epoch 2108: 0.323\n",
      "test accuracy on epoch 2108: 0.769\n",
      "train loss on epoch 2109 : 0.330\n",
      "train accuracy on epoch 2109: 0.889\n",
      "test loss on epoch 2109: 0.340\n",
      "test accuracy on epoch 2109: 0.769\n",
      "train loss on epoch 2110 : 0.207\n",
      "train accuracy on epoch 2110: 0.889\n",
      "test loss on epoch 2110: 0.341\n",
      "test accuracy on epoch 2110: 0.769\n",
      "train loss on epoch 2111 : 0.584\n",
      "train accuracy on epoch 2111: 0.778\n",
      "test loss on epoch 2111: 0.341\n",
      "test accuracy on epoch 2111: 0.769\n",
      "train loss on epoch 2112 : 0.136\n",
      "train accuracy on epoch 2112: 0.944\n",
      "test loss on epoch 2112: 0.339\n",
      "test accuracy on epoch 2112: 0.692\n",
      "train loss on epoch 2113 : 0.204\n",
      "train accuracy on epoch 2113: 0.944\n",
      "test loss on epoch 2113: 0.323\n",
      "test accuracy on epoch 2113: 0.846\n",
      "train loss on epoch 2114 : 0.035\n",
      "train accuracy on epoch 2114: 1.000\n",
      "test loss on epoch 2114: 0.338\n",
      "test accuracy on epoch 2114: 0.692\n",
      "train loss on epoch 2115 : 0.145\n",
      "train accuracy on epoch 2115: 0.944\n",
      "test loss on epoch 2115: 0.325\n",
      "test accuracy on epoch 2115: 0.846\n",
      "train loss on epoch 2116 : 0.082\n",
      "train accuracy on epoch 2116: 0.944\n",
      "test loss on epoch 2116: 0.330\n",
      "test accuracy on epoch 2116: 0.769\n",
      "train loss on epoch 2117 : 0.164\n",
      "train accuracy on epoch 2117: 0.944\n",
      "test loss on epoch 2117: 0.334\n",
      "test accuracy on epoch 2117: 0.769\n",
      "train loss on epoch 2118 : 0.124\n",
      "train accuracy on epoch 2118: 0.944\n",
      "test loss on epoch 2118: 0.345\n",
      "test accuracy on epoch 2118: 0.769\n",
      "train loss on epoch 2119 : 0.087\n",
      "train accuracy on epoch 2119: 1.000\n",
      "test loss on epoch 2119: 0.353\n",
      "test accuracy on epoch 2119: 0.769\n",
      "train loss on epoch 2120 : 0.305\n",
      "train accuracy on epoch 2120: 0.889\n",
      "test loss on epoch 2120: 0.351\n",
      "test accuracy on epoch 2120: 0.769\n",
      "train loss on epoch 2121 : 0.054\n",
      "train accuracy on epoch 2121: 1.000\n",
      "test loss on epoch 2121: 0.352\n",
      "test accuracy on epoch 2121: 0.769\n",
      "train loss on epoch 2122 : 0.359\n",
      "train accuracy on epoch 2122: 0.833\n",
      "test loss on epoch 2122: 0.344\n",
      "test accuracy on epoch 2122: 0.769\n",
      "train loss on epoch 2123 : 0.228\n",
      "train accuracy on epoch 2123: 0.889\n",
      "test loss on epoch 2123: 0.337\n",
      "test accuracy on epoch 2123: 0.769\n",
      "train loss on epoch 2124 : 0.145\n",
      "train accuracy on epoch 2124: 0.944\n",
      "test loss on epoch 2124: 0.341\n",
      "test accuracy on epoch 2124: 0.692\n",
      "train loss on epoch 2125 : 0.518\n",
      "train accuracy on epoch 2125: 0.889\n",
      "test loss on epoch 2125: 0.322\n",
      "test accuracy on epoch 2125: 0.846\n",
      "train loss on epoch 2126 : 0.264\n",
      "train accuracy on epoch 2126: 0.944\n",
      "test loss on epoch 2126: 0.326\n",
      "test accuracy on epoch 2126: 0.846\n",
      "train loss on epoch 2127 : 0.195\n",
      "train accuracy on epoch 2127: 0.889\n",
      "test loss on epoch 2127: 0.333\n",
      "test accuracy on epoch 2127: 0.769\n",
      "train loss on epoch 2128 : 0.112\n",
      "train accuracy on epoch 2128: 1.000\n",
      "test loss on epoch 2128: 0.343\n",
      "test accuracy on epoch 2128: 0.769\n",
      "train loss on epoch 2129 : 0.147\n",
      "train accuracy on epoch 2129: 0.889\n",
      "test loss on epoch 2129: 0.341\n",
      "test accuracy on epoch 2129: 0.769\n",
      "train loss on epoch 2130 : 0.154\n",
      "train accuracy on epoch 2130: 0.944\n",
      "test loss on epoch 2130: 0.350\n",
      "test accuracy on epoch 2130: 0.769\n",
      "train loss on epoch 2131 : 0.315\n",
      "train accuracy on epoch 2131: 0.889\n",
      "test loss on epoch 2131: 0.362\n",
      "test accuracy on epoch 2131: 0.769\n",
      "train loss on epoch 2132 : 0.330\n",
      "train accuracy on epoch 2132: 0.889\n",
      "test loss on epoch 2132: 0.359\n",
      "test accuracy on epoch 2132: 0.769\n",
      "train loss on epoch 2133 : 0.125\n",
      "train accuracy on epoch 2133: 0.944\n",
      "test loss on epoch 2133: 0.343\n",
      "test accuracy on epoch 2133: 0.769\n",
      "train loss on epoch 2134 : 0.246\n",
      "train accuracy on epoch 2134: 0.944\n",
      "test loss on epoch 2134: 0.335\n",
      "test accuracy on epoch 2134: 0.769\n",
      "train loss on epoch 2135 : 0.089\n",
      "train accuracy on epoch 2135: 0.944\n",
      "test loss on epoch 2135: 0.334\n",
      "test accuracy on epoch 2135: 0.769\n",
      "train loss on epoch 2136 : 0.097\n",
      "train accuracy on epoch 2136: 1.000\n",
      "test loss on epoch 2136: 0.336\n",
      "test accuracy on epoch 2136: 0.692\n",
      "train loss on epoch 2137 : 0.103\n",
      "train accuracy on epoch 2137: 0.944\n",
      "test loss on epoch 2137: 0.340\n",
      "test accuracy on epoch 2137: 0.692\n",
      "train loss on epoch 2138 : 0.131\n",
      "train accuracy on epoch 2138: 0.944\n",
      "test loss on epoch 2138: 0.337\n",
      "test accuracy on epoch 2138: 0.692\n",
      "train loss on epoch 2139 : 0.173\n",
      "train accuracy on epoch 2139: 0.944\n",
      "test loss on epoch 2139: 0.326\n",
      "test accuracy on epoch 2139: 0.769\n",
      "train loss on epoch 2140 : 0.381\n",
      "train accuracy on epoch 2140: 0.833\n",
      "test loss on epoch 2140: 0.333\n",
      "test accuracy on epoch 2140: 0.769\n",
      "train loss on epoch 2141 : 0.410\n",
      "train accuracy on epoch 2141: 0.833\n",
      "test loss on epoch 2141: 0.337\n",
      "test accuracy on epoch 2141: 0.769\n",
      "train loss on epoch 2142 : 0.154\n",
      "train accuracy on epoch 2142: 0.944\n",
      "test loss on epoch 2142: 0.325\n",
      "test accuracy on epoch 2142: 0.769\n",
      "train loss on epoch 2143 : 0.320\n",
      "train accuracy on epoch 2143: 0.833\n",
      "test loss on epoch 2143: 0.341\n",
      "test accuracy on epoch 2143: 0.769\n",
      "train loss on epoch 2144 : 0.434\n",
      "train accuracy on epoch 2144: 0.833\n",
      "test loss on epoch 2144: 0.342\n",
      "test accuracy on epoch 2144: 0.769\n",
      "train loss on epoch 2145 : 0.247\n",
      "train accuracy on epoch 2145: 0.889\n",
      "test loss on epoch 2145: 0.322\n",
      "test accuracy on epoch 2145: 0.769\n",
      "train loss on epoch 2146 : 0.189\n",
      "train accuracy on epoch 2146: 0.944\n",
      "test loss on epoch 2146: 0.337\n",
      "test accuracy on epoch 2146: 0.692\n",
      "train loss on epoch 2147 : 0.091\n",
      "train accuracy on epoch 2147: 0.944\n",
      "test loss on epoch 2147: 0.330\n",
      "test accuracy on epoch 2147: 0.769\n",
      "train loss on epoch 2148 : 0.173\n",
      "train accuracy on epoch 2148: 0.944\n",
      "test loss on epoch 2148: 0.322\n",
      "test accuracy on epoch 2148: 0.846\n",
      "train loss on epoch 2149 : 0.436\n",
      "train accuracy on epoch 2149: 0.889\n",
      "test loss on epoch 2149: 0.323\n",
      "test accuracy on epoch 2149: 0.846\n",
      "train loss on epoch 2150 : 0.136\n",
      "train accuracy on epoch 2150: 0.944\n",
      "test loss on epoch 2150: 0.334\n",
      "test accuracy on epoch 2150: 0.769\n",
      "train loss on epoch 2151 : 0.275\n",
      "train accuracy on epoch 2151: 0.833\n",
      "test loss on epoch 2151: 0.342\n",
      "test accuracy on epoch 2151: 0.692\n",
      "train loss on epoch 2152 : 0.123\n",
      "train accuracy on epoch 2152: 0.944\n",
      "test loss on epoch 2152: 0.329\n",
      "test accuracy on epoch 2152: 0.769\n",
      "train loss on epoch 2153 : 0.043\n",
      "train accuracy on epoch 2153: 1.000\n",
      "test loss on epoch 2153: 0.334\n",
      "test accuracy on epoch 2153: 0.769\n",
      "train loss on epoch 2154 : 0.207\n",
      "train accuracy on epoch 2154: 0.833\n",
      "test loss on epoch 2154: 0.345\n",
      "test accuracy on epoch 2154: 0.769\n",
      "train loss on epoch 2155 : 0.178\n",
      "train accuracy on epoch 2155: 0.944\n",
      "test loss on epoch 2155: 0.338\n",
      "test accuracy on epoch 2155: 0.769\n",
      "train loss on epoch 2156 : 0.165\n",
      "train accuracy on epoch 2156: 0.889\n",
      "test loss on epoch 2156: 0.345\n",
      "test accuracy on epoch 2156: 0.692\n",
      "train loss on epoch 2157 : 0.190\n",
      "train accuracy on epoch 2157: 0.889\n",
      "test loss on epoch 2157: 0.335\n",
      "test accuracy on epoch 2157: 0.769\n",
      "train loss on epoch 2158 : 0.186\n",
      "train accuracy on epoch 2158: 0.889\n",
      "test loss on epoch 2158: 0.335\n",
      "test accuracy on epoch 2158: 0.769\n",
      "train loss on epoch 2159 : 0.209\n",
      "train accuracy on epoch 2159: 0.944\n",
      "test loss on epoch 2159: 0.343\n",
      "test accuracy on epoch 2159: 0.692\n",
      "train loss on epoch 2160 : 0.199\n",
      "train accuracy on epoch 2160: 0.889\n",
      "test loss on epoch 2160: 0.344\n",
      "test accuracy on epoch 2160: 0.692\n",
      "train loss on epoch 2161 : 0.092\n",
      "train accuracy on epoch 2161: 0.944\n",
      "test loss on epoch 2161: 0.330\n",
      "test accuracy on epoch 2161: 0.846\n",
      "train loss on epoch 2162 : 0.118\n",
      "train accuracy on epoch 2162: 0.944\n",
      "test loss on epoch 2162: 0.344\n",
      "test accuracy on epoch 2162: 0.769\n",
      "train loss on epoch 2163 : 0.148\n",
      "train accuracy on epoch 2163: 0.944\n",
      "test loss on epoch 2163: 0.340\n",
      "test accuracy on epoch 2163: 0.769\n",
      "train loss on epoch 2164 : 0.360\n",
      "train accuracy on epoch 2164: 0.889\n",
      "test loss on epoch 2164: 0.347\n",
      "test accuracy on epoch 2164: 0.692\n",
      "train loss on epoch 2165 : 0.124\n",
      "train accuracy on epoch 2165: 0.944\n",
      "test loss on epoch 2165: 0.345\n",
      "test accuracy on epoch 2165: 0.692\n",
      "train loss on epoch 2166 : 0.186\n",
      "train accuracy on epoch 2166: 0.889\n",
      "test loss on epoch 2166: 0.344\n",
      "test accuracy on epoch 2166: 0.692\n",
      "train loss on epoch 2167 : 0.441\n",
      "train accuracy on epoch 2167: 0.833\n",
      "test loss on epoch 2167: 0.341\n",
      "test accuracy on epoch 2167: 0.769\n",
      "train loss on epoch 2168 : 0.261\n",
      "train accuracy on epoch 2168: 0.889\n",
      "test loss on epoch 2168: 0.342\n",
      "test accuracy on epoch 2168: 0.769\n",
      "train loss on epoch 2169 : 0.289\n",
      "train accuracy on epoch 2169: 0.889\n",
      "test loss on epoch 2169: 0.335\n",
      "test accuracy on epoch 2169: 0.846\n",
      "train loss on epoch 2170 : 0.197\n",
      "train accuracy on epoch 2170: 0.944\n",
      "test loss on epoch 2170: 0.347\n",
      "test accuracy on epoch 2170: 0.692\n",
      "train loss on epoch 2171 : 0.092\n",
      "train accuracy on epoch 2171: 1.000\n",
      "test loss on epoch 2171: 0.348\n",
      "test accuracy on epoch 2171: 0.692\n",
      "train loss on epoch 2172 : 0.062\n",
      "train accuracy on epoch 2172: 1.000\n",
      "test loss on epoch 2172: 0.335\n",
      "test accuracy on epoch 2172: 0.846\n",
      "train loss on epoch 2173 : 0.098\n",
      "train accuracy on epoch 2173: 0.944\n",
      "test loss on epoch 2173: 0.333\n",
      "test accuracy on epoch 2173: 0.846\n",
      "train loss on epoch 2174 : 0.094\n",
      "train accuracy on epoch 2174: 0.944\n",
      "test loss on epoch 2174: 0.340\n",
      "test accuracy on epoch 2174: 0.769\n",
      "train loss on epoch 2175 : 0.208\n",
      "train accuracy on epoch 2175: 0.833\n",
      "test loss on epoch 2175: 0.333\n",
      "test accuracy on epoch 2175: 0.769\n",
      "train loss on epoch 2176 : 0.187\n",
      "train accuracy on epoch 2176: 0.944\n",
      "test loss on epoch 2176: 0.345\n",
      "test accuracy on epoch 2176: 0.769\n",
      "train loss on epoch 2177 : 0.091\n",
      "train accuracy on epoch 2177: 0.944\n",
      "test loss on epoch 2177: 0.350\n",
      "test accuracy on epoch 2177: 0.769\n",
      "train loss on epoch 2178 : 0.116\n",
      "train accuracy on epoch 2178: 0.944\n",
      "test loss on epoch 2178: 0.340\n",
      "test accuracy on epoch 2178: 0.769\n",
      "train loss on epoch 2179 : 0.111\n",
      "train accuracy on epoch 2179: 0.944\n",
      "test loss on epoch 2179: 0.347\n",
      "test accuracy on epoch 2179: 0.769\n",
      "train loss on epoch 2180 : 0.154\n",
      "train accuracy on epoch 2180: 0.944\n",
      "test loss on epoch 2180: 0.346\n",
      "test accuracy on epoch 2180: 0.769\n",
      "train loss on epoch 2181 : 0.214\n",
      "train accuracy on epoch 2181: 0.944\n",
      "test loss on epoch 2181: 0.325\n",
      "test accuracy on epoch 2181: 0.769\n",
      "train loss on epoch 2182 : 0.162\n",
      "train accuracy on epoch 2182: 0.944\n",
      "test loss on epoch 2182: 0.326\n",
      "test accuracy on epoch 2182: 0.769\n",
      "train loss on epoch 2183 : 0.337\n",
      "train accuracy on epoch 2183: 0.889\n",
      "test loss on epoch 2183: 0.325\n",
      "test accuracy on epoch 2183: 0.846\n",
      "train loss on epoch 2184 : 0.167\n",
      "train accuracy on epoch 2184: 0.889\n",
      "test loss on epoch 2184: 0.328\n",
      "test accuracy on epoch 2184: 0.769\n",
      "train loss on epoch 2185 : 0.072\n",
      "train accuracy on epoch 2185: 0.944\n",
      "test loss on epoch 2185: 0.324\n",
      "test accuracy on epoch 2185: 0.769\n",
      "train loss on epoch 2186 : 0.235\n",
      "train accuracy on epoch 2186: 0.889\n",
      "test loss on epoch 2186: 0.331\n",
      "test accuracy on epoch 2186: 0.769\n",
      "train loss on epoch 2187 : 0.154\n",
      "train accuracy on epoch 2187: 0.944\n",
      "test loss on epoch 2187: 0.341\n",
      "test accuracy on epoch 2187: 0.769\n",
      "train loss on epoch 2188 : 0.376\n",
      "train accuracy on epoch 2188: 0.889\n",
      "test loss on epoch 2188: 0.334\n",
      "test accuracy on epoch 2188: 0.769\n",
      "train loss on epoch 2189 : 0.289\n",
      "train accuracy on epoch 2189: 0.833\n",
      "test loss on epoch 2189: 0.323\n",
      "test accuracy on epoch 2189: 0.846\n",
      "train loss on epoch 2190 : 0.192\n",
      "train accuracy on epoch 2190: 0.889\n",
      "test loss on epoch 2190: 0.318\n",
      "test accuracy on epoch 2190: 0.846\n",
      "train loss on epoch 2191 : 0.148\n",
      "train accuracy on epoch 2191: 0.944\n",
      "test loss on epoch 2191: 0.324\n",
      "test accuracy on epoch 2191: 0.769\n",
      "train loss on epoch 2192 : 0.206\n",
      "train accuracy on epoch 2192: 0.944\n",
      "test loss on epoch 2192: 0.316\n",
      "test accuracy on epoch 2192: 0.769\n",
      "train loss on epoch 2193 : 0.164\n",
      "train accuracy on epoch 2193: 0.944\n",
      "test loss on epoch 2193: 0.338\n",
      "test accuracy on epoch 2193: 0.769\n",
      "train loss on epoch 2194 : 0.174\n",
      "train accuracy on epoch 2194: 0.889\n",
      "test loss on epoch 2194: 0.340\n",
      "test accuracy on epoch 2194: 0.769\n",
      "train loss on epoch 2195 : 0.217\n",
      "train accuracy on epoch 2195: 0.889\n",
      "test loss on epoch 2195: 0.341\n",
      "test accuracy on epoch 2195: 0.769\n",
      "train loss on epoch 2196 : 0.161\n",
      "train accuracy on epoch 2196: 0.889\n",
      "test loss on epoch 2196: 0.333\n",
      "test accuracy on epoch 2196: 0.769\n",
      "train loss on epoch 2197 : 0.250\n",
      "train accuracy on epoch 2197: 0.889\n",
      "test loss on epoch 2197: 0.340\n",
      "test accuracy on epoch 2197: 0.769\n",
      "train loss on epoch 2198 : 0.247\n",
      "train accuracy on epoch 2198: 0.889\n",
      "test loss on epoch 2198: 0.315\n",
      "test accuracy on epoch 2198: 0.769\n",
      "train loss on epoch 2199 : 0.096\n",
      "train accuracy on epoch 2199: 1.000\n",
      "test loss on epoch 2199: 0.317\n",
      "test accuracy on epoch 2199: 0.769\n",
      "train loss on epoch 2200 : 0.079\n",
      "train accuracy on epoch 2200: 0.944\n",
      "test loss on epoch 2200: 0.336\n",
      "test accuracy on epoch 2200: 0.769\n",
      "train loss on epoch 2201 : 0.112\n",
      "train accuracy on epoch 2201: 0.944\n",
      "test loss on epoch 2201: 0.319\n",
      "test accuracy on epoch 2201: 0.846\n",
      "train loss on epoch 2202 : 0.155\n",
      "train accuracy on epoch 2202: 0.889\n",
      "test loss on epoch 2202: 0.338\n",
      "test accuracy on epoch 2202: 0.692\n",
      "train loss on epoch 2203 : 0.118\n",
      "train accuracy on epoch 2203: 0.944\n",
      "test loss on epoch 2203: 0.339\n",
      "test accuracy on epoch 2203: 0.692\n",
      "train loss on epoch 2204 : 0.421\n",
      "train accuracy on epoch 2204: 0.778\n",
      "test loss on epoch 2204: 0.341\n",
      "test accuracy on epoch 2204: 0.692\n",
      "train loss on epoch 2205 : 0.271\n",
      "train accuracy on epoch 2205: 0.944\n",
      "test loss on epoch 2205: 0.338\n",
      "test accuracy on epoch 2205: 0.692\n",
      "train loss on epoch 2206 : 0.196\n",
      "train accuracy on epoch 2206: 0.889\n",
      "test loss on epoch 2206: 0.331\n",
      "test accuracy on epoch 2206: 0.769\n",
      "train loss on epoch 2207 : 0.090\n",
      "train accuracy on epoch 2207: 1.000\n",
      "test loss on epoch 2207: 0.321\n",
      "test accuracy on epoch 2207: 0.846\n",
      "train loss on epoch 2208 : 0.106\n",
      "train accuracy on epoch 2208: 0.944\n",
      "test loss on epoch 2208: 0.320\n",
      "test accuracy on epoch 2208: 0.769\n",
      "train loss on epoch 2209 : 0.295\n",
      "train accuracy on epoch 2209: 0.944\n",
      "test loss on epoch 2209: 0.323\n",
      "test accuracy on epoch 2209: 0.769\n",
      "train loss on epoch 2210 : 0.140\n",
      "train accuracy on epoch 2210: 0.944\n",
      "test loss on epoch 2210: 0.333\n",
      "test accuracy on epoch 2210: 0.692\n",
      "train loss on epoch 2211 : 0.106\n",
      "train accuracy on epoch 2211: 0.944\n",
      "test loss on epoch 2211: 0.328\n",
      "test accuracy on epoch 2211: 0.769\n",
      "train loss on epoch 2212 : 0.092\n",
      "train accuracy on epoch 2212: 1.000\n",
      "test loss on epoch 2212: 0.332\n",
      "test accuracy on epoch 2212: 0.769\n",
      "train loss on epoch 2213 : 0.191\n",
      "train accuracy on epoch 2213: 0.889\n",
      "test loss on epoch 2213: 0.321\n",
      "test accuracy on epoch 2213: 0.769\n",
      "train loss on epoch 2214 : 0.217\n",
      "train accuracy on epoch 2214: 0.944\n",
      "test loss on epoch 2214: 0.323\n",
      "test accuracy on epoch 2214: 0.846\n",
      "train loss on epoch 2215 : 0.144\n",
      "train accuracy on epoch 2215: 0.944\n",
      "test loss on epoch 2215: 0.334\n",
      "test accuracy on epoch 2215: 0.769\n",
      "train loss on epoch 2216 : 0.203\n",
      "train accuracy on epoch 2216: 0.944\n",
      "test loss on epoch 2216: 0.327\n",
      "test accuracy on epoch 2216: 0.769\n",
      "train loss on epoch 2217 : 0.239\n",
      "train accuracy on epoch 2217: 0.833\n",
      "test loss on epoch 2217: 0.329\n",
      "test accuracy on epoch 2217: 0.769\n",
      "train loss on epoch 2218 : 0.094\n",
      "train accuracy on epoch 2218: 0.944\n",
      "test loss on epoch 2218: 0.331\n",
      "test accuracy on epoch 2218: 0.769\n",
      "train loss on epoch 2219 : 0.142\n",
      "train accuracy on epoch 2219: 1.000\n",
      "test loss on epoch 2219: 0.325\n",
      "test accuracy on epoch 2219: 0.769\n",
      "train loss on epoch 2220 : 0.371\n",
      "train accuracy on epoch 2220: 0.778\n",
      "test loss on epoch 2220: 0.327\n",
      "test accuracy on epoch 2220: 0.769\n",
      "train loss on epoch 2221 : 0.216\n",
      "train accuracy on epoch 2221: 0.889\n",
      "test loss on epoch 2221: 0.332\n",
      "test accuracy on epoch 2221: 0.769\n",
      "train loss on epoch 2222 : 0.084\n",
      "train accuracy on epoch 2222: 1.000\n",
      "test loss on epoch 2222: 0.338\n",
      "test accuracy on epoch 2222: 0.692\n",
      "train loss on epoch 2223 : 0.171\n",
      "train accuracy on epoch 2223: 0.944\n",
      "test loss on epoch 2223: 0.330\n",
      "test accuracy on epoch 2223: 0.769\n",
      "train loss on epoch 2224 : 0.141\n",
      "train accuracy on epoch 2224: 0.944\n",
      "test loss on epoch 2224: 0.320\n",
      "test accuracy on epoch 2224: 0.769\n",
      "train loss on epoch 2225 : 0.479\n",
      "train accuracy on epoch 2225: 0.833\n",
      "test loss on epoch 2225: 0.321\n",
      "test accuracy on epoch 2225: 0.769\n",
      "train loss on epoch 2226 : 0.356\n",
      "train accuracy on epoch 2226: 0.833\n",
      "test loss on epoch 2226: 0.336\n",
      "test accuracy on epoch 2226: 0.769\n",
      "train loss on epoch 2227 : 0.223\n",
      "train accuracy on epoch 2227: 0.889\n",
      "test loss on epoch 2227: 0.318\n",
      "test accuracy on epoch 2227: 0.769\n",
      "train loss on epoch 2228 : 0.147\n",
      "train accuracy on epoch 2228: 0.944\n",
      "test loss on epoch 2228: 0.320\n",
      "test accuracy on epoch 2228: 0.769\n",
      "train loss on epoch 2229 : 0.077\n",
      "train accuracy on epoch 2229: 1.000\n",
      "test loss on epoch 2229: 0.338\n",
      "test accuracy on epoch 2229: 0.769\n",
      "train loss on epoch 2230 : 0.175\n",
      "train accuracy on epoch 2230: 0.889\n",
      "test loss on epoch 2230: 0.319\n",
      "test accuracy on epoch 2230: 0.769\n",
      "train loss on epoch 2231 : 0.069\n",
      "train accuracy on epoch 2231: 1.000\n",
      "test loss on epoch 2231: 0.321\n",
      "test accuracy on epoch 2231: 0.769\n",
      "train loss on epoch 2232 : 0.398\n",
      "train accuracy on epoch 2232: 0.833\n",
      "test loss on epoch 2232: 0.337\n",
      "test accuracy on epoch 2232: 0.769\n",
      "train loss on epoch 2233 : 0.411\n",
      "train accuracy on epoch 2233: 0.889\n",
      "test loss on epoch 2233: 0.340\n",
      "test accuracy on epoch 2233: 0.769\n",
      "train loss on epoch 2234 : 0.273\n",
      "train accuracy on epoch 2234: 0.833\n",
      "test loss on epoch 2234: 0.338\n",
      "test accuracy on epoch 2234: 0.769\n",
      "train loss on epoch 2235 : 0.206\n",
      "train accuracy on epoch 2235: 0.944\n",
      "test loss on epoch 2235: 0.322\n",
      "test accuracy on epoch 2235: 0.769\n",
      "train loss on epoch 2236 : 0.190\n",
      "train accuracy on epoch 2236: 0.889\n",
      "test loss on epoch 2236: 0.328\n",
      "test accuracy on epoch 2236: 0.769\n",
      "train loss on epoch 2237 : 0.163\n",
      "train accuracy on epoch 2237: 0.889\n",
      "test loss on epoch 2237: 0.341\n",
      "test accuracy on epoch 2237: 0.769\n",
      "train loss on epoch 2238 : 0.202\n",
      "train accuracy on epoch 2238: 0.944\n",
      "test loss on epoch 2238: 0.341\n",
      "test accuracy on epoch 2238: 0.769\n",
      "train loss on epoch 2239 : 0.125\n",
      "train accuracy on epoch 2239: 0.944\n",
      "test loss on epoch 2239: 0.328\n",
      "test accuracy on epoch 2239: 0.769\n",
      "train loss on epoch 2240 : 0.308\n",
      "train accuracy on epoch 2240: 0.833\n",
      "test loss on epoch 2240: 0.340\n",
      "test accuracy on epoch 2240: 0.769\n",
      "train loss on epoch 2241 : 0.067\n",
      "train accuracy on epoch 2241: 1.000\n",
      "test loss on epoch 2241: 0.336\n",
      "test accuracy on epoch 2241: 0.769\n",
      "train loss on epoch 2242 : 0.120\n",
      "train accuracy on epoch 2242: 0.944\n",
      "test loss on epoch 2242: 0.324\n",
      "test accuracy on epoch 2242: 0.769\n",
      "train loss on epoch 2243 : 0.475\n",
      "train accuracy on epoch 2243: 0.833\n",
      "test loss on epoch 2243: 0.344\n",
      "test accuracy on epoch 2243: 0.769\n",
      "train loss on epoch 2244 : 0.293\n",
      "train accuracy on epoch 2244: 0.833\n",
      "test loss on epoch 2244: 0.345\n",
      "test accuracy on epoch 2244: 0.769\n",
      "train loss on epoch 2245 : 0.298\n",
      "train accuracy on epoch 2245: 0.889\n",
      "test loss on epoch 2245: 0.339\n",
      "test accuracy on epoch 2245: 0.769\n",
      "train loss on epoch 2246 : 0.089\n",
      "train accuracy on epoch 2246: 1.000\n",
      "test loss on epoch 2246: 0.346\n",
      "test accuracy on epoch 2246: 0.769\n",
      "train loss on epoch 2247 : 0.096\n",
      "train accuracy on epoch 2247: 1.000\n",
      "test loss on epoch 2247: 0.333\n",
      "test accuracy on epoch 2247: 0.769\n",
      "train loss on epoch 2248 : 0.115\n",
      "train accuracy on epoch 2248: 0.944\n",
      "test loss on epoch 2248: 0.343\n",
      "test accuracy on epoch 2248: 0.769\n",
      "train loss on epoch 2249 : 0.259\n",
      "train accuracy on epoch 2249: 0.944\n",
      "test loss on epoch 2249: 0.339\n",
      "test accuracy on epoch 2249: 0.769\n",
      "train loss on epoch 2250 : 0.286\n",
      "train accuracy on epoch 2250: 0.778\n",
      "test loss on epoch 2250: 0.344\n",
      "test accuracy on epoch 2250: 0.769\n",
      "train loss on epoch 2251 : 0.113\n",
      "train accuracy on epoch 2251: 0.944\n",
      "test loss on epoch 2251: 0.338\n",
      "test accuracy on epoch 2251: 0.769\n",
      "train loss on epoch 2252 : 0.163\n",
      "train accuracy on epoch 2252: 0.889\n",
      "test loss on epoch 2252: 0.323\n",
      "test accuracy on epoch 2252: 0.769\n",
      "train loss on epoch 2253 : 0.091\n",
      "train accuracy on epoch 2253: 1.000\n",
      "test loss on epoch 2253: 0.332\n",
      "test accuracy on epoch 2253: 0.769\n",
      "train loss on epoch 2254 : 0.143\n",
      "train accuracy on epoch 2254: 0.944\n",
      "test loss on epoch 2254: 0.326\n",
      "test accuracy on epoch 2254: 0.769\n",
      "train loss on epoch 2255 : 0.188\n",
      "train accuracy on epoch 2255: 0.889\n",
      "test loss on epoch 2255: 0.325\n",
      "test accuracy on epoch 2255: 0.769\n",
      "train loss on epoch 2256 : 0.085\n",
      "train accuracy on epoch 2256: 1.000\n",
      "test loss on epoch 2256: 0.349\n",
      "test accuracy on epoch 2256: 0.769\n",
      "train loss on epoch 2257 : 0.107\n",
      "train accuracy on epoch 2257: 1.000\n",
      "test loss on epoch 2257: 0.327\n",
      "test accuracy on epoch 2257: 0.769\n",
      "train loss on epoch 2258 : 0.193\n",
      "train accuracy on epoch 2258: 0.944\n",
      "test loss on epoch 2258: 0.326\n",
      "test accuracy on epoch 2258: 0.769\n",
      "train loss on epoch 2259 : 0.218\n",
      "train accuracy on epoch 2259: 0.944\n",
      "test loss on epoch 2259: 0.331\n",
      "test accuracy on epoch 2259: 0.846\n",
      "train loss on epoch 2260 : 0.110\n",
      "train accuracy on epoch 2260: 0.944\n",
      "test loss on epoch 2260: 0.343\n",
      "test accuracy on epoch 2260: 0.769\n",
      "train loss on epoch 2261 : 0.142\n",
      "train accuracy on epoch 2261: 0.944\n",
      "test loss on epoch 2261: 0.351\n",
      "test accuracy on epoch 2261: 0.692\n",
      "train loss on epoch 2262 : 0.085\n",
      "train accuracy on epoch 2262: 1.000\n",
      "test loss on epoch 2262: 0.341\n",
      "test accuracy on epoch 2262: 0.769\n",
      "train loss on epoch 2263 : 0.218\n",
      "train accuracy on epoch 2263: 0.833\n",
      "test loss on epoch 2263: 0.341\n",
      "test accuracy on epoch 2263: 0.769\n",
      "train loss on epoch 2264 : 0.119\n",
      "train accuracy on epoch 2264: 0.944\n",
      "test loss on epoch 2264: 0.327\n",
      "test accuracy on epoch 2264: 0.846\n",
      "train loss on epoch 2265 : 0.066\n",
      "train accuracy on epoch 2265: 1.000\n",
      "test loss on epoch 2265: 0.329\n",
      "test accuracy on epoch 2265: 0.846\n",
      "train loss on epoch 2266 : 0.116\n",
      "train accuracy on epoch 2266: 0.944\n",
      "test loss on epoch 2266: 0.331\n",
      "test accuracy on epoch 2266: 0.846\n",
      "train loss on epoch 2267 : 0.103\n",
      "train accuracy on epoch 2267: 0.944\n",
      "test loss on epoch 2267: 0.352\n",
      "test accuracy on epoch 2267: 0.692\n",
      "train loss on epoch 2268 : 0.308\n",
      "train accuracy on epoch 2268: 0.889\n",
      "test loss on epoch 2268: 0.353\n",
      "test accuracy on epoch 2268: 0.769\n",
      "train loss on epoch 2269 : 0.184\n",
      "train accuracy on epoch 2269: 0.944\n",
      "test loss on epoch 2269: 0.351\n",
      "test accuracy on epoch 2269: 0.769\n",
      "train loss on epoch 2270 : 0.082\n",
      "train accuracy on epoch 2270: 0.944\n",
      "test loss on epoch 2270: 0.331\n",
      "test accuracy on epoch 2270: 0.846\n",
      "train loss on epoch 2271 : 0.299\n",
      "train accuracy on epoch 2271: 0.944\n",
      "test loss on epoch 2271: 0.349\n",
      "test accuracy on epoch 2271: 0.769\n",
      "train loss on epoch 2272 : 0.270\n",
      "train accuracy on epoch 2272: 0.944\n",
      "test loss on epoch 2272: 0.343\n",
      "test accuracy on epoch 2272: 0.769\n",
      "train loss on epoch 2273 : 0.104\n",
      "train accuracy on epoch 2273: 0.944\n",
      "test loss on epoch 2273: 0.352\n",
      "test accuracy on epoch 2273: 0.769\n",
      "train loss on epoch 2274 : 0.351\n",
      "train accuracy on epoch 2274: 0.833\n",
      "test loss on epoch 2274: 0.351\n",
      "test accuracy on epoch 2274: 0.769\n",
      "train loss on epoch 2275 : 0.329\n",
      "train accuracy on epoch 2275: 0.944\n",
      "test loss on epoch 2275: 0.343\n",
      "test accuracy on epoch 2275: 0.769\n",
      "train loss on epoch 2276 : 0.090\n",
      "train accuracy on epoch 2276: 0.944\n",
      "test loss on epoch 2276: 0.329\n",
      "test accuracy on epoch 2276: 0.769\n",
      "train loss on epoch 2277 : 0.171\n",
      "train accuracy on epoch 2277: 0.944\n",
      "test loss on epoch 2277: 0.329\n",
      "test accuracy on epoch 2277: 0.769\n",
      "train loss on epoch 2278 : 0.064\n",
      "train accuracy on epoch 2278: 1.000\n",
      "test loss on epoch 2278: 0.343\n",
      "test accuracy on epoch 2278: 0.769\n",
      "train loss on epoch 2279 : 0.306\n",
      "train accuracy on epoch 2279: 0.833\n",
      "test loss on epoch 2279: 0.326\n",
      "test accuracy on epoch 2279: 0.769\n",
      "train loss on epoch 2280 : 0.375\n",
      "train accuracy on epoch 2280: 0.833\n",
      "test loss on epoch 2280: 0.327\n",
      "test accuracy on epoch 2280: 0.769\n",
      "train loss on epoch 2281 : 0.070\n",
      "train accuracy on epoch 2281: 1.000\n",
      "test loss on epoch 2281: 0.327\n",
      "test accuracy on epoch 2281: 0.769\n",
      "train loss on epoch 2282 : 0.037\n",
      "train accuracy on epoch 2282: 1.000\n",
      "test loss on epoch 2282: 0.323\n",
      "test accuracy on epoch 2282: 0.769\n",
      "train loss on epoch 2283 : 0.092\n",
      "train accuracy on epoch 2283: 0.944\n",
      "test loss on epoch 2283: 0.333\n",
      "test accuracy on epoch 2283: 0.769\n",
      "train loss on epoch 2284 : 0.193\n",
      "train accuracy on epoch 2284: 0.944\n",
      "test loss on epoch 2284: 0.341\n",
      "test accuracy on epoch 2284: 0.769\n",
      "train loss on epoch 2285 : 0.103\n",
      "train accuracy on epoch 2285: 0.944\n",
      "test loss on epoch 2285: 0.341\n",
      "test accuracy on epoch 2285: 0.769\n",
      "train loss on epoch 2286 : 0.202\n",
      "train accuracy on epoch 2286: 0.833\n",
      "test loss on epoch 2286: 0.353\n",
      "test accuracy on epoch 2286: 0.692\n",
      "train loss on epoch 2287 : 0.343\n",
      "train accuracy on epoch 2287: 0.889\n",
      "test loss on epoch 2287: 0.341\n",
      "test accuracy on epoch 2287: 0.769\n",
      "train loss on epoch 2288 : 0.211\n",
      "train accuracy on epoch 2288: 0.944\n",
      "test loss on epoch 2288: 0.353\n",
      "test accuracy on epoch 2288: 0.769\n",
      "train loss on epoch 2289 : 0.199\n",
      "train accuracy on epoch 2289: 0.889\n",
      "test loss on epoch 2289: 0.340\n",
      "test accuracy on epoch 2289: 0.769\n",
      "train loss on epoch 2290 : 0.206\n",
      "train accuracy on epoch 2290: 0.889\n",
      "test loss on epoch 2290: 0.337\n",
      "test accuracy on epoch 2290: 0.769\n",
      "train loss on epoch 2291 : 0.141\n",
      "train accuracy on epoch 2291: 0.944\n",
      "test loss on epoch 2291: 0.331\n",
      "test accuracy on epoch 2291: 0.769\n",
      "train loss on epoch 2292 : 0.098\n",
      "train accuracy on epoch 2292: 1.000\n",
      "test loss on epoch 2292: 0.322\n",
      "test accuracy on epoch 2292: 0.769\n",
      "train loss on epoch 2293 : 0.220\n",
      "train accuracy on epoch 2293: 0.944\n",
      "test loss on epoch 2293: 0.341\n",
      "test accuracy on epoch 2293: 0.692\n",
      "train loss on epoch 2294 : 0.155\n",
      "train accuracy on epoch 2294: 0.944\n",
      "test loss on epoch 2294: 0.332\n",
      "test accuracy on epoch 2294: 0.769\n",
      "train loss on epoch 2295 : 0.065\n",
      "train accuracy on epoch 2295: 1.000\n",
      "test loss on epoch 2295: 0.320\n",
      "test accuracy on epoch 2295: 0.846\n",
      "train loss on epoch 2296 : 0.097\n",
      "train accuracy on epoch 2296: 0.944\n",
      "test loss on epoch 2296: 0.321\n",
      "test accuracy on epoch 2296: 0.846\n",
      "train loss on epoch 2297 : 0.131\n",
      "train accuracy on epoch 2297: 0.889\n",
      "test loss on epoch 2297: 0.329\n",
      "test accuracy on epoch 2297: 0.769\n",
      "train loss on epoch 2298 : 0.215\n",
      "train accuracy on epoch 2298: 0.944\n",
      "test loss on epoch 2298: 0.338\n",
      "test accuracy on epoch 2298: 0.692\n",
      "train loss on epoch 2299 : 0.232\n",
      "train accuracy on epoch 2299: 0.889\n",
      "test loss on epoch 2299: 0.317\n",
      "test accuracy on epoch 2299: 0.846\n",
      "train loss on epoch 2300 : 0.232\n",
      "train accuracy on epoch 2300: 0.833\n",
      "test loss on epoch 2300: 0.325\n",
      "test accuracy on epoch 2300: 0.769\n",
      "train loss on epoch 2301 : 0.071\n",
      "train accuracy on epoch 2301: 0.944\n",
      "test loss on epoch 2301: 0.326\n",
      "test accuracy on epoch 2301: 0.769\n",
      "train loss on epoch 2302 : 0.173\n",
      "train accuracy on epoch 2302: 0.944\n",
      "test loss on epoch 2302: 0.322\n",
      "test accuracy on epoch 2302: 0.846\n",
      "train loss on epoch 2303 : 0.242\n",
      "train accuracy on epoch 2303: 0.889\n",
      "test loss on epoch 2303: 0.331\n",
      "test accuracy on epoch 2303: 0.769\n",
      "train loss on epoch 2304 : 0.214\n",
      "train accuracy on epoch 2304: 0.833\n",
      "test loss on epoch 2304: 0.323\n",
      "test accuracy on epoch 2304: 0.846\n",
      "train loss on epoch 2305 : 0.432\n",
      "train accuracy on epoch 2305: 0.833\n",
      "test loss on epoch 2305: 0.324\n",
      "test accuracy on epoch 2305: 0.846\n",
      "train loss on epoch 2306 : 0.231\n",
      "train accuracy on epoch 2306: 0.944\n",
      "test loss on epoch 2306: 0.335\n",
      "test accuracy on epoch 2306: 0.769\n",
      "train loss on epoch 2307 : 0.099\n",
      "train accuracy on epoch 2307: 0.889\n",
      "test loss on epoch 2307: 0.348\n",
      "test accuracy on epoch 2307: 0.769\n",
      "train loss on epoch 2308 : 0.166\n",
      "train accuracy on epoch 2308: 0.889\n",
      "test loss on epoch 2308: 0.335\n",
      "test accuracy on epoch 2308: 0.769\n",
      "train loss on epoch 2309 : 0.138\n",
      "train accuracy on epoch 2309: 0.944\n",
      "test loss on epoch 2309: 0.343\n",
      "test accuracy on epoch 2309: 0.769\n",
      "train loss on epoch 2310 : 0.228\n",
      "train accuracy on epoch 2310: 0.944\n",
      "test loss on epoch 2310: 0.347\n",
      "test accuracy on epoch 2310: 0.769\n",
      "train loss on epoch 2311 : 0.425\n",
      "train accuracy on epoch 2311: 0.889\n",
      "test loss on epoch 2311: 0.346\n",
      "test accuracy on epoch 2311: 0.769\n",
      "train loss on epoch 2312 : 0.091\n",
      "train accuracy on epoch 2312: 1.000\n",
      "test loss on epoch 2312: 0.345\n",
      "test accuracy on epoch 2312: 0.769\n",
      "train loss on epoch 2313 : 0.307\n",
      "train accuracy on epoch 2313: 0.889\n",
      "test loss on epoch 2313: 0.345\n",
      "test accuracy on epoch 2313: 0.769\n",
      "train loss on epoch 2314 : 0.106\n",
      "train accuracy on epoch 2314: 1.000\n",
      "test loss on epoch 2314: 0.346\n",
      "test accuracy on epoch 2314: 0.769\n",
      "train loss on epoch 2315 : 0.218\n",
      "train accuracy on epoch 2315: 0.889\n",
      "test loss on epoch 2315: 0.337\n",
      "test accuracy on epoch 2315: 0.769\n",
      "train loss on epoch 2316 : 0.365\n",
      "train accuracy on epoch 2316: 0.833\n",
      "test loss on epoch 2316: 0.322\n",
      "test accuracy on epoch 2316: 0.846\n",
      "train loss on epoch 2317 : 0.154\n",
      "train accuracy on epoch 2317: 0.944\n",
      "test loss on epoch 2317: 0.332\n",
      "test accuracy on epoch 2317: 0.769\n",
      "train loss on epoch 2318 : 0.462\n",
      "train accuracy on epoch 2318: 0.889\n",
      "test loss on epoch 2318: 0.316\n",
      "test accuracy on epoch 2318: 0.769\n",
      "train loss on epoch 2319 : 0.128\n",
      "train accuracy on epoch 2319: 0.889\n",
      "test loss on epoch 2319: 0.316\n",
      "test accuracy on epoch 2319: 0.769\n",
      "train loss on epoch 2320 : 0.197\n",
      "train accuracy on epoch 2320: 0.944\n",
      "test loss on epoch 2320: 0.322\n",
      "test accuracy on epoch 2320: 0.769\n",
      "train loss on epoch 2321 : 0.178\n",
      "train accuracy on epoch 2321: 0.944\n",
      "test loss on epoch 2321: 0.316\n",
      "test accuracy on epoch 2321: 0.846\n",
      "train loss on epoch 2322 : 0.214\n",
      "train accuracy on epoch 2322: 0.889\n",
      "test loss on epoch 2322: 0.325\n",
      "test accuracy on epoch 2322: 0.769\n",
      "train loss on epoch 2323 : 0.290\n",
      "train accuracy on epoch 2323: 0.889\n",
      "test loss on epoch 2323: 0.331\n",
      "test accuracy on epoch 2323: 0.769\n",
      "train loss on epoch 2324 : 0.105\n",
      "train accuracy on epoch 2324: 0.944\n",
      "test loss on epoch 2324: 0.336\n",
      "test accuracy on epoch 2324: 0.769\n",
      "train loss on epoch 2325 : 0.260\n",
      "train accuracy on epoch 2325: 0.889\n",
      "test loss on epoch 2325: 0.343\n",
      "test accuracy on epoch 2325: 0.769\n",
      "train loss on epoch 2326 : 0.164\n",
      "train accuracy on epoch 2326: 0.889\n",
      "test loss on epoch 2326: 0.344\n",
      "test accuracy on epoch 2326: 0.769\n",
      "train loss on epoch 2327 : 0.357\n",
      "train accuracy on epoch 2327: 0.889\n",
      "test loss on epoch 2327: 0.341\n",
      "test accuracy on epoch 2327: 0.769\n",
      "train loss on epoch 2328 : 0.256\n",
      "train accuracy on epoch 2328: 0.889\n",
      "test loss on epoch 2328: 0.340\n",
      "test accuracy on epoch 2328: 0.769\n",
      "train loss on epoch 2329 : 0.047\n",
      "train accuracy on epoch 2329: 1.000\n",
      "test loss on epoch 2329: 0.339\n",
      "test accuracy on epoch 2329: 0.769\n",
      "train loss on epoch 2330 : 0.080\n",
      "train accuracy on epoch 2330: 1.000\n",
      "test loss on epoch 2330: 0.334\n",
      "test accuracy on epoch 2330: 0.769\n",
      "train loss on epoch 2331 : 0.168\n",
      "train accuracy on epoch 2331: 0.889\n",
      "test loss on epoch 2331: 0.331\n",
      "test accuracy on epoch 2331: 0.769\n",
      "train loss on epoch 2332 : 0.065\n",
      "train accuracy on epoch 2332: 1.000\n",
      "test loss on epoch 2332: 0.326\n",
      "test accuracy on epoch 2332: 0.769\n",
      "train loss on epoch 2333 : 0.215\n",
      "train accuracy on epoch 2333: 0.889\n",
      "test loss on epoch 2333: 0.329\n",
      "test accuracy on epoch 2333: 0.769\n",
      "train loss on epoch 2334 : 0.392\n",
      "train accuracy on epoch 2334: 0.889\n",
      "test loss on epoch 2334: 0.331\n",
      "test accuracy on epoch 2334: 0.769\n",
      "train loss on epoch 2335 : 0.259\n",
      "train accuracy on epoch 2335: 0.944\n",
      "test loss on epoch 2335: 0.332\n",
      "test accuracy on epoch 2335: 0.769\n",
      "train loss on epoch 2336 : 0.267\n",
      "train accuracy on epoch 2336: 0.833\n",
      "test loss on epoch 2336: 0.337\n",
      "test accuracy on epoch 2336: 0.769\n",
      "train loss on epoch 2337 : 0.076\n",
      "train accuracy on epoch 2337: 1.000\n",
      "test loss on epoch 2337: 0.341\n",
      "test accuracy on epoch 2337: 0.769\n",
      "train loss on epoch 2338 : 0.287\n",
      "train accuracy on epoch 2338: 0.944\n",
      "test loss on epoch 2338: 0.338\n",
      "test accuracy on epoch 2338: 0.769\n",
      "train loss on epoch 2339 : 0.116\n",
      "train accuracy on epoch 2339: 0.944\n",
      "test loss on epoch 2339: 0.336\n",
      "test accuracy on epoch 2339: 0.769\n",
      "train loss on epoch 2340 : 0.108\n",
      "train accuracy on epoch 2340: 0.944\n",
      "test loss on epoch 2340: 0.334\n",
      "test accuracy on epoch 2340: 0.769\n",
      "train loss on epoch 2341 : 0.093\n",
      "train accuracy on epoch 2341: 1.000\n",
      "test loss on epoch 2341: 0.336\n",
      "test accuracy on epoch 2341: 0.769\n",
      "train loss on epoch 2342 : 0.351\n",
      "train accuracy on epoch 2342: 0.944\n",
      "test loss on epoch 2342: 0.335\n",
      "test accuracy on epoch 2342: 0.769\n",
      "train loss on epoch 2343 : 0.096\n",
      "train accuracy on epoch 2343: 1.000\n",
      "test loss on epoch 2343: 0.339\n",
      "test accuracy on epoch 2343: 0.769\n",
      "train loss on epoch 2344 : 0.266\n",
      "train accuracy on epoch 2344: 0.944\n",
      "test loss on epoch 2344: 0.340\n",
      "test accuracy on epoch 2344: 0.769\n",
      "train loss on epoch 2345 : 0.274\n",
      "train accuracy on epoch 2345: 0.889\n",
      "test loss on epoch 2345: 0.332\n",
      "test accuracy on epoch 2345: 0.769\n",
      "train loss on epoch 2346 : 0.123\n",
      "train accuracy on epoch 2346: 0.944\n",
      "test loss on epoch 2346: 0.332\n",
      "test accuracy on epoch 2346: 0.769\n",
      "train loss on epoch 2347 : 0.198\n",
      "train accuracy on epoch 2347: 0.889\n",
      "test loss on epoch 2347: 0.334\n",
      "test accuracy on epoch 2347: 0.769\n",
      "train loss on epoch 2348 : 0.355\n",
      "train accuracy on epoch 2348: 0.889\n",
      "test loss on epoch 2348: 0.333\n",
      "test accuracy on epoch 2348: 0.769\n",
      "train loss on epoch 2349 : 0.326\n",
      "train accuracy on epoch 2349: 0.889\n",
      "test loss on epoch 2349: 0.336\n",
      "test accuracy on epoch 2349: 0.769\n",
      "train loss on epoch 2350 : 0.271\n",
      "train accuracy on epoch 2350: 0.889\n",
      "test loss on epoch 2350: 0.334\n",
      "test accuracy on epoch 2350: 0.769\n",
      "train loss on epoch 2351 : 0.315\n",
      "train accuracy on epoch 2351: 0.833\n",
      "test loss on epoch 2351: 0.341\n",
      "test accuracy on epoch 2351: 0.769\n",
      "train loss on epoch 2352 : 0.133\n",
      "train accuracy on epoch 2352: 0.889\n",
      "test loss on epoch 2352: 0.333\n",
      "test accuracy on epoch 2352: 0.769\n",
      "train loss on epoch 2353 : 0.210\n",
      "train accuracy on epoch 2353: 0.889\n",
      "test loss on epoch 2353: 0.331\n",
      "test accuracy on epoch 2353: 0.769\n",
      "train loss on epoch 2354 : 0.122\n",
      "train accuracy on epoch 2354: 0.944\n",
      "test loss on epoch 2354: 0.331\n",
      "test accuracy on epoch 2354: 0.769\n",
      "train loss on epoch 2355 : 0.116\n",
      "train accuracy on epoch 2355: 0.944\n",
      "test loss on epoch 2355: 0.337\n",
      "test accuracy on epoch 2355: 0.769\n",
      "train loss on epoch 2356 : 0.064\n",
      "train accuracy on epoch 2356: 1.000\n",
      "test loss on epoch 2356: 0.332\n",
      "test accuracy on epoch 2356: 0.769\n",
      "train loss on epoch 2357 : 0.206\n",
      "train accuracy on epoch 2357: 0.889\n",
      "test loss on epoch 2357: 0.337\n",
      "test accuracy on epoch 2357: 0.769\n",
      "train loss on epoch 2358 : 0.094\n",
      "train accuracy on epoch 2358: 0.944\n",
      "test loss on epoch 2358: 0.331\n",
      "test accuracy on epoch 2358: 0.769\n",
      "train loss on epoch 2359 : 0.195\n",
      "train accuracy on epoch 2359: 0.944\n",
      "test loss on epoch 2359: 0.334\n",
      "test accuracy on epoch 2359: 0.769\n",
      "train loss on epoch 2360 : 0.236\n",
      "train accuracy on epoch 2360: 0.833\n",
      "test loss on epoch 2360: 0.336\n",
      "test accuracy on epoch 2360: 0.769\n",
      "train loss on epoch 2361 : 0.178\n",
      "train accuracy on epoch 2361: 0.889\n",
      "test loss on epoch 2361: 0.340\n",
      "test accuracy on epoch 2361: 0.769\n",
      "train loss on epoch 2362 : 0.199\n",
      "train accuracy on epoch 2362: 0.944\n",
      "test loss on epoch 2362: 0.335\n",
      "test accuracy on epoch 2362: 0.769\n",
      "train loss on epoch 2363 : 0.213\n",
      "train accuracy on epoch 2363: 0.944\n",
      "test loss on epoch 2363: 0.333\n",
      "test accuracy on epoch 2363: 0.769\n",
      "train loss on epoch 2364 : 0.506\n",
      "train accuracy on epoch 2364: 0.889\n",
      "test loss on epoch 2364: 0.332\n",
      "test accuracy on epoch 2364: 0.769\n",
      "train loss on epoch 2365 : 0.268\n",
      "train accuracy on epoch 2365: 0.944\n",
      "test loss on epoch 2365: 0.327\n",
      "test accuracy on epoch 2365: 0.769\n",
      "train loss on epoch 2366 : 0.101\n",
      "train accuracy on epoch 2366: 0.944\n",
      "test loss on epoch 2366: 0.331\n",
      "test accuracy on epoch 2366: 0.769\n",
      "train loss on epoch 2367 : 0.078\n",
      "train accuracy on epoch 2367: 1.000\n",
      "test loss on epoch 2367: 0.324\n",
      "test accuracy on epoch 2367: 0.769\n",
      "train loss on epoch 2368 : 0.163\n",
      "train accuracy on epoch 2368: 0.944\n",
      "test loss on epoch 2368: 0.323\n",
      "test accuracy on epoch 2368: 0.769\n",
      "train loss on epoch 2369 : 0.068\n",
      "train accuracy on epoch 2369: 1.000\n",
      "test loss on epoch 2369: 0.329\n",
      "test accuracy on epoch 2369: 0.769\n",
      "train loss on epoch 2370 : 0.108\n",
      "train accuracy on epoch 2370: 1.000\n",
      "test loss on epoch 2370: 0.322\n",
      "test accuracy on epoch 2370: 0.769\n",
      "train loss on epoch 2371 : 0.200\n",
      "train accuracy on epoch 2371: 0.889\n",
      "test loss on epoch 2371: 0.328\n",
      "test accuracy on epoch 2371: 0.769\n",
      "train loss on epoch 2372 : 0.307\n",
      "train accuracy on epoch 2372: 0.833\n",
      "test loss on epoch 2372: 0.329\n",
      "test accuracy on epoch 2372: 0.769\n",
      "train loss on epoch 2373 : 0.154\n",
      "train accuracy on epoch 2373: 0.944\n",
      "test loss on epoch 2373: 0.338\n",
      "test accuracy on epoch 2373: 0.769\n",
      "train loss on epoch 2374 : 0.224\n",
      "train accuracy on epoch 2374: 0.944\n",
      "test loss on epoch 2374: 0.343\n",
      "test accuracy on epoch 2374: 0.769\n",
      "train loss on epoch 2375 : 0.169\n",
      "train accuracy on epoch 2375: 0.944\n",
      "test loss on epoch 2375: 0.347\n",
      "test accuracy on epoch 2375: 0.769\n",
      "train loss on epoch 2376 : 0.183\n",
      "train accuracy on epoch 2376: 0.889\n",
      "test loss on epoch 2376: 0.353\n",
      "test accuracy on epoch 2376: 0.769\n",
      "train loss on epoch 2377 : 0.073\n",
      "train accuracy on epoch 2377: 1.000\n",
      "test loss on epoch 2377: 0.349\n",
      "test accuracy on epoch 2377: 0.769\n",
      "train loss on epoch 2378 : 0.296\n",
      "train accuracy on epoch 2378: 0.889\n",
      "test loss on epoch 2378: 0.346\n",
      "test accuracy on epoch 2378: 0.769\n",
      "train loss on epoch 2379 : 0.163\n",
      "train accuracy on epoch 2379: 0.889\n",
      "test loss on epoch 2379: 0.355\n",
      "test accuracy on epoch 2379: 0.769\n",
      "train loss on epoch 2380 : 0.219\n",
      "train accuracy on epoch 2380: 0.889\n",
      "test loss on epoch 2380: 0.351\n",
      "test accuracy on epoch 2380: 0.769\n",
      "train loss on epoch 2381 : 0.218\n",
      "train accuracy on epoch 2381: 0.944\n",
      "test loss on epoch 2381: 0.357\n",
      "test accuracy on epoch 2381: 0.769\n",
      "train loss on epoch 2382 : 0.150\n",
      "train accuracy on epoch 2382: 0.944\n",
      "test loss on epoch 2382: 0.351\n",
      "test accuracy on epoch 2382: 0.769\n",
      "train loss on epoch 2383 : 0.128\n",
      "train accuracy on epoch 2383: 0.944\n",
      "test loss on epoch 2383: 0.354\n",
      "test accuracy on epoch 2383: 0.769\n",
      "train loss on epoch 2384 : 0.154\n",
      "train accuracy on epoch 2384: 0.889\n",
      "test loss on epoch 2384: 0.354\n",
      "test accuracy on epoch 2384: 0.769\n",
      "train loss on epoch 2385 : 0.187\n",
      "train accuracy on epoch 2385: 0.944\n",
      "test loss on epoch 2385: 0.365\n",
      "test accuracy on epoch 2385: 0.769\n",
      "train loss on epoch 2386 : 0.354\n",
      "train accuracy on epoch 2386: 0.889\n",
      "test loss on epoch 2386: 0.361\n",
      "test accuracy on epoch 2386: 0.769\n",
      "train loss on epoch 2387 : 0.165\n",
      "train accuracy on epoch 2387: 0.944\n",
      "test loss on epoch 2387: 0.357\n",
      "test accuracy on epoch 2387: 0.769\n",
      "train loss on epoch 2388 : 0.088\n",
      "train accuracy on epoch 2388: 0.944\n",
      "test loss on epoch 2388: 0.349\n",
      "test accuracy on epoch 2388: 0.769\n",
      "train loss on epoch 2389 : 0.128\n",
      "train accuracy on epoch 2389: 0.889\n",
      "test loss on epoch 2389: 0.349\n",
      "test accuracy on epoch 2389: 0.769\n",
      "train loss on epoch 2390 : 0.210\n",
      "train accuracy on epoch 2390: 0.889\n",
      "test loss on epoch 2390: 0.352\n",
      "test accuracy on epoch 2390: 0.769\n",
      "train loss on epoch 2391 : 0.288\n",
      "train accuracy on epoch 2391: 0.889\n",
      "test loss on epoch 2391: 0.347\n",
      "test accuracy on epoch 2391: 0.769\n",
      "train loss on epoch 2392 : 0.218\n",
      "train accuracy on epoch 2392: 0.944\n",
      "test loss on epoch 2392: 0.347\n",
      "test accuracy on epoch 2392: 0.769\n",
      "train loss on epoch 2393 : 0.306\n",
      "train accuracy on epoch 2393: 0.778\n",
      "test loss on epoch 2393: 0.347\n",
      "test accuracy on epoch 2393: 0.769\n",
      "train loss on epoch 2394 : 0.194\n",
      "train accuracy on epoch 2394: 0.889\n",
      "test loss on epoch 2394: 0.347\n",
      "test accuracy on epoch 2394: 0.769\n",
      "train loss on epoch 2395 : 0.175\n",
      "train accuracy on epoch 2395: 0.944\n",
      "test loss on epoch 2395: 0.356\n",
      "test accuracy on epoch 2395: 0.769\n",
      "train loss on epoch 2396 : 0.069\n",
      "train accuracy on epoch 2396: 1.000\n",
      "test loss on epoch 2396: 0.358\n",
      "test accuracy on epoch 2396: 0.769\n",
      "train loss on epoch 2397 : 0.086\n",
      "train accuracy on epoch 2397: 0.944\n",
      "test loss on epoch 2397: 0.359\n",
      "test accuracy on epoch 2397: 0.769\n",
      "train loss on epoch 2398 : 0.111\n",
      "train accuracy on epoch 2398: 0.944\n",
      "test loss on epoch 2398: 0.365\n",
      "test accuracy on epoch 2398: 0.769\n",
      "train loss on epoch 2399 : 0.387\n",
      "train accuracy on epoch 2399: 0.889\n",
      "test loss on epoch 2399: 0.357\n",
      "test accuracy on epoch 2399: 0.769\n",
      "train loss on epoch 2400 : 0.281\n",
      "train accuracy on epoch 2400: 0.889\n",
      "test loss on epoch 2400: 0.348\n",
      "test accuracy on epoch 2400: 0.769\n",
      "train loss on epoch 2401 : 0.050\n",
      "train accuracy on epoch 2401: 1.000\n",
      "test loss on epoch 2401: 0.347\n",
      "test accuracy on epoch 2401: 0.769\n",
      "train loss on epoch 2402 : 0.148\n",
      "train accuracy on epoch 2402: 0.944\n",
      "test loss on epoch 2402: 0.345\n",
      "test accuracy on epoch 2402: 0.769\n",
      "train loss on epoch 2403 : 0.105\n",
      "train accuracy on epoch 2403: 0.944\n",
      "test loss on epoch 2403: 0.345\n",
      "test accuracy on epoch 2403: 0.769\n",
      "train loss on epoch 2404 : 0.158\n",
      "train accuracy on epoch 2404: 0.889\n",
      "test loss on epoch 2404: 0.337\n",
      "test accuracy on epoch 2404: 0.769\n",
      "train loss on epoch 2405 : 0.368\n",
      "train accuracy on epoch 2405: 0.889\n",
      "test loss on epoch 2405: 0.342\n",
      "test accuracy on epoch 2405: 0.769\n",
      "train loss on epoch 2406 : 0.134\n",
      "train accuracy on epoch 2406: 0.944\n",
      "test loss on epoch 2406: 0.337\n",
      "test accuracy on epoch 2406: 0.769\n",
      "train loss on epoch 2407 : 0.173\n",
      "train accuracy on epoch 2407: 0.889\n",
      "test loss on epoch 2407: 0.338\n",
      "test accuracy on epoch 2407: 0.769\n",
      "train loss on epoch 2408 : 0.211\n",
      "train accuracy on epoch 2408: 0.944\n",
      "test loss on epoch 2408: 0.331\n",
      "test accuracy on epoch 2408: 0.769\n",
      "train loss on epoch 2409 : 0.157\n",
      "train accuracy on epoch 2409: 0.944\n",
      "test loss on epoch 2409: 0.327\n",
      "test accuracy on epoch 2409: 0.769\n",
      "train loss on epoch 2410 : 0.034\n",
      "train accuracy on epoch 2410: 1.000\n",
      "test loss on epoch 2410: 0.324\n",
      "test accuracy on epoch 2410: 0.769\n",
      "train loss on epoch 2411 : 0.189\n",
      "train accuracy on epoch 2411: 0.889\n",
      "test loss on epoch 2411: 0.324\n",
      "test accuracy on epoch 2411: 0.769\n",
      "train loss on epoch 2412 : 0.204\n",
      "train accuracy on epoch 2412: 0.889\n",
      "test loss on epoch 2412: 0.324\n",
      "test accuracy on epoch 2412: 0.769\n",
      "train loss on epoch 2413 : 0.136\n",
      "train accuracy on epoch 2413: 0.944\n",
      "test loss on epoch 2413: 0.321\n",
      "test accuracy on epoch 2413: 0.769\n",
      "train loss on epoch 2414 : 0.142\n",
      "train accuracy on epoch 2414: 0.944\n",
      "test loss on epoch 2414: 0.326\n",
      "test accuracy on epoch 2414: 0.769\n",
      "train loss on epoch 2415 : 0.383\n",
      "train accuracy on epoch 2415: 0.889\n",
      "test loss on epoch 2415: 0.334\n",
      "test accuracy on epoch 2415: 0.769\n",
      "train loss on epoch 2416 : 0.190\n",
      "train accuracy on epoch 2416: 0.944\n",
      "test loss on epoch 2416: 0.337\n",
      "test accuracy on epoch 2416: 0.769\n",
      "train loss on epoch 2417 : 0.247\n",
      "train accuracy on epoch 2417: 0.889\n",
      "test loss on epoch 2417: 0.329\n",
      "test accuracy on epoch 2417: 0.769\n",
      "train loss on epoch 2418 : 0.101\n",
      "train accuracy on epoch 2418: 0.889\n",
      "test loss on epoch 2418: 0.331\n",
      "test accuracy on epoch 2418: 0.769\n",
      "train loss on epoch 2419 : 0.157\n",
      "train accuracy on epoch 2419: 0.944\n",
      "test loss on epoch 2419: 0.328\n",
      "test accuracy on epoch 2419: 0.769\n",
      "train loss on epoch 2420 : 0.221\n",
      "train accuracy on epoch 2420: 0.944\n",
      "test loss on epoch 2420: 0.337\n",
      "test accuracy on epoch 2420: 0.769\n",
      "train loss on epoch 2421 : 0.129\n",
      "train accuracy on epoch 2421: 0.944\n",
      "test loss on epoch 2421: 0.343\n",
      "test accuracy on epoch 2421: 0.769\n",
      "train loss on epoch 2422 : 0.093\n",
      "train accuracy on epoch 2422: 1.000\n",
      "test loss on epoch 2422: 0.336\n",
      "test accuracy on epoch 2422: 0.769\n",
      "train loss on epoch 2423 : 0.106\n",
      "train accuracy on epoch 2423: 1.000\n",
      "test loss on epoch 2423: 0.341\n",
      "test accuracy on epoch 2423: 0.769\n",
      "train loss on epoch 2424 : 0.078\n",
      "train accuracy on epoch 2424: 0.944\n",
      "test loss on epoch 2424: 0.334\n",
      "test accuracy on epoch 2424: 0.769\n",
      "train loss on epoch 2425 : 0.208\n",
      "train accuracy on epoch 2425: 0.889\n",
      "test loss on epoch 2425: 0.339\n",
      "test accuracy on epoch 2425: 0.769\n",
      "train loss on epoch 2426 : 0.175\n",
      "train accuracy on epoch 2426: 0.944\n",
      "test loss on epoch 2426: 0.337\n",
      "test accuracy on epoch 2426: 0.769\n",
      "train loss on epoch 2427 : 0.139\n",
      "train accuracy on epoch 2427: 0.889\n",
      "test loss on epoch 2427: 0.331\n",
      "test accuracy on epoch 2427: 0.769\n",
      "train loss on epoch 2428 : 0.303\n",
      "train accuracy on epoch 2428: 0.889\n",
      "test loss on epoch 2428: 0.335\n",
      "test accuracy on epoch 2428: 0.769\n",
      "train loss on epoch 2429 : 0.107\n",
      "train accuracy on epoch 2429: 0.944\n",
      "test loss on epoch 2429: 0.339\n",
      "test accuracy on epoch 2429: 0.769\n",
      "train loss on epoch 2430 : 0.238\n",
      "train accuracy on epoch 2430: 0.944\n",
      "test loss on epoch 2430: 0.322\n",
      "test accuracy on epoch 2430: 0.846\n",
      "train loss on epoch 2431 : 0.199\n",
      "train accuracy on epoch 2431: 0.889\n",
      "test loss on epoch 2431: 0.337\n",
      "test accuracy on epoch 2431: 0.769\n",
      "train loss on epoch 2432 : 0.164\n",
      "train accuracy on epoch 2432: 0.944\n",
      "test loss on epoch 2432: 0.337\n",
      "test accuracy on epoch 2432: 0.692\n",
      "train loss on epoch 2433 : 0.116\n",
      "train accuracy on epoch 2433: 0.944\n",
      "test loss on epoch 2433: 0.339\n",
      "test accuracy on epoch 2433: 0.692\n",
      "train loss on epoch 2434 : 0.315\n",
      "train accuracy on epoch 2434: 0.889\n",
      "test loss on epoch 2434: 0.332\n",
      "test accuracy on epoch 2434: 0.769\n",
      "train loss on epoch 2435 : 0.132\n",
      "train accuracy on epoch 2435: 0.944\n",
      "test loss on epoch 2435: 0.328\n",
      "test accuracy on epoch 2435: 0.769\n",
      "train loss on epoch 2436 : 0.342\n",
      "train accuracy on epoch 2436: 0.889\n",
      "test loss on epoch 2436: 0.332\n",
      "test accuracy on epoch 2436: 0.769\n",
      "train loss on epoch 2437 : 0.255\n",
      "train accuracy on epoch 2437: 0.889\n",
      "test loss on epoch 2437: 0.334\n",
      "test accuracy on epoch 2437: 0.769\n",
      "train loss on epoch 2438 : 0.189\n",
      "train accuracy on epoch 2438: 0.944\n",
      "test loss on epoch 2438: 0.329\n",
      "test accuracy on epoch 2438: 0.769\n",
      "train loss on epoch 2439 : 0.119\n",
      "train accuracy on epoch 2439: 0.944\n",
      "test loss on epoch 2439: 0.346\n",
      "test accuracy on epoch 2439: 0.769\n",
      "train loss on epoch 2440 : 0.336\n",
      "train accuracy on epoch 2440: 0.944\n",
      "test loss on epoch 2440: 0.323\n",
      "test accuracy on epoch 2440: 0.769\n",
      "train loss on epoch 2441 : 0.103\n",
      "train accuracy on epoch 2441: 1.000\n",
      "test loss on epoch 2441: 0.338\n",
      "test accuracy on epoch 2441: 0.769\n",
      "train loss on epoch 2442 : 0.111\n",
      "train accuracy on epoch 2442: 0.944\n",
      "test loss on epoch 2442: 0.344\n",
      "test accuracy on epoch 2442: 0.769\n",
      "train loss on epoch 2443 : 0.167\n",
      "train accuracy on epoch 2443: 0.889\n",
      "test loss on epoch 2443: 0.324\n",
      "test accuracy on epoch 2443: 0.769\n",
      "train loss on epoch 2444 : 0.057\n",
      "train accuracy on epoch 2444: 1.000\n",
      "test loss on epoch 2444: 0.324\n",
      "test accuracy on epoch 2444: 0.769\n",
      "train loss on epoch 2445 : 0.264\n",
      "train accuracy on epoch 2445: 0.944\n",
      "test loss on epoch 2445: 0.322\n",
      "test accuracy on epoch 2445: 0.769\n",
      "train loss on epoch 2446 : 0.107\n",
      "train accuracy on epoch 2446: 1.000\n",
      "test loss on epoch 2446: 0.344\n",
      "test accuracy on epoch 2446: 0.769\n",
      "train loss on epoch 2447 : 0.331\n",
      "train accuracy on epoch 2447: 0.833\n",
      "test loss on epoch 2447: 0.325\n",
      "test accuracy on epoch 2447: 0.769\n",
      "train loss on epoch 2448 : 0.159\n",
      "train accuracy on epoch 2448: 0.889\n",
      "test loss on epoch 2448: 0.323\n",
      "test accuracy on epoch 2448: 0.769\n",
      "train loss on epoch 2449 : 0.236\n",
      "train accuracy on epoch 2449: 0.833\n",
      "test loss on epoch 2449: 0.341\n",
      "test accuracy on epoch 2449: 0.769\n",
      "train loss on epoch 2450 : 0.121\n",
      "train accuracy on epoch 2450: 0.944\n",
      "test loss on epoch 2450: 0.336\n",
      "test accuracy on epoch 2450: 0.769\n",
      "train loss on epoch 2451 : 0.183\n",
      "train accuracy on epoch 2451: 0.944\n",
      "test loss on epoch 2451: 0.336\n",
      "test accuracy on epoch 2451: 0.769\n",
      "train loss on epoch 2452 : 0.083\n",
      "train accuracy on epoch 2452: 0.944\n",
      "test loss on epoch 2452: 0.339\n",
      "test accuracy on epoch 2452: 0.692\n",
      "train loss on epoch 2453 : 0.288\n",
      "train accuracy on epoch 2453: 0.833\n",
      "test loss on epoch 2453: 0.343\n",
      "test accuracy on epoch 2453: 0.692\n",
      "train loss on epoch 2454 : 0.195\n",
      "train accuracy on epoch 2454: 0.944\n",
      "test loss on epoch 2454: 0.335\n",
      "test accuracy on epoch 2454: 0.769\n",
      "train loss on epoch 2455 : 0.133\n",
      "train accuracy on epoch 2455: 0.944\n",
      "test loss on epoch 2455: 0.324\n",
      "test accuracy on epoch 2455: 0.769\n",
      "train loss on epoch 2456 : 0.096\n",
      "train accuracy on epoch 2456: 1.000\n",
      "test loss on epoch 2456: 0.332\n",
      "test accuracy on epoch 2456: 0.769\n",
      "train loss on epoch 2457 : 0.097\n",
      "train accuracy on epoch 2457: 0.944\n",
      "test loss on epoch 2457: 0.342\n",
      "test accuracy on epoch 2457: 0.769\n",
      "train loss on epoch 2458 : 0.119\n",
      "train accuracy on epoch 2458: 0.944\n",
      "test loss on epoch 2458: 0.331\n",
      "test accuracy on epoch 2458: 0.769\n",
      "train loss on epoch 2459 : 0.088\n",
      "train accuracy on epoch 2459: 1.000\n",
      "test loss on epoch 2459: 0.346\n",
      "test accuracy on epoch 2459: 0.769\n",
      "train loss on epoch 2460 : 0.227\n",
      "train accuracy on epoch 2460: 0.889\n",
      "test loss on epoch 2460: 0.347\n",
      "test accuracy on epoch 2460: 0.769\n",
      "train loss on epoch 2461 : 0.108\n",
      "train accuracy on epoch 2461: 1.000\n",
      "test loss on epoch 2461: 0.334\n",
      "test accuracy on epoch 2461: 0.769\n",
      "train loss on epoch 2462 : 0.252\n",
      "train accuracy on epoch 2462: 0.833\n",
      "test loss on epoch 2462: 0.335\n",
      "test accuracy on epoch 2462: 0.846\n",
      "train loss on epoch 2463 : 0.056\n",
      "train accuracy on epoch 2463: 1.000\n",
      "test loss on epoch 2463: 0.349\n",
      "test accuracy on epoch 2463: 0.769\n",
      "train loss on epoch 2464 : 0.164\n",
      "train accuracy on epoch 2464: 0.889\n",
      "test loss on epoch 2464: 0.347\n",
      "test accuracy on epoch 2464: 0.769\n",
      "train loss on epoch 2465 : 0.114\n",
      "train accuracy on epoch 2465: 0.944\n",
      "test loss on epoch 2465: 0.354\n",
      "test accuracy on epoch 2465: 0.769\n",
      "train loss on epoch 2466 : 0.074\n",
      "train accuracy on epoch 2466: 1.000\n",
      "test loss on epoch 2466: 0.355\n",
      "test accuracy on epoch 2466: 0.769\n",
      "train loss on epoch 2467 : 0.149\n",
      "train accuracy on epoch 2467: 0.944\n",
      "test loss on epoch 2467: 0.355\n",
      "test accuracy on epoch 2467: 0.769\n",
      "train loss on epoch 2468 : 0.259\n",
      "train accuracy on epoch 2468: 0.944\n",
      "test loss on epoch 2468: 0.356\n",
      "test accuracy on epoch 2468: 0.769\n",
      "train loss on epoch 2469 : 0.090\n",
      "train accuracy on epoch 2469: 0.944\n",
      "test loss on epoch 2469: 0.359\n",
      "test accuracy on epoch 2469: 0.769\n",
      "train loss on epoch 2470 : 0.110\n",
      "train accuracy on epoch 2470: 0.944\n",
      "test loss on epoch 2470: 0.365\n",
      "test accuracy on epoch 2470: 0.769\n",
      "train loss on epoch 2471 : 0.237\n",
      "train accuracy on epoch 2471: 0.944\n",
      "test loss on epoch 2471: 0.360\n",
      "test accuracy on epoch 2471: 0.769\n",
      "train loss on epoch 2472 : 0.173\n",
      "train accuracy on epoch 2472: 0.889\n",
      "test loss on epoch 2472: 0.359\n",
      "test accuracy on epoch 2472: 0.769\n",
      "train loss on epoch 2473 : 0.061\n",
      "train accuracy on epoch 2473: 1.000\n",
      "test loss on epoch 2473: 0.349\n",
      "test accuracy on epoch 2473: 0.769\n",
      "train loss on epoch 2474 : 0.160\n",
      "train accuracy on epoch 2474: 0.944\n",
      "test loss on epoch 2474: 0.348\n",
      "test accuracy on epoch 2474: 0.769\n",
      "train loss on epoch 2475 : 0.278\n",
      "train accuracy on epoch 2475: 0.889\n",
      "test loss on epoch 2475: 0.351\n",
      "test accuracy on epoch 2475: 0.769\n",
      "train loss on epoch 2476 : 0.089\n",
      "train accuracy on epoch 2476: 0.944\n",
      "test loss on epoch 2476: 0.353\n",
      "test accuracy on epoch 2476: 0.769\n",
      "train loss on epoch 2477 : 0.239\n",
      "train accuracy on epoch 2477: 0.889\n",
      "test loss on epoch 2477: 0.359\n",
      "test accuracy on epoch 2477: 0.769\n",
      "train loss on epoch 2478 : 0.024\n",
      "train accuracy on epoch 2478: 1.000\n",
      "test loss on epoch 2478: 0.361\n",
      "test accuracy on epoch 2478: 0.769\n",
      "train loss on epoch 2479 : 0.061\n",
      "train accuracy on epoch 2479: 1.000\n",
      "test loss on epoch 2479: 0.358\n",
      "test accuracy on epoch 2479: 0.769\n",
      "train loss on epoch 2480 : 0.119\n",
      "train accuracy on epoch 2480: 0.889\n",
      "test loss on epoch 2480: 0.358\n",
      "test accuracy on epoch 2480: 0.769\n",
      "train loss on epoch 2481 : 0.254\n",
      "train accuracy on epoch 2481: 0.889\n",
      "test loss on epoch 2481: 0.353\n",
      "test accuracy on epoch 2481: 0.769\n",
      "train loss on epoch 2482 : 0.173\n",
      "train accuracy on epoch 2482: 0.889\n",
      "test loss on epoch 2482: 0.347\n",
      "test accuracy on epoch 2482: 0.769\n",
      "train loss on epoch 2483 : 0.087\n",
      "train accuracy on epoch 2483: 0.944\n",
      "test loss on epoch 2483: 0.340\n",
      "test accuracy on epoch 2483: 0.769\n",
      "train loss on epoch 2484 : 0.217\n",
      "train accuracy on epoch 2484: 0.889\n",
      "test loss on epoch 2484: 0.336\n",
      "test accuracy on epoch 2484: 0.769\n",
      "train loss on epoch 2485 : 0.163\n",
      "train accuracy on epoch 2485: 0.944\n",
      "test loss on epoch 2485: 0.338\n",
      "test accuracy on epoch 2485: 0.769\n",
      "train loss on epoch 2486 : 0.124\n",
      "train accuracy on epoch 2486: 0.944\n",
      "test loss on epoch 2486: 0.342\n",
      "test accuracy on epoch 2486: 0.769\n",
      "train loss on epoch 2487 : 0.298\n",
      "train accuracy on epoch 2487: 0.944\n",
      "test loss on epoch 2487: 0.334\n",
      "test accuracy on epoch 2487: 0.769\n",
      "train loss on epoch 2488 : 0.347\n",
      "train accuracy on epoch 2488: 0.833\n",
      "test loss on epoch 2488: 0.340\n",
      "test accuracy on epoch 2488: 0.769\n",
      "train loss on epoch 2489 : 0.133\n",
      "train accuracy on epoch 2489: 0.889\n",
      "test loss on epoch 2489: 0.338\n",
      "test accuracy on epoch 2489: 0.769\n",
      "train loss on epoch 2490 : 0.322\n",
      "train accuracy on epoch 2490: 0.889\n",
      "test loss on epoch 2490: 0.350\n",
      "test accuracy on epoch 2490: 0.769\n",
      "train loss on epoch 2491 : 0.283\n",
      "train accuracy on epoch 2491: 0.833\n",
      "test loss on epoch 2491: 0.352\n",
      "test accuracy on epoch 2491: 0.769\n",
      "train loss on epoch 2492 : 0.328\n",
      "train accuracy on epoch 2492: 0.889\n",
      "test loss on epoch 2492: 0.350\n",
      "test accuracy on epoch 2492: 0.769\n",
      "train loss on epoch 2493 : 0.172\n",
      "train accuracy on epoch 2493: 0.889\n",
      "test loss on epoch 2493: 0.344\n",
      "test accuracy on epoch 2493: 0.769\n",
      "train loss on epoch 2494 : 0.187\n",
      "train accuracy on epoch 2494: 0.889\n",
      "test loss on epoch 2494: 0.343\n",
      "test accuracy on epoch 2494: 0.769\n",
      "train loss on epoch 2495 : 0.200\n",
      "train accuracy on epoch 2495: 0.944\n",
      "test loss on epoch 2495: 0.335\n",
      "test accuracy on epoch 2495: 0.769\n",
      "train loss on epoch 2496 : 0.054\n",
      "train accuracy on epoch 2496: 1.000\n",
      "test loss on epoch 2496: 0.331\n",
      "test accuracy on epoch 2496: 0.769\n",
      "train loss on epoch 2497 : 0.311\n",
      "train accuracy on epoch 2497: 0.944\n",
      "test loss on epoch 2497: 0.341\n",
      "test accuracy on epoch 2497: 0.769\n",
      "train loss on epoch 2498 : 0.241\n",
      "train accuracy on epoch 2498: 0.833\n",
      "test loss on epoch 2498: 0.329\n",
      "test accuracy on epoch 2498: 0.769\n",
      "train loss on epoch 2499 : 0.319\n",
      "train accuracy on epoch 2499: 0.889\n",
      "test loss on epoch 2499: 0.336\n",
      "test accuracy on epoch 2499: 0.769\n",
      "train loss on epoch 2500 : 0.220\n",
      "train accuracy on epoch 2500: 0.833\n",
      "test loss on epoch 2500: 0.330\n",
      "test accuracy on epoch 2500: 0.769\n",
      "train loss on epoch 2501 : 0.079\n",
      "train accuracy on epoch 2501: 0.944\n",
      "test loss on epoch 2501: 0.339\n",
      "test accuracy on epoch 2501: 0.769\n",
      "train loss on epoch 2502 : 0.473\n",
      "train accuracy on epoch 2502: 0.889\n",
      "test loss on epoch 2502: 0.339\n",
      "test accuracy on epoch 2502: 0.769\n",
      "train loss on epoch 2503 : 0.205\n",
      "train accuracy on epoch 2503: 0.889\n",
      "test loss on epoch 2503: 0.330\n",
      "test accuracy on epoch 2503: 0.769\n",
      "train loss on epoch 2504 : 0.130\n",
      "train accuracy on epoch 2504: 0.944\n",
      "test loss on epoch 2504: 0.349\n",
      "test accuracy on epoch 2504: 0.769\n",
      "train loss on epoch 2505 : 0.220\n",
      "train accuracy on epoch 2505: 0.889\n",
      "test loss on epoch 2505: 0.356\n",
      "test accuracy on epoch 2505: 0.769\n",
      "train loss on epoch 2506 : 0.082\n",
      "train accuracy on epoch 2506: 1.000\n",
      "test loss on epoch 2506: 0.360\n",
      "test accuracy on epoch 2506: 0.769\n",
      "train loss on epoch 2507 : 0.079\n",
      "train accuracy on epoch 2507: 1.000\n",
      "test loss on epoch 2507: 0.370\n",
      "test accuracy on epoch 2507: 0.769\n",
      "train loss on epoch 2508 : 0.104\n",
      "train accuracy on epoch 2508: 0.944\n",
      "test loss on epoch 2508: 0.364\n",
      "test accuracy on epoch 2508: 0.769\n",
      "train loss on epoch 2509 : 0.300\n",
      "train accuracy on epoch 2509: 0.833\n",
      "test loss on epoch 2509: 0.356\n",
      "test accuracy on epoch 2509: 0.769\n",
      "train loss on epoch 2510 : 0.183\n",
      "train accuracy on epoch 2510: 0.944\n",
      "test loss on epoch 2510: 0.351\n",
      "test accuracy on epoch 2510: 0.769\n",
      "train loss on epoch 2511 : 0.298\n",
      "train accuracy on epoch 2511: 0.889\n",
      "test loss on epoch 2511: 0.341\n",
      "test accuracy on epoch 2511: 0.769\n",
      "train loss on epoch 2512 : 0.224\n",
      "train accuracy on epoch 2512: 0.889\n",
      "test loss on epoch 2512: 0.337\n",
      "test accuracy on epoch 2512: 0.769\n",
      "train loss on epoch 2513 : 0.111\n",
      "train accuracy on epoch 2513: 1.000\n",
      "test loss on epoch 2513: 0.326\n",
      "test accuracy on epoch 2513: 0.769\n",
      "train loss on epoch 2514 : 0.268\n",
      "train accuracy on epoch 2514: 0.833\n",
      "test loss on epoch 2514: 0.329\n",
      "test accuracy on epoch 2514: 0.769\n",
      "train loss on epoch 2515 : 0.207\n",
      "train accuracy on epoch 2515: 0.944\n",
      "test loss on epoch 2515: 0.332\n",
      "test accuracy on epoch 2515: 0.769\n",
      "train loss on epoch 2516 : 0.119\n",
      "train accuracy on epoch 2516: 0.944\n",
      "test loss on epoch 2516: 0.346\n",
      "test accuracy on epoch 2516: 0.769\n",
      "train loss on epoch 2517 : 0.186\n",
      "train accuracy on epoch 2517: 0.944\n",
      "test loss on epoch 2517: 0.337\n",
      "test accuracy on epoch 2517: 0.769\n",
      "train loss on epoch 2518 : 0.081\n",
      "train accuracy on epoch 2518: 1.000\n",
      "test loss on epoch 2518: 0.336\n",
      "test accuracy on epoch 2518: 0.769\n",
      "train loss on epoch 2519 : 0.082\n",
      "train accuracy on epoch 2519: 0.944\n",
      "test loss on epoch 2519: 0.342\n",
      "test accuracy on epoch 2519: 0.769\n",
      "train loss on epoch 2520 : 0.418\n",
      "train accuracy on epoch 2520: 0.889\n",
      "test loss on epoch 2520: 0.332\n",
      "test accuracy on epoch 2520: 0.769\n",
      "train loss on epoch 2521 : 0.153\n",
      "train accuracy on epoch 2521: 0.944\n",
      "test loss on epoch 2521: 0.343\n",
      "test accuracy on epoch 2521: 0.692\n",
      "train loss on epoch 2522 : 0.244\n",
      "train accuracy on epoch 2522: 0.889\n",
      "test loss on epoch 2522: 0.327\n",
      "test accuracy on epoch 2522: 0.769\n",
      "train loss on epoch 2523 : 0.167\n",
      "train accuracy on epoch 2523: 0.833\n",
      "test loss on epoch 2523: 0.349\n",
      "test accuracy on epoch 2523: 0.769\n",
      "train loss on epoch 2524 : 0.070\n",
      "train accuracy on epoch 2524: 1.000\n",
      "test loss on epoch 2524: 0.349\n",
      "test accuracy on epoch 2524: 0.769\n",
      "train loss on epoch 2525 : 0.174\n",
      "train accuracy on epoch 2525: 0.889\n",
      "test loss on epoch 2525: 0.330\n",
      "test accuracy on epoch 2525: 0.769\n",
      "train loss on epoch 2526 : 0.090\n",
      "train accuracy on epoch 2526: 0.944\n",
      "test loss on epoch 2526: 0.345\n",
      "test accuracy on epoch 2526: 0.692\n",
      "train loss on epoch 2527 : 0.054\n",
      "train accuracy on epoch 2527: 1.000\n",
      "test loss on epoch 2527: 0.347\n",
      "test accuracy on epoch 2527: 0.692\n",
      "train loss on epoch 2528 : 0.106\n",
      "train accuracy on epoch 2528: 0.944\n",
      "test loss on epoch 2528: 0.343\n",
      "test accuracy on epoch 2528: 0.692\n",
      "train loss on epoch 2529 : 0.261\n",
      "train accuracy on epoch 2529: 0.944\n",
      "test loss on epoch 2529: 0.346\n",
      "test accuracy on epoch 2529: 0.769\n",
      "train loss on epoch 2530 : 0.388\n",
      "train accuracy on epoch 2530: 0.889\n",
      "test loss on epoch 2530: 0.325\n",
      "test accuracy on epoch 2530: 0.846\n",
      "train loss on epoch 2531 : 0.213\n",
      "train accuracy on epoch 2531: 0.889\n",
      "test loss on epoch 2531: 0.331\n",
      "test accuracy on epoch 2531: 0.769\n",
      "train loss on epoch 2532 : 0.233\n",
      "train accuracy on epoch 2532: 0.833\n",
      "test loss on epoch 2532: 0.345\n",
      "test accuracy on epoch 2532: 0.769\n",
      "train loss on epoch 2533 : 0.121\n",
      "train accuracy on epoch 2533: 1.000\n",
      "test loss on epoch 2533: 0.347\n",
      "test accuracy on epoch 2533: 0.769\n",
      "train loss on epoch 2534 : 0.159\n",
      "train accuracy on epoch 2534: 0.944\n",
      "test loss on epoch 2534: 0.319\n",
      "test accuracy on epoch 2534: 0.769\n",
      "train loss on epoch 2535 : 0.469\n",
      "train accuracy on epoch 2535: 0.889\n",
      "test loss on epoch 2535: 0.337\n",
      "test accuracy on epoch 2535: 0.769\n",
      "train loss on epoch 2536 : 0.212\n",
      "train accuracy on epoch 2536: 0.889\n",
      "test loss on epoch 2536: 0.348\n",
      "test accuracy on epoch 2536: 0.692\n",
      "train loss on epoch 2537 : 0.121\n",
      "train accuracy on epoch 2537: 0.944\n",
      "test loss on epoch 2537: 0.325\n",
      "test accuracy on epoch 2537: 0.846\n",
      "train loss on epoch 2538 : 0.074\n",
      "train accuracy on epoch 2538: 1.000\n",
      "test loss on epoch 2538: 0.329\n",
      "test accuracy on epoch 2538: 0.846\n",
      "train loss on epoch 2539 : 0.358\n",
      "train accuracy on epoch 2539: 0.889\n",
      "test loss on epoch 2539: 0.355\n",
      "test accuracy on epoch 2539: 0.769\n",
      "train loss on epoch 2540 : 0.370\n",
      "train accuracy on epoch 2540: 0.889\n",
      "test loss on epoch 2540: 0.337\n",
      "test accuracy on epoch 2540: 0.769\n",
      "train loss on epoch 2541 : 0.288\n",
      "train accuracy on epoch 2541: 0.833\n",
      "test loss on epoch 2541: 0.357\n",
      "test accuracy on epoch 2541: 0.769\n",
      "train loss on epoch 2542 : 0.106\n",
      "train accuracy on epoch 2542: 1.000\n",
      "test loss on epoch 2542: 0.343\n",
      "test accuracy on epoch 2542: 0.769\n",
      "train loss on epoch 2543 : 0.371\n",
      "train accuracy on epoch 2543: 0.889\n",
      "test loss on epoch 2543: 0.327\n",
      "test accuracy on epoch 2543: 0.846\n",
      "train loss on epoch 2544 : 0.254\n",
      "train accuracy on epoch 2544: 0.944\n",
      "test loss on epoch 2544: 0.330\n",
      "test accuracy on epoch 2544: 0.846\n",
      "train loss on epoch 2545 : 0.411\n",
      "train accuracy on epoch 2545: 0.889\n",
      "test loss on epoch 2545: 0.354\n",
      "test accuracy on epoch 2545: 0.692\n",
      "train loss on epoch 2546 : 0.119\n",
      "train accuracy on epoch 2546: 0.944\n",
      "test loss on epoch 2546: 0.352\n",
      "test accuracy on epoch 2546: 0.692\n",
      "train loss on epoch 2547 : 0.062\n",
      "train accuracy on epoch 2547: 1.000\n",
      "test loss on epoch 2547: 0.337\n",
      "test accuracy on epoch 2547: 0.769\n",
      "train loss on epoch 2548 : 0.324\n",
      "train accuracy on epoch 2548: 0.889\n",
      "test loss on epoch 2548: 0.353\n",
      "test accuracy on epoch 2548: 0.692\n",
      "train loss on epoch 2549 : 0.126\n",
      "train accuracy on epoch 2549: 0.944\n",
      "test loss on epoch 2549: 0.349\n",
      "test accuracy on epoch 2549: 0.692\n",
      "train loss on epoch 2550 : 0.206\n",
      "train accuracy on epoch 2550: 0.944\n",
      "test loss on epoch 2550: 0.338\n",
      "test accuracy on epoch 2550: 0.769\n",
      "train loss on epoch 2551 : 0.073\n",
      "train accuracy on epoch 2551: 0.944\n",
      "test loss on epoch 2551: 0.339\n",
      "test accuracy on epoch 2551: 0.769\n",
      "train loss on epoch 2552 : 0.404\n",
      "train accuracy on epoch 2552: 0.889\n",
      "test loss on epoch 2552: 0.330\n",
      "test accuracy on epoch 2552: 0.769\n",
      "train loss on epoch 2553 : 0.090\n",
      "train accuracy on epoch 2553: 0.944\n",
      "test loss on epoch 2553: 0.345\n",
      "test accuracy on epoch 2553: 0.769\n",
      "train loss on epoch 2554 : 0.385\n",
      "train accuracy on epoch 2554: 0.944\n",
      "test loss on epoch 2554: 0.334\n",
      "test accuracy on epoch 2554: 0.769\n",
      "train loss on epoch 2555 : 0.183\n",
      "train accuracy on epoch 2555: 0.889\n",
      "test loss on epoch 2555: 0.348\n",
      "test accuracy on epoch 2555: 0.769\n",
      "train loss on epoch 2556 : 0.099\n",
      "train accuracy on epoch 2556: 0.944\n",
      "test loss on epoch 2556: 0.335\n",
      "test accuracy on epoch 2556: 0.769\n",
      "train loss on epoch 2557 : 0.184\n",
      "train accuracy on epoch 2557: 0.944\n",
      "test loss on epoch 2557: 0.328\n",
      "test accuracy on epoch 2557: 0.769\n",
      "train loss on epoch 2558 : 0.144\n",
      "train accuracy on epoch 2558: 0.944\n",
      "test loss on epoch 2558: 0.341\n",
      "test accuracy on epoch 2558: 0.692\n",
      "train loss on epoch 2559 : 0.215\n",
      "train accuracy on epoch 2559: 0.944\n",
      "test loss on epoch 2559: 0.330\n",
      "test accuracy on epoch 2559: 0.769\n",
      "train loss on epoch 2560 : 0.083\n",
      "train accuracy on epoch 2560: 1.000\n",
      "test loss on epoch 2560: 0.318\n",
      "test accuracy on epoch 2560: 0.769\n",
      "train loss on epoch 2561 : 0.289\n",
      "train accuracy on epoch 2561: 0.944\n",
      "test loss on epoch 2561: 0.330\n",
      "test accuracy on epoch 2561: 0.769\n",
      "train loss on epoch 2562 : 0.195\n",
      "train accuracy on epoch 2562: 0.944\n",
      "test loss on epoch 2562: 0.319\n",
      "test accuracy on epoch 2562: 0.846\n",
      "train loss on epoch 2563 : 0.244\n",
      "train accuracy on epoch 2563: 0.889\n",
      "test loss on epoch 2563: 0.323\n",
      "test accuracy on epoch 2563: 0.769\n",
      "train loss on epoch 2564 : 0.144\n",
      "train accuracy on epoch 2564: 0.944\n",
      "test loss on epoch 2564: 0.338\n",
      "test accuracy on epoch 2564: 0.769\n",
      "train loss on epoch 2565 : 0.236\n",
      "train accuracy on epoch 2565: 0.889\n",
      "test loss on epoch 2565: 0.335\n",
      "test accuracy on epoch 2565: 0.769\n",
      "train loss on epoch 2566 : 0.229\n",
      "train accuracy on epoch 2566: 0.833\n",
      "test loss on epoch 2566: 0.345\n",
      "test accuracy on epoch 2566: 0.769\n",
      "train loss on epoch 2567 : 0.159\n",
      "train accuracy on epoch 2567: 0.944\n",
      "test loss on epoch 2567: 0.342\n",
      "test accuracy on epoch 2567: 0.769\n",
      "train loss on epoch 2568 : 0.080\n",
      "train accuracy on epoch 2568: 1.000\n",
      "test loss on epoch 2568: 0.339\n",
      "test accuracy on epoch 2568: 0.769\n",
      "train loss on epoch 2569 : 0.202\n",
      "train accuracy on epoch 2569: 0.944\n",
      "test loss on epoch 2569: 0.336\n",
      "test accuracy on epoch 2569: 0.769\n",
      "train loss on epoch 2570 : 0.188\n",
      "train accuracy on epoch 2570: 0.889\n",
      "test loss on epoch 2570: 0.350\n",
      "test accuracy on epoch 2570: 0.769\n",
      "train loss on epoch 2571 : 0.109\n",
      "train accuracy on epoch 2571: 0.944\n",
      "test loss on epoch 2571: 0.334\n",
      "test accuracy on epoch 2571: 0.769\n",
      "train loss on epoch 2572 : 0.146\n",
      "train accuracy on epoch 2572: 0.944\n",
      "test loss on epoch 2572: 0.332\n",
      "test accuracy on epoch 2572: 0.769\n",
      "train loss on epoch 2573 : 0.123\n",
      "train accuracy on epoch 2573: 0.944\n",
      "test loss on epoch 2573: 0.339\n",
      "test accuracy on epoch 2573: 0.769\n",
      "train loss on epoch 2574 : 0.171\n",
      "train accuracy on epoch 2574: 0.944\n",
      "test loss on epoch 2574: 0.337\n",
      "test accuracy on epoch 2574: 0.769\n",
      "train loss on epoch 2575 : 0.184\n",
      "train accuracy on epoch 2575: 0.944\n",
      "test loss on epoch 2575: 0.343\n",
      "test accuracy on epoch 2575: 0.769\n",
      "train loss on epoch 2576 : 0.177\n",
      "train accuracy on epoch 2576: 0.944\n",
      "test loss on epoch 2576: 0.348\n",
      "test accuracy on epoch 2576: 0.769\n",
      "train loss on epoch 2577 : 0.130\n",
      "train accuracy on epoch 2577: 0.944\n",
      "test loss on epoch 2577: 0.332\n",
      "test accuracy on epoch 2577: 0.769\n",
      "train loss on epoch 2578 : 0.127\n",
      "train accuracy on epoch 2578: 0.889\n",
      "test loss on epoch 2578: 0.332\n",
      "test accuracy on epoch 2578: 0.769\n",
      "train loss on epoch 2579 : 0.165\n",
      "train accuracy on epoch 2579: 0.944\n",
      "test loss on epoch 2579: 0.345\n",
      "test accuracy on epoch 2579: 0.692\n",
      "train loss on epoch 2580 : 0.060\n",
      "train accuracy on epoch 2580: 1.000\n",
      "test loss on epoch 2580: 0.324\n",
      "test accuracy on epoch 2580: 0.846\n",
      "train loss on epoch 2581 : 0.178\n",
      "train accuracy on epoch 2581: 0.889\n",
      "test loss on epoch 2581: 0.333\n",
      "test accuracy on epoch 2581: 0.769\n",
      "train loss on epoch 2582 : 0.286\n",
      "train accuracy on epoch 2582: 0.889\n",
      "test loss on epoch 2582: 0.323\n",
      "test accuracy on epoch 2582: 0.846\n",
      "train loss on epoch 2583 : 0.156\n",
      "train accuracy on epoch 2583: 0.889\n",
      "test loss on epoch 2583: 0.337\n",
      "test accuracy on epoch 2583: 0.769\n",
      "train loss on epoch 2584 : 0.051\n",
      "train accuracy on epoch 2584: 1.000\n",
      "test loss on epoch 2584: 0.335\n",
      "test accuracy on epoch 2584: 0.769\n",
      "train loss on epoch 2585 : 0.247\n",
      "train accuracy on epoch 2585: 0.944\n",
      "test loss on epoch 2585: 0.344\n",
      "test accuracy on epoch 2585: 0.692\n",
      "train loss on epoch 2586 : 0.092\n",
      "train accuracy on epoch 2586: 1.000\n",
      "test loss on epoch 2586: 0.327\n",
      "test accuracy on epoch 2586: 0.846\n",
      "train loss on epoch 2587 : 0.118\n",
      "train accuracy on epoch 2587: 0.944\n",
      "test loss on epoch 2587: 0.328\n",
      "test accuracy on epoch 2587: 0.846\n",
      "train loss on epoch 2588 : 0.243\n",
      "train accuracy on epoch 2588: 0.889\n",
      "test loss on epoch 2588: 0.327\n",
      "test accuracy on epoch 2588: 0.846\n",
      "train loss on epoch 2589 : 0.530\n",
      "train accuracy on epoch 2589: 0.889\n",
      "test loss on epoch 2589: 0.327\n",
      "test accuracy on epoch 2589: 0.846\n",
      "train loss on epoch 2590 : 0.147\n",
      "train accuracy on epoch 2590: 0.889\n",
      "test loss on epoch 2590: 0.336\n",
      "test accuracy on epoch 2590: 0.769\n",
      "train loss on epoch 2591 : 0.089\n",
      "train accuracy on epoch 2591: 0.944\n",
      "test loss on epoch 2591: 0.336\n",
      "test accuracy on epoch 2591: 0.769\n",
      "train loss on epoch 2592 : 0.199\n",
      "train accuracy on epoch 2592: 0.889\n",
      "test loss on epoch 2592: 0.324\n",
      "test accuracy on epoch 2592: 0.769\n",
      "train loss on epoch 2593 : 0.132\n",
      "train accuracy on epoch 2593: 0.944\n",
      "test loss on epoch 2593: 0.331\n",
      "test accuracy on epoch 2593: 0.769\n",
      "train loss on epoch 2594 : 0.380\n",
      "train accuracy on epoch 2594: 0.722\n",
      "test loss on epoch 2594: 0.328\n",
      "test accuracy on epoch 2594: 0.769\n",
      "train loss on epoch 2595 : 0.118\n",
      "train accuracy on epoch 2595: 0.944\n",
      "test loss on epoch 2595: 0.341\n",
      "test accuracy on epoch 2595: 0.692\n",
      "train loss on epoch 2596 : 0.088\n",
      "train accuracy on epoch 2596: 1.000\n",
      "test loss on epoch 2596: 0.327\n",
      "test accuracy on epoch 2596: 0.769\n",
      "train loss on epoch 2597 : 0.128\n",
      "train accuracy on epoch 2597: 0.944\n",
      "test loss on epoch 2597: 0.331\n",
      "test accuracy on epoch 2597: 0.769\n",
      "train loss on epoch 2598 : 0.227\n",
      "train accuracy on epoch 2598: 0.889\n",
      "test loss on epoch 2598: 0.329\n",
      "test accuracy on epoch 2598: 0.769\n",
      "train loss on epoch 2599 : 0.233\n",
      "train accuracy on epoch 2599: 0.944\n",
      "test loss on epoch 2599: 0.336\n",
      "test accuracy on epoch 2599: 0.769\n",
      "train loss on epoch 2600 : 0.273\n",
      "train accuracy on epoch 2600: 0.889\n",
      "test loss on epoch 2600: 0.327\n",
      "test accuracy on epoch 2600: 0.769\n",
      "train loss on epoch 2601 : 0.202\n",
      "train accuracy on epoch 2601: 0.889\n",
      "test loss on epoch 2601: 0.337\n",
      "test accuracy on epoch 2601: 0.769\n",
      "train loss on epoch 2602 : 0.159\n",
      "train accuracy on epoch 2602: 0.889\n",
      "test loss on epoch 2602: 0.338\n",
      "test accuracy on epoch 2602: 0.769\n",
      "train loss on epoch 2603 : 0.255\n",
      "train accuracy on epoch 2603: 0.889\n",
      "test loss on epoch 2603: 0.324\n",
      "test accuracy on epoch 2603: 0.846\n",
      "train loss on epoch 2604 : 0.130\n",
      "train accuracy on epoch 2604: 0.944\n",
      "test loss on epoch 2604: 0.325\n",
      "test accuracy on epoch 2604: 0.769\n",
      "train loss on epoch 2605 : 0.189\n",
      "train accuracy on epoch 2605: 0.944\n",
      "test loss on epoch 2605: 0.337\n",
      "test accuracy on epoch 2605: 0.769\n",
      "train loss on epoch 2606 : 0.086\n",
      "train accuracy on epoch 2606: 1.000\n",
      "test loss on epoch 2606: 0.345\n",
      "test accuracy on epoch 2606: 0.769\n",
      "train loss on epoch 2607 : 0.177\n",
      "train accuracy on epoch 2607: 0.889\n",
      "test loss on epoch 2607: 0.343\n",
      "test accuracy on epoch 2607: 0.769\n",
      "train loss on epoch 2608 : 0.122\n",
      "train accuracy on epoch 2608: 0.944\n",
      "test loss on epoch 2608: 0.320\n",
      "test accuracy on epoch 2608: 0.769\n",
      "train loss on epoch 2609 : 0.242\n",
      "train accuracy on epoch 2609: 0.889\n",
      "test loss on epoch 2609: 0.333\n",
      "test accuracy on epoch 2609: 0.769\n",
      "train loss on epoch 2610 : 0.109\n",
      "train accuracy on epoch 2610: 0.944\n",
      "test loss on epoch 2610: 0.337\n",
      "test accuracy on epoch 2610: 0.769\n",
      "train loss on epoch 2611 : 0.119\n",
      "train accuracy on epoch 2611: 0.944\n",
      "test loss on epoch 2611: 0.332\n",
      "test accuracy on epoch 2611: 0.769\n",
      "train loss on epoch 2612 : 0.141\n",
      "train accuracy on epoch 2612: 0.889\n",
      "test loss on epoch 2612: 0.338\n",
      "test accuracy on epoch 2612: 0.769\n",
      "train loss on epoch 2613 : 0.057\n",
      "train accuracy on epoch 2613: 1.000\n",
      "test loss on epoch 2613: 0.347\n",
      "test accuracy on epoch 2613: 0.769\n",
      "train loss on epoch 2614 : 0.060\n",
      "train accuracy on epoch 2614: 1.000\n",
      "test loss on epoch 2614: 0.344\n",
      "test accuracy on epoch 2614: 0.769\n",
      "train loss on epoch 2615 : 0.115\n",
      "train accuracy on epoch 2615: 0.944\n",
      "test loss on epoch 2615: 0.329\n",
      "test accuracy on epoch 2615: 0.769\n",
      "train loss on epoch 2616 : 0.078\n",
      "train accuracy on epoch 2616: 1.000\n",
      "test loss on epoch 2616: 0.326\n",
      "test accuracy on epoch 2616: 0.769\n",
      "train loss on epoch 2617 : 0.226\n",
      "train accuracy on epoch 2617: 0.944\n",
      "test loss on epoch 2617: 0.330\n",
      "test accuracy on epoch 2617: 0.846\n",
      "train loss on epoch 2618 : 0.221\n",
      "train accuracy on epoch 2618: 0.889\n",
      "test loss on epoch 2618: 0.337\n",
      "test accuracy on epoch 2618: 0.769\n",
      "train loss on epoch 2619 : 0.413\n",
      "train accuracy on epoch 2619: 0.833\n",
      "test loss on epoch 2619: 0.345\n",
      "test accuracy on epoch 2619: 0.769\n",
      "train loss on epoch 2620 : 0.179\n",
      "train accuracy on epoch 2620: 0.944\n",
      "test loss on epoch 2620: 0.344\n",
      "test accuracy on epoch 2620: 0.769\n",
      "train loss on epoch 2621 : 0.076\n",
      "train accuracy on epoch 2621: 0.944\n",
      "test loss on epoch 2621: 0.328\n",
      "test accuracy on epoch 2621: 0.769\n",
      "train loss on epoch 2622 : 0.283\n",
      "train accuracy on epoch 2622: 0.889\n",
      "test loss on epoch 2622: 0.341\n",
      "test accuracy on epoch 2622: 0.769\n",
      "train loss on epoch 2623 : 0.192\n",
      "train accuracy on epoch 2623: 0.889\n",
      "test loss on epoch 2623: 0.339\n",
      "test accuracy on epoch 2623: 0.692\n",
      "train loss on epoch 2624 : 0.116\n",
      "train accuracy on epoch 2624: 0.944\n",
      "test loss on epoch 2624: 0.332\n",
      "test accuracy on epoch 2624: 0.769\n",
      "train loss on epoch 2625 : 0.231\n",
      "train accuracy on epoch 2625: 0.833\n",
      "test loss on epoch 2625: 0.335\n",
      "test accuracy on epoch 2625: 0.769\n",
      "train loss on epoch 2626 : 0.192\n",
      "train accuracy on epoch 2626: 0.889\n",
      "test loss on epoch 2626: 0.343\n",
      "test accuracy on epoch 2626: 0.769\n",
      "train loss on epoch 2627 : 0.147\n",
      "train accuracy on epoch 2627: 0.889\n",
      "test loss on epoch 2627: 0.349\n",
      "test accuracy on epoch 2627: 0.769\n",
      "train loss on epoch 2628 : 0.114\n",
      "train accuracy on epoch 2628: 0.944\n",
      "test loss on epoch 2628: 0.335\n",
      "test accuracy on epoch 2628: 0.769\n",
      "train loss on epoch 2629 : 0.206\n",
      "train accuracy on epoch 2629: 0.889\n",
      "test loss on epoch 2629: 0.337\n",
      "test accuracy on epoch 2629: 0.769\n",
      "train loss on epoch 2630 : 0.119\n",
      "train accuracy on epoch 2630: 0.889\n",
      "test loss on epoch 2630: 0.348\n",
      "test accuracy on epoch 2630: 0.769\n",
      "train loss on epoch 2631 : 0.176\n",
      "train accuracy on epoch 2631: 0.944\n",
      "test loss on epoch 2631: 0.345\n",
      "test accuracy on epoch 2631: 0.769\n",
      "train loss on epoch 2632 : 0.195\n",
      "train accuracy on epoch 2632: 0.944\n",
      "test loss on epoch 2632: 0.339\n",
      "test accuracy on epoch 2632: 0.769\n",
      "train loss on epoch 2633 : 0.088\n",
      "train accuracy on epoch 2633: 1.000\n",
      "test loss on epoch 2633: 0.344\n",
      "test accuracy on epoch 2633: 0.769\n",
      "train loss on epoch 2634 : 0.097\n",
      "train accuracy on epoch 2634: 1.000\n",
      "test loss on epoch 2634: 0.351\n",
      "test accuracy on epoch 2634: 0.769\n",
      "train loss on epoch 2635 : 0.168\n",
      "train accuracy on epoch 2635: 0.944\n",
      "test loss on epoch 2635: 0.333\n",
      "test accuracy on epoch 2635: 0.769\n",
      "train loss on epoch 2636 : 0.280\n",
      "train accuracy on epoch 2636: 0.889\n",
      "test loss on epoch 2636: 0.331\n",
      "test accuracy on epoch 2636: 0.846\n",
      "train loss on epoch 2637 : 0.242\n",
      "train accuracy on epoch 2637: 0.889\n",
      "test loss on epoch 2637: 0.334\n",
      "test accuracy on epoch 2637: 0.769\n",
      "train loss on epoch 2638 : 0.212\n",
      "train accuracy on epoch 2638: 0.889\n",
      "test loss on epoch 2638: 0.337\n",
      "test accuracy on epoch 2638: 0.769\n",
      "train loss on epoch 2639 : 0.131\n",
      "train accuracy on epoch 2639: 0.944\n",
      "test loss on epoch 2639: 0.333\n",
      "test accuracy on epoch 2639: 0.769\n",
      "train loss on epoch 2640 : 0.115\n",
      "train accuracy on epoch 2640: 1.000\n",
      "test loss on epoch 2640: 0.325\n",
      "test accuracy on epoch 2640: 0.846\n",
      "train loss on epoch 2641 : 0.466\n",
      "train accuracy on epoch 2641: 0.833\n",
      "test loss on epoch 2641: 0.342\n",
      "test accuracy on epoch 2641: 0.692\n",
      "train loss on epoch 2642 : 0.091\n",
      "train accuracy on epoch 2642: 0.944\n",
      "test loss on epoch 2642: 0.336\n",
      "test accuracy on epoch 2642: 0.769\n",
      "train loss on epoch 2643 : 0.116\n",
      "train accuracy on epoch 2643: 0.944\n",
      "test loss on epoch 2643: 0.340\n",
      "test accuracy on epoch 2643: 0.692\n",
      "train loss on epoch 2644 : 0.070\n",
      "train accuracy on epoch 2644: 1.000\n",
      "test loss on epoch 2644: 0.343\n",
      "test accuracy on epoch 2644: 0.692\n",
      "train loss on epoch 2645 : 0.337\n",
      "train accuracy on epoch 2645: 0.778\n",
      "test loss on epoch 2645: 0.324\n",
      "test accuracy on epoch 2645: 0.846\n",
      "train loss on epoch 2646 : 0.203\n",
      "train accuracy on epoch 2646: 0.944\n",
      "test loss on epoch 2646: 0.328\n",
      "test accuracy on epoch 2646: 0.769\n",
      "train loss on epoch 2647 : 0.151\n",
      "train accuracy on epoch 2647: 0.944\n",
      "test loss on epoch 2647: 0.326\n",
      "test accuracy on epoch 2647: 0.846\n",
      "train loss on epoch 2648 : 0.246\n",
      "train accuracy on epoch 2648: 0.944\n",
      "test loss on epoch 2648: 0.324\n",
      "test accuracy on epoch 2648: 0.769\n",
      "train loss on epoch 2649 : 0.139\n",
      "train accuracy on epoch 2649: 0.944\n",
      "test loss on epoch 2649: 0.334\n",
      "test accuracy on epoch 2649: 0.769\n",
      "train loss on epoch 2650 : 0.276\n",
      "train accuracy on epoch 2650: 0.889\n",
      "test loss on epoch 2650: 0.329\n",
      "test accuracy on epoch 2650: 0.769\n",
      "train loss on epoch 2651 : 0.177\n",
      "train accuracy on epoch 2651: 0.944\n",
      "test loss on epoch 2651: 0.322\n",
      "test accuracy on epoch 2651: 0.769\n",
      "train loss on epoch 2652 : 0.243\n",
      "train accuracy on epoch 2652: 0.833\n",
      "test loss on epoch 2652: 0.327\n",
      "test accuracy on epoch 2652: 0.769\n",
      "train loss on epoch 2653 : 0.419\n",
      "train accuracy on epoch 2653: 0.889\n",
      "test loss on epoch 2653: 0.331\n",
      "test accuracy on epoch 2653: 0.769\n",
      "train loss on epoch 2654 : 0.306\n",
      "train accuracy on epoch 2654: 0.944\n",
      "test loss on epoch 2654: 0.339\n",
      "test accuracy on epoch 2654: 0.769\n",
      "train loss on epoch 2655 : 0.207\n",
      "train accuracy on epoch 2655: 0.944\n",
      "test loss on epoch 2655: 0.340\n",
      "test accuracy on epoch 2655: 0.769\n",
      "train loss on epoch 2656 : 0.133\n",
      "train accuracy on epoch 2656: 0.889\n",
      "test loss on epoch 2656: 0.351\n",
      "test accuracy on epoch 2656: 0.769\n",
      "train loss on epoch 2657 : 0.053\n",
      "train accuracy on epoch 2657: 1.000\n",
      "test loss on epoch 2657: 0.357\n",
      "test accuracy on epoch 2657: 0.769\n",
      "train loss on epoch 2658 : 0.173\n",
      "train accuracy on epoch 2658: 0.944\n",
      "test loss on epoch 2658: 0.352\n",
      "test accuracy on epoch 2658: 0.769\n",
      "train loss on epoch 2659 : 0.261\n",
      "train accuracy on epoch 2659: 0.833\n",
      "test loss on epoch 2659: 0.349\n",
      "test accuracy on epoch 2659: 0.769\n",
      "train loss on epoch 2660 : 0.173\n",
      "train accuracy on epoch 2660: 0.833\n",
      "test loss on epoch 2660: 0.347\n",
      "test accuracy on epoch 2660: 0.769\n",
      "train loss on epoch 2661 : 0.063\n",
      "train accuracy on epoch 2661: 1.000\n",
      "test loss on epoch 2661: 0.345\n",
      "test accuracy on epoch 2661: 0.769\n",
      "train loss on epoch 2662 : 0.093\n",
      "train accuracy on epoch 2662: 0.944\n",
      "test loss on epoch 2662: 0.345\n",
      "test accuracy on epoch 2662: 0.769\n",
      "train loss on epoch 2663 : 0.120\n",
      "train accuracy on epoch 2663: 0.944\n",
      "test loss on epoch 2663: 0.345\n",
      "test accuracy on epoch 2663: 0.769\n",
      "train loss on epoch 2664 : 0.504\n",
      "train accuracy on epoch 2664: 0.778\n",
      "test loss on epoch 2664: 0.345\n",
      "test accuracy on epoch 2664: 0.769\n",
      "train loss on epoch 2665 : 0.185\n",
      "train accuracy on epoch 2665: 0.944\n",
      "test loss on epoch 2665: 0.343\n",
      "test accuracy on epoch 2665: 0.769\n",
      "train loss on epoch 2666 : 0.154\n",
      "train accuracy on epoch 2666: 0.944\n",
      "test loss on epoch 2666: 0.337\n",
      "test accuracy on epoch 2666: 0.769\n",
      "train loss on epoch 2667 : 0.225\n",
      "train accuracy on epoch 2667: 0.944\n",
      "test loss on epoch 2667: 0.325\n",
      "test accuracy on epoch 2667: 0.769\n",
      "train loss on epoch 2668 : 0.351\n",
      "train accuracy on epoch 2668: 0.889\n",
      "test loss on epoch 2668: 0.324\n",
      "test accuracy on epoch 2668: 0.769\n",
      "train loss on epoch 2669 : 0.256\n",
      "train accuracy on epoch 2669: 0.833\n",
      "test loss on epoch 2669: 0.318\n",
      "test accuracy on epoch 2669: 0.769\n",
      "train loss on epoch 2670 : 0.225\n",
      "train accuracy on epoch 2670: 0.889\n",
      "test loss on epoch 2670: 0.330\n",
      "test accuracy on epoch 2670: 0.769\n",
      "train loss on epoch 2671 : 0.127\n",
      "train accuracy on epoch 2671: 0.889\n",
      "test loss on epoch 2671: 0.331\n",
      "test accuracy on epoch 2671: 0.769\n",
      "train loss on epoch 2672 : 0.282\n",
      "train accuracy on epoch 2672: 0.833\n",
      "test loss on epoch 2672: 0.332\n",
      "test accuracy on epoch 2672: 0.769\n",
      "train loss on epoch 2673 : 0.309\n",
      "train accuracy on epoch 2673: 0.889\n",
      "test loss on epoch 2673: 0.330\n",
      "test accuracy on epoch 2673: 0.769\n",
      "train loss on epoch 2674 : 0.060\n",
      "train accuracy on epoch 2674: 1.000\n",
      "test loss on epoch 2674: 0.336\n",
      "test accuracy on epoch 2674: 0.769\n",
      "train loss on epoch 2675 : 0.212\n",
      "train accuracy on epoch 2675: 0.833\n",
      "test loss on epoch 2675: 0.336\n",
      "test accuracy on epoch 2675: 0.769\n",
      "train loss on epoch 2676 : 0.092\n",
      "train accuracy on epoch 2676: 0.944\n",
      "test loss on epoch 2676: 0.329\n",
      "test accuracy on epoch 2676: 0.769\n",
      "train loss on epoch 2677 : 0.136\n",
      "train accuracy on epoch 2677: 0.944\n",
      "test loss on epoch 2677: 0.334\n",
      "test accuracy on epoch 2677: 0.769\n",
      "train loss on epoch 2678 : 0.100\n",
      "train accuracy on epoch 2678: 1.000\n",
      "test loss on epoch 2678: 0.331\n",
      "test accuracy on epoch 2678: 0.769\n",
      "train loss on epoch 2679 : 0.047\n",
      "train accuracy on epoch 2679: 1.000\n",
      "test loss on epoch 2679: 0.333\n",
      "test accuracy on epoch 2679: 0.769\n",
      "train loss on epoch 2680 : 0.068\n",
      "train accuracy on epoch 2680: 1.000\n",
      "test loss on epoch 2680: 0.328\n",
      "test accuracy on epoch 2680: 0.769\n",
      "train loss on epoch 2681 : 0.153\n",
      "train accuracy on epoch 2681: 0.944\n",
      "test loss on epoch 2681: 0.332\n",
      "test accuracy on epoch 2681: 0.769\n",
      "train loss on epoch 2682 : 0.129\n",
      "train accuracy on epoch 2682: 0.889\n",
      "test loss on epoch 2682: 0.333\n",
      "test accuracy on epoch 2682: 0.769\n",
      "train loss on epoch 2683 : 0.185\n",
      "train accuracy on epoch 2683: 0.944\n",
      "test loss on epoch 2683: 0.331\n",
      "test accuracy on epoch 2683: 0.769\n",
      "train loss on epoch 2684 : 0.183\n",
      "train accuracy on epoch 2684: 0.889\n",
      "test loss on epoch 2684: 0.325\n",
      "test accuracy on epoch 2684: 0.769\n",
      "train loss on epoch 2685 : 0.103\n",
      "train accuracy on epoch 2685: 1.000\n",
      "test loss on epoch 2685: 0.325\n",
      "test accuracy on epoch 2685: 0.769\n",
      "train loss on epoch 2686 : 0.047\n",
      "train accuracy on epoch 2686: 1.000\n",
      "test loss on epoch 2686: 0.334\n",
      "test accuracy on epoch 2686: 0.769\n",
      "train loss on epoch 2687 : 0.090\n",
      "train accuracy on epoch 2687: 1.000\n",
      "test loss on epoch 2687: 0.326\n",
      "test accuracy on epoch 2687: 0.769\n",
      "train loss on epoch 2688 : 0.125\n",
      "train accuracy on epoch 2688: 0.944\n",
      "test loss on epoch 2688: 0.326\n",
      "test accuracy on epoch 2688: 0.769\n",
      "train loss on epoch 2689 : 0.097\n",
      "train accuracy on epoch 2689: 1.000\n",
      "test loss on epoch 2689: 0.327\n",
      "test accuracy on epoch 2689: 0.769\n",
      "train loss on epoch 2690 : 0.091\n",
      "train accuracy on epoch 2690: 0.944\n",
      "test loss on epoch 2690: 0.340\n",
      "test accuracy on epoch 2690: 0.769\n",
      "train loss on epoch 2691 : 0.176\n",
      "train accuracy on epoch 2691: 0.889\n",
      "test loss on epoch 2691: 0.322\n",
      "test accuracy on epoch 2691: 0.769\n",
      "train loss on epoch 2692 : 0.071\n",
      "train accuracy on epoch 2692: 1.000\n",
      "test loss on epoch 2692: 0.339\n",
      "test accuracy on epoch 2692: 0.769\n",
      "train loss on epoch 2693 : 0.135\n",
      "train accuracy on epoch 2693: 0.944\n",
      "test loss on epoch 2693: 0.337\n",
      "test accuracy on epoch 2693: 0.769\n",
      "train loss on epoch 2694 : 0.308\n",
      "train accuracy on epoch 2694: 0.889\n",
      "test loss on epoch 2694: 0.325\n",
      "test accuracy on epoch 2694: 0.769\n",
      "train loss on epoch 2695 : 0.095\n",
      "train accuracy on epoch 2695: 0.944\n",
      "test loss on epoch 2695: 0.316\n",
      "test accuracy on epoch 2695: 0.846\n",
      "train loss on epoch 2696 : 0.257\n",
      "train accuracy on epoch 2696: 0.889\n",
      "test loss on epoch 2696: 0.334\n",
      "test accuracy on epoch 2696: 0.769\n",
      "train loss on epoch 2697 : 0.064\n",
      "train accuracy on epoch 2697: 1.000\n",
      "test loss on epoch 2697: 0.338\n",
      "test accuracy on epoch 2697: 0.769\n",
      "train loss on epoch 2698 : 0.094\n",
      "train accuracy on epoch 2698: 1.000\n",
      "test loss on epoch 2698: 0.322\n",
      "test accuracy on epoch 2698: 0.769\n",
      "train loss on epoch 2699 : 0.309\n",
      "train accuracy on epoch 2699: 0.889\n",
      "test loss on epoch 2699: 0.336\n",
      "test accuracy on epoch 2699: 0.769\n",
      "train loss on epoch 2700 : 0.190\n",
      "train accuracy on epoch 2700: 0.889\n",
      "test loss on epoch 2700: 0.336\n",
      "test accuracy on epoch 2700: 0.769\n",
      "train loss on epoch 2701 : 0.050\n",
      "train accuracy on epoch 2701: 1.000\n",
      "test loss on epoch 2701: 0.325\n",
      "test accuracy on epoch 2701: 0.769\n",
      "train loss on epoch 2702 : 0.125\n",
      "train accuracy on epoch 2702: 0.944\n",
      "test loss on epoch 2702: 0.318\n",
      "test accuracy on epoch 2702: 0.769\n",
      "train loss on epoch 2703 : 0.108\n",
      "train accuracy on epoch 2703: 0.889\n",
      "test loss on epoch 2703: 0.322\n",
      "test accuracy on epoch 2703: 0.769\n",
      "train loss on epoch 2704 : 0.297\n",
      "train accuracy on epoch 2704: 0.833\n",
      "test loss on epoch 2704: 0.325\n",
      "test accuracy on epoch 2704: 0.769\n",
      "train loss on epoch 2705 : 0.533\n",
      "train accuracy on epoch 2705: 0.889\n",
      "test loss on epoch 2705: 0.312\n",
      "test accuracy on epoch 2705: 0.769\n",
      "train loss on epoch 2706 : 0.124\n",
      "train accuracy on epoch 2706: 0.944\n",
      "test loss on epoch 2706: 0.313\n",
      "test accuracy on epoch 2706: 0.769\n",
      "train loss on epoch 2707 : 0.210\n",
      "train accuracy on epoch 2707: 0.889\n",
      "test loss on epoch 2707: 0.318\n",
      "test accuracy on epoch 2707: 0.692\n",
      "train loss on epoch 2708 : 0.062\n",
      "train accuracy on epoch 2708: 1.000\n",
      "test loss on epoch 2708: 0.318\n",
      "test accuracy on epoch 2708: 0.769\n",
      "train loss on epoch 2709 : 0.203\n",
      "train accuracy on epoch 2709: 0.944\n",
      "test loss on epoch 2709: 0.324\n",
      "test accuracy on epoch 2709: 0.769\n",
      "train loss on epoch 2710 : 0.239\n",
      "train accuracy on epoch 2710: 0.889\n",
      "test loss on epoch 2710: 0.334\n",
      "test accuracy on epoch 2710: 0.769\n",
      "train loss on epoch 2711 : 0.249\n",
      "train accuracy on epoch 2711: 0.889\n",
      "test loss on epoch 2711: 0.332\n",
      "test accuracy on epoch 2711: 0.769\n",
      "train loss on epoch 2712 : 0.068\n",
      "train accuracy on epoch 2712: 1.000\n",
      "test loss on epoch 2712: 0.337\n",
      "test accuracy on epoch 2712: 0.769\n",
      "train loss on epoch 2713 : 0.055\n",
      "train accuracy on epoch 2713: 1.000\n",
      "test loss on epoch 2713: 0.325\n",
      "test accuracy on epoch 2713: 0.769\n",
      "train loss on epoch 2714 : 0.144\n",
      "train accuracy on epoch 2714: 0.944\n",
      "test loss on epoch 2714: 0.318\n",
      "test accuracy on epoch 2714: 0.769\n",
      "train loss on epoch 2715 : 0.122\n",
      "train accuracy on epoch 2715: 0.944\n",
      "test loss on epoch 2715: 0.326\n",
      "test accuracy on epoch 2715: 0.769\n",
      "train loss on epoch 2716 : 0.174\n",
      "train accuracy on epoch 2716: 0.944\n",
      "test loss on epoch 2716: 0.320\n",
      "test accuracy on epoch 2716: 0.769\n",
      "train loss on epoch 2717 : 0.341\n",
      "train accuracy on epoch 2717: 0.889\n",
      "test loss on epoch 2717: 0.326\n",
      "test accuracy on epoch 2717: 0.769\n",
      "train loss on epoch 2718 : 0.148\n",
      "train accuracy on epoch 2718: 0.944\n",
      "test loss on epoch 2718: 0.327\n",
      "test accuracy on epoch 2718: 0.769\n",
      "train loss on epoch 2719 : 0.253\n",
      "train accuracy on epoch 2719: 0.889\n",
      "test loss on epoch 2719: 0.335\n",
      "test accuracy on epoch 2719: 0.769\n",
      "train loss on epoch 2720 : 0.419\n",
      "train accuracy on epoch 2720: 0.833\n",
      "test loss on epoch 2720: 0.322\n",
      "test accuracy on epoch 2720: 0.769\n",
      "train loss on epoch 2721 : 0.104\n",
      "train accuracy on epoch 2721: 0.944\n",
      "test loss on epoch 2721: 0.324\n",
      "test accuracy on epoch 2721: 0.769\n",
      "train loss on epoch 2722 : 0.280\n",
      "train accuracy on epoch 2722: 0.944\n",
      "test loss on epoch 2722: 0.323\n",
      "test accuracy on epoch 2722: 0.769\n",
      "train loss on epoch 2723 : 0.033\n",
      "train accuracy on epoch 2723: 1.000\n",
      "test loss on epoch 2723: 0.318\n",
      "test accuracy on epoch 2723: 0.769\n",
      "train loss on epoch 2724 : 0.124\n",
      "train accuracy on epoch 2724: 0.944\n",
      "test loss on epoch 2724: 0.327\n",
      "test accuracy on epoch 2724: 0.769\n",
      "train loss on epoch 2725 : 0.050\n",
      "train accuracy on epoch 2725: 1.000\n",
      "test loss on epoch 2725: 0.322\n",
      "test accuracy on epoch 2725: 0.769\n",
      "train loss on epoch 2726 : 0.162\n",
      "train accuracy on epoch 2726: 0.889\n",
      "test loss on epoch 2726: 0.321\n",
      "test accuracy on epoch 2726: 0.769\n",
      "train loss on epoch 2727 : 0.059\n",
      "train accuracy on epoch 2727: 1.000\n",
      "test loss on epoch 2727: 0.329\n",
      "test accuracy on epoch 2727: 0.769\n",
      "train loss on epoch 2728 : 0.070\n",
      "train accuracy on epoch 2728: 1.000\n",
      "test loss on epoch 2728: 0.324\n",
      "test accuracy on epoch 2728: 0.769\n",
      "train loss on epoch 2729 : 0.143\n",
      "train accuracy on epoch 2729: 0.944\n",
      "test loss on epoch 2729: 0.320\n",
      "test accuracy on epoch 2729: 0.769\n",
      "train loss on epoch 2730 : 0.176\n",
      "train accuracy on epoch 2730: 0.944\n",
      "test loss on epoch 2730: 0.314\n",
      "test accuracy on epoch 2730: 0.769\n",
      "train loss on epoch 2731 : 0.449\n",
      "train accuracy on epoch 2731: 0.944\n",
      "test loss on epoch 2731: 0.325\n",
      "test accuracy on epoch 2731: 0.769\n",
      "train loss on epoch 2732 : 0.143\n",
      "train accuracy on epoch 2732: 0.944\n",
      "test loss on epoch 2732: 0.322\n",
      "test accuracy on epoch 2732: 0.769\n",
      "train loss on epoch 2733 : 0.127\n",
      "train accuracy on epoch 2733: 0.944\n",
      "test loss on epoch 2733: 0.321\n",
      "test accuracy on epoch 2733: 0.769\n",
      "train loss on epoch 2734 : 0.079\n",
      "train accuracy on epoch 2734: 1.000\n",
      "test loss on epoch 2734: 0.328\n",
      "test accuracy on epoch 2734: 0.769\n",
      "train loss on epoch 2735 : 0.315\n",
      "train accuracy on epoch 2735: 0.944\n",
      "test loss on epoch 2735: 0.333\n",
      "test accuracy on epoch 2735: 0.769\n",
      "train loss on epoch 2736 : 0.133\n",
      "train accuracy on epoch 2736: 0.889\n",
      "test loss on epoch 2736: 0.328\n",
      "test accuracy on epoch 2736: 0.769\n",
      "train loss on epoch 2737 : 0.198\n",
      "train accuracy on epoch 2737: 0.889\n",
      "test loss on epoch 2737: 0.330\n",
      "test accuracy on epoch 2737: 0.769\n",
      "train loss on epoch 2738 : 0.162\n",
      "train accuracy on epoch 2738: 0.944\n",
      "test loss on epoch 2738: 0.321\n",
      "test accuracy on epoch 2738: 0.769\n",
      "train loss on epoch 2739 : 0.332\n",
      "train accuracy on epoch 2739: 0.889\n",
      "test loss on epoch 2739: 0.323\n",
      "test accuracy on epoch 2739: 0.769\n",
      "train loss on epoch 2740 : 0.159\n",
      "train accuracy on epoch 2740: 0.944\n",
      "test loss on epoch 2740: 0.317\n",
      "test accuracy on epoch 2740: 0.769\n",
      "train loss on epoch 2741 : 0.159\n",
      "train accuracy on epoch 2741: 0.944\n",
      "test loss on epoch 2741: 0.327\n",
      "test accuracy on epoch 2741: 0.769\n",
      "train loss on epoch 2742 : 0.213\n",
      "train accuracy on epoch 2742: 0.944\n",
      "test loss on epoch 2742: 0.323\n",
      "test accuracy on epoch 2742: 0.769\n",
      "train loss on epoch 2743 : 0.371\n",
      "train accuracy on epoch 2743: 0.778\n",
      "test loss on epoch 2743: 0.326\n",
      "test accuracy on epoch 2743: 0.769\n",
      "train loss on epoch 2744 : 0.320\n",
      "train accuracy on epoch 2744: 0.889\n",
      "test loss on epoch 2744: 0.321\n",
      "test accuracy on epoch 2744: 0.769\n",
      "train loss on epoch 2745 : 0.078\n",
      "train accuracy on epoch 2745: 1.000\n",
      "test loss on epoch 2745: 0.318\n",
      "test accuracy on epoch 2745: 0.769\n",
      "train loss on epoch 2746 : 0.192\n",
      "train accuracy on epoch 2746: 0.889\n",
      "test loss on epoch 2746: 0.312\n",
      "test accuracy on epoch 2746: 0.769\n",
      "train loss on epoch 2747 : 0.212\n",
      "train accuracy on epoch 2747: 0.944\n",
      "test loss on epoch 2747: 0.316\n",
      "test accuracy on epoch 2747: 0.769\n",
      "train loss on epoch 2748 : 0.140\n",
      "train accuracy on epoch 2748: 0.889\n",
      "test loss on epoch 2748: 0.322\n",
      "test accuracy on epoch 2748: 0.769\n",
      "train loss on epoch 2749 : 0.396\n",
      "train accuracy on epoch 2749: 0.833\n",
      "test loss on epoch 2749: 0.311\n",
      "test accuracy on epoch 2749: 0.769\n",
      "train loss on epoch 2750 : 0.441\n",
      "train accuracy on epoch 2750: 0.833\n",
      "test loss on epoch 2750: 0.311\n",
      "test accuracy on epoch 2750: 0.769\n",
      "train loss on epoch 2751 : 0.094\n",
      "train accuracy on epoch 2751: 1.000\n",
      "test loss on epoch 2751: 0.314\n",
      "test accuracy on epoch 2751: 0.769\n",
      "train loss on epoch 2752 : 0.283\n",
      "train accuracy on epoch 2752: 0.944\n",
      "test loss on epoch 2752: 0.321\n",
      "test accuracy on epoch 2752: 0.769\n",
      "train loss on epoch 2753 : 0.070\n",
      "train accuracy on epoch 2753: 0.944\n",
      "test loss on epoch 2753: 0.323\n",
      "test accuracy on epoch 2753: 0.769\n",
      "train loss on epoch 2754 : 0.126\n",
      "train accuracy on epoch 2754: 0.944\n",
      "test loss on epoch 2754: 0.324\n",
      "test accuracy on epoch 2754: 0.769\n",
      "train loss on epoch 2755 : 0.176\n",
      "train accuracy on epoch 2755: 0.889\n",
      "test loss on epoch 2755: 0.323\n",
      "test accuracy on epoch 2755: 0.769\n",
      "train loss on epoch 2756 : 0.123\n",
      "train accuracy on epoch 2756: 0.944\n",
      "test loss on epoch 2756: 0.316\n",
      "test accuracy on epoch 2756: 0.769\n",
      "train loss on epoch 2757 : 0.126\n",
      "train accuracy on epoch 2757: 0.944\n",
      "test loss on epoch 2757: 0.328\n",
      "test accuracy on epoch 2757: 0.769\n",
      "train loss on epoch 2758 : 0.298\n",
      "train accuracy on epoch 2758: 0.833\n",
      "test loss on epoch 2758: 0.324\n",
      "test accuracy on epoch 2758: 0.769\n",
      "train loss on epoch 2759 : 0.067\n",
      "train accuracy on epoch 2759: 1.000\n",
      "test loss on epoch 2759: 0.326\n",
      "test accuracy on epoch 2759: 0.769\n",
      "train loss on epoch 2760 : 0.141\n",
      "train accuracy on epoch 2760: 0.944\n",
      "test loss on epoch 2760: 0.335\n",
      "test accuracy on epoch 2760: 0.769\n",
      "train loss on epoch 2761 : 0.289\n",
      "train accuracy on epoch 2761: 0.833\n",
      "test loss on epoch 2761: 0.335\n",
      "test accuracy on epoch 2761: 0.769\n",
      "train loss on epoch 2762 : 0.052\n",
      "train accuracy on epoch 2762: 1.000\n",
      "test loss on epoch 2762: 0.338\n",
      "test accuracy on epoch 2762: 0.769\n",
      "train loss on epoch 2763 : 0.113\n",
      "train accuracy on epoch 2763: 0.944\n",
      "test loss on epoch 2763: 0.337\n",
      "test accuracy on epoch 2763: 0.769\n",
      "train loss on epoch 2764 : 0.114\n",
      "train accuracy on epoch 2764: 0.944\n",
      "test loss on epoch 2764: 0.332\n",
      "test accuracy on epoch 2764: 0.769\n",
      "train loss on epoch 2765 : 0.202\n",
      "train accuracy on epoch 2765: 0.889\n",
      "test loss on epoch 2765: 0.331\n",
      "test accuracy on epoch 2765: 0.769\n",
      "train loss on epoch 2766 : 0.179\n",
      "train accuracy on epoch 2766: 0.944\n",
      "test loss on epoch 2766: 0.332\n",
      "test accuracy on epoch 2766: 0.769\n",
      "train loss on epoch 2767 : 0.073\n",
      "train accuracy on epoch 2767: 0.944\n",
      "test loss on epoch 2767: 0.330\n",
      "test accuracy on epoch 2767: 0.769\n",
      "train loss on epoch 2768 : 0.105\n",
      "train accuracy on epoch 2768: 1.000\n",
      "test loss on epoch 2768: 0.331\n",
      "test accuracy on epoch 2768: 0.769\n",
      "train loss on epoch 2769 : 0.251\n",
      "train accuracy on epoch 2769: 0.889\n",
      "test loss on epoch 2769: 0.330\n",
      "test accuracy on epoch 2769: 0.769\n",
      "train loss on epoch 2770 : 0.054\n",
      "train accuracy on epoch 2770: 1.000\n",
      "test loss on epoch 2770: 0.330\n",
      "test accuracy on epoch 2770: 0.769\n",
      "train loss on epoch 2771 : 0.077\n",
      "train accuracy on epoch 2771: 1.000\n",
      "test loss on epoch 2771: 0.330\n",
      "test accuracy on epoch 2771: 0.769\n",
      "train loss on epoch 2772 : 0.276\n",
      "train accuracy on epoch 2772: 0.889\n",
      "test loss on epoch 2772: 0.326\n",
      "test accuracy on epoch 2772: 0.769\n",
      "train loss on epoch 2773 : 0.281\n",
      "train accuracy on epoch 2773: 0.889\n",
      "test loss on epoch 2773: 0.324\n",
      "test accuracy on epoch 2773: 0.769\n",
      "train loss on epoch 2774 : 0.245\n",
      "train accuracy on epoch 2774: 0.889\n",
      "test loss on epoch 2774: 0.317\n",
      "test accuracy on epoch 2774: 0.769\n",
      "train loss on epoch 2775 : 0.160\n",
      "train accuracy on epoch 2775: 0.889\n",
      "test loss on epoch 2775: 0.317\n",
      "test accuracy on epoch 2775: 0.769\n",
      "train loss on epoch 2776 : 0.082\n",
      "train accuracy on epoch 2776: 0.944\n",
      "test loss on epoch 2776: 0.320\n",
      "test accuracy on epoch 2776: 0.692\n",
      "train loss on epoch 2777 : 0.153\n",
      "train accuracy on epoch 2777: 0.889\n",
      "test loss on epoch 2777: 0.316\n",
      "test accuracy on epoch 2777: 0.846\n",
      "train loss on epoch 2778 : 0.266\n",
      "train accuracy on epoch 2778: 0.889\n",
      "test loss on epoch 2778: 0.310\n",
      "test accuracy on epoch 2778: 0.769\n",
      "train loss on epoch 2779 : 0.285\n",
      "train accuracy on epoch 2779: 0.889\n",
      "test loss on epoch 2779: 0.319\n",
      "test accuracy on epoch 2779: 0.769\n",
      "train loss on epoch 2780 : 0.095\n",
      "train accuracy on epoch 2780: 1.000\n",
      "test loss on epoch 2780: 0.329\n",
      "test accuracy on epoch 2780: 0.769\n",
      "train loss on epoch 2781 : 0.327\n",
      "train accuracy on epoch 2781: 0.944\n",
      "test loss on epoch 2781: 0.326\n",
      "test accuracy on epoch 2781: 0.769\n",
      "train loss on epoch 2782 : 0.179\n",
      "train accuracy on epoch 2782: 0.889\n",
      "test loss on epoch 2782: 0.328\n",
      "test accuracy on epoch 2782: 0.769\n",
      "train loss on epoch 2783 : 0.140\n",
      "train accuracy on epoch 2783: 0.889\n",
      "test loss on epoch 2783: 0.328\n",
      "test accuracy on epoch 2783: 0.769\n",
      "train loss on epoch 2784 : 0.070\n",
      "train accuracy on epoch 2784: 1.000\n",
      "test loss on epoch 2784: 0.328\n",
      "test accuracy on epoch 2784: 0.769\n",
      "train loss on epoch 2785 : 0.211\n",
      "train accuracy on epoch 2785: 0.889\n",
      "test loss on epoch 2785: 0.328\n",
      "test accuracy on epoch 2785: 0.769\n",
      "train loss on epoch 2786 : 0.179\n",
      "train accuracy on epoch 2786: 0.889\n",
      "test loss on epoch 2786: 0.322\n",
      "test accuracy on epoch 2786: 0.769\n",
      "train loss on epoch 2787 : 0.241\n",
      "train accuracy on epoch 2787: 0.944\n",
      "test loss on epoch 2787: 0.319\n",
      "test accuracy on epoch 2787: 0.769\n",
      "train loss on epoch 2788 : 0.059\n",
      "train accuracy on epoch 2788: 1.000\n",
      "test loss on epoch 2788: 0.320\n",
      "test accuracy on epoch 2788: 0.769\n",
      "train loss on epoch 2789 : 0.411\n",
      "train accuracy on epoch 2789: 0.833\n",
      "test loss on epoch 2789: 0.313\n",
      "test accuracy on epoch 2789: 0.769\n",
      "train loss on epoch 2790 : 0.155\n",
      "train accuracy on epoch 2790: 0.944\n",
      "test loss on epoch 2790: 0.311\n",
      "test accuracy on epoch 2790: 0.769\n",
      "train loss on epoch 2791 : 0.085\n",
      "train accuracy on epoch 2791: 0.944\n",
      "test loss on epoch 2791: 0.326\n",
      "test accuracy on epoch 2791: 0.769\n",
      "train loss on epoch 2792 : 0.335\n",
      "train accuracy on epoch 2792: 0.889\n",
      "test loss on epoch 2792: 0.324\n",
      "test accuracy on epoch 2792: 0.769\n",
      "train loss on epoch 2793 : 0.260\n",
      "train accuracy on epoch 2793: 0.833\n",
      "test loss on epoch 2793: 0.317\n",
      "test accuracy on epoch 2793: 0.769\n",
      "train loss on epoch 2794 : 0.213\n",
      "train accuracy on epoch 2794: 0.889\n",
      "test loss on epoch 2794: 0.329\n",
      "test accuracy on epoch 2794: 0.769\n",
      "train loss on epoch 2795 : 0.227\n",
      "train accuracy on epoch 2795: 0.944\n",
      "test loss on epoch 2795: 0.333\n",
      "test accuracy on epoch 2795: 0.769\n",
      "train loss on epoch 2796 : 0.340\n",
      "train accuracy on epoch 2796: 0.944\n",
      "test loss on epoch 2796: 0.346\n",
      "test accuracy on epoch 2796: 0.769\n",
      "train loss on epoch 2797 : 0.276\n",
      "train accuracy on epoch 2797: 0.944\n",
      "test loss on epoch 2797: 0.356\n",
      "test accuracy on epoch 2797: 0.769\n",
      "train loss on epoch 2798 : 0.122\n",
      "train accuracy on epoch 2798: 0.944\n",
      "test loss on epoch 2798: 0.358\n",
      "test accuracy on epoch 2798: 0.769\n",
      "train loss on epoch 2799 : 0.150\n",
      "train accuracy on epoch 2799: 0.944\n",
      "test loss on epoch 2799: 0.364\n",
      "test accuracy on epoch 2799: 0.769\n",
      "train loss on epoch 2800 : 0.146\n",
      "train accuracy on epoch 2800: 0.944\n",
      "test loss on epoch 2800: 0.359\n",
      "test accuracy on epoch 2800: 0.769\n",
      "train loss on epoch 2801 : 0.179\n",
      "train accuracy on epoch 2801: 0.944\n",
      "test loss on epoch 2801: 0.358\n",
      "test accuracy on epoch 2801: 0.769\n",
      "train loss on epoch 2802 : 0.075\n",
      "train accuracy on epoch 2802: 1.000\n",
      "test loss on epoch 2802: 0.360\n",
      "test accuracy on epoch 2802: 0.769\n",
      "train loss on epoch 2803 : 0.073\n",
      "train accuracy on epoch 2803: 1.000\n",
      "test loss on epoch 2803: 0.368\n",
      "test accuracy on epoch 2803: 0.769\n",
      "train loss on epoch 2804 : 0.103\n",
      "train accuracy on epoch 2804: 0.944\n",
      "test loss on epoch 2804: 0.358\n",
      "test accuracy on epoch 2804: 0.769\n",
      "train loss on epoch 2805 : 0.088\n",
      "train accuracy on epoch 2805: 0.944\n",
      "test loss on epoch 2805: 0.355\n",
      "test accuracy on epoch 2805: 0.769\n",
      "train loss on epoch 2806 : 0.143\n",
      "train accuracy on epoch 2806: 0.944\n",
      "test loss on epoch 2806: 0.347\n",
      "test accuracy on epoch 2806: 0.769\n",
      "train loss on epoch 2807 : 0.105\n",
      "train accuracy on epoch 2807: 0.944\n",
      "test loss on epoch 2807: 0.341\n",
      "test accuracy on epoch 2807: 0.769\n",
      "train loss on epoch 2808 : 0.223\n",
      "train accuracy on epoch 2808: 0.944\n",
      "test loss on epoch 2808: 0.333\n",
      "test accuracy on epoch 2808: 0.769\n",
      "train loss on epoch 2809 : 0.126\n",
      "train accuracy on epoch 2809: 0.944\n",
      "test loss on epoch 2809: 0.335\n",
      "test accuracy on epoch 2809: 0.769\n",
      "train loss on epoch 2810 : 0.207\n",
      "train accuracy on epoch 2810: 0.889\n",
      "test loss on epoch 2810: 0.331\n",
      "test accuracy on epoch 2810: 0.769\n",
      "train loss on epoch 2811 : 0.150\n",
      "train accuracy on epoch 2811: 0.944\n",
      "test loss on epoch 2811: 0.333\n",
      "test accuracy on epoch 2811: 0.769\n",
      "train loss on epoch 2812 : 0.138\n",
      "train accuracy on epoch 2812: 1.000\n",
      "test loss on epoch 2812: 0.337\n",
      "test accuracy on epoch 2812: 0.769\n",
      "train loss on epoch 2813 : 0.178\n",
      "train accuracy on epoch 2813: 0.889\n",
      "test loss on epoch 2813: 0.341\n",
      "test accuracy on epoch 2813: 0.769\n",
      "train loss on epoch 2814 : 0.254\n",
      "train accuracy on epoch 2814: 0.889\n",
      "test loss on epoch 2814: 0.335\n",
      "test accuracy on epoch 2814: 0.769\n",
      "train loss on epoch 2815 : 0.233\n",
      "train accuracy on epoch 2815: 0.889\n",
      "test loss on epoch 2815: 0.336\n",
      "test accuracy on epoch 2815: 0.769\n",
      "train loss on epoch 2816 : 0.080\n",
      "train accuracy on epoch 2816: 1.000\n",
      "test loss on epoch 2816: 0.332\n",
      "test accuracy on epoch 2816: 0.769\n",
      "train loss on epoch 2817 : 0.137\n",
      "train accuracy on epoch 2817: 0.889\n",
      "test loss on epoch 2817: 0.336\n",
      "test accuracy on epoch 2817: 0.769\n",
      "train loss on epoch 2818 : 0.108\n",
      "train accuracy on epoch 2818: 0.944\n",
      "test loss on epoch 2818: 0.333\n",
      "test accuracy on epoch 2818: 0.769\n",
      "train loss on epoch 2819 : 0.308\n",
      "train accuracy on epoch 2819: 0.889\n",
      "test loss on epoch 2819: 0.348\n",
      "test accuracy on epoch 2819: 0.769\n",
      "train loss on epoch 2820 : 0.094\n",
      "train accuracy on epoch 2820: 0.944\n",
      "test loss on epoch 2820: 0.346\n",
      "test accuracy on epoch 2820: 0.769\n",
      "train loss on epoch 2821 : 0.137\n",
      "train accuracy on epoch 2821: 0.944\n",
      "test loss on epoch 2821: 0.355\n",
      "test accuracy on epoch 2821: 0.769\n",
      "train loss on epoch 2822 : 0.167\n",
      "train accuracy on epoch 2822: 0.889\n",
      "test loss on epoch 2822: 0.357\n",
      "test accuracy on epoch 2822: 0.769\n",
      "train loss on epoch 2823 : 0.046\n",
      "train accuracy on epoch 2823: 1.000\n",
      "test loss on epoch 2823: 0.359\n",
      "test accuracy on epoch 2823: 0.769\n",
      "train loss on epoch 2824 : 0.276\n",
      "train accuracy on epoch 2824: 0.889\n",
      "test loss on epoch 2824: 0.358\n",
      "test accuracy on epoch 2824: 0.769\n",
      "train loss on epoch 2825 : 0.083\n",
      "train accuracy on epoch 2825: 0.944\n",
      "test loss on epoch 2825: 0.351\n",
      "test accuracy on epoch 2825: 0.769\n",
      "train loss on epoch 2826 : 0.436\n",
      "train accuracy on epoch 2826: 0.944\n",
      "test loss on epoch 2826: 0.351\n",
      "test accuracy on epoch 2826: 0.769\n",
      "train loss on epoch 2827 : 0.086\n",
      "train accuracy on epoch 2827: 0.944\n",
      "test loss on epoch 2827: 0.344\n",
      "test accuracy on epoch 2827: 0.769\n",
      "train loss on epoch 2828 : 0.180\n",
      "train accuracy on epoch 2828: 0.944\n",
      "test loss on epoch 2828: 0.355\n",
      "test accuracy on epoch 2828: 0.769\n",
      "train loss on epoch 2829 : 0.134\n",
      "train accuracy on epoch 2829: 0.889\n",
      "test loss on epoch 2829: 0.354\n",
      "test accuracy on epoch 2829: 0.769\n",
      "train loss on epoch 2830 : 0.105\n",
      "train accuracy on epoch 2830: 0.944\n",
      "test loss on epoch 2830: 0.337\n",
      "test accuracy on epoch 2830: 0.769\n",
      "train loss on epoch 2831 : 0.174\n",
      "train accuracy on epoch 2831: 0.889\n",
      "test loss on epoch 2831: 0.333\n",
      "test accuracy on epoch 2831: 0.769\n",
      "train loss on epoch 2832 : 0.098\n",
      "train accuracy on epoch 2832: 0.944\n",
      "test loss on epoch 2832: 0.341\n",
      "test accuracy on epoch 2832: 0.769\n",
      "train loss on epoch 2833 : 0.126\n",
      "train accuracy on epoch 2833: 0.944\n",
      "test loss on epoch 2833: 0.330\n",
      "test accuracy on epoch 2833: 0.769\n",
      "train loss on epoch 2834 : 0.188\n",
      "train accuracy on epoch 2834: 0.944\n",
      "test loss on epoch 2834: 0.342\n",
      "test accuracy on epoch 2834: 0.769\n",
      "train loss on epoch 2835 : 0.124\n",
      "train accuracy on epoch 2835: 0.944\n",
      "test loss on epoch 2835: 0.332\n",
      "test accuracy on epoch 2835: 0.769\n",
      "train loss on epoch 2836 : 0.131\n",
      "train accuracy on epoch 2836: 0.944\n",
      "test loss on epoch 2836: 0.332\n",
      "test accuracy on epoch 2836: 0.769\n",
      "train loss on epoch 2837 : 0.073\n",
      "train accuracy on epoch 2837: 1.000\n",
      "test loss on epoch 2837: 0.329\n",
      "test accuracy on epoch 2837: 0.769\n",
      "train loss on epoch 2838 : 0.123\n",
      "train accuracy on epoch 2838: 0.889\n",
      "test loss on epoch 2838: 0.349\n",
      "test accuracy on epoch 2838: 0.769\n",
      "train loss on epoch 2839 : 0.121\n",
      "train accuracy on epoch 2839: 1.000\n",
      "test loss on epoch 2839: 0.341\n",
      "test accuracy on epoch 2839: 0.769\n",
      "train loss on epoch 2840 : 0.199\n",
      "train accuracy on epoch 2840: 0.889\n",
      "test loss on epoch 2840: 0.343\n",
      "test accuracy on epoch 2840: 0.769\n",
      "train loss on epoch 2841 : 0.088\n",
      "train accuracy on epoch 2841: 1.000\n",
      "test loss on epoch 2841: 0.340\n",
      "test accuracy on epoch 2841: 0.769\n",
      "train loss on epoch 2842 : 0.314\n",
      "train accuracy on epoch 2842: 0.889\n",
      "test loss on epoch 2842: 0.347\n",
      "test accuracy on epoch 2842: 0.769\n",
      "train loss on epoch 2843 : 0.138\n",
      "train accuracy on epoch 2843: 0.944\n",
      "test loss on epoch 2843: 0.352\n",
      "test accuracy on epoch 2843: 0.769\n",
      "train loss on epoch 2844 : 0.345\n",
      "train accuracy on epoch 2844: 0.778\n",
      "test loss on epoch 2844: 0.349\n",
      "test accuracy on epoch 2844: 0.769\n",
      "train loss on epoch 2845 : 0.211\n",
      "train accuracy on epoch 2845: 0.944\n",
      "test loss on epoch 2845: 0.335\n",
      "test accuracy on epoch 2845: 0.769\n",
      "train loss on epoch 2846 : 0.079\n",
      "train accuracy on epoch 2846: 1.000\n",
      "test loss on epoch 2846: 0.338\n",
      "test accuracy on epoch 2846: 0.769\n",
      "train loss on epoch 2847 : 0.053\n",
      "train accuracy on epoch 2847: 1.000\n",
      "test loss on epoch 2847: 0.346\n",
      "test accuracy on epoch 2847: 0.769\n",
      "train loss on epoch 2848 : 0.060\n",
      "train accuracy on epoch 2848: 0.944\n",
      "test loss on epoch 2848: 0.335\n",
      "test accuracy on epoch 2848: 0.769\n",
      "train loss on epoch 2849 : 0.061\n",
      "train accuracy on epoch 2849: 1.000\n",
      "test loss on epoch 2849: 0.337\n",
      "test accuracy on epoch 2849: 0.769\n",
      "train loss on epoch 2850 : 0.288\n",
      "train accuracy on epoch 2850: 0.944\n",
      "test loss on epoch 2850: 0.346\n",
      "test accuracy on epoch 2850: 0.769\n",
      "train loss on epoch 2851 : 0.243\n",
      "train accuracy on epoch 2851: 0.889\n",
      "test loss on epoch 2851: 0.344\n",
      "test accuracy on epoch 2851: 0.769\n",
      "train loss on epoch 2852 : 0.123\n",
      "train accuracy on epoch 2852: 0.944\n",
      "test loss on epoch 2852: 0.342\n",
      "test accuracy on epoch 2852: 0.692\n",
      "train loss on epoch 2853 : 0.215\n",
      "train accuracy on epoch 2853: 0.944\n",
      "test loss on epoch 2853: 0.336\n",
      "test accuracy on epoch 2853: 0.769\n",
      "train loss on epoch 2854 : 0.212\n",
      "train accuracy on epoch 2854: 0.889\n",
      "test loss on epoch 2854: 0.342\n",
      "test accuracy on epoch 2854: 0.769\n",
      "train loss on epoch 2855 : 0.128\n",
      "train accuracy on epoch 2855: 0.944\n",
      "test loss on epoch 2855: 0.349\n",
      "test accuracy on epoch 2855: 0.769\n",
      "train loss on epoch 2856 : 0.036\n",
      "train accuracy on epoch 2856: 1.000\n",
      "test loss on epoch 2856: 0.365\n",
      "test accuracy on epoch 2856: 0.769\n",
      "train loss on epoch 2857 : 0.233\n",
      "train accuracy on epoch 2857: 0.889\n",
      "test loss on epoch 2857: 0.366\n",
      "test accuracy on epoch 2857: 0.769\n",
      "train loss on epoch 2858 : 0.282\n",
      "train accuracy on epoch 2858: 0.889\n",
      "test loss on epoch 2858: 0.357\n",
      "test accuracy on epoch 2858: 0.769\n",
      "train loss on epoch 2859 : 0.197\n",
      "train accuracy on epoch 2859: 0.889\n",
      "test loss on epoch 2859: 0.358\n",
      "test accuracy on epoch 2859: 0.769\n",
      "train loss on epoch 2860 : 0.097\n",
      "train accuracy on epoch 2860: 0.944\n",
      "test loss on epoch 2860: 0.346\n",
      "test accuracy on epoch 2860: 0.769\n",
      "train loss on epoch 2861 : 0.100\n",
      "train accuracy on epoch 2861: 0.944\n",
      "test loss on epoch 2861: 0.358\n",
      "test accuracy on epoch 2861: 0.769\n",
      "train loss on epoch 2862 : 0.162\n",
      "train accuracy on epoch 2862: 0.889\n",
      "test loss on epoch 2862: 0.358\n",
      "test accuracy on epoch 2862: 0.769\n",
      "train loss on epoch 2863 : 0.041\n",
      "train accuracy on epoch 2863: 1.000\n",
      "test loss on epoch 2863: 0.371\n",
      "test accuracy on epoch 2863: 0.769\n",
      "train loss on epoch 2864 : 0.375\n",
      "train accuracy on epoch 2864: 0.889\n",
      "test loss on epoch 2864: 0.372\n",
      "test accuracy on epoch 2864: 0.769\n",
      "train loss on epoch 2865 : 0.297\n",
      "train accuracy on epoch 2865: 0.889\n",
      "test loss on epoch 2865: 0.383\n",
      "test accuracy on epoch 2865: 0.769\n",
      "train loss on epoch 2866 : 0.129\n",
      "train accuracy on epoch 2866: 0.944\n",
      "test loss on epoch 2866: 0.382\n",
      "test accuracy on epoch 2866: 0.769\n",
      "train loss on epoch 2867 : 0.126\n",
      "train accuracy on epoch 2867: 0.944\n",
      "test loss on epoch 2867: 0.384\n",
      "test accuracy on epoch 2867: 0.769\n",
      "train loss on epoch 2868 : 0.206\n",
      "train accuracy on epoch 2868: 0.944\n",
      "test loss on epoch 2868: 0.363\n",
      "test accuracy on epoch 2868: 0.769\n",
      "train loss on epoch 2869 : 0.164\n",
      "train accuracy on epoch 2869: 0.889\n",
      "test loss on epoch 2869: 0.366\n",
      "test accuracy on epoch 2869: 0.769\n",
      "train loss on epoch 2870 : 0.092\n",
      "train accuracy on epoch 2870: 0.944\n",
      "test loss on epoch 2870: 0.355\n",
      "test accuracy on epoch 2870: 0.769\n",
      "train loss on epoch 2871 : 0.068\n",
      "train accuracy on epoch 2871: 1.000\n",
      "test loss on epoch 2871: 0.358\n",
      "test accuracy on epoch 2871: 0.769\n",
      "train loss on epoch 2872 : 0.135\n",
      "train accuracy on epoch 2872: 0.944\n",
      "test loss on epoch 2872: 0.363\n",
      "test accuracy on epoch 2872: 0.769\n",
      "train loss on epoch 2873 : 0.162\n",
      "train accuracy on epoch 2873: 0.944\n",
      "test loss on epoch 2873: 0.361\n",
      "test accuracy on epoch 2873: 0.769\n",
      "train loss on epoch 2874 : 0.072\n",
      "train accuracy on epoch 2874: 1.000\n",
      "test loss on epoch 2874: 0.351\n",
      "test accuracy on epoch 2874: 0.769\n",
      "train loss on epoch 2875 : 0.068\n",
      "train accuracy on epoch 2875: 1.000\n",
      "test loss on epoch 2875: 0.359\n",
      "test accuracy on epoch 2875: 0.769\n",
      "train loss on epoch 2876 : 0.185\n",
      "train accuracy on epoch 2876: 0.944\n",
      "test loss on epoch 2876: 0.350\n",
      "test accuracy on epoch 2876: 0.769\n",
      "train loss on epoch 2877 : 0.110\n",
      "train accuracy on epoch 2877: 0.944\n",
      "test loss on epoch 2877: 0.349\n",
      "test accuracy on epoch 2877: 0.769\n",
      "train loss on epoch 2878 : 0.145\n",
      "train accuracy on epoch 2878: 0.889\n",
      "test loss on epoch 2878: 0.342\n",
      "test accuracy on epoch 2878: 0.769\n",
      "train loss on epoch 2879 : 0.289\n",
      "train accuracy on epoch 2879: 0.889\n",
      "test loss on epoch 2879: 0.339\n",
      "test accuracy on epoch 2879: 0.769\n",
      "train loss on epoch 2880 : 0.124\n",
      "train accuracy on epoch 2880: 0.944\n",
      "test loss on epoch 2880: 0.346\n",
      "test accuracy on epoch 2880: 0.769\n",
      "train loss on epoch 2881 : 0.066\n",
      "train accuracy on epoch 2881: 0.944\n",
      "test loss on epoch 2881: 0.343\n",
      "test accuracy on epoch 2881: 0.769\n",
      "train loss on epoch 2882 : 0.185\n",
      "train accuracy on epoch 2882: 0.889\n",
      "test loss on epoch 2882: 0.347\n",
      "test accuracy on epoch 2882: 0.769\n",
      "train loss on epoch 2883 : 0.326\n",
      "train accuracy on epoch 2883: 0.889\n",
      "test loss on epoch 2883: 0.337\n",
      "test accuracy on epoch 2883: 0.769\n",
      "train loss on epoch 2884 : 0.078\n",
      "train accuracy on epoch 2884: 0.944\n",
      "test loss on epoch 2884: 0.348\n",
      "test accuracy on epoch 2884: 0.769\n",
      "train loss on epoch 2885 : 0.205\n",
      "train accuracy on epoch 2885: 0.833\n",
      "test loss on epoch 2885: 0.352\n",
      "test accuracy on epoch 2885: 0.769\n",
      "train loss on epoch 2886 : 0.169\n",
      "train accuracy on epoch 2886: 0.889\n",
      "test loss on epoch 2886: 0.356\n",
      "test accuracy on epoch 2886: 0.769\n",
      "train loss on epoch 2887 : 0.294\n",
      "train accuracy on epoch 2887: 0.944\n",
      "test loss on epoch 2887: 0.357\n",
      "test accuracy on epoch 2887: 0.769\n",
      "train loss on epoch 2888 : 0.136\n",
      "train accuracy on epoch 2888: 0.944\n",
      "test loss on epoch 2888: 0.352\n",
      "test accuracy on epoch 2888: 0.769\n",
      "train loss on epoch 2889 : 0.201\n",
      "train accuracy on epoch 2889: 0.889\n",
      "test loss on epoch 2889: 0.349\n",
      "test accuracy on epoch 2889: 0.769\n",
      "train loss on epoch 2890 : 0.150\n",
      "train accuracy on epoch 2890: 0.944\n",
      "test loss on epoch 2890: 0.330\n",
      "test accuracy on epoch 2890: 0.769\n",
      "train loss on epoch 2891 : 0.147\n",
      "train accuracy on epoch 2891: 0.944\n",
      "test loss on epoch 2891: 0.348\n",
      "test accuracy on epoch 2891: 0.769\n",
      "train loss on epoch 2892 : 0.257\n",
      "train accuracy on epoch 2892: 0.833\n",
      "test loss on epoch 2892: 0.348\n",
      "test accuracy on epoch 2892: 0.692\n",
      "train loss on epoch 2893 : 0.094\n",
      "train accuracy on epoch 2893: 1.000\n",
      "test loss on epoch 2893: 0.341\n",
      "test accuracy on epoch 2893: 0.769\n",
      "train loss on epoch 2894 : 0.441\n",
      "train accuracy on epoch 2894: 0.889\n",
      "test loss on epoch 2894: 0.336\n",
      "test accuracy on epoch 2894: 0.846\n",
      "train loss on epoch 2895 : 0.304\n",
      "train accuracy on epoch 2895: 0.833\n",
      "test loss on epoch 2895: 0.334\n",
      "test accuracy on epoch 2895: 0.769\n",
      "train loss on epoch 2896 : 0.053\n",
      "train accuracy on epoch 2896: 1.000\n",
      "test loss on epoch 2896: 0.345\n",
      "test accuracy on epoch 2896: 0.692\n",
      "train loss on epoch 2897 : 0.047\n",
      "train accuracy on epoch 2897: 1.000\n",
      "test loss on epoch 2897: 0.346\n",
      "test accuracy on epoch 2897: 0.692\n",
      "train loss on epoch 2898 : 0.121\n",
      "train accuracy on epoch 2898: 0.944\n",
      "test loss on epoch 2898: 0.332\n",
      "test accuracy on epoch 2898: 0.769\n",
      "train loss on epoch 2899 : 0.131\n",
      "train accuracy on epoch 2899: 0.944\n",
      "test loss on epoch 2899: 0.339\n",
      "test accuracy on epoch 2899: 0.769\n",
      "train loss on epoch 2900 : 0.192\n",
      "train accuracy on epoch 2900: 0.944\n",
      "test loss on epoch 2900: 0.340\n",
      "test accuracy on epoch 2900: 0.769\n",
      "train loss on epoch 2901 : 0.157\n",
      "train accuracy on epoch 2901: 0.889\n",
      "test loss on epoch 2901: 0.350\n",
      "test accuracy on epoch 2901: 0.769\n",
      "train loss on epoch 2902 : 0.065\n",
      "train accuracy on epoch 2902: 1.000\n",
      "test loss on epoch 2902: 0.346\n",
      "test accuracy on epoch 2902: 0.769\n",
      "train loss on epoch 2903 : 0.109\n",
      "train accuracy on epoch 2903: 0.944\n",
      "test loss on epoch 2903: 0.351\n",
      "test accuracy on epoch 2903: 0.769\n",
      "train loss on epoch 2904 : 0.215\n",
      "train accuracy on epoch 2904: 0.944\n",
      "test loss on epoch 2904: 0.354\n",
      "test accuracy on epoch 2904: 0.769\n",
      "train loss on epoch 2905 : 0.286\n",
      "train accuracy on epoch 2905: 0.833\n",
      "test loss on epoch 2905: 0.361\n",
      "test accuracy on epoch 2905: 0.769\n",
      "train loss on epoch 2906 : 0.291\n",
      "train accuracy on epoch 2906: 0.833\n",
      "test loss on epoch 2906: 0.365\n",
      "test accuracy on epoch 2906: 0.769\n",
      "train loss on epoch 2907 : 0.132\n",
      "train accuracy on epoch 2907: 0.889\n",
      "test loss on epoch 2907: 0.369\n",
      "test accuracy on epoch 2907: 0.769\n",
      "train loss on epoch 2908 : 0.093\n",
      "train accuracy on epoch 2908: 0.944\n",
      "test loss on epoch 2908: 0.369\n",
      "test accuracy on epoch 2908: 0.769\n",
      "train loss on epoch 2909 : 0.059\n",
      "train accuracy on epoch 2909: 1.000\n",
      "test loss on epoch 2909: 0.364\n",
      "test accuracy on epoch 2909: 0.769\n",
      "train loss on epoch 2910 : 0.151\n",
      "train accuracy on epoch 2910: 0.944\n",
      "test loss on epoch 2910: 0.355\n",
      "test accuracy on epoch 2910: 0.769\n",
      "train loss on epoch 2911 : 0.274\n",
      "train accuracy on epoch 2911: 0.889\n",
      "test loss on epoch 2911: 0.356\n",
      "test accuracy on epoch 2911: 0.769\n",
      "train loss on epoch 2912 : 0.150\n",
      "train accuracy on epoch 2912: 0.889\n",
      "test loss on epoch 2912: 0.353\n",
      "test accuracy on epoch 2912: 0.769\n",
      "train loss on epoch 2913 : 0.106\n",
      "train accuracy on epoch 2913: 0.944\n",
      "test loss on epoch 2913: 0.348\n",
      "test accuracy on epoch 2913: 0.769\n",
      "train loss on epoch 2914 : 0.072\n",
      "train accuracy on epoch 2914: 0.944\n",
      "test loss on epoch 2914: 0.347\n",
      "test accuracy on epoch 2914: 0.769\n",
      "train loss on epoch 2915 : 0.107\n",
      "train accuracy on epoch 2915: 0.944\n",
      "test loss on epoch 2915: 0.343\n",
      "test accuracy on epoch 2915: 0.769\n",
      "train loss on epoch 2916 : 0.201\n",
      "train accuracy on epoch 2916: 0.889\n",
      "test loss on epoch 2916: 0.341\n",
      "test accuracy on epoch 2916: 0.769\n",
      "train loss on epoch 2917 : 0.293\n",
      "train accuracy on epoch 2917: 0.889\n",
      "test loss on epoch 2917: 0.335\n",
      "test accuracy on epoch 2917: 0.769\n",
      "train loss on epoch 2918 : 0.189\n",
      "train accuracy on epoch 2918: 0.833\n",
      "test loss on epoch 2918: 0.332\n",
      "test accuracy on epoch 2918: 0.769\n",
      "train loss on epoch 2919 : 0.139\n",
      "train accuracy on epoch 2919: 0.944\n",
      "test loss on epoch 2919: 0.327\n",
      "test accuracy on epoch 2919: 0.769\n",
      "train loss on epoch 2920 : 0.080\n",
      "train accuracy on epoch 2920: 0.944\n",
      "test loss on epoch 2920: 0.323\n",
      "test accuracy on epoch 2920: 0.769\n",
      "train loss on epoch 2921 : 0.068\n",
      "train accuracy on epoch 2921: 1.000\n",
      "test loss on epoch 2921: 0.332\n",
      "test accuracy on epoch 2921: 0.769\n",
      "train loss on epoch 2922 : 0.167\n",
      "train accuracy on epoch 2922: 0.944\n",
      "test loss on epoch 2922: 0.331\n",
      "test accuracy on epoch 2922: 0.769\n",
      "train loss on epoch 2923 : 0.173\n",
      "train accuracy on epoch 2923: 0.889\n",
      "test loss on epoch 2923: 0.325\n",
      "test accuracy on epoch 2923: 0.846\n",
      "train loss on epoch 2924 : 0.197\n",
      "train accuracy on epoch 2924: 0.944\n",
      "test loss on epoch 2924: 0.330\n",
      "test accuracy on epoch 2924: 0.769\n",
      "train loss on epoch 2925 : 0.098\n",
      "train accuracy on epoch 2925: 0.944\n",
      "test loss on epoch 2925: 0.326\n",
      "test accuracy on epoch 2925: 0.769\n",
      "train loss on epoch 2926 : 0.480\n",
      "train accuracy on epoch 2926: 0.833\n",
      "test loss on epoch 2926: 0.336\n",
      "test accuracy on epoch 2926: 0.769\n",
      "train loss on epoch 2927 : 0.060\n",
      "train accuracy on epoch 2927: 1.000\n",
      "test loss on epoch 2927: 0.325\n",
      "test accuracy on epoch 2927: 0.769\n",
      "train loss on epoch 2928 : 0.236\n",
      "train accuracy on epoch 2928: 0.944\n",
      "test loss on epoch 2928: 0.328\n",
      "test accuracy on epoch 2928: 0.769\n",
      "train loss on epoch 2929 : 0.306\n",
      "train accuracy on epoch 2929: 0.833\n",
      "test loss on epoch 2929: 0.327\n",
      "test accuracy on epoch 2929: 0.769\n",
      "train loss on epoch 2930 : 0.235\n",
      "train accuracy on epoch 2930: 0.944\n",
      "test loss on epoch 2930: 0.335\n",
      "test accuracy on epoch 2930: 0.769\n",
      "train loss on epoch 2931 : 0.065\n",
      "train accuracy on epoch 2931: 1.000\n",
      "test loss on epoch 2931: 0.335\n",
      "test accuracy on epoch 2931: 0.769\n",
      "train loss on epoch 2932 : 0.191\n",
      "train accuracy on epoch 2932: 0.889\n",
      "test loss on epoch 2932: 0.335\n",
      "test accuracy on epoch 2932: 0.769\n",
      "train loss on epoch 2933 : 0.107\n",
      "train accuracy on epoch 2933: 0.944\n",
      "test loss on epoch 2933: 0.339\n",
      "test accuracy on epoch 2933: 0.769\n",
      "train loss on epoch 2934 : 0.221\n",
      "train accuracy on epoch 2934: 0.889\n",
      "test loss on epoch 2934: 0.336\n",
      "test accuracy on epoch 2934: 0.769\n",
      "train loss on epoch 2935 : 0.164\n",
      "train accuracy on epoch 2935: 0.889\n",
      "test loss on epoch 2935: 0.336\n",
      "test accuracy on epoch 2935: 0.769\n",
      "train loss on epoch 2936 : 0.110\n",
      "train accuracy on epoch 2936: 1.000\n",
      "test loss on epoch 2936: 0.337\n",
      "test accuracy on epoch 2936: 0.769\n",
      "train loss on epoch 2937 : 0.198\n",
      "train accuracy on epoch 2937: 0.889\n",
      "test loss on epoch 2937: 0.336\n",
      "test accuracy on epoch 2937: 0.769\n",
      "train loss on epoch 2938 : 0.194\n",
      "train accuracy on epoch 2938: 0.944\n",
      "test loss on epoch 2938: 0.340\n",
      "test accuracy on epoch 2938: 0.769\n",
      "train loss on epoch 2939 : 0.095\n",
      "train accuracy on epoch 2939: 0.944\n",
      "test loss on epoch 2939: 0.340\n",
      "test accuracy on epoch 2939: 0.769\n",
      "train loss on epoch 2940 : 0.071\n",
      "train accuracy on epoch 2940: 1.000\n",
      "test loss on epoch 2940: 0.348\n",
      "test accuracy on epoch 2940: 0.769\n",
      "train loss on epoch 2941 : 0.193\n",
      "train accuracy on epoch 2941: 0.889\n",
      "test loss on epoch 2941: 0.355\n",
      "test accuracy on epoch 2941: 0.769\n",
      "train loss on epoch 2942 : 0.378\n",
      "train accuracy on epoch 2942: 0.889\n",
      "test loss on epoch 2942: 0.365\n",
      "test accuracy on epoch 2942: 0.769\n",
      "train loss on epoch 2943 : 0.154\n",
      "train accuracy on epoch 2943: 0.889\n",
      "test loss on epoch 2943: 0.376\n",
      "test accuracy on epoch 2943: 0.769\n",
      "train loss on epoch 2944 : 0.052\n",
      "train accuracy on epoch 2944: 1.000\n",
      "test loss on epoch 2944: 0.377\n",
      "test accuracy on epoch 2944: 0.769\n",
      "train loss on epoch 2945 : 0.133\n",
      "train accuracy on epoch 2945: 0.889\n",
      "test loss on epoch 2945: 0.375\n",
      "test accuracy on epoch 2945: 0.769\n",
      "train loss on epoch 2946 : 0.285\n",
      "train accuracy on epoch 2946: 0.889\n",
      "test loss on epoch 2946: 0.372\n",
      "test accuracy on epoch 2946: 0.769\n",
      "train loss on epoch 2947 : 0.406\n",
      "train accuracy on epoch 2947: 0.889\n",
      "test loss on epoch 2947: 0.363\n",
      "test accuracy on epoch 2947: 0.769\n",
      "train loss on epoch 2948 : 0.135\n",
      "train accuracy on epoch 2948: 0.944\n",
      "test loss on epoch 2948: 0.355\n",
      "test accuracy on epoch 2948: 0.769\n",
      "train loss on epoch 2949 : 0.334\n",
      "train accuracy on epoch 2949: 0.889\n",
      "test loss on epoch 2949: 0.355\n",
      "test accuracy on epoch 2949: 0.769\n",
      "train loss on epoch 2950 : 0.220\n",
      "train accuracy on epoch 2950: 0.889\n",
      "test loss on epoch 2950: 0.355\n",
      "test accuracy on epoch 2950: 0.769\n",
      "train loss on epoch 2951 : 0.182\n",
      "train accuracy on epoch 2951: 0.944\n",
      "test loss on epoch 2951: 0.361\n",
      "test accuracy on epoch 2951: 0.769\n",
      "train loss on epoch 2952 : 0.052\n",
      "train accuracy on epoch 2952: 1.000\n",
      "test loss on epoch 2952: 0.361\n",
      "test accuracy on epoch 2952: 0.769\n",
      "train loss on epoch 2953 : 0.064\n",
      "train accuracy on epoch 2953: 1.000\n",
      "test loss on epoch 2953: 0.360\n",
      "test accuracy on epoch 2953: 0.769\n",
      "train loss on epoch 2954 : 0.128\n",
      "train accuracy on epoch 2954: 0.944\n",
      "test loss on epoch 2954: 0.356\n",
      "test accuracy on epoch 2954: 0.769\n",
      "train loss on epoch 2955 : 0.277\n",
      "train accuracy on epoch 2955: 0.833\n",
      "test loss on epoch 2955: 0.361\n",
      "test accuracy on epoch 2955: 0.769\n",
      "train loss on epoch 2956 : 0.073\n",
      "train accuracy on epoch 2956: 1.000\n",
      "test loss on epoch 2956: 0.360\n",
      "test accuracy on epoch 2956: 0.769\n",
      "train loss on epoch 2957 : 0.340\n",
      "train accuracy on epoch 2957: 0.889\n",
      "test loss on epoch 2957: 0.361\n",
      "test accuracy on epoch 2957: 0.769\n",
      "train loss on epoch 2958 : 0.364\n",
      "train accuracy on epoch 2958: 0.889\n",
      "test loss on epoch 2958: 0.368\n",
      "test accuracy on epoch 2958: 0.769\n",
      "train loss on epoch 2959 : 0.226\n",
      "train accuracy on epoch 2959: 0.833\n",
      "test loss on epoch 2959: 0.372\n",
      "test accuracy on epoch 2959: 0.769\n",
      "train loss on epoch 2960 : 0.068\n",
      "train accuracy on epoch 2960: 1.000\n",
      "test loss on epoch 2960: 0.367\n",
      "test accuracy on epoch 2960: 0.769\n",
      "train loss on epoch 2961 : 0.139\n",
      "train accuracy on epoch 2961: 0.944\n",
      "test loss on epoch 2961: 0.370\n",
      "test accuracy on epoch 2961: 0.769\n",
      "train loss on epoch 2962 : 0.202\n",
      "train accuracy on epoch 2962: 0.944\n",
      "test loss on epoch 2962: 0.369\n",
      "test accuracy on epoch 2962: 0.769\n",
      "train loss on epoch 2963 : 0.147\n",
      "train accuracy on epoch 2963: 0.889\n",
      "test loss on epoch 2963: 0.369\n",
      "test accuracy on epoch 2963: 0.769\n",
      "train loss on epoch 2964 : 0.110\n",
      "train accuracy on epoch 2964: 1.000\n",
      "test loss on epoch 2964: 0.368\n",
      "test accuracy on epoch 2964: 0.769\n",
      "train loss on epoch 2965 : 0.042\n",
      "train accuracy on epoch 2965: 1.000\n",
      "test loss on epoch 2965: 0.368\n",
      "test accuracy on epoch 2965: 0.769\n",
      "train loss on epoch 2966 : 0.071\n",
      "train accuracy on epoch 2966: 1.000\n",
      "test loss on epoch 2966: 0.370\n",
      "test accuracy on epoch 2966: 0.769\n",
      "train loss on epoch 2967 : 0.097\n",
      "train accuracy on epoch 2967: 0.944\n",
      "test loss on epoch 2967: 0.375\n",
      "test accuracy on epoch 2967: 0.769\n",
      "train loss on epoch 2968 : 0.399\n",
      "train accuracy on epoch 2968: 0.833\n",
      "test loss on epoch 2968: 0.385\n",
      "test accuracy on epoch 2968: 0.769\n",
      "train loss on epoch 2969 : 0.077\n",
      "train accuracy on epoch 2969: 0.944\n",
      "test loss on epoch 2969: 0.397\n",
      "test accuracy on epoch 2969: 0.769\n",
      "train loss on epoch 2970 : 0.221\n",
      "train accuracy on epoch 2970: 0.889\n",
      "test loss on epoch 2970: 0.402\n",
      "test accuracy on epoch 2970: 0.769\n",
      "train loss on epoch 2971 : 0.346\n",
      "train accuracy on epoch 2971: 0.889\n",
      "test loss on epoch 2971: 0.398\n",
      "test accuracy on epoch 2971: 0.769\n",
      "train loss on epoch 2972 : 0.105\n",
      "train accuracy on epoch 2972: 0.944\n",
      "test loss on epoch 2972: 0.391\n",
      "test accuracy on epoch 2972: 0.769\n",
      "train loss on epoch 2973 : 0.234\n",
      "train accuracy on epoch 2973: 0.889\n",
      "test loss on epoch 2973: 0.382\n",
      "test accuracy on epoch 2973: 0.769\n",
      "train loss on epoch 2974 : 0.180\n",
      "train accuracy on epoch 2974: 0.889\n",
      "test loss on epoch 2974: 0.375\n",
      "test accuracy on epoch 2974: 0.769\n",
      "train loss on epoch 2975 : 0.265\n",
      "train accuracy on epoch 2975: 0.889\n",
      "test loss on epoch 2975: 0.370\n",
      "test accuracy on epoch 2975: 0.769\n",
      "train loss on epoch 2976 : 0.472\n",
      "train accuracy on epoch 2976: 0.833\n",
      "test loss on epoch 2976: 0.376\n",
      "test accuracy on epoch 2976: 0.769\n",
      "train loss on epoch 2977 : 0.211\n",
      "train accuracy on epoch 2977: 0.889\n",
      "test loss on epoch 2977: 0.371\n",
      "test accuracy on epoch 2977: 0.769\n",
      "train loss on epoch 2978 : 0.172\n",
      "train accuracy on epoch 2978: 0.889\n",
      "test loss on epoch 2978: 0.368\n",
      "test accuracy on epoch 2978: 0.769\n",
      "train loss on epoch 2979 : 0.279\n",
      "train accuracy on epoch 2979: 0.889\n",
      "test loss on epoch 2979: 0.364\n",
      "test accuracy on epoch 2979: 0.769\n",
      "train loss on epoch 2980 : 0.098\n",
      "train accuracy on epoch 2980: 1.000\n",
      "test loss on epoch 2980: 0.356\n",
      "test accuracy on epoch 2980: 0.769\n",
      "train loss on epoch 2981 : 0.105\n",
      "train accuracy on epoch 2981: 0.944\n",
      "test loss on epoch 2981: 0.365\n",
      "test accuracy on epoch 2981: 0.769\n",
      "train loss on epoch 2982 : 0.256\n",
      "train accuracy on epoch 2982: 0.833\n",
      "test loss on epoch 2982: 0.361\n",
      "test accuracy on epoch 2982: 0.769\n",
      "train loss on epoch 2983 : 0.024\n",
      "train accuracy on epoch 2983: 1.000\n",
      "test loss on epoch 2983: 0.357\n",
      "test accuracy on epoch 2983: 0.769\n",
      "train loss on epoch 2984 : 0.135\n",
      "train accuracy on epoch 2984: 0.944\n",
      "test loss on epoch 2984: 0.353\n",
      "test accuracy on epoch 2984: 0.769\n",
      "train loss on epoch 2985 : 0.050\n",
      "train accuracy on epoch 2985: 1.000\n",
      "test loss on epoch 2985: 0.353\n",
      "test accuracy on epoch 2985: 0.769\n",
      "train loss on epoch 2986 : 0.266\n",
      "train accuracy on epoch 2986: 0.889\n",
      "test loss on epoch 2986: 0.348\n",
      "test accuracy on epoch 2986: 0.769\n",
      "train loss on epoch 2987 : 0.094\n",
      "train accuracy on epoch 2987: 1.000\n",
      "test loss on epoch 2987: 0.364\n",
      "test accuracy on epoch 2987: 0.769\n",
      "train loss on epoch 2988 : 0.153\n",
      "train accuracy on epoch 2988: 1.000\n",
      "test loss on epoch 2988: 0.351\n",
      "test accuracy on epoch 2988: 0.769\n",
      "train loss on epoch 2989 : 0.278\n",
      "train accuracy on epoch 2989: 0.944\n",
      "test loss on epoch 2989: 0.361\n",
      "test accuracy on epoch 2989: 0.769\n",
      "train loss on epoch 2990 : 0.210\n",
      "train accuracy on epoch 2990: 0.944\n",
      "test loss on epoch 2990: 0.346\n",
      "test accuracy on epoch 2990: 0.769\n",
      "train loss on epoch 2991 : 0.147\n",
      "train accuracy on epoch 2991: 0.944\n",
      "test loss on epoch 2991: 0.360\n",
      "test accuracy on epoch 2991: 0.769\n",
      "train loss on epoch 2992 : 0.344\n",
      "train accuracy on epoch 2992: 0.944\n",
      "test loss on epoch 2992: 0.356\n",
      "test accuracy on epoch 2992: 0.769\n",
      "train loss on epoch 2993 : 0.187\n",
      "train accuracy on epoch 2993: 0.889\n",
      "test loss on epoch 2993: 0.364\n",
      "test accuracy on epoch 2993: 0.769\n",
      "train loss on epoch 2994 : 0.353\n",
      "train accuracy on epoch 2994: 0.889\n",
      "test loss on epoch 2994: 0.357\n",
      "test accuracy on epoch 2994: 0.769\n",
      "train loss on epoch 2995 : 0.193\n",
      "train accuracy on epoch 2995: 0.944\n",
      "test loss on epoch 2995: 0.359\n",
      "test accuracy on epoch 2995: 0.769\n",
      "train loss on epoch 2996 : 0.429\n",
      "train accuracy on epoch 2996: 0.833\n",
      "test loss on epoch 2996: 0.365\n",
      "test accuracy on epoch 2996: 0.769\n",
      "train loss on epoch 2997 : 0.208\n",
      "train accuracy on epoch 2997: 0.944\n",
      "test loss on epoch 2997: 0.358\n",
      "test accuracy on epoch 2997: 0.769\n",
      "train loss on epoch 2998 : 0.426\n",
      "train accuracy on epoch 2998: 0.833\n",
      "test loss on epoch 2998: 0.336\n",
      "test accuracy on epoch 2998: 0.769\n",
      "train loss on epoch 2999 : 0.261\n",
      "train accuracy on epoch 2999: 0.889\n",
      "test loss on epoch 2999: 0.357\n",
      "test accuracy on epoch 2999: 0.692\n",
      "train loss on epoch 3000 : 0.337\n",
      "train accuracy on epoch 3000: 0.889\n",
      "test loss on epoch 3000: 0.338\n",
      "test accuracy on epoch 3000: 0.846\n",
      "train loss on epoch 3001 : 0.060\n",
      "train accuracy on epoch 3001: 1.000\n",
      "test loss on epoch 3001: 0.357\n",
      "test accuracy on epoch 3001: 0.692\n",
      "train loss on epoch 3002 : 0.216\n",
      "train accuracy on epoch 3002: 0.944\n",
      "test loss on epoch 3002: 0.339\n",
      "test accuracy on epoch 3002: 0.769\n",
      "train loss on epoch 3003 : 0.235\n",
      "train accuracy on epoch 3003: 0.889\n",
      "test loss on epoch 3003: 0.364\n",
      "test accuracy on epoch 3003: 0.769\n",
      "train loss on epoch 3004 : 0.182\n",
      "train accuracy on epoch 3004: 0.944\n",
      "test loss on epoch 3004: 0.354\n",
      "test accuracy on epoch 3004: 0.769\n",
      "train loss on epoch 3005 : 0.204\n",
      "train accuracy on epoch 3005: 0.889\n",
      "test loss on epoch 3005: 0.378\n",
      "test accuracy on epoch 3005: 0.769\n",
      "train loss on epoch 3006 : 0.155\n",
      "train accuracy on epoch 3006: 0.944\n",
      "test loss on epoch 3006: 0.368\n",
      "test accuracy on epoch 3006: 0.769\n",
      "train loss on epoch 3007 : 0.066\n",
      "train accuracy on epoch 3007: 0.944\n",
      "test loss on epoch 3007: 0.373\n",
      "test accuracy on epoch 3007: 0.769\n",
      "train loss on epoch 3008 : 0.206\n",
      "train accuracy on epoch 3008: 0.944\n",
      "test loss on epoch 3008: 0.373\n",
      "test accuracy on epoch 3008: 0.769\n",
      "train loss on epoch 3009 : 0.219\n",
      "train accuracy on epoch 3009: 0.944\n",
      "test loss on epoch 3009: 0.386\n",
      "test accuracy on epoch 3009: 0.769\n",
      "train loss on epoch 3010 : 0.249\n",
      "train accuracy on epoch 3010: 0.833\n",
      "test loss on epoch 3010: 0.390\n",
      "test accuracy on epoch 3010: 0.769\n",
      "train loss on epoch 3011 : 0.044\n",
      "train accuracy on epoch 3011: 1.000\n",
      "test loss on epoch 3011: 0.384\n",
      "test accuracy on epoch 3011: 0.769\n",
      "train loss on epoch 3012 : 0.074\n",
      "train accuracy on epoch 3012: 0.944\n",
      "test loss on epoch 3012: 0.383\n",
      "test accuracy on epoch 3012: 0.769\n",
      "train loss on epoch 3013 : 0.076\n",
      "train accuracy on epoch 3013: 1.000\n",
      "test loss on epoch 3013: 0.379\n",
      "test accuracy on epoch 3013: 0.769\n",
      "train loss on epoch 3014 : 0.298\n",
      "train accuracy on epoch 3014: 0.889\n",
      "test loss on epoch 3014: 0.379\n",
      "test accuracy on epoch 3014: 0.769\n",
      "train loss on epoch 3015 : 0.272\n",
      "train accuracy on epoch 3015: 0.889\n",
      "test loss on epoch 3015: 0.376\n",
      "test accuracy on epoch 3015: 0.769\n",
      "train loss on epoch 3016 : 0.314\n",
      "train accuracy on epoch 3016: 0.944\n",
      "test loss on epoch 3016: 0.359\n",
      "test accuracy on epoch 3016: 0.769\n",
      "train loss on epoch 3017 : 0.213\n",
      "train accuracy on epoch 3017: 0.944\n",
      "test loss on epoch 3017: 0.357\n",
      "test accuracy on epoch 3017: 0.769\n",
      "train loss on epoch 3018 : 0.184\n",
      "train accuracy on epoch 3018: 0.944\n",
      "test loss on epoch 3018: 0.357\n",
      "test accuracy on epoch 3018: 0.769\n",
      "train loss on epoch 3019 : 0.199\n",
      "train accuracy on epoch 3019: 0.944\n",
      "test loss on epoch 3019: 0.356\n",
      "test accuracy on epoch 3019: 0.769\n",
      "train loss on epoch 3020 : 0.236\n",
      "train accuracy on epoch 3020: 0.833\n",
      "test loss on epoch 3020: 0.365\n",
      "test accuracy on epoch 3020: 0.769\n",
      "train loss on epoch 3021 : 0.183\n",
      "train accuracy on epoch 3021: 0.889\n",
      "test loss on epoch 3021: 0.362\n",
      "test accuracy on epoch 3021: 0.769\n",
      "train loss on epoch 3022 : 0.126\n",
      "train accuracy on epoch 3022: 1.000\n",
      "test loss on epoch 3022: 0.365\n",
      "test accuracy on epoch 3022: 0.769\n",
      "train loss on epoch 3023 : 0.084\n",
      "train accuracy on epoch 3023: 0.944\n",
      "test loss on epoch 3023: 0.360\n",
      "test accuracy on epoch 3023: 0.769\n",
      "train loss on epoch 3024 : 0.103\n",
      "train accuracy on epoch 3024: 1.000\n",
      "test loss on epoch 3024: 0.355\n",
      "test accuracy on epoch 3024: 0.769\n",
      "train loss on epoch 3025 : 0.190\n",
      "train accuracy on epoch 3025: 0.889\n",
      "test loss on epoch 3025: 0.361\n",
      "test accuracy on epoch 3025: 0.769\n",
      "train loss on epoch 3026 : 0.100\n",
      "train accuracy on epoch 3026: 0.944\n",
      "test loss on epoch 3026: 0.350\n",
      "test accuracy on epoch 3026: 0.846\n",
      "train loss on epoch 3027 : 0.162\n",
      "train accuracy on epoch 3027: 0.889\n",
      "test loss on epoch 3027: 0.362\n",
      "test accuracy on epoch 3027: 0.769\n",
      "train loss on epoch 3028 : 0.129\n",
      "train accuracy on epoch 3028: 0.944\n",
      "test loss on epoch 3028: 0.371\n",
      "test accuracy on epoch 3028: 0.692\n",
      "train loss on epoch 3029 : 0.090\n",
      "train accuracy on epoch 3029: 0.944\n",
      "test loss on epoch 3029: 0.363\n",
      "test accuracy on epoch 3029: 0.769\n",
      "train loss on epoch 3030 : 0.137\n",
      "train accuracy on epoch 3030: 0.889\n",
      "test loss on epoch 3030: 0.353\n",
      "test accuracy on epoch 3030: 0.846\n",
      "train loss on epoch 3031 : 0.112\n",
      "train accuracy on epoch 3031: 0.944\n",
      "test loss on epoch 3031: 0.354\n",
      "test accuracy on epoch 3031: 0.769\n",
      "train loss on epoch 3032 : 0.135\n",
      "train accuracy on epoch 3032: 0.944\n",
      "test loss on epoch 3032: 0.352\n",
      "test accuracy on epoch 3032: 0.846\n",
      "train loss on epoch 3033 : 0.086\n",
      "train accuracy on epoch 3033: 1.000\n",
      "test loss on epoch 3033: 0.366\n",
      "test accuracy on epoch 3033: 0.692\n",
      "train loss on epoch 3034 : 0.165\n",
      "train accuracy on epoch 3034: 0.889\n",
      "test loss on epoch 3034: 0.356\n",
      "test accuracy on epoch 3034: 0.769\n",
      "train loss on epoch 3035 : 0.183\n",
      "train accuracy on epoch 3035: 0.889\n",
      "test loss on epoch 3035: 0.357\n",
      "test accuracy on epoch 3035: 0.769\n",
      "train loss on epoch 3036 : 0.173\n",
      "train accuracy on epoch 3036: 0.833\n",
      "test loss on epoch 3036: 0.370\n",
      "test accuracy on epoch 3036: 0.769\n",
      "train loss on epoch 3037 : 0.079\n",
      "train accuracy on epoch 3037: 1.000\n",
      "test loss on epoch 3037: 0.373\n",
      "test accuracy on epoch 3037: 0.769\n",
      "train loss on epoch 3038 : 0.179\n",
      "train accuracy on epoch 3038: 0.944\n",
      "test loss on epoch 3038: 0.351\n",
      "test accuracy on epoch 3038: 0.769\n",
      "train loss on epoch 3039 : 0.118\n",
      "train accuracy on epoch 3039: 0.944\n",
      "test loss on epoch 3039: 0.369\n",
      "test accuracy on epoch 3039: 0.692\n",
      "train loss on epoch 3040 : 0.166\n",
      "train accuracy on epoch 3040: 0.889\n",
      "test loss on epoch 3040: 0.357\n",
      "test accuracy on epoch 3040: 0.846\n",
      "train loss on epoch 3041 : 0.277\n",
      "train accuracy on epoch 3041: 0.889\n",
      "test loss on epoch 3041: 0.368\n",
      "test accuracy on epoch 3041: 0.769\n",
      "train loss on epoch 3042 : 0.228\n",
      "train accuracy on epoch 3042: 0.944\n",
      "test loss on epoch 3042: 0.374\n",
      "test accuracy on epoch 3042: 0.769\n",
      "train loss on epoch 3043 : 0.322\n",
      "train accuracy on epoch 3043: 0.889\n",
      "test loss on epoch 3043: 0.374\n",
      "test accuracy on epoch 3043: 0.692\n",
      "train loss on epoch 3044 : 0.325\n",
      "train accuracy on epoch 3044: 0.889\n",
      "test loss on epoch 3044: 0.359\n",
      "test accuracy on epoch 3044: 0.846\n",
      "train loss on epoch 3045 : 0.140\n",
      "train accuracy on epoch 3045: 0.944\n",
      "test loss on epoch 3045: 0.354\n",
      "test accuracy on epoch 3045: 0.846\n",
      "train loss on epoch 3046 : 0.265\n",
      "train accuracy on epoch 3046: 0.889\n",
      "test loss on epoch 3046: 0.368\n",
      "test accuracy on epoch 3046: 0.692\n",
      "train loss on epoch 3047 : 0.420\n",
      "train accuracy on epoch 3047: 0.833\n",
      "test loss on epoch 3047: 0.369\n",
      "test accuracy on epoch 3047: 0.692\n",
      "train loss on epoch 3048 : 0.211\n",
      "train accuracy on epoch 3048: 0.944\n",
      "test loss on epoch 3048: 0.368\n",
      "test accuracy on epoch 3048: 0.692\n",
      "train loss on epoch 3049 : 0.116\n",
      "train accuracy on epoch 3049: 1.000\n",
      "test loss on epoch 3049: 0.369\n",
      "test accuracy on epoch 3049: 0.692\n",
      "train loss on epoch 3050 : 0.259\n",
      "train accuracy on epoch 3050: 0.889\n",
      "test loss on epoch 3050: 0.366\n",
      "test accuracy on epoch 3050: 0.692\n",
      "train loss on epoch 3051 : 0.325\n",
      "train accuracy on epoch 3051: 0.944\n",
      "test loss on epoch 3051: 0.363\n",
      "test accuracy on epoch 3051: 0.769\n",
      "train loss on epoch 3052 : 0.240\n",
      "train accuracy on epoch 3052: 0.889\n",
      "test loss on epoch 3052: 0.361\n",
      "test accuracy on epoch 3052: 0.769\n",
      "train loss on epoch 3053 : 0.110\n",
      "train accuracy on epoch 3053: 0.944\n",
      "test loss on epoch 3053: 0.357\n",
      "test accuracy on epoch 3053: 0.769\n",
      "train loss on epoch 3054 : 0.110\n",
      "train accuracy on epoch 3054: 0.944\n",
      "test loss on epoch 3054: 0.360\n",
      "test accuracy on epoch 3054: 0.769\n",
      "train loss on epoch 3055 : 0.196\n",
      "train accuracy on epoch 3055: 0.944\n",
      "test loss on epoch 3055: 0.359\n",
      "test accuracy on epoch 3055: 0.769\n",
      "train loss on epoch 3056 : 0.141\n",
      "train accuracy on epoch 3056: 0.944\n",
      "test loss on epoch 3056: 0.366\n",
      "test accuracy on epoch 3056: 0.769\n",
      "train loss on epoch 3057 : 0.352\n",
      "train accuracy on epoch 3057: 0.889\n",
      "test loss on epoch 3057: 0.370\n",
      "test accuracy on epoch 3057: 0.769\n",
      "train loss on epoch 3058 : 0.184\n",
      "train accuracy on epoch 3058: 0.889\n",
      "test loss on epoch 3058: 0.373\n",
      "test accuracy on epoch 3058: 0.769\n",
      "train loss on epoch 3059 : 0.115\n",
      "train accuracy on epoch 3059: 0.944\n",
      "test loss on epoch 3059: 0.375\n",
      "test accuracy on epoch 3059: 0.769\n",
      "train loss on epoch 3060 : 0.090\n",
      "train accuracy on epoch 3060: 1.000\n",
      "test loss on epoch 3060: 0.379\n",
      "test accuracy on epoch 3060: 0.769\n",
      "train loss on epoch 3061 : 0.152\n",
      "train accuracy on epoch 3061: 0.944\n",
      "test loss on epoch 3061: 0.383\n",
      "test accuracy on epoch 3061: 0.769\n",
      "train loss on epoch 3062 : 0.082\n",
      "train accuracy on epoch 3062: 1.000\n",
      "test loss on epoch 3062: 0.387\n",
      "test accuracy on epoch 3062: 0.769\n",
      "train loss on epoch 3063 : 0.021\n",
      "train accuracy on epoch 3063: 1.000\n",
      "test loss on epoch 3063: 0.387\n",
      "test accuracy on epoch 3063: 0.769\n",
      "train loss on epoch 3064 : 0.187\n",
      "train accuracy on epoch 3064: 0.833\n",
      "test loss on epoch 3064: 0.380\n",
      "test accuracy on epoch 3064: 0.769\n",
      "train loss on epoch 3065 : 0.130\n",
      "train accuracy on epoch 3065: 0.944\n",
      "test loss on epoch 3065: 0.369\n",
      "test accuracy on epoch 3065: 0.769\n",
      "train loss on epoch 3066 : 0.040\n",
      "train accuracy on epoch 3066: 1.000\n",
      "test loss on epoch 3066: 0.364\n",
      "test accuracy on epoch 3066: 0.769\n",
      "train loss on epoch 3067 : 0.279\n",
      "train accuracy on epoch 3067: 0.889\n",
      "test loss on epoch 3067: 0.361\n",
      "test accuracy on epoch 3067: 0.769\n",
      "train loss on epoch 3068 : 0.193\n",
      "train accuracy on epoch 3068: 0.944\n",
      "test loss on epoch 3068: 0.356\n",
      "test accuracy on epoch 3068: 0.769\n",
      "train loss on epoch 3069 : 0.116\n",
      "train accuracy on epoch 3069: 0.944\n",
      "test loss on epoch 3069: 0.345\n",
      "test accuracy on epoch 3069: 0.769\n",
      "train loss on epoch 3070 : 0.311\n",
      "train accuracy on epoch 3070: 0.833\n",
      "test loss on epoch 3070: 0.362\n",
      "test accuracy on epoch 3070: 0.769\n",
      "train loss on epoch 3071 : 0.062\n",
      "train accuracy on epoch 3071: 1.000\n",
      "test loss on epoch 3071: 0.352\n",
      "test accuracy on epoch 3071: 0.769\n",
      "train loss on epoch 3072 : 0.172\n",
      "train accuracy on epoch 3072: 0.889\n",
      "test loss on epoch 3072: 0.360\n",
      "test accuracy on epoch 3072: 0.769\n",
      "train loss on epoch 3073 : 0.125\n",
      "train accuracy on epoch 3073: 0.944\n",
      "test loss on epoch 3073: 0.358\n",
      "test accuracy on epoch 3073: 0.769\n",
      "train loss on epoch 3074 : 0.079\n",
      "train accuracy on epoch 3074: 1.000\n",
      "test loss on epoch 3074: 0.355\n",
      "test accuracy on epoch 3074: 0.769\n",
      "train loss on epoch 3075 : 0.101\n",
      "train accuracy on epoch 3075: 0.944\n",
      "test loss on epoch 3075: 0.365\n",
      "test accuracy on epoch 3075: 0.769\n",
      "train loss on epoch 3076 : 0.318\n",
      "train accuracy on epoch 3076: 0.889\n",
      "test loss on epoch 3076: 0.357\n",
      "test accuracy on epoch 3076: 0.769\n",
      "train loss on epoch 3077 : 0.156\n",
      "train accuracy on epoch 3077: 0.889\n",
      "test loss on epoch 3077: 0.360\n",
      "test accuracy on epoch 3077: 0.769\n",
      "train loss on epoch 3078 : 0.251\n",
      "train accuracy on epoch 3078: 0.833\n",
      "test loss on epoch 3078: 0.342\n",
      "test accuracy on epoch 3078: 0.769\n",
      "train loss on epoch 3079 : 0.179\n",
      "train accuracy on epoch 3079: 0.944\n",
      "test loss on epoch 3079: 0.337\n",
      "test accuracy on epoch 3079: 0.846\n",
      "train loss on epoch 3080 : 0.135\n",
      "train accuracy on epoch 3080: 0.944\n",
      "test loss on epoch 3080: 0.354\n",
      "test accuracy on epoch 3080: 0.692\n",
      "train loss on epoch 3081 : 0.157\n",
      "train accuracy on epoch 3081: 0.944\n",
      "test loss on epoch 3081: 0.344\n",
      "test accuracy on epoch 3081: 0.769\n",
      "train loss on epoch 3082 : 0.106\n",
      "train accuracy on epoch 3082: 0.944\n",
      "test loss on epoch 3082: 0.351\n",
      "test accuracy on epoch 3082: 0.692\n",
      "train loss on epoch 3083 : 0.097\n",
      "train accuracy on epoch 3083: 1.000\n",
      "test loss on epoch 3083: 0.354\n",
      "test accuracy on epoch 3083: 0.769\n",
      "train loss on epoch 3084 : 0.190\n",
      "train accuracy on epoch 3084: 0.944\n",
      "test loss on epoch 3084: 0.358\n",
      "test accuracy on epoch 3084: 0.769\n",
      "train loss on epoch 3085 : 0.108\n",
      "train accuracy on epoch 3085: 0.944\n",
      "test loss on epoch 3085: 0.342\n",
      "test accuracy on epoch 3085: 0.769\n",
      "train loss on epoch 3086 : 0.337\n",
      "train accuracy on epoch 3086: 0.889\n",
      "test loss on epoch 3086: 0.358\n",
      "test accuracy on epoch 3086: 0.769\n",
      "train loss on epoch 3087 : 0.105\n",
      "train accuracy on epoch 3087: 0.944\n",
      "test loss on epoch 3087: 0.356\n",
      "test accuracy on epoch 3087: 0.769\n",
      "train loss on epoch 3088 : 0.083\n",
      "train accuracy on epoch 3088: 0.944\n",
      "test loss on epoch 3088: 0.356\n",
      "test accuracy on epoch 3088: 0.769\n",
      "train loss on epoch 3089 : 0.080\n",
      "train accuracy on epoch 3089: 1.000\n",
      "test loss on epoch 3089: 0.361\n",
      "test accuracy on epoch 3089: 0.769\n",
      "train loss on epoch 3090 : 0.232\n",
      "train accuracy on epoch 3090: 0.833\n",
      "test loss on epoch 3090: 0.367\n",
      "test accuracy on epoch 3090: 0.769\n",
      "train loss on epoch 3091 : 0.076\n",
      "train accuracy on epoch 3091: 1.000\n",
      "test loss on epoch 3091: 0.357\n",
      "test accuracy on epoch 3091: 0.769\n",
      "train loss on epoch 3092 : 0.144\n",
      "train accuracy on epoch 3092: 0.944\n",
      "test loss on epoch 3092: 0.353\n",
      "test accuracy on epoch 3092: 0.769\n",
      "train loss on epoch 3093 : 0.256\n",
      "train accuracy on epoch 3093: 0.889\n",
      "test loss on epoch 3093: 0.353\n",
      "test accuracy on epoch 3093: 0.769\n",
      "train loss on epoch 3094 : 0.036\n",
      "train accuracy on epoch 3094: 1.000\n",
      "test loss on epoch 3094: 0.355\n",
      "test accuracy on epoch 3094: 0.769\n",
      "train loss on epoch 3095 : 0.293\n",
      "train accuracy on epoch 3095: 0.889\n",
      "test loss on epoch 3095: 0.346\n",
      "test accuracy on epoch 3095: 0.769\n",
      "train loss on epoch 3096 : 0.158\n",
      "train accuracy on epoch 3096: 0.944\n",
      "test loss on epoch 3096: 0.344\n",
      "test accuracy on epoch 3096: 0.769\n",
      "train loss on epoch 3097 : 0.304\n",
      "train accuracy on epoch 3097: 0.889\n",
      "test loss on epoch 3097: 0.354\n",
      "test accuracy on epoch 3097: 0.769\n",
      "train loss on epoch 3098 : 0.047\n",
      "train accuracy on epoch 3098: 1.000\n",
      "test loss on epoch 3098: 0.350\n",
      "test accuracy on epoch 3098: 0.769\n",
      "train loss on epoch 3099 : 0.116\n",
      "train accuracy on epoch 3099: 0.944\n",
      "test loss on epoch 3099: 0.352\n",
      "test accuracy on epoch 3099: 0.769\n",
      "train loss on epoch 3100 : 0.142\n",
      "train accuracy on epoch 3100: 0.889\n",
      "test loss on epoch 3100: 0.361\n",
      "test accuracy on epoch 3100: 0.769\n",
      "train loss on epoch 3101 : 0.047\n",
      "train accuracy on epoch 3101: 1.000\n",
      "test loss on epoch 3101: 0.349\n",
      "test accuracy on epoch 3101: 0.769\n",
      "train loss on epoch 3102 : 0.217\n",
      "train accuracy on epoch 3102: 0.944\n",
      "test loss on epoch 3102: 0.345\n",
      "test accuracy on epoch 3102: 0.769\n",
      "train loss on epoch 3103 : 0.099\n",
      "train accuracy on epoch 3103: 0.944\n",
      "test loss on epoch 3103: 0.344\n",
      "test accuracy on epoch 3103: 0.769\n",
      "train loss on epoch 3104 : 0.247\n",
      "train accuracy on epoch 3104: 0.944\n",
      "test loss on epoch 3104: 0.340\n",
      "test accuracy on epoch 3104: 0.769\n",
      "train loss on epoch 3105 : 0.092\n",
      "train accuracy on epoch 3105: 1.000\n",
      "test loss on epoch 3105: 0.341\n",
      "test accuracy on epoch 3105: 0.769\n",
      "train loss on epoch 3106 : 0.185\n",
      "train accuracy on epoch 3106: 0.889\n",
      "test loss on epoch 3106: 0.342\n",
      "test accuracy on epoch 3106: 0.769\n",
      "train loss on epoch 3107 : 0.130\n",
      "train accuracy on epoch 3107: 1.000\n",
      "test loss on epoch 3107: 0.357\n",
      "test accuracy on epoch 3107: 0.769\n",
      "train loss on epoch 3108 : 0.105\n",
      "train accuracy on epoch 3108: 0.944\n",
      "test loss on epoch 3108: 0.351\n",
      "test accuracy on epoch 3108: 0.769\n",
      "train loss on epoch 3109 : 0.167\n",
      "train accuracy on epoch 3109: 0.889\n",
      "test loss on epoch 3109: 0.353\n",
      "test accuracy on epoch 3109: 0.769\n",
      "train loss on epoch 3110 : 0.388\n",
      "train accuracy on epoch 3110: 0.889\n",
      "test loss on epoch 3110: 0.344\n",
      "test accuracy on epoch 3110: 0.769\n",
      "train loss on epoch 3111 : 0.062\n",
      "train accuracy on epoch 3111: 1.000\n",
      "test loss on epoch 3111: 0.358\n",
      "test accuracy on epoch 3111: 0.769\n",
      "train loss on epoch 3112 : 0.101\n",
      "train accuracy on epoch 3112: 0.944\n",
      "test loss on epoch 3112: 0.341\n",
      "test accuracy on epoch 3112: 0.769\n",
      "train loss on epoch 3113 : 0.104\n",
      "train accuracy on epoch 3113: 0.944\n",
      "test loss on epoch 3113: 0.340\n",
      "test accuracy on epoch 3113: 0.769\n",
      "train loss on epoch 3114 : 0.033\n",
      "train accuracy on epoch 3114: 1.000\n",
      "test loss on epoch 3114: 0.349\n",
      "test accuracy on epoch 3114: 0.769\n",
      "train loss on epoch 3115 : 0.106\n",
      "train accuracy on epoch 3115: 0.944\n",
      "test loss on epoch 3115: 0.360\n",
      "test accuracy on epoch 3115: 0.769\n",
      "train loss on epoch 3116 : 0.114\n",
      "train accuracy on epoch 3116: 0.944\n",
      "test loss on epoch 3116: 0.356\n",
      "test accuracy on epoch 3116: 0.769\n",
      "train loss on epoch 3117 : 0.198\n",
      "train accuracy on epoch 3117: 0.944\n",
      "test loss on epoch 3117: 0.345\n",
      "test accuracy on epoch 3117: 0.769\n",
      "train loss on epoch 3118 : 0.168\n",
      "train accuracy on epoch 3118: 0.944\n",
      "test loss on epoch 3118: 0.354\n",
      "test accuracy on epoch 3118: 0.769\n",
      "train loss on epoch 3119 : 0.361\n",
      "train accuracy on epoch 3119: 0.944\n",
      "test loss on epoch 3119: 0.340\n",
      "test accuracy on epoch 3119: 0.769\n",
      "train loss on epoch 3120 : 0.328\n",
      "train accuracy on epoch 3120: 0.833\n",
      "test loss on epoch 3120: 0.340\n",
      "test accuracy on epoch 3120: 0.769\n",
      "train loss on epoch 3121 : 0.112\n",
      "train accuracy on epoch 3121: 0.944\n",
      "test loss on epoch 3121: 0.333\n",
      "test accuracy on epoch 3121: 0.769\n",
      "train loss on epoch 3122 : 0.108\n",
      "train accuracy on epoch 3122: 0.944\n",
      "test loss on epoch 3122: 0.335\n",
      "test accuracy on epoch 3122: 0.769\n",
      "train loss on epoch 3123 : 0.172\n",
      "train accuracy on epoch 3123: 0.889\n",
      "test loss on epoch 3123: 0.353\n",
      "test accuracy on epoch 3123: 0.769\n",
      "train loss on epoch 3124 : 0.125\n",
      "train accuracy on epoch 3124: 0.889\n",
      "test loss on epoch 3124: 0.351\n",
      "test accuracy on epoch 3124: 0.769\n",
      "train loss on epoch 3125 : 0.279\n",
      "train accuracy on epoch 3125: 0.944\n",
      "test loss on epoch 3125: 0.351\n",
      "test accuracy on epoch 3125: 0.769\n",
      "train loss on epoch 3126 : 0.271\n",
      "train accuracy on epoch 3126: 0.889\n",
      "test loss on epoch 3126: 0.363\n",
      "test accuracy on epoch 3126: 0.769\n",
      "train loss on epoch 3127 : 0.324\n",
      "train accuracy on epoch 3127: 0.889\n",
      "test loss on epoch 3127: 0.360\n",
      "test accuracy on epoch 3127: 0.769\n",
      "train loss on epoch 3128 : 0.072\n",
      "train accuracy on epoch 3128: 1.000\n",
      "test loss on epoch 3128: 0.358\n",
      "test accuracy on epoch 3128: 0.769\n",
      "train loss on epoch 3129 : 0.063\n",
      "train accuracy on epoch 3129: 1.000\n",
      "test loss on epoch 3129: 0.350\n",
      "test accuracy on epoch 3129: 0.769\n",
      "train loss on epoch 3130 : 0.174\n",
      "train accuracy on epoch 3130: 0.944\n",
      "test loss on epoch 3130: 0.349\n",
      "test accuracy on epoch 3130: 0.769\n",
      "train loss on epoch 3131 : 0.135\n",
      "train accuracy on epoch 3131: 0.944\n",
      "test loss on epoch 3131: 0.356\n",
      "test accuracy on epoch 3131: 0.769\n",
      "train loss on epoch 3132 : 0.066\n",
      "train accuracy on epoch 3132: 1.000\n",
      "test loss on epoch 3132: 0.345\n",
      "test accuracy on epoch 3132: 0.769\n",
      "train loss on epoch 3133 : 0.069\n",
      "train accuracy on epoch 3133: 1.000\n",
      "test loss on epoch 3133: 0.343\n",
      "test accuracy on epoch 3133: 0.769\n",
      "train loss on epoch 3134 : 0.139\n",
      "train accuracy on epoch 3134: 0.944\n",
      "test loss on epoch 3134: 0.343\n",
      "test accuracy on epoch 3134: 0.769\n",
      "train loss on epoch 3135 : 0.056\n",
      "train accuracy on epoch 3135: 1.000\n",
      "test loss on epoch 3135: 0.351\n",
      "test accuracy on epoch 3135: 0.692\n",
      "train loss on epoch 3136 : 0.111\n",
      "train accuracy on epoch 3136: 1.000\n",
      "test loss on epoch 3136: 0.339\n",
      "test accuracy on epoch 3136: 0.769\n",
      "train loss on epoch 3137 : 0.104\n",
      "train accuracy on epoch 3137: 1.000\n",
      "test loss on epoch 3137: 0.351\n",
      "test accuracy on epoch 3137: 0.692\n",
      "train loss on epoch 3138 : 0.137\n",
      "train accuracy on epoch 3138: 0.944\n",
      "test loss on epoch 3138: 0.341\n",
      "test accuracy on epoch 3138: 0.769\n",
      "train loss on epoch 3139 : 0.135\n",
      "train accuracy on epoch 3139: 0.944\n",
      "test loss on epoch 3139: 0.346\n",
      "test accuracy on epoch 3139: 0.769\n",
      "train loss on epoch 3140 : 0.352\n",
      "train accuracy on epoch 3140: 0.889\n",
      "test loss on epoch 3140: 0.340\n",
      "test accuracy on epoch 3140: 0.769\n",
      "train loss on epoch 3141 : 0.207\n",
      "train accuracy on epoch 3141: 0.944\n",
      "test loss on epoch 3141: 0.341\n",
      "test accuracy on epoch 3141: 0.769\n",
      "train loss on epoch 3142 : 0.192\n",
      "train accuracy on epoch 3142: 0.944\n",
      "test loss on epoch 3142: 0.341\n",
      "test accuracy on epoch 3142: 0.769\n",
      "train loss on epoch 3143 : 0.114\n",
      "train accuracy on epoch 3143: 0.944\n",
      "test loss on epoch 3143: 0.329\n",
      "test accuracy on epoch 3143: 0.846\n",
      "train loss on epoch 3144 : 0.133\n",
      "train accuracy on epoch 3144: 0.944\n",
      "test loss on epoch 3144: 0.348\n",
      "test accuracy on epoch 3144: 0.769\n",
      "train loss on epoch 3145 : 0.533\n",
      "train accuracy on epoch 3145: 0.778\n",
      "test loss on epoch 3145: 0.341\n",
      "test accuracy on epoch 3145: 0.769\n",
      "train loss on epoch 3146 : 0.303\n",
      "train accuracy on epoch 3146: 0.889\n",
      "test loss on epoch 3146: 0.348\n",
      "test accuracy on epoch 3146: 0.692\n",
      "train loss on epoch 3147 : 0.136\n",
      "train accuracy on epoch 3147: 0.944\n",
      "test loss on epoch 3147: 0.340\n",
      "test accuracy on epoch 3147: 0.769\n",
      "train loss on epoch 3148 : 0.097\n",
      "train accuracy on epoch 3148: 1.000\n",
      "test loss on epoch 3148: 0.347\n",
      "test accuracy on epoch 3148: 0.692\n",
      "train loss on epoch 3149 : 0.225\n",
      "train accuracy on epoch 3149: 0.889\n",
      "test loss on epoch 3149: 0.334\n",
      "test accuracy on epoch 3149: 0.769\n",
      "train loss on epoch 3150 : 0.097\n",
      "train accuracy on epoch 3150: 0.944\n",
      "test loss on epoch 3150: 0.342\n",
      "test accuracy on epoch 3150: 0.769\n",
      "train loss on epoch 3151 : 0.111\n",
      "train accuracy on epoch 3151: 0.944\n",
      "test loss on epoch 3151: 0.335\n",
      "test accuracy on epoch 3151: 0.769\n",
      "train loss on epoch 3152 : 0.079\n",
      "train accuracy on epoch 3152: 0.944\n",
      "test loss on epoch 3152: 0.336\n",
      "test accuracy on epoch 3152: 0.769\n",
      "train loss on epoch 3153 : 0.184\n",
      "train accuracy on epoch 3153: 0.889\n",
      "test loss on epoch 3153: 0.335\n",
      "test accuracy on epoch 3153: 0.769\n",
      "train loss on epoch 3154 : 0.193\n",
      "train accuracy on epoch 3154: 0.889\n",
      "test loss on epoch 3154: 0.344\n",
      "test accuracy on epoch 3154: 0.769\n",
      "train loss on epoch 3155 : 0.542\n",
      "train accuracy on epoch 3155: 0.833\n",
      "test loss on epoch 3155: 0.348\n",
      "test accuracy on epoch 3155: 0.769\n",
      "train loss on epoch 3156 : 0.111\n",
      "train accuracy on epoch 3156: 0.944\n",
      "test loss on epoch 3156: 0.351\n",
      "test accuracy on epoch 3156: 0.769\n",
      "train loss on epoch 3157 : 0.172\n",
      "train accuracy on epoch 3157: 0.889\n",
      "test loss on epoch 3157: 0.345\n",
      "test accuracy on epoch 3157: 0.769\n",
      "train loss on epoch 3158 : 0.195\n",
      "train accuracy on epoch 3158: 0.944\n",
      "test loss on epoch 3158: 0.336\n",
      "test accuracy on epoch 3158: 0.769\n",
      "train loss on epoch 3159 : 0.224\n",
      "train accuracy on epoch 3159: 0.833\n",
      "test loss on epoch 3159: 0.353\n",
      "test accuracy on epoch 3159: 0.769\n",
      "train loss on epoch 3160 : 0.132\n",
      "train accuracy on epoch 3160: 0.944\n",
      "test loss on epoch 3160: 0.353\n",
      "test accuracy on epoch 3160: 0.692\n",
      "train loss on epoch 3161 : 0.263\n",
      "train accuracy on epoch 3161: 0.833\n",
      "test loss on epoch 3161: 0.329\n",
      "test accuracy on epoch 3161: 0.846\n",
      "train loss on epoch 3162 : 0.253\n",
      "train accuracy on epoch 3162: 0.889\n",
      "test loss on epoch 3162: 0.339\n",
      "test accuracy on epoch 3162: 0.769\n",
      "train loss on epoch 3163 : 0.173\n",
      "train accuracy on epoch 3163: 0.889\n",
      "test loss on epoch 3163: 0.340\n",
      "test accuracy on epoch 3163: 0.769\n",
      "train loss on epoch 3164 : 0.117\n",
      "train accuracy on epoch 3164: 0.944\n",
      "test loss on epoch 3164: 0.347\n",
      "test accuracy on epoch 3164: 0.692\n",
      "train loss on epoch 3165 : 0.064\n",
      "train accuracy on epoch 3165: 1.000\n",
      "test loss on epoch 3165: 0.349\n",
      "test accuracy on epoch 3165: 0.769\n",
      "train loss on epoch 3166 : 0.187\n",
      "train accuracy on epoch 3166: 0.944\n",
      "test loss on epoch 3166: 0.331\n",
      "test accuracy on epoch 3166: 0.769\n",
      "train loss on epoch 3167 : 0.130\n",
      "train accuracy on epoch 3167: 0.889\n",
      "test loss on epoch 3167: 0.340\n",
      "test accuracy on epoch 3167: 0.769\n",
      "train loss on epoch 3168 : 0.053\n",
      "train accuracy on epoch 3168: 1.000\n",
      "test loss on epoch 3168: 0.349\n",
      "test accuracy on epoch 3168: 0.769\n",
      "train loss on epoch 3169 : 0.159\n",
      "train accuracy on epoch 3169: 0.944\n",
      "test loss on epoch 3169: 0.327\n",
      "test accuracy on epoch 3169: 0.769\n",
      "train loss on epoch 3170 : 0.227\n",
      "train accuracy on epoch 3170: 0.889\n",
      "test loss on epoch 3170: 0.333\n",
      "test accuracy on epoch 3170: 0.769\n",
      "train loss on epoch 3171 : 0.175\n",
      "train accuracy on epoch 3171: 0.944\n",
      "test loss on epoch 3171: 0.323\n",
      "test accuracy on epoch 3171: 0.769\n",
      "train loss on epoch 3172 : 0.304\n",
      "train accuracy on epoch 3172: 0.889\n",
      "test loss on epoch 3172: 0.326\n",
      "test accuracy on epoch 3172: 0.769\n",
      "train loss on epoch 3173 : 0.228\n",
      "train accuracy on epoch 3173: 0.944\n",
      "test loss on epoch 3173: 0.321\n",
      "test accuracy on epoch 3173: 0.769\n",
      "train loss on epoch 3174 : 0.329\n",
      "train accuracy on epoch 3174: 0.833\n",
      "test loss on epoch 3174: 0.340\n",
      "test accuracy on epoch 3174: 0.769\n",
      "train loss on epoch 3175 : 0.116\n",
      "train accuracy on epoch 3175: 0.944\n",
      "test loss on epoch 3175: 0.332\n",
      "test accuracy on epoch 3175: 0.769\n",
      "train loss on epoch 3176 : 0.183\n",
      "train accuracy on epoch 3176: 0.944\n",
      "test loss on epoch 3176: 0.323\n",
      "test accuracy on epoch 3176: 0.769\n",
      "train loss on epoch 3177 : 0.084\n",
      "train accuracy on epoch 3177: 1.000\n",
      "test loss on epoch 3177: 0.322\n",
      "test accuracy on epoch 3177: 0.769\n",
      "train loss on epoch 3178 : 0.238\n",
      "train accuracy on epoch 3178: 0.889\n",
      "test loss on epoch 3178: 0.339\n",
      "test accuracy on epoch 3178: 0.769\n",
      "train loss on epoch 3179 : 0.144\n",
      "train accuracy on epoch 3179: 0.944\n",
      "test loss on epoch 3179: 0.319\n",
      "test accuracy on epoch 3179: 0.769\n",
      "train loss on epoch 3180 : 0.137\n",
      "train accuracy on epoch 3180: 0.944\n",
      "test loss on epoch 3180: 0.337\n",
      "test accuracy on epoch 3180: 0.769\n",
      "train loss on epoch 3181 : 0.184\n",
      "train accuracy on epoch 3181: 0.889\n",
      "test loss on epoch 3181: 0.322\n",
      "test accuracy on epoch 3181: 0.769\n",
      "train loss on epoch 3182 : 0.121\n",
      "train accuracy on epoch 3182: 0.944\n",
      "test loss on epoch 3182: 0.338\n",
      "test accuracy on epoch 3182: 0.769\n",
      "train loss on epoch 3183 : 0.154\n",
      "train accuracy on epoch 3183: 0.889\n",
      "test loss on epoch 3183: 0.340\n",
      "test accuracy on epoch 3183: 0.769\n",
      "train loss on epoch 3184 : 0.252\n",
      "train accuracy on epoch 3184: 0.889\n",
      "test loss on epoch 3184: 0.336\n",
      "test accuracy on epoch 3184: 0.769\n",
      "train loss on epoch 3185 : 0.129\n",
      "train accuracy on epoch 3185: 0.944\n",
      "test loss on epoch 3185: 0.333\n",
      "test accuracy on epoch 3185: 0.769\n",
      "train loss on epoch 3186 : 0.183\n",
      "train accuracy on epoch 3186: 0.944\n",
      "test loss on epoch 3186: 0.335\n",
      "test accuracy on epoch 3186: 0.769\n",
      "train loss on epoch 3187 : 0.024\n",
      "train accuracy on epoch 3187: 1.000\n",
      "test loss on epoch 3187: 0.333\n",
      "test accuracy on epoch 3187: 0.769\n",
      "train loss on epoch 3188 : 0.230\n",
      "train accuracy on epoch 3188: 0.889\n",
      "test loss on epoch 3188: 0.325\n",
      "test accuracy on epoch 3188: 0.769\n",
      "train loss on epoch 3189 : 0.244\n",
      "train accuracy on epoch 3189: 0.889\n",
      "test loss on epoch 3189: 0.326\n",
      "test accuracy on epoch 3189: 0.769\n",
      "train loss on epoch 3190 : 0.212\n",
      "train accuracy on epoch 3190: 0.889\n",
      "test loss on epoch 3190: 0.341\n",
      "test accuracy on epoch 3190: 0.769\n",
      "train loss on epoch 3191 : 0.355\n",
      "train accuracy on epoch 3191: 0.889\n",
      "test loss on epoch 3191: 0.337\n",
      "test accuracy on epoch 3191: 0.769\n",
      "train loss on epoch 3192 : 0.077\n",
      "train accuracy on epoch 3192: 0.944\n",
      "test loss on epoch 3192: 0.334\n",
      "test accuracy on epoch 3192: 0.769\n",
      "train loss on epoch 3193 : 0.406\n",
      "train accuracy on epoch 3193: 0.833\n",
      "test loss on epoch 3193: 0.324\n",
      "test accuracy on epoch 3193: 0.769\n",
      "train loss on epoch 3194 : 0.102\n",
      "train accuracy on epoch 3194: 0.889\n",
      "test loss on epoch 3194: 0.339\n",
      "test accuracy on epoch 3194: 0.692\n",
      "train loss on epoch 3195 : 0.030\n",
      "train accuracy on epoch 3195: 1.000\n",
      "test loss on epoch 3195: 0.337\n",
      "test accuracy on epoch 3195: 0.692\n",
      "train loss on epoch 3196 : 0.289\n",
      "train accuracy on epoch 3196: 0.833\n",
      "test loss on epoch 3196: 0.321\n",
      "test accuracy on epoch 3196: 0.846\n",
      "train loss on epoch 3197 : 0.189\n",
      "train accuracy on epoch 3197: 0.944\n",
      "test loss on epoch 3197: 0.340\n",
      "test accuracy on epoch 3197: 0.769\n",
      "train loss on epoch 3198 : 0.188\n",
      "train accuracy on epoch 3198: 0.944\n",
      "test loss on epoch 3198: 0.341\n",
      "test accuracy on epoch 3198: 0.769\n",
      "train loss on epoch 3199 : 0.071\n",
      "train accuracy on epoch 3199: 0.944\n",
      "test loss on epoch 3199: 0.336\n",
      "test accuracy on epoch 3199: 0.769\n",
      "train loss on epoch 3200 : 0.127\n",
      "train accuracy on epoch 3200: 0.944\n",
      "test loss on epoch 3200: 0.332\n",
      "test accuracy on epoch 3200: 0.769\n",
      "train loss on epoch 3201 : 0.189\n",
      "train accuracy on epoch 3201: 0.944\n",
      "test loss on epoch 3201: 0.323\n",
      "test accuracy on epoch 3201: 0.769\n",
      "train loss on epoch 3202 : 0.185\n",
      "train accuracy on epoch 3202: 0.944\n",
      "test loss on epoch 3202: 0.329\n",
      "test accuracy on epoch 3202: 0.769\n",
      "train loss on epoch 3203 : 0.168\n",
      "train accuracy on epoch 3203: 0.944\n",
      "test loss on epoch 3203: 0.330\n",
      "test accuracy on epoch 3203: 0.769\n",
      "train loss on epoch 3204 : 0.091\n",
      "train accuracy on epoch 3204: 0.944\n",
      "test loss on epoch 3204: 0.325\n",
      "test accuracy on epoch 3204: 0.769\n",
      "train loss on epoch 3205 : 0.430\n",
      "train accuracy on epoch 3205: 0.722\n",
      "test loss on epoch 3205: 0.331\n",
      "test accuracy on epoch 3205: 0.769\n",
      "train loss on epoch 3206 : 0.329\n",
      "train accuracy on epoch 3206: 0.944\n",
      "test loss on epoch 3206: 0.320\n",
      "test accuracy on epoch 3206: 0.846\n",
      "train loss on epoch 3207 : 0.356\n",
      "train accuracy on epoch 3207: 0.889\n",
      "test loss on epoch 3207: 0.340\n",
      "test accuracy on epoch 3207: 0.769\n",
      "train loss on epoch 3208 : 0.083\n",
      "train accuracy on epoch 3208: 1.000\n",
      "test loss on epoch 3208: 0.332\n",
      "test accuracy on epoch 3208: 0.769\n",
      "train loss on epoch 3209 : 0.544\n",
      "train accuracy on epoch 3209: 0.889\n",
      "test loss on epoch 3209: 0.352\n",
      "test accuracy on epoch 3209: 0.769\n",
      "train loss on epoch 3210 : 0.169\n",
      "train accuracy on epoch 3210: 0.889\n",
      "test loss on epoch 3210: 0.349\n",
      "test accuracy on epoch 3210: 0.769\n",
      "train loss on epoch 3211 : 0.195\n",
      "train accuracy on epoch 3211: 0.944\n",
      "test loss on epoch 3211: 0.363\n",
      "test accuracy on epoch 3211: 0.769\n",
      "train loss on epoch 3212 : 0.104\n",
      "train accuracy on epoch 3212: 0.944\n",
      "test loss on epoch 3212: 0.359\n",
      "test accuracy on epoch 3212: 0.769\n",
      "train loss on epoch 3213 : 0.247\n",
      "train accuracy on epoch 3213: 0.944\n",
      "test loss on epoch 3213: 0.361\n",
      "test accuracy on epoch 3213: 0.769\n",
      "train loss on epoch 3214 : 0.168\n",
      "train accuracy on epoch 3214: 0.944\n",
      "test loss on epoch 3214: 0.345\n",
      "test accuracy on epoch 3214: 0.769\n",
      "train loss on epoch 3215 : 0.089\n",
      "train accuracy on epoch 3215: 1.000\n",
      "test loss on epoch 3215: 0.349\n",
      "test accuracy on epoch 3215: 0.769\n",
      "train loss on epoch 3216 : 0.041\n",
      "train accuracy on epoch 3216: 1.000\n",
      "test loss on epoch 3216: 0.337\n",
      "test accuracy on epoch 3216: 0.769\n",
      "train loss on epoch 3217 : 0.167\n",
      "train accuracy on epoch 3217: 0.889\n",
      "test loss on epoch 3217: 0.334\n",
      "test accuracy on epoch 3217: 0.769\n",
      "train loss on epoch 3218 : 0.199\n",
      "train accuracy on epoch 3218: 0.889\n",
      "test loss on epoch 3218: 0.342\n",
      "test accuracy on epoch 3218: 0.769\n",
      "train loss on epoch 3219 : 0.141\n",
      "train accuracy on epoch 3219: 0.944\n",
      "test loss on epoch 3219: 0.357\n",
      "test accuracy on epoch 3219: 0.769\n",
      "train loss on epoch 3220 : 0.124\n",
      "train accuracy on epoch 3220: 0.944\n",
      "test loss on epoch 3220: 0.361\n",
      "test accuracy on epoch 3220: 0.769\n",
      "train loss on epoch 3221 : 0.264\n",
      "train accuracy on epoch 3221: 0.889\n",
      "test loss on epoch 3221: 0.367\n",
      "test accuracy on epoch 3221: 0.769\n",
      "train loss on epoch 3222 : 0.272\n",
      "train accuracy on epoch 3222: 0.889\n",
      "test loss on epoch 3222: 0.369\n",
      "test accuracy on epoch 3222: 0.769\n",
      "train loss on epoch 3223 : 0.303\n",
      "train accuracy on epoch 3223: 0.944\n",
      "test loss on epoch 3223: 0.371\n",
      "test accuracy on epoch 3223: 0.769\n",
      "train loss on epoch 3224 : 0.108\n",
      "train accuracy on epoch 3224: 0.944\n",
      "test loss on epoch 3224: 0.375\n",
      "test accuracy on epoch 3224: 0.769\n",
      "train loss on epoch 3225 : 0.103\n",
      "train accuracy on epoch 3225: 0.944\n",
      "test loss on epoch 3225: 0.378\n",
      "test accuracy on epoch 3225: 0.769\n",
      "train loss on epoch 3226 : 0.153\n",
      "train accuracy on epoch 3226: 0.889\n",
      "test loss on epoch 3226: 0.382\n",
      "test accuracy on epoch 3226: 0.769\n",
      "train loss on epoch 3227 : 0.391\n",
      "train accuracy on epoch 3227: 0.889\n",
      "test loss on epoch 3227: 0.373\n",
      "test accuracy on epoch 3227: 0.769\n",
      "train loss on epoch 3228 : 0.071\n",
      "train accuracy on epoch 3228: 1.000\n",
      "test loss on epoch 3228: 0.370\n",
      "test accuracy on epoch 3228: 0.769\n",
      "train loss on epoch 3229 : 0.049\n",
      "train accuracy on epoch 3229: 1.000\n",
      "test loss on epoch 3229: 0.366\n",
      "test accuracy on epoch 3229: 0.769\n",
      "train loss on epoch 3230 : 0.101\n",
      "train accuracy on epoch 3230: 0.944\n",
      "test loss on epoch 3230: 0.373\n",
      "test accuracy on epoch 3230: 0.769\n",
      "train loss on epoch 3231 : 0.284\n",
      "train accuracy on epoch 3231: 0.944\n",
      "test loss on epoch 3231: 0.363\n",
      "test accuracy on epoch 3231: 0.769\n",
      "train loss on epoch 3232 : 0.355\n",
      "train accuracy on epoch 3232: 0.889\n",
      "test loss on epoch 3232: 0.370\n",
      "test accuracy on epoch 3232: 0.769\n",
      "train loss on epoch 3233 : 0.136\n",
      "train accuracy on epoch 3233: 0.944\n",
      "test loss on epoch 3233: 0.359\n",
      "test accuracy on epoch 3233: 0.769\n",
      "train loss on epoch 3234 : 0.143\n",
      "train accuracy on epoch 3234: 0.889\n",
      "test loss on epoch 3234: 0.354\n",
      "test accuracy on epoch 3234: 0.769\n",
      "train loss on epoch 3235 : 0.173\n",
      "train accuracy on epoch 3235: 0.944\n",
      "test loss on epoch 3235: 0.353\n",
      "test accuracy on epoch 3235: 0.769\n",
      "train loss on epoch 3236 : 0.157\n",
      "train accuracy on epoch 3236: 0.944\n",
      "test loss on epoch 3236: 0.347\n",
      "test accuracy on epoch 3236: 0.769\n",
      "train loss on epoch 3237 : 0.261\n",
      "train accuracy on epoch 3237: 0.889\n",
      "test loss on epoch 3237: 0.342\n",
      "test accuracy on epoch 3237: 0.769\n",
      "train loss on epoch 3238 : 0.184\n",
      "train accuracy on epoch 3238: 0.889\n",
      "test loss on epoch 3238: 0.346\n",
      "test accuracy on epoch 3238: 0.769\n",
      "train loss on epoch 3239 : 0.074\n",
      "train accuracy on epoch 3239: 1.000\n",
      "test loss on epoch 3239: 0.340\n",
      "test accuracy on epoch 3239: 0.769\n",
      "train loss on epoch 3240 : 0.188\n",
      "train accuracy on epoch 3240: 0.944\n",
      "test loss on epoch 3240: 0.350\n",
      "test accuracy on epoch 3240: 0.769\n",
      "train loss on epoch 3241 : 0.214\n",
      "train accuracy on epoch 3241: 0.889\n",
      "test loss on epoch 3241: 0.356\n",
      "test accuracy on epoch 3241: 0.769\n",
      "train loss on epoch 3242 : 0.127\n",
      "train accuracy on epoch 3242: 0.944\n",
      "test loss on epoch 3242: 0.355\n",
      "test accuracy on epoch 3242: 0.769\n",
      "train loss on epoch 3243 : 0.463\n",
      "train accuracy on epoch 3243: 0.889\n",
      "test loss on epoch 3243: 0.350\n",
      "test accuracy on epoch 3243: 0.769\n",
      "train loss on epoch 3244 : 0.324\n",
      "train accuracy on epoch 3244: 0.889\n",
      "test loss on epoch 3244: 0.344\n",
      "test accuracy on epoch 3244: 0.769\n",
      "train loss on epoch 3245 : 0.065\n",
      "train accuracy on epoch 3245: 0.944\n",
      "test loss on epoch 3245: 0.324\n",
      "test accuracy on epoch 3245: 0.846\n",
      "train loss on epoch 3246 : 0.067\n",
      "train accuracy on epoch 3246: 1.000\n",
      "test loss on epoch 3246: 0.324\n",
      "test accuracy on epoch 3246: 0.846\n",
      "train loss on epoch 3247 : 0.333\n",
      "train accuracy on epoch 3247: 0.833\n",
      "test loss on epoch 3247: 0.344\n",
      "test accuracy on epoch 3247: 0.769\n",
      "train loss on epoch 3248 : 0.284\n",
      "train accuracy on epoch 3248: 0.889\n",
      "test loss on epoch 3248: 0.338\n",
      "test accuracy on epoch 3248: 0.769\n",
      "train loss on epoch 3249 : 0.208\n",
      "train accuracy on epoch 3249: 0.944\n",
      "test loss on epoch 3249: 0.346\n",
      "test accuracy on epoch 3249: 0.769\n",
      "train loss on epoch 3250 : 0.304\n",
      "train accuracy on epoch 3250: 0.889\n",
      "test loss on epoch 3250: 0.325\n",
      "test accuracy on epoch 3250: 0.846\n",
      "train loss on epoch 3251 : 0.111\n",
      "train accuracy on epoch 3251: 0.944\n",
      "test loss on epoch 3251: 0.327\n",
      "test accuracy on epoch 3251: 0.846\n",
      "train loss on epoch 3252 : 0.169\n",
      "train accuracy on epoch 3252: 0.889\n",
      "test loss on epoch 3252: 0.341\n",
      "test accuracy on epoch 3252: 0.692\n",
      "train loss on epoch 3253 : 0.199\n",
      "train accuracy on epoch 3253: 0.944\n",
      "test loss on epoch 3253: 0.332\n",
      "test accuracy on epoch 3253: 0.769\n",
      "train loss on epoch 3254 : 0.165\n",
      "train accuracy on epoch 3254: 0.889\n",
      "test loss on epoch 3254: 0.332\n",
      "test accuracy on epoch 3254: 0.769\n",
      "train loss on epoch 3255 : 0.196\n",
      "train accuracy on epoch 3255: 0.944\n",
      "test loss on epoch 3255: 0.332\n",
      "test accuracy on epoch 3255: 0.769\n",
      "train loss on epoch 3256 : 0.101\n",
      "train accuracy on epoch 3256: 0.944\n",
      "test loss on epoch 3256: 0.334\n",
      "test accuracy on epoch 3256: 0.769\n",
      "train loss on epoch 3257 : 0.268\n",
      "train accuracy on epoch 3257: 0.889\n",
      "test loss on epoch 3257: 0.340\n",
      "test accuracy on epoch 3257: 0.769\n",
      "train loss on epoch 3258 : 0.141\n",
      "train accuracy on epoch 3258: 0.944\n",
      "test loss on epoch 3258: 0.342\n",
      "test accuracy on epoch 3258: 0.769\n",
      "train loss on epoch 3259 : 0.045\n",
      "train accuracy on epoch 3259: 1.000\n",
      "test loss on epoch 3259: 0.338\n",
      "test accuracy on epoch 3259: 0.769\n",
      "train loss on epoch 3260 : 0.017\n",
      "train accuracy on epoch 3260: 1.000\n",
      "test loss on epoch 3260: 0.348\n",
      "test accuracy on epoch 3260: 0.769\n",
      "train loss on epoch 3261 : 0.197\n",
      "train accuracy on epoch 3261: 0.944\n",
      "test loss on epoch 3261: 0.349\n",
      "test accuracy on epoch 3261: 0.769\n",
      "train loss on epoch 3262 : 0.220\n",
      "train accuracy on epoch 3262: 0.944\n",
      "test loss on epoch 3262: 0.345\n",
      "test accuracy on epoch 3262: 0.769\n",
      "train loss on epoch 3263 : 0.215\n",
      "train accuracy on epoch 3263: 0.944\n",
      "test loss on epoch 3263: 0.345\n",
      "test accuracy on epoch 3263: 0.769\n",
      "train loss on epoch 3264 : 0.080\n",
      "train accuracy on epoch 3264: 0.944\n",
      "test loss on epoch 3264: 0.340\n",
      "test accuracy on epoch 3264: 0.769\n",
      "train loss on epoch 3265 : 0.084\n",
      "train accuracy on epoch 3265: 0.944\n",
      "test loss on epoch 3265: 0.349\n",
      "test accuracy on epoch 3265: 0.769\n",
      "train loss on epoch 3266 : 0.139\n",
      "train accuracy on epoch 3266: 0.889\n",
      "test loss on epoch 3266: 0.358\n",
      "test accuracy on epoch 3266: 0.769\n",
      "train loss on epoch 3267 : 0.313\n",
      "train accuracy on epoch 3267: 0.889\n",
      "test loss on epoch 3267: 0.356\n",
      "test accuracy on epoch 3267: 0.769\n",
      "train loss on epoch 3268 : 0.245\n",
      "train accuracy on epoch 3268: 0.833\n",
      "test loss on epoch 3268: 0.360\n",
      "test accuracy on epoch 3268: 0.769\n",
      "train loss on epoch 3269 : 0.356\n",
      "train accuracy on epoch 3269: 0.889\n",
      "test loss on epoch 3269: 0.352\n",
      "test accuracy on epoch 3269: 0.769\n",
      "train loss on epoch 3270 : 0.207\n",
      "train accuracy on epoch 3270: 0.889\n",
      "test loss on epoch 3270: 0.358\n",
      "test accuracy on epoch 3270: 0.769\n",
      "train loss on epoch 3271 : 0.049\n",
      "train accuracy on epoch 3271: 1.000\n",
      "test loss on epoch 3271: 0.350\n",
      "test accuracy on epoch 3271: 0.769\n",
      "train loss on epoch 3272 : 0.105\n",
      "train accuracy on epoch 3272: 1.000\n",
      "test loss on epoch 3272: 0.344\n",
      "test accuracy on epoch 3272: 0.769\n",
      "train loss on epoch 3273 : 0.323\n",
      "train accuracy on epoch 3273: 0.889\n",
      "test loss on epoch 3273: 0.333\n",
      "test accuracy on epoch 3273: 0.769\n",
      "train loss on epoch 3274 : 0.176\n",
      "train accuracy on epoch 3274: 0.944\n",
      "test loss on epoch 3274: 0.335\n",
      "test accuracy on epoch 3274: 0.769\n",
      "train loss on epoch 3275 : 0.272\n",
      "train accuracy on epoch 3275: 0.944\n",
      "test loss on epoch 3275: 0.327\n",
      "test accuracy on epoch 3275: 0.846\n",
      "train loss on epoch 3276 : 0.044\n",
      "train accuracy on epoch 3276: 1.000\n",
      "test loss on epoch 3276: 0.333\n",
      "test accuracy on epoch 3276: 0.769\n",
      "train loss on epoch 3277 : 0.056\n",
      "train accuracy on epoch 3277: 1.000\n",
      "test loss on epoch 3277: 0.331\n",
      "test accuracy on epoch 3277: 0.769\n",
      "train loss on epoch 3278 : 0.061\n",
      "train accuracy on epoch 3278: 1.000\n",
      "test loss on epoch 3278: 0.339\n",
      "test accuracy on epoch 3278: 0.692\n",
      "train loss on epoch 3279 : 0.150\n",
      "train accuracy on epoch 3279: 0.889\n",
      "test loss on epoch 3279: 0.324\n",
      "test accuracy on epoch 3279: 0.769\n",
      "train loss on epoch 3280 : 0.148\n",
      "train accuracy on epoch 3280: 1.000\n",
      "test loss on epoch 3280: 0.321\n",
      "test accuracy on epoch 3280: 0.769\n",
      "train loss on epoch 3281 : 0.283\n",
      "train accuracy on epoch 3281: 0.889\n",
      "test loss on epoch 3281: 0.324\n",
      "test accuracy on epoch 3281: 0.769\n",
      "train loss on epoch 3282 : 0.049\n",
      "train accuracy on epoch 3282: 1.000\n",
      "test loss on epoch 3282: 0.324\n",
      "test accuracy on epoch 3282: 0.846\n",
      "train loss on epoch 3283 : 0.245\n",
      "train accuracy on epoch 3283: 0.833\n",
      "test loss on epoch 3283: 0.337\n",
      "test accuracy on epoch 3283: 0.769\n",
      "train loss on epoch 3284 : 0.184\n",
      "train accuracy on epoch 3284: 0.944\n",
      "test loss on epoch 3284: 0.340\n",
      "test accuracy on epoch 3284: 0.769\n",
      "train loss on epoch 3285 : 0.107\n",
      "train accuracy on epoch 3285: 1.000\n",
      "test loss on epoch 3285: 0.330\n",
      "test accuracy on epoch 3285: 0.769\n",
      "train loss on epoch 3286 : 0.120\n",
      "train accuracy on epoch 3286: 0.944\n",
      "test loss on epoch 3286: 0.325\n",
      "test accuracy on epoch 3286: 0.769\n",
      "train loss on epoch 3287 : 0.093\n",
      "train accuracy on epoch 3287: 1.000\n",
      "test loss on epoch 3287: 0.323\n",
      "test accuracy on epoch 3287: 0.769\n",
      "train loss on epoch 3288 : 0.570\n",
      "train accuracy on epoch 3288: 0.889\n",
      "test loss on epoch 3288: 0.331\n",
      "test accuracy on epoch 3288: 0.769\n",
      "train loss on epoch 3289 : 0.122\n",
      "train accuracy on epoch 3289: 0.944\n",
      "test loss on epoch 3289: 0.328\n",
      "test accuracy on epoch 3289: 0.769\n",
      "train loss on epoch 3290 : 0.057\n",
      "train accuracy on epoch 3290: 1.000\n",
      "test loss on epoch 3290: 0.336\n",
      "test accuracy on epoch 3290: 0.769\n",
      "train loss on epoch 3291 : 0.218\n",
      "train accuracy on epoch 3291: 0.833\n",
      "test loss on epoch 3291: 0.337\n",
      "test accuracy on epoch 3291: 0.769\n",
      "train loss on epoch 3292 : 0.195\n",
      "train accuracy on epoch 3292: 0.833\n",
      "test loss on epoch 3292: 0.337\n",
      "test accuracy on epoch 3292: 0.769\n",
      "train loss on epoch 3293 : 0.149\n",
      "train accuracy on epoch 3293: 0.944\n",
      "test loss on epoch 3293: 0.340\n",
      "test accuracy on epoch 3293: 0.769\n",
      "train loss on epoch 3294 : 0.122\n",
      "train accuracy on epoch 3294: 0.944\n",
      "test loss on epoch 3294: 0.336\n",
      "test accuracy on epoch 3294: 0.769\n",
      "train loss on epoch 3295 : 0.130\n",
      "train accuracy on epoch 3295: 0.944\n",
      "test loss on epoch 3295: 0.330\n",
      "test accuracy on epoch 3295: 0.769\n",
      "train loss on epoch 3296 : 0.027\n",
      "train accuracy on epoch 3296: 1.000\n",
      "test loss on epoch 3296: 0.333\n",
      "test accuracy on epoch 3296: 0.692\n",
      "train loss on epoch 3297 : 0.188\n",
      "train accuracy on epoch 3297: 0.944\n",
      "test loss on epoch 3297: 0.326\n",
      "test accuracy on epoch 3297: 0.769\n",
      "train loss on epoch 3298 : 0.262\n",
      "train accuracy on epoch 3298: 0.889\n",
      "test loss on epoch 3298: 0.324\n",
      "test accuracy on epoch 3298: 0.769\n",
      "train loss on epoch 3299 : 0.122\n",
      "train accuracy on epoch 3299: 0.944\n",
      "test loss on epoch 3299: 0.328\n",
      "test accuracy on epoch 3299: 0.769\n",
      "train loss on epoch 3300 : 0.296\n",
      "train accuracy on epoch 3300: 0.944\n",
      "test loss on epoch 3300: 0.328\n",
      "test accuracy on epoch 3300: 0.769\n",
      "train loss on epoch 3301 : 0.234\n",
      "train accuracy on epoch 3301: 0.944\n",
      "test loss on epoch 3301: 0.343\n",
      "test accuracy on epoch 3301: 0.769\n",
      "train loss on epoch 3302 : 0.107\n",
      "train accuracy on epoch 3302: 0.944\n",
      "test loss on epoch 3302: 0.344\n",
      "test accuracy on epoch 3302: 0.769\n",
      "train loss on epoch 3303 : 0.142\n",
      "train accuracy on epoch 3303: 0.944\n",
      "test loss on epoch 3303: 0.338\n",
      "test accuracy on epoch 3303: 0.769\n",
      "train loss on epoch 3304 : 0.138\n",
      "train accuracy on epoch 3304: 0.944\n",
      "test loss on epoch 3304: 0.340\n",
      "test accuracy on epoch 3304: 0.769\n",
      "train loss on epoch 3305 : 0.154\n",
      "train accuracy on epoch 3305: 0.944\n",
      "test loss on epoch 3305: 0.333\n",
      "test accuracy on epoch 3305: 0.769\n",
      "train loss on epoch 3306 : 0.224\n",
      "train accuracy on epoch 3306: 0.889\n",
      "test loss on epoch 3306: 0.342\n",
      "test accuracy on epoch 3306: 0.692\n",
      "train loss on epoch 3307 : 0.176\n",
      "train accuracy on epoch 3307: 0.944\n",
      "test loss on epoch 3307: 0.324\n",
      "test accuracy on epoch 3307: 0.769\n",
      "train loss on epoch 3308 : 0.281\n",
      "train accuracy on epoch 3308: 0.944\n",
      "test loss on epoch 3308: 0.324\n",
      "test accuracy on epoch 3308: 0.769\n",
      "train loss on epoch 3309 : 0.084\n",
      "train accuracy on epoch 3309: 1.000\n",
      "test loss on epoch 3309: 0.328\n",
      "test accuracy on epoch 3309: 0.769\n",
      "train loss on epoch 3310 : 0.318\n",
      "train accuracy on epoch 3310: 0.833\n",
      "test loss on epoch 3310: 0.343\n",
      "test accuracy on epoch 3310: 0.769\n",
      "train loss on epoch 3311 : 0.167\n",
      "train accuracy on epoch 3311: 0.889\n",
      "test loss on epoch 3311: 0.332\n",
      "test accuracy on epoch 3311: 0.769\n",
      "train loss on epoch 3312 : 0.075\n",
      "train accuracy on epoch 3312: 1.000\n",
      "test loss on epoch 3312: 0.326\n",
      "test accuracy on epoch 3312: 0.769\n",
      "train loss on epoch 3313 : 0.120\n",
      "train accuracy on epoch 3313: 0.944\n",
      "test loss on epoch 3313: 0.326\n",
      "test accuracy on epoch 3313: 0.769\n",
      "train loss on epoch 3314 : 0.323\n",
      "train accuracy on epoch 3314: 0.889\n",
      "test loss on epoch 3314: 0.345\n",
      "test accuracy on epoch 3314: 0.769\n",
      "train loss on epoch 3315 : 0.269\n",
      "train accuracy on epoch 3315: 0.833\n",
      "test loss on epoch 3315: 0.339\n",
      "test accuracy on epoch 3315: 0.769\n",
      "train loss on epoch 3316 : 0.253\n",
      "train accuracy on epoch 3316: 0.944\n",
      "test loss on epoch 3316: 0.337\n",
      "test accuracy on epoch 3316: 0.769\n",
      "train loss on epoch 3317 : 0.181\n",
      "train accuracy on epoch 3317: 0.944\n",
      "test loss on epoch 3317: 0.324\n",
      "test accuracy on epoch 3317: 0.769\n",
      "train loss on epoch 3318 : 0.029\n",
      "train accuracy on epoch 3318: 1.000\n",
      "test loss on epoch 3318: 0.325\n",
      "test accuracy on epoch 3318: 0.769\n",
      "train loss on epoch 3319 : 0.379\n",
      "train accuracy on epoch 3319: 0.833\n",
      "test loss on epoch 3319: 0.323\n",
      "test accuracy on epoch 3319: 0.769\n",
      "train loss on epoch 3320 : 0.226\n",
      "train accuracy on epoch 3320: 0.944\n",
      "test loss on epoch 3320: 0.325\n",
      "test accuracy on epoch 3320: 0.769\n",
      "train loss on epoch 3321 : 0.231\n",
      "train accuracy on epoch 3321: 0.944\n",
      "test loss on epoch 3321: 0.342\n",
      "test accuracy on epoch 3321: 0.769\n",
      "train loss on epoch 3322 : 0.097\n",
      "train accuracy on epoch 3322: 0.944\n",
      "test loss on epoch 3322: 0.324\n",
      "test accuracy on epoch 3322: 0.769\n",
      "train loss on epoch 3323 : 0.112\n",
      "train accuracy on epoch 3323: 0.944\n",
      "test loss on epoch 3323: 0.337\n",
      "test accuracy on epoch 3323: 0.769\n",
      "train loss on epoch 3324 : 0.096\n",
      "train accuracy on epoch 3324: 0.944\n",
      "test loss on epoch 3324: 0.320\n",
      "test accuracy on epoch 3324: 0.769\n",
      "train loss on epoch 3325 : 0.130\n",
      "train accuracy on epoch 3325: 0.944\n",
      "test loss on epoch 3325: 0.324\n",
      "test accuracy on epoch 3325: 0.769\n",
      "train loss on epoch 3326 : 0.227\n",
      "train accuracy on epoch 3326: 0.944\n",
      "test loss on epoch 3326: 0.340\n",
      "test accuracy on epoch 3326: 0.769\n",
      "train loss on epoch 3327 : 0.153\n",
      "train accuracy on epoch 3327: 0.944\n",
      "test loss on epoch 3327: 0.328\n",
      "test accuracy on epoch 3327: 0.769\n",
      "train loss on epoch 3328 : 0.230\n",
      "train accuracy on epoch 3328: 0.944\n",
      "test loss on epoch 3328: 0.342\n",
      "test accuracy on epoch 3328: 0.769\n",
      "train loss on epoch 3329 : 0.116\n",
      "train accuracy on epoch 3329: 1.000\n",
      "test loss on epoch 3329: 0.341\n",
      "test accuracy on epoch 3329: 0.769\n",
      "train loss on epoch 3330 : 0.248\n",
      "train accuracy on epoch 3330: 0.944\n",
      "test loss on epoch 3330: 0.323\n",
      "test accuracy on epoch 3330: 0.769\n",
      "train loss on epoch 3331 : 0.146\n",
      "train accuracy on epoch 3331: 0.944\n",
      "test loss on epoch 3331: 0.321\n",
      "test accuracy on epoch 3331: 0.769\n",
      "train loss on epoch 3332 : 0.168\n",
      "train accuracy on epoch 3332: 0.944\n",
      "test loss on epoch 3332: 0.330\n",
      "test accuracy on epoch 3332: 0.769\n",
      "train loss on epoch 3333 : 0.194\n",
      "train accuracy on epoch 3333: 0.889\n",
      "test loss on epoch 3333: 0.330\n",
      "test accuracy on epoch 3333: 0.769\n",
      "train loss on epoch 3334 : 0.251\n",
      "train accuracy on epoch 3334: 0.889\n",
      "test loss on epoch 3334: 0.326\n",
      "test accuracy on epoch 3334: 0.769\n",
      "train loss on epoch 3335 : 0.173\n",
      "train accuracy on epoch 3335: 0.944\n",
      "test loss on epoch 3335: 0.326\n",
      "test accuracy on epoch 3335: 0.769\n",
      "train loss on epoch 3336 : 0.097\n",
      "train accuracy on epoch 3336: 0.944\n",
      "test loss on epoch 3336: 0.324\n",
      "test accuracy on epoch 3336: 0.769\n",
      "train loss on epoch 3337 : 0.087\n",
      "train accuracy on epoch 3337: 0.944\n",
      "test loss on epoch 3337: 0.320\n",
      "test accuracy on epoch 3337: 0.769\n",
      "train loss on epoch 3338 : 0.068\n",
      "train accuracy on epoch 3338: 1.000\n",
      "test loss on epoch 3338: 0.325\n",
      "test accuracy on epoch 3338: 0.769\n",
      "train loss on epoch 3339 : 0.153\n",
      "train accuracy on epoch 3339: 0.944\n",
      "test loss on epoch 3339: 0.318\n",
      "test accuracy on epoch 3339: 0.769\n",
      "train loss on epoch 3340 : 0.132\n",
      "train accuracy on epoch 3340: 1.000\n",
      "test loss on epoch 3340: 0.321\n",
      "test accuracy on epoch 3340: 0.769\n",
      "train loss on epoch 3341 : 0.180\n",
      "train accuracy on epoch 3341: 0.944\n",
      "test loss on epoch 3341: 0.317\n",
      "test accuracy on epoch 3341: 0.769\n",
      "train loss on epoch 3342 : 0.363\n",
      "train accuracy on epoch 3342: 0.889\n",
      "test loss on epoch 3342: 0.316\n",
      "test accuracy on epoch 3342: 0.769\n",
      "train loss on epoch 3343 : 0.160\n",
      "train accuracy on epoch 3343: 0.944\n",
      "test loss on epoch 3343: 0.320\n",
      "test accuracy on epoch 3343: 0.769\n",
      "train loss on epoch 3344 : 0.128\n",
      "train accuracy on epoch 3344: 0.944\n",
      "test loss on epoch 3344: 0.316\n",
      "test accuracy on epoch 3344: 0.769\n",
      "train loss on epoch 3345 : 0.181\n",
      "train accuracy on epoch 3345: 0.889\n",
      "test loss on epoch 3345: 0.315\n",
      "test accuracy on epoch 3345: 0.769\n",
      "train loss on epoch 3346 : 0.124\n",
      "train accuracy on epoch 3346: 0.944\n",
      "test loss on epoch 3346: 0.317\n",
      "test accuracy on epoch 3346: 0.769\n",
      "train loss on epoch 3347 : 0.129\n",
      "train accuracy on epoch 3347: 0.889\n",
      "test loss on epoch 3347: 0.309\n",
      "test accuracy on epoch 3347: 0.769\n",
      "train loss on epoch 3348 : 0.239\n",
      "train accuracy on epoch 3348: 0.833\n",
      "test loss on epoch 3348: 0.311\n",
      "test accuracy on epoch 3348: 0.769\n",
      "train loss on epoch 3349 : 0.258\n",
      "train accuracy on epoch 3349: 0.889\n",
      "test loss on epoch 3349: 0.316\n",
      "test accuracy on epoch 3349: 0.769\n",
      "train loss on epoch 3350 : 0.383\n",
      "train accuracy on epoch 3350: 0.889\n",
      "test loss on epoch 3350: 0.320\n",
      "test accuracy on epoch 3350: 0.769\n",
      "train loss on epoch 3351 : 0.511\n",
      "train accuracy on epoch 3351: 0.833\n",
      "test loss on epoch 3351: 0.321\n",
      "test accuracy on epoch 3351: 0.769\n",
      "train loss on epoch 3352 : 0.164\n",
      "train accuracy on epoch 3352: 0.889\n",
      "test loss on epoch 3352: 0.323\n",
      "test accuracy on epoch 3352: 0.769\n",
      "train loss on epoch 3353 : 0.065\n",
      "train accuracy on epoch 3353: 1.000\n",
      "test loss on epoch 3353: 0.332\n",
      "test accuracy on epoch 3353: 0.769\n",
      "train loss on epoch 3354 : 0.068\n",
      "train accuracy on epoch 3354: 1.000\n",
      "test loss on epoch 3354: 0.331\n",
      "test accuracy on epoch 3354: 0.769\n",
      "train loss on epoch 3355 : 0.076\n",
      "train accuracy on epoch 3355: 0.944\n",
      "test loss on epoch 3355: 0.328\n",
      "test accuracy on epoch 3355: 0.769\n",
      "train loss on epoch 3356 : 0.182\n",
      "train accuracy on epoch 3356: 0.944\n",
      "test loss on epoch 3356: 0.322\n",
      "test accuracy on epoch 3356: 0.769\n",
      "train loss on epoch 3357 : 0.192\n",
      "train accuracy on epoch 3357: 0.889\n",
      "test loss on epoch 3357: 0.324\n",
      "test accuracy on epoch 3357: 0.769\n",
      "train loss on epoch 3358 : 0.129\n",
      "train accuracy on epoch 3358: 0.944\n",
      "test loss on epoch 3358: 0.317\n",
      "test accuracy on epoch 3358: 0.769\n",
      "train loss on epoch 3359 : 0.231\n",
      "train accuracy on epoch 3359: 0.944\n",
      "test loss on epoch 3359: 0.323\n",
      "test accuracy on epoch 3359: 0.769\n",
      "train loss on epoch 3360 : 0.232\n",
      "train accuracy on epoch 3360: 0.833\n",
      "test loss on epoch 3360: 0.322\n",
      "test accuracy on epoch 3360: 0.769\n",
      "train loss on epoch 3361 : 0.097\n",
      "train accuracy on epoch 3361: 0.944\n",
      "test loss on epoch 3361: 0.318\n",
      "test accuracy on epoch 3361: 0.769\n",
      "train loss on epoch 3362 : 0.093\n",
      "train accuracy on epoch 3362: 0.944\n",
      "test loss on epoch 3362: 0.320\n",
      "test accuracy on epoch 3362: 0.769\n",
      "train loss on epoch 3363 : 0.096\n",
      "train accuracy on epoch 3363: 0.944\n",
      "test loss on epoch 3363: 0.319\n",
      "test accuracy on epoch 3363: 0.769\n",
      "train loss on epoch 3364 : 0.088\n",
      "train accuracy on epoch 3364: 0.944\n",
      "test loss on epoch 3364: 0.320\n",
      "test accuracy on epoch 3364: 0.769\n",
      "train loss on epoch 3365 : 0.156\n",
      "train accuracy on epoch 3365: 0.944\n",
      "test loss on epoch 3365: 0.318\n",
      "test accuracy on epoch 3365: 0.769\n",
      "train loss on epoch 3366 : 0.101\n",
      "train accuracy on epoch 3366: 1.000\n",
      "test loss on epoch 3366: 0.313\n",
      "test accuracy on epoch 3366: 0.769\n",
      "train loss on epoch 3367 : 0.147\n",
      "train accuracy on epoch 3367: 0.944\n",
      "test loss on epoch 3367: 0.317\n",
      "test accuracy on epoch 3367: 0.769\n",
      "train loss on epoch 3368 : 0.162\n",
      "train accuracy on epoch 3368: 0.944\n",
      "test loss on epoch 3368: 0.324\n",
      "test accuracy on epoch 3368: 0.769\n",
      "train loss on epoch 3369 : 0.071\n",
      "train accuracy on epoch 3369: 1.000\n",
      "test loss on epoch 3369: 0.323\n",
      "test accuracy on epoch 3369: 0.769\n",
      "train loss on epoch 3370 : 0.360\n",
      "train accuracy on epoch 3370: 0.889\n",
      "test loss on epoch 3370: 0.323\n",
      "test accuracy on epoch 3370: 0.769\n",
      "train loss on epoch 3371 : 0.112\n",
      "train accuracy on epoch 3371: 0.944\n",
      "test loss on epoch 3371: 0.323\n",
      "test accuracy on epoch 3371: 0.769\n",
      "train loss on epoch 3372 : 0.081\n",
      "train accuracy on epoch 3372: 0.944\n",
      "test loss on epoch 3372: 0.325\n",
      "test accuracy on epoch 3372: 0.769\n",
      "train loss on epoch 3373 : 0.244\n",
      "train accuracy on epoch 3373: 0.889\n",
      "test loss on epoch 3373: 0.316\n",
      "test accuracy on epoch 3373: 0.769\n",
      "train loss on epoch 3374 : 0.023\n",
      "train accuracy on epoch 3374: 1.000\n",
      "test loss on epoch 3374: 0.320\n",
      "test accuracy on epoch 3374: 0.769\n",
      "train loss on epoch 3375 : 0.191\n",
      "train accuracy on epoch 3375: 0.889\n",
      "test loss on epoch 3375: 0.323\n",
      "test accuracy on epoch 3375: 0.769\n",
      "train loss on epoch 3376 : 0.534\n",
      "train accuracy on epoch 3376: 0.889\n",
      "test loss on epoch 3376: 0.319\n",
      "test accuracy on epoch 3376: 0.769\n",
      "train loss on epoch 3377 : 0.270\n",
      "train accuracy on epoch 3377: 0.889\n",
      "test loss on epoch 3377: 0.311\n",
      "test accuracy on epoch 3377: 0.769\n",
      "train loss on epoch 3378 : 0.155\n",
      "train accuracy on epoch 3378: 0.944\n",
      "test loss on epoch 3378: 0.310\n",
      "test accuracy on epoch 3378: 0.769\n",
      "train loss on epoch 3379 : 0.102\n",
      "train accuracy on epoch 3379: 0.944\n",
      "test loss on epoch 3379: 0.316\n",
      "test accuracy on epoch 3379: 0.769\n",
      "train loss on epoch 3380 : 0.174\n",
      "train accuracy on epoch 3380: 0.944\n",
      "test loss on epoch 3380: 0.321\n",
      "test accuracy on epoch 3380: 0.769\n",
      "train loss on epoch 3381 : 0.129\n",
      "train accuracy on epoch 3381: 0.944\n",
      "test loss on epoch 3381: 0.328\n",
      "test accuracy on epoch 3381: 0.769\n",
      "train loss on epoch 3382 : 0.160\n",
      "train accuracy on epoch 3382: 0.889\n",
      "test loss on epoch 3382: 0.332\n",
      "test accuracy on epoch 3382: 0.769\n",
      "train loss on epoch 3383 : 0.144\n",
      "train accuracy on epoch 3383: 0.944\n",
      "test loss on epoch 3383: 0.332\n",
      "test accuracy on epoch 3383: 0.769\n",
      "train loss on epoch 3384 : 0.114\n",
      "train accuracy on epoch 3384: 0.944\n",
      "test loss on epoch 3384: 0.330\n",
      "test accuracy on epoch 3384: 0.769\n",
      "train loss on epoch 3385 : 0.056\n",
      "train accuracy on epoch 3385: 1.000\n",
      "test loss on epoch 3385: 0.331\n",
      "test accuracy on epoch 3385: 0.769\n",
      "train loss on epoch 3386 : 0.230\n",
      "train accuracy on epoch 3386: 0.889\n",
      "test loss on epoch 3386: 0.327\n",
      "test accuracy on epoch 3386: 0.769\n",
      "train loss on epoch 3387 : 0.042\n",
      "train accuracy on epoch 3387: 1.000\n",
      "test loss on epoch 3387: 0.328\n",
      "test accuracy on epoch 3387: 0.769\n",
      "train loss on epoch 3388 : 0.210\n",
      "train accuracy on epoch 3388: 0.889\n",
      "test loss on epoch 3388: 0.328\n",
      "test accuracy on epoch 3388: 0.769\n",
      "train loss on epoch 3389 : 0.253\n",
      "train accuracy on epoch 3389: 0.889\n",
      "test loss on epoch 3389: 0.322\n",
      "test accuracy on epoch 3389: 0.769\n",
      "train loss on epoch 3390 : 0.197\n",
      "train accuracy on epoch 3390: 0.889\n",
      "test loss on epoch 3390: 0.319\n",
      "test accuracy on epoch 3390: 0.769\n",
      "train loss on epoch 3391 : 0.295\n",
      "train accuracy on epoch 3391: 0.889\n",
      "test loss on epoch 3391: 0.318\n",
      "test accuracy on epoch 3391: 0.769\n",
      "train loss on epoch 3392 : 0.286\n",
      "train accuracy on epoch 3392: 0.944\n",
      "test loss on epoch 3392: 0.314\n",
      "test accuracy on epoch 3392: 0.769\n",
      "train loss on epoch 3393 : 0.344\n",
      "train accuracy on epoch 3393: 0.889\n",
      "test loss on epoch 3393: 0.314\n",
      "test accuracy on epoch 3393: 0.769\n",
      "train loss on epoch 3394 : 0.168\n",
      "train accuracy on epoch 3394: 0.944\n",
      "test loss on epoch 3394: 0.318\n",
      "test accuracy on epoch 3394: 0.769\n",
      "train loss on epoch 3395 : 0.164\n",
      "train accuracy on epoch 3395: 0.944\n",
      "test loss on epoch 3395: 0.321\n",
      "test accuracy on epoch 3395: 0.769\n",
      "train loss on epoch 3396 : 0.099\n",
      "train accuracy on epoch 3396: 0.944\n",
      "test loss on epoch 3396: 0.321\n",
      "test accuracy on epoch 3396: 0.769\n",
      "train loss on epoch 3397 : 0.336\n",
      "train accuracy on epoch 3397: 0.889\n",
      "test loss on epoch 3397: 0.331\n",
      "test accuracy on epoch 3397: 0.769\n",
      "train loss on epoch 3398 : 0.078\n",
      "train accuracy on epoch 3398: 1.000\n",
      "test loss on epoch 3398: 0.336\n",
      "test accuracy on epoch 3398: 0.769\n",
      "train loss on epoch 3399 : 0.248\n",
      "train accuracy on epoch 3399: 0.889\n",
      "test loss on epoch 3399: 0.341\n",
      "test accuracy on epoch 3399: 0.769\n",
      "train loss on epoch 3400 : 0.309\n",
      "train accuracy on epoch 3400: 0.944\n",
      "test loss on epoch 3400: 0.354\n",
      "test accuracy on epoch 3400: 0.769\n",
      "train loss on epoch 3401 : 0.129\n",
      "train accuracy on epoch 3401: 0.944\n",
      "test loss on epoch 3401: 0.362\n",
      "test accuracy on epoch 3401: 0.769\n",
      "train loss on epoch 3402 : 0.059\n",
      "train accuracy on epoch 3402: 1.000\n",
      "test loss on epoch 3402: 0.365\n",
      "test accuracy on epoch 3402: 0.769\n",
      "train loss on epoch 3403 : 0.130\n",
      "train accuracy on epoch 3403: 0.944\n",
      "test loss on epoch 3403: 0.354\n",
      "test accuracy on epoch 3403: 0.769\n",
      "train loss on epoch 3404 : 0.072\n",
      "train accuracy on epoch 3404: 0.944\n",
      "test loss on epoch 3404: 0.343\n",
      "test accuracy on epoch 3404: 0.769\n",
      "train loss on epoch 3405 : 0.116\n",
      "train accuracy on epoch 3405: 0.944\n",
      "test loss on epoch 3405: 0.341\n",
      "test accuracy on epoch 3405: 0.769\n",
      "train loss on epoch 3406 : 0.267\n",
      "train accuracy on epoch 3406: 0.889\n",
      "test loss on epoch 3406: 0.339\n",
      "test accuracy on epoch 3406: 0.769\n",
      "train loss on epoch 3407 : 0.045\n",
      "train accuracy on epoch 3407: 1.000\n",
      "test loss on epoch 3407: 0.337\n",
      "test accuracy on epoch 3407: 0.769\n",
      "train loss on epoch 3408 : 0.340\n",
      "train accuracy on epoch 3408: 0.889\n",
      "test loss on epoch 3408: 0.337\n",
      "test accuracy on epoch 3408: 0.769\n",
      "train loss on epoch 3409 : 0.080\n",
      "train accuracy on epoch 3409: 1.000\n",
      "test loss on epoch 3409: 0.331\n",
      "test accuracy on epoch 3409: 0.769\n",
      "train loss on epoch 3410 : 0.291\n",
      "train accuracy on epoch 3410: 0.833\n",
      "test loss on epoch 3410: 0.328\n",
      "test accuracy on epoch 3410: 0.769\n",
      "train loss on epoch 3411 : 0.137\n",
      "train accuracy on epoch 3411: 0.944\n",
      "test loss on epoch 3411: 0.326\n",
      "test accuracy on epoch 3411: 0.769\n",
      "train loss on epoch 3412 : 0.326\n",
      "train accuracy on epoch 3412: 0.889\n",
      "test loss on epoch 3412: 0.321\n",
      "test accuracy on epoch 3412: 0.769\n",
      "train loss on epoch 3413 : 0.182\n",
      "train accuracy on epoch 3413: 0.944\n",
      "test loss on epoch 3413: 0.317\n",
      "test accuracy on epoch 3413: 0.769\n",
      "train loss on epoch 3414 : 0.093\n",
      "train accuracy on epoch 3414: 1.000\n",
      "test loss on epoch 3414: 0.313\n",
      "test accuracy on epoch 3414: 0.769\n",
      "train loss on epoch 3415 : 0.157\n",
      "train accuracy on epoch 3415: 0.944\n",
      "test loss on epoch 3415: 0.317\n",
      "test accuracy on epoch 3415: 0.769\n",
      "train loss on epoch 3416 : 0.289\n",
      "train accuracy on epoch 3416: 0.833\n",
      "test loss on epoch 3416: 0.312\n",
      "test accuracy on epoch 3416: 0.769\n",
      "train loss on epoch 3417 : 0.035\n",
      "train accuracy on epoch 3417: 1.000\n",
      "test loss on epoch 3417: 0.312\n",
      "test accuracy on epoch 3417: 0.769\n",
      "train loss on epoch 3418 : 0.081\n",
      "train accuracy on epoch 3418: 1.000\n",
      "test loss on epoch 3418: 0.319\n",
      "test accuracy on epoch 3418: 0.769\n",
      "train loss on epoch 3419 : 0.334\n",
      "train accuracy on epoch 3419: 0.833\n",
      "test loss on epoch 3419: 0.310\n",
      "test accuracy on epoch 3419: 0.769\n",
      "train loss on epoch 3420 : 0.160\n",
      "train accuracy on epoch 3420: 0.944\n",
      "test loss on epoch 3420: 0.316\n",
      "test accuracy on epoch 3420: 0.769\n",
      "train loss on epoch 3421 : 0.280\n",
      "train accuracy on epoch 3421: 0.944\n",
      "test loss on epoch 3421: 0.320\n",
      "test accuracy on epoch 3421: 0.769\n",
      "train loss on epoch 3422 : 0.275\n",
      "train accuracy on epoch 3422: 0.833\n",
      "test loss on epoch 3422: 0.318\n",
      "test accuracy on epoch 3422: 0.769\n",
      "train loss on epoch 3423 : 0.100\n",
      "train accuracy on epoch 3423: 0.944\n",
      "test loss on epoch 3423: 0.320\n",
      "test accuracy on epoch 3423: 0.769\n",
      "train loss on epoch 3424 : 0.172\n",
      "train accuracy on epoch 3424: 0.889\n",
      "test loss on epoch 3424: 0.324\n",
      "test accuracy on epoch 3424: 0.769\n",
      "train loss on epoch 3425 : 0.298\n",
      "train accuracy on epoch 3425: 0.889\n",
      "test loss on epoch 3425: 0.315\n",
      "test accuracy on epoch 3425: 0.769\n",
      "train loss on epoch 3426 : 0.226\n",
      "train accuracy on epoch 3426: 0.944\n",
      "test loss on epoch 3426: 0.319\n",
      "test accuracy on epoch 3426: 0.769\n",
      "train loss on epoch 3427 : 0.061\n",
      "train accuracy on epoch 3427: 1.000\n",
      "test loss on epoch 3427: 0.319\n",
      "test accuracy on epoch 3427: 0.769\n",
      "train loss on epoch 3428 : 0.134\n",
      "train accuracy on epoch 3428: 0.944\n",
      "test loss on epoch 3428: 0.317\n",
      "test accuracy on epoch 3428: 0.769\n",
      "train loss on epoch 3429 : 0.162\n",
      "train accuracy on epoch 3429: 0.944\n",
      "test loss on epoch 3429: 0.316\n",
      "test accuracy on epoch 3429: 0.769\n",
      "train loss on epoch 3430 : 0.139\n",
      "train accuracy on epoch 3430: 0.944\n",
      "test loss on epoch 3430: 0.317\n",
      "test accuracy on epoch 3430: 0.769\n",
      "train loss on epoch 3431 : 0.162\n",
      "train accuracy on epoch 3431: 0.889\n",
      "test loss on epoch 3431: 0.321\n",
      "test accuracy on epoch 3431: 0.769\n",
      "train loss on epoch 3432 : 0.180\n",
      "train accuracy on epoch 3432: 0.889\n",
      "test loss on epoch 3432: 0.323\n",
      "test accuracy on epoch 3432: 0.769\n",
      "train loss on epoch 3433 : 0.199\n",
      "train accuracy on epoch 3433: 0.889\n",
      "test loss on epoch 3433: 0.324\n",
      "test accuracy on epoch 3433: 0.769\n",
      "train loss on epoch 3434 : 0.117\n",
      "train accuracy on epoch 3434: 0.944\n",
      "test loss on epoch 3434: 0.328\n",
      "test accuracy on epoch 3434: 0.769\n",
      "train loss on epoch 3435 : 0.137\n",
      "train accuracy on epoch 3435: 0.944\n",
      "test loss on epoch 3435: 0.323\n",
      "test accuracy on epoch 3435: 0.769\n",
      "train loss on epoch 3436 : 0.109\n",
      "train accuracy on epoch 3436: 0.889\n",
      "test loss on epoch 3436: 0.322\n",
      "test accuracy on epoch 3436: 0.769\n",
      "train loss on epoch 3437 : 0.175\n",
      "train accuracy on epoch 3437: 0.944\n",
      "test loss on epoch 3437: 0.314\n",
      "test accuracy on epoch 3437: 0.769\n",
      "train loss on epoch 3438 : 0.091\n",
      "train accuracy on epoch 3438: 0.944\n",
      "test loss on epoch 3438: 0.319\n",
      "test accuracy on epoch 3438: 0.769\n",
      "train loss on epoch 3439 : 0.171\n",
      "train accuracy on epoch 3439: 0.944\n",
      "test loss on epoch 3439: 0.317\n",
      "test accuracy on epoch 3439: 0.769\n",
      "train loss on epoch 3440 : 0.402\n",
      "train accuracy on epoch 3440: 0.889\n",
      "test loss on epoch 3440: 0.318\n",
      "test accuracy on epoch 3440: 0.769\n",
      "train loss on epoch 3441 : 0.226\n",
      "train accuracy on epoch 3441: 0.944\n",
      "test loss on epoch 3441: 0.320\n",
      "test accuracy on epoch 3441: 0.769\n",
      "train loss on epoch 3442 : 0.395\n",
      "train accuracy on epoch 3442: 0.833\n",
      "test loss on epoch 3442: 0.316\n",
      "test accuracy on epoch 3442: 0.769\n",
      "train loss on epoch 3443 : 0.134\n",
      "train accuracy on epoch 3443: 0.944\n",
      "test loss on epoch 3443: 0.316\n",
      "test accuracy on epoch 3443: 0.769\n",
      "train loss on epoch 3444 : 0.164\n",
      "train accuracy on epoch 3444: 0.889\n",
      "test loss on epoch 3444: 0.312\n",
      "test accuracy on epoch 3444: 0.769\n",
      "train loss on epoch 3445 : 0.346\n",
      "train accuracy on epoch 3445: 0.944\n",
      "test loss on epoch 3445: 0.314\n",
      "test accuracy on epoch 3445: 0.769\n",
      "train loss on epoch 3446 : 0.055\n",
      "train accuracy on epoch 3446: 1.000\n",
      "test loss on epoch 3446: 0.316\n",
      "test accuracy on epoch 3446: 0.769\n",
      "train loss on epoch 3447 : 0.207\n",
      "train accuracy on epoch 3447: 0.944\n",
      "test loss on epoch 3447: 0.319\n",
      "test accuracy on epoch 3447: 0.769\n",
      "train loss on epoch 3448 : 0.189\n",
      "train accuracy on epoch 3448: 0.944\n",
      "test loss on epoch 3448: 0.325\n",
      "test accuracy on epoch 3448: 0.769\n",
      "train loss on epoch 3449 : 0.034\n",
      "train accuracy on epoch 3449: 1.000\n",
      "test loss on epoch 3449: 0.330\n",
      "test accuracy on epoch 3449: 0.769\n",
      "train loss on epoch 3450 : 0.211\n",
      "train accuracy on epoch 3450: 0.889\n",
      "test loss on epoch 3450: 0.336\n",
      "test accuracy on epoch 3450: 0.769\n",
      "train loss on epoch 3451 : 0.088\n",
      "train accuracy on epoch 3451: 1.000\n",
      "test loss on epoch 3451: 0.331\n",
      "test accuracy on epoch 3451: 0.769\n",
      "train loss on epoch 3452 : 0.066\n",
      "train accuracy on epoch 3452: 0.944\n",
      "test loss on epoch 3452: 0.338\n",
      "test accuracy on epoch 3452: 0.769\n",
      "train loss on epoch 3453 : 0.342\n",
      "train accuracy on epoch 3453: 0.889\n",
      "test loss on epoch 3453: 0.341\n",
      "test accuracy on epoch 3453: 0.769\n",
      "train loss on epoch 3454 : 0.148\n",
      "train accuracy on epoch 3454: 0.944\n",
      "test loss on epoch 3454: 0.339\n",
      "test accuracy on epoch 3454: 0.769\n",
      "train loss on epoch 3455 : 0.124\n",
      "train accuracy on epoch 3455: 0.944\n",
      "test loss on epoch 3455: 0.331\n",
      "test accuracy on epoch 3455: 0.769\n",
      "train loss on epoch 3456 : 0.147\n",
      "train accuracy on epoch 3456: 0.944\n",
      "test loss on epoch 3456: 0.330\n",
      "test accuracy on epoch 3456: 0.769\n",
      "train loss on epoch 3457 : 0.467\n",
      "train accuracy on epoch 3457: 0.889\n",
      "test loss on epoch 3457: 0.323\n",
      "test accuracy on epoch 3457: 0.769\n",
      "train loss on epoch 3458 : 0.193\n",
      "train accuracy on epoch 3458: 0.889\n",
      "test loss on epoch 3458: 0.330\n",
      "test accuracy on epoch 3458: 0.769\n",
      "train loss on epoch 3459 : 0.497\n",
      "train accuracy on epoch 3459: 0.889\n",
      "test loss on epoch 3459: 0.323\n",
      "test accuracy on epoch 3459: 0.769\n",
      "train loss on epoch 3460 : 0.055\n",
      "train accuracy on epoch 3460: 1.000\n",
      "test loss on epoch 3460: 0.332\n",
      "test accuracy on epoch 3460: 0.769\n",
      "train loss on epoch 3461 : 0.061\n",
      "train accuracy on epoch 3461: 0.944\n",
      "test loss on epoch 3461: 0.329\n",
      "test accuracy on epoch 3461: 0.769\n",
      "train loss on epoch 3462 : 0.290\n",
      "train accuracy on epoch 3462: 0.833\n",
      "test loss on epoch 3462: 0.330\n",
      "test accuracy on epoch 3462: 0.769\n",
      "train loss on epoch 3463 : 0.195\n",
      "train accuracy on epoch 3463: 0.889\n",
      "test loss on epoch 3463: 0.320\n",
      "test accuracy on epoch 3463: 0.769\n",
      "train loss on epoch 3464 : 0.104\n",
      "train accuracy on epoch 3464: 0.944\n",
      "test loss on epoch 3464: 0.324\n",
      "test accuracy on epoch 3464: 0.769\n",
      "train loss on epoch 3465 : 0.077\n",
      "train accuracy on epoch 3465: 0.944\n",
      "test loss on epoch 3465: 0.327\n",
      "test accuracy on epoch 3465: 0.769\n",
      "train loss on epoch 3466 : 0.163\n",
      "train accuracy on epoch 3466: 0.889\n",
      "test loss on epoch 3466: 0.322\n",
      "test accuracy on epoch 3466: 0.769\n",
      "train loss on epoch 3467 : 0.340\n",
      "train accuracy on epoch 3467: 0.889\n",
      "test loss on epoch 3467: 0.322\n",
      "test accuracy on epoch 3467: 0.769\n",
      "train loss on epoch 3468 : 0.055\n",
      "train accuracy on epoch 3468: 1.000\n",
      "test loss on epoch 3468: 0.324\n",
      "test accuracy on epoch 3468: 0.769\n",
      "train loss on epoch 3469 : 0.289\n",
      "train accuracy on epoch 3469: 0.944\n",
      "test loss on epoch 3469: 0.322\n",
      "test accuracy on epoch 3469: 0.769\n",
      "train loss on epoch 3470 : 0.103\n",
      "train accuracy on epoch 3470: 0.944\n",
      "test loss on epoch 3470: 0.320\n",
      "test accuracy on epoch 3470: 0.769\n",
      "train loss on epoch 3471 : 0.137\n",
      "train accuracy on epoch 3471: 0.944\n",
      "test loss on epoch 3471: 0.318\n",
      "test accuracy on epoch 3471: 0.692\n",
      "train loss on epoch 3472 : 0.224\n",
      "train accuracy on epoch 3472: 0.889\n",
      "test loss on epoch 3472: 0.319\n",
      "test accuracy on epoch 3472: 0.769\n",
      "train loss on epoch 3473 : 0.184\n",
      "train accuracy on epoch 3473: 0.889\n",
      "test loss on epoch 3473: 0.314\n",
      "test accuracy on epoch 3473: 0.769\n",
      "train loss on epoch 3474 : 0.107\n",
      "train accuracy on epoch 3474: 0.944\n",
      "test loss on epoch 3474: 0.309\n",
      "test accuracy on epoch 3474: 0.769\n",
      "train loss on epoch 3475 : 0.397\n",
      "train accuracy on epoch 3475: 0.944\n",
      "test loss on epoch 3475: 0.308\n",
      "test accuracy on epoch 3475: 0.769\n",
      "train loss on epoch 3476 : 0.059\n",
      "train accuracy on epoch 3476: 0.944\n",
      "test loss on epoch 3476: 0.300\n",
      "test accuracy on epoch 3476: 0.769\n",
      "train loss on epoch 3477 : 0.204\n",
      "train accuracy on epoch 3477: 0.944\n",
      "test loss on epoch 3477: 0.301\n",
      "test accuracy on epoch 3477: 0.769\n",
      "train loss on epoch 3478 : 0.081\n",
      "train accuracy on epoch 3478: 0.944\n",
      "test loss on epoch 3478: 0.320\n",
      "test accuracy on epoch 3478: 0.769\n",
      "train loss on epoch 3479 : 0.076\n",
      "train accuracy on epoch 3479: 1.000\n",
      "test loss on epoch 3479: 0.309\n",
      "test accuracy on epoch 3479: 0.769\n",
      "train loss on epoch 3480 : 0.170\n",
      "train accuracy on epoch 3480: 0.944\n",
      "test loss on epoch 3480: 0.308\n",
      "test accuracy on epoch 3480: 0.769\n",
      "train loss on epoch 3481 : 0.224\n",
      "train accuracy on epoch 3481: 0.889\n",
      "test loss on epoch 3481: 0.306\n",
      "test accuracy on epoch 3481: 0.846\n",
      "train loss on epoch 3482 : 0.128\n",
      "train accuracy on epoch 3482: 0.944\n",
      "test loss on epoch 3482: 0.306\n",
      "test accuracy on epoch 3482: 0.846\n",
      "train loss on epoch 3483 : 0.328\n",
      "train accuracy on epoch 3483: 0.944\n",
      "test loss on epoch 3483: 0.320\n",
      "test accuracy on epoch 3483: 0.769\n",
      "train loss on epoch 3484 : 0.075\n",
      "train accuracy on epoch 3484: 1.000\n",
      "test loss on epoch 3484: 0.320\n",
      "test accuracy on epoch 3484: 0.769\n",
      "train loss on epoch 3485 : 0.274\n",
      "train accuracy on epoch 3485: 0.889\n",
      "test loss on epoch 3485: 0.318\n",
      "test accuracy on epoch 3485: 0.769\n",
      "train loss on epoch 3486 : 0.057\n",
      "train accuracy on epoch 3486: 0.944\n",
      "test loss on epoch 3486: 0.321\n",
      "test accuracy on epoch 3486: 0.769\n",
      "train loss on epoch 3487 : 0.202\n",
      "train accuracy on epoch 3487: 0.889\n",
      "test loss on epoch 3487: 0.308\n",
      "test accuracy on epoch 3487: 0.846\n",
      "train loss on epoch 3488 : 0.137\n",
      "train accuracy on epoch 3488: 0.944\n",
      "test loss on epoch 3488: 0.308\n",
      "test accuracy on epoch 3488: 0.769\n",
      "train loss on epoch 3489 : 0.252\n",
      "train accuracy on epoch 3489: 0.944\n",
      "test loss on epoch 3489: 0.318\n",
      "test accuracy on epoch 3489: 0.769\n",
      "train loss on epoch 3490 : 0.265\n",
      "train accuracy on epoch 3490: 0.833\n",
      "test loss on epoch 3490: 0.320\n",
      "test accuracy on epoch 3490: 0.769\n",
      "train loss on epoch 3491 : 0.202\n",
      "train accuracy on epoch 3491: 0.944\n",
      "test loss on epoch 3491: 0.305\n",
      "test accuracy on epoch 3491: 0.769\n",
      "train loss on epoch 3492 : 0.145\n",
      "train accuracy on epoch 3492: 0.889\n",
      "test loss on epoch 3492: 0.310\n",
      "test accuracy on epoch 3492: 0.769\n",
      "train loss on epoch 3493 : 0.104\n",
      "train accuracy on epoch 3493: 0.889\n",
      "test loss on epoch 3493: 0.307\n",
      "test accuracy on epoch 3493: 0.769\n",
      "train loss on epoch 3494 : 0.260\n",
      "train accuracy on epoch 3494: 0.944\n",
      "test loss on epoch 3494: 0.316\n",
      "test accuracy on epoch 3494: 0.769\n",
      "train loss on epoch 3495 : 0.076\n",
      "train accuracy on epoch 3495: 0.944\n",
      "test loss on epoch 3495: 0.311\n",
      "test accuracy on epoch 3495: 0.769\n",
      "train loss on epoch 3496 : 0.103\n",
      "train accuracy on epoch 3496: 0.944\n",
      "test loss on epoch 3496: 0.312\n",
      "test accuracy on epoch 3496: 0.769\n",
      "train loss on epoch 3497 : 0.189\n",
      "train accuracy on epoch 3497: 0.944\n",
      "test loss on epoch 3497: 0.319\n",
      "test accuracy on epoch 3497: 0.769\n",
      "train loss on epoch 3498 : 0.071\n",
      "train accuracy on epoch 3498: 1.000\n",
      "test loss on epoch 3498: 0.308\n",
      "test accuracy on epoch 3498: 0.769\n",
      "train loss on epoch 3499 : 0.313\n",
      "train accuracy on epoch 3499: 0.889\n",
      "test loss on epoch 3499: 0.303\n",
      "test accuracy on epoch 3499: 0.769\n",
      "train loss on epoch 3500 : 0.188\n",
      "train accuracy on epoch 3500: 0.889\n",
      "test loss on epoch 3500: 0.320\n",
      "test accuracy on epoch 3500: 0.769\n",
      "train loss on epoch 3501 : 0.121\n",
      "train accuracy on epoch 3501: 0.944\n",
      "test loss on epoch 3501: 0.303\n",
      "test accuracy on epoch 3501: 0.769\n",
      "train loss on epoch 3502 : 0.306\n",
      "train accuracy on epoch 3502: 0.889\n",
      "test loss on epoch 3502: 0.309\n",
      "test accuracy on epoch 3502: 0.769\n",
      "train loss on epoch 3503 : 0.091\n",
      "train accuracy on epoch 3503: 0.944\n",
      "test loss on epoch 3503: 0.317\n",
      "test accuracy on epoch 3503: 0.769\n",
      "train loss on epoch 3504 : 0.238\n",
      "train accuracy on epoch 3504: 0.944\n",
      "test loss on epoch 3504: 0.309\n",
      "test accuracy on epoch 3504: 0.769\n",
      "train loss on epoch 3505 : 0.154\n",
      "train accuracy on epoch 3505: 0.889\n",
      "test loss on epoch 3505: 0.303\n",
      "test accuracy on epoch 3505: 0.769\n",
      "train loss on epoch 3506 : 0.097\n",
      "train accuracy on epoch 3506: 0.944\n",
      "test loss on epoch 3506: 0.321\n",
      "test accuracy on epoch 3506: 0.769\n",
      "train loss on epoch 3507 : 0.135\n",
      "train accuracy on epoch 3507: 1.000\n",
      "test loss on epoch 3507: 0.309\n",
      "test accuracy on epoch 3507: 0.769\n",
      "train loss on epoch 3508 : 0.061\n",
      "train accuracy on epoch 3508: 1.000\n",
      "test loss on epoch 3508: 0.315\n",
      "test accuracy on epoch 3508: 0.769\n",
      "train loss on epoch 3509 : 0.199\n",
      "train accuracy on epoch 3509: 0.944\n",
      "test loss on epoch 3509: 0.319\n",
      "test accuracy on epoch 3509: 0.769\n",
      "train loss on epoch 3510 : 0.185\n",
      "train accuracy on epoch 3510: 0.889\n",
      "test loss on epoch 3510: 0.314\n",
      "test accuracy on epoch 3510: 0.769\n",
      "train loss on epoch 3511 : 0.079\n",
      "train accuracy on epoch 3511: 1.000\n",
      "test loss on epoch 3511: 0.316\n",
      "test accuracy on epoch 3511: 0.692\n",
      "train loss on epoch 3512 : 0.037\n",
      "train accuracy on epoch 3512: 1.000\n",
      "test loss on epoch 3512: 0.319\n",
      "test accuracy on epoch 3512: 0.692\n",
      "train loss on epoch 3513 : 0.115\n",
      "train accuracy on epoch 3513: 0.944\n",
      "test loss on epoch 3513: 0.316\n",
      "test accuracy on epoch 3513: 0.769\n",
      "train loss on epoch 3514 : 0.275\n",
      "train accuracy on epoch 3514: 0.944\n",
      "test loss on epoch 3514: 0.317\n",
      "test accuracy on epoch 3514: 0.769\n",
      "train loss on epoch 3515 : 0.076\n",
      "train accuracy on epoch 3515: 0.944\n",
      "test loss on epoch 3515: 0.324\n",
      "test accuracy on epoch 3515: 0.769\n",
      "train loss on epoch 3516 : 0.078\n",
      "train accuracy on epoch 3516: 0.944\n",
      "test loss on epoch 3516: 0.327\n",
      "test accuracy on epoch 3516: 0.769\n",
      "train loss on epoch 3517 : 0.227\n",
      "train accuracy on epoch 3517: 0.889\n",
      "test loss on epoch 3517: 0.316\n",
      "test accuracy on epoch 3517: 0.769\n",
      "train loss on epoch 3518 : 0.139\n",
      "train accuracy on epoch 3518: 0.944\n",
      "test loss on epoch 3518: 0.321\n",
      "test accuracy on epoch 3518: 0.769\n",
      "train loss on epoch 3519 : 0.066\n",
      "train accuracy on epoch 3519: 1.000\n",
      "test loss on epoch 3519: 0.317\n",
      "test accuracy on epoch 3519: 0.769\n",
      "train loss on epoch 3520 : 0.124\n",
      "train accuracy on epoch 3520: 0.944\n",
      "test loss on epoch 3520: 0.316\n",
      "test accuracy on epoch 3520: 0.769\n",
      "train loss on epoch 3521 : 0.205\n",
      "train accuracy on epoch 3521: 0.889\n",
      "test loss on epoch 3521: 0.309\n",
      "test accuracy on epoch 3521: 0.769\n",
      "train loss on epoch 3522 : 0.194\n",
      "train accuracy on epoch 3522: 0.889\n",
      "test loss on epoch 3522: 0.312\n",
      "test accuracy on epoch 3522: 0.769\n",
      "train loss on epoch 3523 : 0.394\n",
      "train accuracy on epoch 3523: 0.889\n",
      "test loss on epoch 3523: 0.317\n",
      "test accuracy on epoch 3523: 0.769\n",
      "train loss on epoch 3524 : 0.311\n",
      "train accuracy on epoch 3524: 0.944\n",
      "test loss on epoch 3524: 0.314\n",
      "test accuracy on epoch 3524: 0.769\n",
      "train loss on epoch 3525 : 0.045\n",
      "train accuracy on epoch 3525: 1.000\n",
      "test loss on epoch 3525: 0.314\n",
      "test accuracy on epoch 3525: 0.769\n",
      "train loss on epoch 3526 : 0.154\n",
      "train accuracy on epoch 3526: 0.944\n",
      "test loss on epoch 3526: 0.314\n",
      "test accuracy on epoch 3526: 0.769\n",
      "train loss on epoch 3527 : 0.255\n",
      "train accuracy on epoch 3527: 0.944\n",
      "test loss on epoch 3527: 0.316\n",
      "test accuracy on epoch 3527: 0.769\n",
      "train loss on epoch 3528 : 0.239\n",
      "train accuracy on epoch 3528: 0.889\n",
      "test loss on epoch 3528: 0.306\n",
      "test accuracy on epoch 3528: 0.846\n",
      "train loss on epoch 3529 : 0.283\n",
      "train accuracy on epoch 3529: 0.889\n",
      "test loss on epoch 3529: 0.309\n",
      "test accuracy on epoch 3529: 0.769\n",
      "train loss on epoch 3530 : 0.230\n",
      "train accuracy on epoch 3530: 0.889\n",
      "test loss on epoch 3530: 0.301\n",
      "test accuracy on epoch 3530: 0.846\n",
      "train loss on epoch 3531 : 0.119\n",
      "train accuracy on epoch 3531: 0.944\n",
      "test loss on epoch 3531: 0.309\n",
      "test accuracy on epoch 3531: 0.769\n",
      "train loss on epoch 3532 : 0.132\n",
      "train accuracy on epoch 3532: 0.944\n",
      "test loss on epoch 3532: 0.309\n",
      "test accuracy on epoch 3532: 0.769\n",
      "train loss on epoch 3533 : 0.086\n",
      "train accuracy on epoch 3533: 1.000\n",
      "test loss on epoch 3533: 0.303\n",
      "test accuracy on epoch 3533: 0.769\n",
      "train loss on epoch 3534 : 0.303\n",
      "train accuracy on epoch 3534: 0.889\n",
      "test loss on epoch 3534: 0.310\n",
      "test accuracy on epoch 3534: 0.769\n",
      "train loss on epoch 3535 : 0.169\n",
      "train accuracy on epoch 3535: 0.944\n",
      "test loss on epoch 3535: 0.303\n",
      "test accuracy on epoch 3535: 0.769\n",
      "train loss on epoch 3536 : 0.313\n",
      "train accuracy on epoch 3536: 0.944\n",
      "test loss on epoch 3536: 0.311\n",
      "test accuracy on epoch 3536: 0.769\n",
      "train loss on epoch 3537 : 0.058\n",
      "train accuracy on epoch 3537: 1.000\n",
      "test loss on epoch 3537: 0.316\n",
      "test accuracy on epoch 3537: 0.769\n",
      "train loss on epoch 3538 : 0.180\n",
      "train accuracy on epoch 3538: 0.889\n",
      "test loss on epoch 3538: 0.314\n",
      "test accuracy on epoch 3538: 0.769\n",
      "train loss on epoch 3539 : 0.165\n",
      "train accuracy on epoch 3539: 0.889\n",
      "test loss on epoch 3539: 0.306\n",
      "test accuracy on epoch 3539: 0.769\n",
      "train loss on epoch 3540 : 0.244\n",
      "train accuracy on epoch 3540: 0.944\n",
      "test loss on epoch 3540: 0.309\n",
      "test accuracy on epoch 3540: 0.846\n",
      "train loss on epoch 3541 : 0.162\n",
      "train accuracy on epoch 3541: 0.889\n",
      "test loss on epoch 3541: 0.318\n",
      "test accuracy on epoch 3541: 0.769\n",
      "train loss on epoch 3542 : 0.225\n",
      "train accuracy on epoch 3542: 0.889\n",
      "test loss on epoch 3542: 0.318\n",
      "test accuracy on epoch 3542: 0.769\n",
      "train loss on epoch 3543 : 0.040\n",
      "train accuracy on epoch 3543: 1.000\n",
      "test loss on epoch 3543: 0.319\n",
      "test accuracy on epoch 3543: 0.769\n",
      "train loss on epoch 3544 : 0.273\n",
      "train accuracy on epoch 3544: 0.889\n",
      "test loss on epoch 3544: 0.320\n",
      "test accuracy on epoch 3544: 0.769\n",
      "train loss on epoch 3545 : 0.126\n",
      "train accuracy on epoch 3545: 0.944\n",
      "test loss on epoch 3545: 0.325\n",
      "test accuracy on epoch 3545: 0.769\n",
      "train loss on epoch 3546 : 0.153\n",
      "train accuracy on epoch 3546: 0.944\n",
      "test loss on epoch 3546: 0.330\n",
      "test accuracy on epoch 3546: 0.769\n",
      "train loss on epoch 3547 : 0.389\n",
      "train accuracy on epoch 3547: 0.833\n",
      "test loss on epoch 3547: 0.337\n",
      "test accuracy on epoch 3547: 0.769\n",
      "train loss on epoch 3548 : 0.058\n",
      "train accuracy on epoch 3548: 1.000\n",
      "test loss on epoch 3548: 0.336\n",
      "test accuracy on epoch 3548: 0.769\n",
      "train loss on epoch 3549 : 0.444\n",
      "train accuracy on epoch 3549: 0.889\n",
      "test loss on epoch 3549: 0.333\n",
      "test accuracy on epoch 3549: 0.769\n",
      "train loss on epoch 3550 : 0.075\n",
      "train accuracy on epoch 3550: 0.944\n",
      "test loss on epoch 3550: 0.331\n",
      "test accuracy on epoch 3550: 0.769\n",
      "train loss on epoch 3551 : 0.255\n",
      "train accuracy on epoch 3551: 0.944\n",
      "test loss on epoch 3551: 0.329\n",
      "test accuracy on epoch 3551: 0.769\n",
      "train loss on epoch 3552 : 0.088\n",
      "train accuracy on epoch 3552: 1.000\n",
      "test loss on epoch 3552: 0.329\n",
      "test accuracy on epoch 3552: 0.769\n",
      "train loss on epoch 3553 : 0.180\n",
      "train accuracy on epoch 3553: 0.944\n",
      "test loss on epoch 3553: 0.328\n",
      "test accuracy on epoch 3553: 0.769\n",
      "train loss on epoch 3554 : 0.335\n",
      "train accuracy on epoch 3554: 0.833\n",
      "test loss on epoch 3554: 0.330\n",
      "test accuracy on epoch 3554: 0.769\n",
      "train loss on epoch 3555 : 0.058\n",
      "train accuracy on epoch 3555: 1.000\n",
      "test loss on epoch 3555: 0.333\n",
      "test accuracy on epoch 3555: 0.769\n",
      "train loss on epoch 3556 : 0.132\n",
      "train accuracy on epoch 3556: 0.944\n",
      "test loss on epoch 3556: 0.335\n",
      "test accuracy on epoch 3556: 0.769\n",
      "train loss on epoch 3557 : 0.101\n",
      "train accuracy on epoch 3557: 1.000\n",
      "test loss on epoch 3557: 0.332\n",
      "test accuracy on epoch 3557: 0.769\n",
      "train loss on epoch 3558 : 0.370\n",
      "train accuracy on epoch 3558: 0.833\n",
      "test loss on epoch 3558: 0.326\n",
      "test accuracy on epoch 3558: 0.769\n",
      "train loss on epoch 3559 : 0.429\n",
      "train accuracy on epoch 3559: 0.889\n",
      "test loss on epoch 3559: 0.319\n",
      "test accuracy on epoch 3559: 0.769\n",
      "train loss on epoch 3560 : 0.189\n",
      "train accuracy on epoch 3560: 0.944\n",
      "test loss on epoch 3560: 0.319\n",
      "test accuracy on epoch 3560: 0.769\n",
      "train loss on epoch 3561 : 0.071\n",
      "train accuracy on epoch 3561: 1.000\n",
      "test loss on epoch 3561: 0.318\n",
      "test accuracy on epoch 3561: 0.769\n",
      "train loss on epoch 3562 : 0.355\n",
      "train accuracy on epoch 3562: 0.944\n",
      "test loss on epoch 3562: 0.316\n",
      "test accuracy on epoch 3562: 0.769\n",
      "train loss on epoch 3563 : 0.372\n",
      "train accuracy on epoch 3563: 0.833\n",
      "test loss on epoch 3563: 0.321\n",
      "test accuracy on epoch 3563: 0.769\n",
      "train loss on epoch 3564 : 0.162\n",
      "train accuracy on epoch 3564: 0.889\n",
      "test loss on epoch 3564: 0.326\n",
      "test accuracy on epoch 3564: 0.769\n",
      "train loss on epoch 3565 : 0.231\n",
      "train accuracy on epoch 3565: 0.889\n",
      "test loss on epoch 3565: 0.329\n",
      "test accuracy on epoch 3565: 0.769\n",
      "train loss on epoch 3566 : 0.081\n",
      "train accuracy on epoch 3566: 0.944\n",
      "test loss on epoch 3566: 0.333\n",
      "test accuracy on epoch 3566: 0.769\n",
      "train loss on epoch 3567 : 0.168\n",
      "train accuracy on epoch 3567: 0.889\n",
      "test loss on epoch 3567: 0.341\n",
      "test accuracy on epoch 3567: 0.769\n",
      "train loss on epoch 3568 : 0.053\n",
      "train accuracy on epoch 3568: 1.000\n",
      "test loss on epoch 3568: 0.353\n",
      "test accuracy on epoch 3568: 0.769\n",
      "train loss on epoch 3569 : 0.349\n",
      "train accuracy on epoch 3569: 0.889\n",
      "test loss on epoch 3569: 0.347\n",
      "test accuracy on epoch 3569: 0.769\n",
      "train loss on epoch 3570 : 0.258\n",
      "train accuracy on epoch 3570: 0.889\n",
      "test loss on epoch 3570: 0.349\n",
      "test accuracy on epoch 3570: 0.769\n",
      "train loss on epoch 3571 : 0.134\n",
      "train accuracy on epoch 3571: 0.889\n",
      "test loss on epoch 3571: 0.356\n",
      "test accuracy on epoch 3571: 0.769\n",
      "train loss on epoch 3572 : 0.259\n",
      "train accuracy on epoch 3572: 0.944\n",
      "test loss on epoch 3572: 0.341\n",
      "test accuracy on epoch 3572: 0.769\n",
      "train loss on epoch 3573 : 0.337\n",
      "train accuracy on epoch 3573: 0.889\n",
      "test loss on epoch 3573: 0.341\n",
      "test accuracy on epoch 3573: 0.769\n",
      "train loss on epoch 3574 : 0.029\n",
      "train accuracy on epoch 3574: 1.000\n",
      "test loss on epoch 3574: 0.334\n",
      "test accuracy on epoch 3574: 0.769\n",
      "train loss on epoch 3575 : 0.131\n",
      "train accuracy on epoch 3575: 0.944\n",
      "test loss on epoch 3575: 0.335\n",
      "test accuracy on epoch 3575: 0.769\n",
      "train loss on epoch 3576 : 0.104\n",
      "train accuracy on epoch 3576: 0.944\n",
      "test loss on epoch 3576: 0.324\n",
      "test accuracy on epoch 3576: 0.769\n",
      "train loss on epoch 3577 : 0.176\n",
      "train accuracy on epoch 3577: 0.889\n",
      "test loss on epoch 3577: 0.320\n",
      "test accuracy on epoch 3577: 0.769\n",
      "train loss on epoch 3578 : 0.372\n",
      "train accuracy on epoch 3578: 0.833\n",
      "test loss on epoch 3578: 0.317\n",
      "test accuracy on epoch 3578: 0.769\n",
      "train loss on epoch 3579 : 0.060\n",
      "train accuracy on epoch 3579: 1.000\n",
      "test loss on epoch 3579: 0.318\n",
      "test accuracy on epoch 3579: 0.769\n",
      "train loss on epoch 3580 : 0.211\n",
      "train accuracy on epoch 3580: 0.944\n",
      "test loss on epoch 3580: 0.320\n",
      "test accuracy on epoch 3580: 0.769\n",
      "train loss on epoch 3581 : 0.159\n",
      "train accuracy on epoch 3581: 0.944\n",
      "test loss on epoch 3581: 0.320\n",
      "test accuracy on epoch 3581: 0.769\n",
      "train loss on epoch 3582 : 0.093\n",
      "train accuracy on epoch 3582: 1.000\n",
      "test loss on epoch 3582: 0.321\n",
      "test accuracy on epoch 3582: 0.769\n",
      "train loss on epoch 3583 : 0.185\n",
      "train accuracy on epoch 3583: 0.944\n",
      "test loss on epoch 3583: 0.322\n",
      "test accuracy on epoch 3583: 0.769\n",
      "train loss on epoch 3584 : 0.242\n",
      "train accuracy on epoch 3584: 0.944\n",
      "test loss on epoch 3584: 0.321\n",
      "test accuracy on epoch 3584: 0.769\n",
      "train loss on epoch 3585 : 0.068\n",
      "train accuracy on epoch 3585: 1.000\n",
      "test loss on epoch 3585: 0.320\n",
      "test accuracy on epoch 3585: 0.769\n",
      "train loss on epoch 3586 : 0.251\n",
      "train accuracy on epoch 3586: 0.944\n",
      "test loss on epoch 3586: 0.318\n",
      "test accuracy on epoch 3586: 0.769\n",
      "train loss on epoch 3587 : 0.243\n",
      "train accuracy on epoch 3587: 0.944\n",
      "test loss on epoch 3587: 0.311\n",
      "test accuracy on epoch 3587: 0.846\n",
      "train loss on epoch 3588 : 0.148\n",
      "train accuracy on epoch 3588: 0.944\n",
      "test loss on epoch 3588: 0.319\n",
      "test accuracy on epoch 3588: 0.769\n",
      "train loss on epoch 3589 : 0.047\n",
      "train accuracy on epoch 3589: 1.000\n",
      "test loss on epoch 3589: 0.322\n",
      "test accuracy on epoch 3589: 0.769\n",
      "train loss on epoch 3590 : 0.145\n",
      "train accuracy on epoch 3590: 0.944\n",
      "test loss on epoch 3590: 0.333\n",
      "test accuracy on epoch 3590: 0.769\n",
      "train loss on epoch 3591 : 0.064\n",
      "train accuracy on epoch 3591: 1.000\n",
      "test loss on epoch 3591: 0.330\n",
      "test accuracy on epoch 3591: 0.769\n",
      "train loss on epoch 3592 : 0.186\n",
      "train accuracy on epoch 3592: 0.944\n",
      "test loss on epoch 3592: 0.331\n",
      "test accuracy on epoch 3592: 0.769\n",
      "train loss on epoch 3593 : 0.164\n",
      "train accuracy on epoch 3593: 0.889\n",
      "test loss on epoch 3593: 0.329\n",
      "test accuracy on epoch 3593: 0.769\n",
      "train loss on epoch 3594 : 0.394\n",
      "train accuracy on epoch 3594: 0.944\n",
      "test loss on epoch 3594: 0.327\n",
      "test accuracy on epoch 3594: 0.769\n",
      "train loss on epoch 3595 : 0.103\n",
      "train accuracy on epoch 3595: 0.944\n",
      "test loss on epoch 3595: 0.329\n",
      "test accuracy on epoch 3595: 0.769\n",
      "train loss on epoch 3596 : 0.110\n",
      "train accuracy on epoch 3596: 1.000\n",
      "test loss on epoch 3596: 0.315\n",
      "test accuracy on epoch 3596: 0.769\n",
      "train loss on epoch 3597 : 0.309\n",
      "train accuracy on epoch 3597: 0.778\n",
      "test loss on epoch 3597: 0.326\n",
      "test accuracy on epoch 3597: 0.769\n",
      "train loss on epoch 3598 : 0.369\n",
      "train accuracy on epoch 3598: 0.944\n",
      "test loss on epoch 3598: 0.325\n",
      "test accuracy on epoch 3598: 0.692\n",
      "train loss on epoch 3599 : 0.041\n",
      "train accuracy on epoch 3599: 1.000\n",
      "test loss on epoch 3599: 0.323\n",
      "test accuracy on epoch 3599: 0.692\n",
      "train loss on epoch 3600 : 0.101\n",
      "train accuracy on epoch 3600: 0.944\n",
      "test loss on epoch 3600: 0.306\n",
      "test accuracy on epoch 3600: 0.769\n",
      "train loss on epoch 3601 : 0.072\n",
      "train accuracy on epoch 3601: 1.000\n",
      "test loss on epoch 3601: 0.306\n",
      "test accuracy on epoch 3601: 0.769\n",
      "train loss on epoch 3602 : 0.150\n",
      "train accuracy on epoch 3602: 0.944\n",
      "test loss on epoch 3602: 0.312\n",
      "test accuracy on epoch 3602: 0.769\n",
      "train loss on epoch 3603 : 0.154\n",
      "train accuracy on epoch 3603: 0.944\n",
      "test loss on epoch 3603: 0.308\n",
      "test accuracy on epoch 3603: 0.769\n",
      "train loss on epoch 3604 : 0.095\n",
      "train accuracy on epoch 3604: 1.000\n",
      "test loss on epoch 3604: 0.322\n",
      "test accuracy on epoch 3604: 0.769\n",
      "train loss on epoch 3605 : 0.207\n",
      "train accuracy on epoch 3605: 0.944\n",
      "test loss on epoch 3605: 0.305\n",
      "test accuracy on epoch 3605: 0.769\n",
      "train loss on epoch 3606 : 0.340\n",
      "train accuracy on epoch 3606: 0.889\n",
      "test loss on epoch 3606: 0.325\n",
      "test accuracy on epoch 3606: 0.692\n",
      "train loss on epoch 3607 : 0.317\n",
      "train accuracy on epoch 3607: 0.889\n",
      "test loss on epoch 3607: 0.319\n",
      "test accuracy on epoch 3607: 0.769\n",
      "train loss on epoch 3608 : 0.058\n",
      "train accuracy on epoch 3608: 0.944\n",
      "test loss on epoch 3608: 0.315\n",
      "test accuracy on epoch 3608: 0.769\n",
      "train loss on epoch 3609 : 0.141\n",
      "train accuracy on epoch 3609: 0.889\n",
      "test loss on epoch 3609: 0.325\n",
      "test accuracy on epoch 3609: 0.769\n",
      "train loss on epoch 3610 : 0.155\n",
      "train accuracy on epoch 3610: 0.944\n",
      "test loss on epoch 3610: 0.328\n",
      "test accuracy on epoch 3610: 0.769\n",
      "train loss on epoch 3611 : 0.334\n",
      "train accuracy on epoch 3611: 0.944\n",
      "test loss on epoch 3611: 0.334\n",
      "test accuracy on epoch 3611: 0.769\n",
      "train loss on epoch 3612 : 0.209\n",
      "train accuracy on epoch 3612: 0.944\n",
      "test loss on epoch 3612: 0.325\n",
      "test accuracy on epoch 3612: 0.769\n",
      "train loss on epoch 3613 : 0.141\n",
      "train accuracy on epoch 3613: 0.889\n",
      "test loss on epoch 3613: 0.327\n",
      "test accuracy on epoch 3613: 0.769\n",
      "train loss on epoch 3614 : 0.346\n",
      "train accuracy on epoch 3614: 0.889\n",
      "test loss on epoch 3614: 0.319\n",
      "test accuracy on epoch 3614: 0.769\n",
      "train loss on epoch 3615 : 0.125\n",
      "train accuracy on epoch 3615: 0.944\n",
      "test loss on epoch 3615: 0.320\n",
      "test accuracy on epoch 3615: 0.692\n",
      "train loss on epoch 3616 : 0.163\n",
      "train accuracy on epoch 3616: 0.944\n",
      "test loss on epoch 3616: 0.309\n",
      "test accuracy on epoch 3616: 0.769\n",
      "train loss on epoch 3617 : 0.188\n",
      "train accuracy on epoch 3617: 0.944\n",
      "test loss on epoch 3617: 0.308\n",
      "test accuracy on epoch 3617: 0.769\n",
      "train loss on epoch 3618 : 0.227\n",
      "train accuracy on epoch 3618: 0.889\n",
      "test loss on epoch 3618: 0.306\n",
      "test accuracy on epoch 3618: 0.769\n",
      "train loss on epoch 3619 : 0.105\n",
      "train accuracy on epoch 3619: 0.944\n",
      "test loss on epoch 3619: 0.301\n",
      "test accuracy on epoch 3619: 0.846\n",
      "train loss on epoch 3620 : 0.324\n",
      "train accuracy on epoch 3620: 0.889\n",
      "test loss on epoch 3620: 0.308\n",
      "test accuracy on epoch 3620: 0.769\n",
      "train loss on epoch 3621 : 0.226\n",
      "train accuracy on epoch 3621: 0.889\n",
      "test loss on epoch 3621: 0.311\n",
      "test accuracy on epoch 3621: 0.769\n",
      "train loss on epoch 3622 : 0.165\n",
      "train accuracy on epoch 3622: 0.889\n",
      "test loss on epoch 3622: 0.315\n",
      "test accuracy on epoch 3622: 0.692\n",
      "train loss on epoch 3623 : 0.204\n",
      "train accuracy on epoch 3623: 0.944\n",
      "test loss on epoch 3623: 0.310\n",
      "test accuracy on epoch 3623: 0.769\n",
      "train loss on epoch 3624 : 0.036\n",
      "train accuracy on epoch 3624: 1.000\n",
      "test loss on epoch 3624: 0.318\n",
      "test accuracy on epoch 3624: 0.769\n",
      "train loss on epoch 3625 : 0.097\n",
      "train accuracy on epoch 3625: 0.944\n",
      "test loss on epoch 3625: 0.318\n",
      "test accuracy on epoch 3625: 0.769\n",
      "train loss on epoch 3626 : 0.076\n",
      "train accuracy on epoch 3626: 1.000\n",
      "test loss on epoch 3626: 0.308\n",
      "test accuracy on epoch 3626: 0.769\n",
      "train loss on epoch 3627 : 0.167\n",
      "train accuracy on epoch 3627: 0.944\n",
      "test loss on epoch 3627: 0.315\n",
      "test accuracy on epoch 3627: 0.769\n",
      "train loss on epoch 3628 : 0.171\n",
      "train accuracy on epoch 3628: 0.944\n",
      "test loss on epoch 3628: 0.310\n",
      "test accuracy on epoch 3628: 0.769\n",
      "train loss on epoch 3629 : 0.398\n",
      "train accuracy on epoch 3629: 0.889\n",
      "test loss on epoch 3629: 0.307\n",
      "test accuracy on epoch 3629: 0.846\n",
      "train loss on epoch 3630 : 0.096\n",
      "train accuracy on epoch 3630: 0.944\n",
      "test loss on epoch 3630: 0.319\n",
      "test accuracy on epoch 3630: 0.692\n",
      "train loss on epoch 3631 : 0.244\n",
      "train accuracy on epoch 3631: 0.889\n",
      "test loss on epoch 3631: 0.317\n",
      "test accuracy on epoch 3631: 0.692\n",
      "train loss on epoch 3632 : 0.320\n",
      "train accuracy on epoch 3632: 0.889\n",
      "test loss on epoch 3632: 0.316\n",
      "test accuracy on epoch 3632: 0.769\n",
      "train loss on epoch 3633 : 0.333\n",
      "train accuracy on epoch 3633: 0.889\n",
      "test loss on epoch 3633: 0.305\n",
      "test accuracy on epoch 3633: 0.769\n",
      "train loss on epoch 3634 : 0.110\n",
      "train accuracy on epoch 3634: 0.944\n",
      "test loss on epoch 3634: 0.320\n",
      "test accuracy on epoch 3634: 0.769\n",
      "train loss on epoch 3635 : 0.147\n",
      "train accuracy on epoch 3635: 0.944\n",
      "test loss on epoch 3635: 0.333\n",
      "test accuracy on epoch 3635: 0.769\n",
      "train loss on epoch 3636 : 0.327\n",
      "train accuracy on epoch 3636: 0.833\n",
      "test loss on epoch 3636: 0.313\n",
      "test accuracy on epoch 3636: 0.769\n",
      "train loss on epoch 3637 : 0.177\n",
      "train accuracy on epoch 3637: 0.889\n",
      "test loss on epoch 3637: 0.313\n",
      "test accuracy on epoch 3637: 0.769\n",
      "train loss on epoch 3638 : 0.104\n",
      "train accuracy on epoch 3638: 1.000\n",
      "test loss on epoch 3638: 0.317\n",
      "test accuracy on epoch 3638: 0.769\n",
      "train loss on epoch 3639 : 0.134\n",
      "train accuracy on epoch 3639: 0.944\n",
      "test loss on epoch 3639: 0.332\n",
      "test accuracy on epoch 3639: 0.769\n",
      "train loss on epoch 3640 : 0.286\n",
      "train accuracy on epoch 3640: 0.889\n",
      "test loss on epoch 3640: 0.334\n",
      "test accuracy on epoch 3640: 0.769\n",
      "train loss on epoch 3641 : 0.405\n",
      "train accuracy on epoch 3641: 0.889\n",
      "test loss on epoch 3641: 0.316\n",
      "test accuracy on epoch 3641: 0.769\n",
      "train loss on epoch 3642 : 0.238\n",
      "train accuracy on epoch 3642: 0.944\n",
      "test loss on epoch 3642: 0.311\n",
      "test accuracy on epoch 3642: 0.769\n",
      "train loss on epoch 3643 : 0.410\n",
      "train accuracy on epoch 3643: 0.778\n",
      "test loss on epoch 3643: 0.327\n",
      "test accuracy on epoch 3643: 0.769\n",
      "train loss on epoch 3644 : 0.095\n",
      "train accuracy on epoch 3644: 1.000\n",
      "test loss on epoch 3644: 0.329\n",
      "test accuracy on epoch 3644: 0.769\n",
      "train loss on epoch 3645 : 0.118\n",
      "train accuracy on epoch 3645: 0.944\n",
      "test loss on epoch 3645: 0.315\n",
      "test accuracy on epoch 3645: 0.769\n",
      "train loss on epoch 3646 : 0.314\n",
      "train accuracy on epoch 3646: 0.833\n",
      "test loss on epoch 3646: 0.329\n",
      "test accuracy on epoch 3646: 0.769\n",
      "train loss on epoch 3647 : 0.454\n",
      "train accuracy on epoch 3647: 0.778\n",
      "test loss on epoch 3647: 0.327\n",
      "test accuracy on epoch 3647: 0.769\n",
      "train loss on epoch 3648 : 0.197\n",
      "train accuracy on epoch 3648: 0.944\n",
      "test loss on epoch 3648: 0.318\n",
      "test accuracy on epoch 3648: 0.846\n",
      "train loss on epoch 3649 : 0.313\n",
      "train accuracy on epoch 3649: 0.889\n",
      "test loss on epoch 3649: 0.317\n",
      "test accuracy on epoch 3649: 0.846\n",
      "train loss on epoch 3650 : 0.251\n",
      "train accuracy on epoch 3650: 0.889\n",
      "test loss on epoch 3650: 0.327\n",
      "test accuracy on epoch 3650: 0.769\n",
      "train loss on epoch 3651 : 0.200\n",
      "train accuracy on epoch 3651: 0.944\n",
      "test loss on epoch 3651: 0.321\n",
      "test accuracy on epoch 3651: 0.769\n",
      "train loss on epoch 3652 : 0.216\n",
      "train accuracy on epoch 3652: 0.944\n",
      "test loss on epoch 3652: 0.316\n",
      "test accuracy on epoch 3652: 0.769\n",
      "train loss on epoch 3653 : 0.217\n",
      "train accuracy on epoch 3653: 0.889\n",
      "test loss on epoch 3653: 0.312\n",
      "test accuracy on epoch 3653: 0.846\n",
      "train loss on epoch 3654 : 0.207\n",
      "train accuracy on epoch 3654: 0.889\n",
      "test loss on epoch 3654: 0.315\n",
      "test accuracy on epoch 3654: 0.769\n",
      "train loss on epoch 3655 : 0.067\n",
      "train accuracy on epoch 3655: 1.000\n",
      "test loss on epoch 3655: 0.323\n",
      "test accuracy on epoch 3655: 0.692\n",
      "train loss on epoch 3656 : 0.317\n",
      "train accuracy on epoch 3656: 0.778\n",
      "test loss on epoch 3656: 0.310\n",
      "test accuracy on epoch 3656: 0.846\n",
      "train loss on epoch 3657 : 0.325\n",
      "train accuracy on epoch 3657: 0.889\n",
      "test loss on epoch 3657: 0.317\n",
      "test accuracy on epoch 3657: 0.769\n",
      "train loss on epoch 3658 : 0.109\n",
      "train accuracy on epoch 3658: 0.944\n",
      "test loss on epoch 3658: 0.305\n",
      "test accuracy on epoch 3658: 0.769\n",
      "train loss on epoch 3659 : 0.230\n",
      "train accuracy on epoch 3659: 0.944\n",
      "test loss on epoch 3659: 0.318\n",
      "test accuracy on epoch 3659: 0.769\n",
      "train loss on epoch 3660 : 0.044\n",
      "train accuracy on epoch 3660: 1.000\n",
      "test loss on epoch 3660: 0.316\n",
      "test accuracy on epoch 3660: 0.769\n",
      "train loss on epoch 3661 : 0.137\n",
      "train accuracy on epoch 3661: 0.944\n",
      "test loss on epoch 3661: 0.312\n",
      "test accuracy on epoch 3661: 0.769\n",
      "train loss on epoch 3662 : 0.152\n",
      "train accuracy on epoch 3662: 0.889\n",
      "test loss on epoch 3662: 0.310\n",
      "test accuracy on epoch 3662: 0.769\n",
      "train loss on epoch 3663 : 0.238\n",
      "train accuracy on epoch 3663: 0.833\n",
      "test loss on epoch 3663: 0.320\n",
      "test accuracy on epoch 3663: 0.769\n",
      "train loss on epoch 3664 : 0.089\n",
      "train accuracy on epoch 3664: 0.944\n",
      "test loss on epoch 3664: 0.316\n",
      "test accuracy on epoch 3664: 0.769\n",
      "train loss on epoch 3665 : 0.094\n",
      "train accuracy on epoch 3665: 0.944\n",
      "test loss on epoch 3665: 0.328\n",
      "test accuracy on epoch 3665: 0.769\n",
      "train loss on epoch 3666 : 0.088\n",
      "train accuracy on epoch 3666: 1.000\n",
      "test loss on epoch 3666: 0.315\n",
      "test accuracy on epoch 3666: 0.769\n",
      "train loss on epoch 3667 : 0.074\n",
      "train accuracy on epoch 3667: 0.944\n",
      "test loss on epoch 3667: 0.319\n",
      "test accuracy on epoch 3667: 0.769\n",
      "train loss on epoch 3668 : 0.052\n",
      "train accuracy on epoch 3668: 1.000\n",
      "test loss on epoch 3668: 0.310\n",
      "test accuracy on epoch 3668: 0.846\n",
      "train loss on epoch 3669 : 0.335\n",
      "train accuracy on epoch 3669: 0.889\n",
      "test loss on epoch 3669: 0.322\n",
      "test accuracy on epoch 3669: 0.692\n",
      "train loss on epoch 3670 : 0.183\n",
      "train accuracy on epoch 3670: 0.889\n",
      "test loss on epoch 3670: 0.315\n",
      "test accuracy on epoch 3670: 0.846\n",
      "train loss on epoch 3671 : 0.246\n",
      "train accuracy on epoch 3671: 0.889\n",
      "test loss on epoch 3671: 0.327\n",
      "test accuracy on epoch 3671: 0.692\n",
      "train loss on epoch 3672 : 0.146\n",
      "train accuracy on epoch 3672: 0.889\n",
      "test loss on epoch 3672: 0.326\n",
      "test accuracy on epoch 3672: 0.692\n",
      "train loss on epoch 3673 : 0.262\n",
      "train accuracy on epoch 3673: 0.889\n",
      "test loss on epoch 3673: 0.328\n",
      "test accuracy on epoch 3673: 0.692\n",
      "train loss on epoch 3674 : 0.215\n",
      "train accuracy on epoch 3674: 0.889\n",
      "test loss on epoch 3674: 0.322\n",
      "test accuracy on epoch 3674: 0.769\n",
      "train loss on epoch 3675 : 0.098\n",
      "train accuracy on epoch 3675: 1.000\n",
      "test loss on epoch 3675: 0.321\n",
      "test accuracy on epoch 3675: 0.769\n",
      "train loss on epoch 3676 : 0.146\n",
      "train accuracy on epoch 3676: 0.944\n",
      "test loss on epoch 3676: 0.318\n",
      "test accuracy on epoch 3676: 0.846\n",
      "train loss on epoch 3677 : 0.097\n",
      "train accuracy on epoch 3677: 1.000\n",
      "test loss on epoch 3677: 0.329\n",
      "test accuracy on epoch 3677: 0.769\n",
      "train loss on epoch 3678 : 0.139\n",
      "train accuracy on epoch 3678: 0.944\n",
      "test loss on epoch 3678: 0.327\n",
      "test accuracy on epoch 3678: 0.769\n",
      "train loss on epoch 3679 : 0.057\n",
      "train accuracy on epoch 3679: 1.000\n",
      "test loss on epoch 3679: 0.333\n",
      "test accuracy on epoch 3679: 0.769\n",
      "train loss on epoch 3680 : 0.077\n",
      "train accuracy on epoch 3680: 1.000\n",
      "test loss on epoch 3680: 0.340\n",
      "test accuracy on epoch 3680: 0.769\n",
      "train loss on epoch 3681 : 0.242\n",
      "train accuracy on epoch 3681: 0.944\n",
      "test loss on epoch 3681: 0.344\n",
      "test accuracy on epoch 3681: 0.769\n",
      "train loss on epoch 3682 : 0.098\n",
      "train accuracy on epoch 3682: 1.000\n",
      "test loss on epoch 3682: 0.341\n",
      "test accuracy on epoch 3682: 0.769\n",
      "train loss on epoch 3683 : 0.356\n",
      "train accuracy on epoch 3683: 0.889\n",
      "test loss on epoch 3683: 0.343\n",
      "test accuracy on epoch 3683: 0.769\n",
      "train loss on epoch 3684 : 0.336\n",
      "train accuracy on epoch 3684: 0.833\n",
      "test loss on epoch 3684: 0.343\n",
      "test accuracy on epoch 3684: 0.769\n",
      "train loss on epoch 3685 : 0.118\n",
      "train accuracy on epoch 3685: 1.000\n",
      "test loss on epoch 3685: 0.344\n",
      "test accuracy on epoch 3685: 0.769\n",
      "train loss on epoch 3686 : 0.166\n",
      "train accuracy on epoch 3686: 0.889\n",
      "test loss on epoch 3686: 0.339\n",
      "test accuracy on epoch 3686: 0.769\n",
      "train loss on epoch 3687 : 0.164\n",
      "train accuracy on epoch 3687: 0.889\n",
      "test loss on epoch 3687: 0.335\n",
      "test accuracy on epoch 3687: 0.769\n",
      "train loss on epoch 3688 : 0.246\n",
      "train accuracy on epoch 3688: 0.833\n",
      "test loss on epoch 3688: 0.336\n",
      "test accuracy on epoch 3688: 0.769\n",
      "train loss on epoch 3689 : 0.149\n",
      "train accuracy on epoch 3689: 0.944\n",
      "test loss on epoch 3689: 0.336\n",
      "test accuracy on epoch 3689: 0.692\n",
      "train loss on epoch 3690 : 0.198\n",
      "train accuracy on epoch 3690: 0.889\n",
      "test loss on epoch 3690: 0.328\n",
      "test accuracy on epoch 3690: 0.769\n",
      "train loss on epoch 3691 : 0.177\n",
      "train accuracy on epoch 3691: 0.944\n",
      "test loss on epoch 3691: 0.320\n",
      "test accuracy on epoch 3691: 0.846\n",
      "train loss on epoch 3692 : 0.223\n",
      "train accuracy on epoch 3692: 0.889\n",
      "test loss on epoch 3692: 0.328\n",
      "test accuracy on epoch 3692: 0.769\n",
      "train loss on epoch 3693 : 0.043\n",
      "train accuracy on epoch 3693: 1.000\n",
      "test loss on epoch 3693: 0.336\n",
      "test accuracy on epoch 3693: 0.692\n",
      "train loss on epoch 3694 : 0.135\n",
      "train accuracy on epoch 3694: 0.889\n",
      "test loss on epoch 3694: 0.322\n",
      "test accuracy on epoch 3694: 0.769\n",
      "train loss on epoch 3695 : 0.157\n",
      "train accuracy on epoch 3695: 0.944\n",
      "test loss on epoch 3695: 0.335\n",
      "test accuracy on epoch 3695: 0.769\n",
      "train loss on epoch 3696 : 0.219\n",
      "train accuracy on epoch 3696: 0.889\n",
      "test loss on epoch 3696: 0.328\n",
      "test accuracy on epoch 3696: 0.769\n",
      "train loss on epoch 3697 : 0.161\n",
      "train accuracy on epoch 3697: 0.889\n",
      "test loss on epoch 3697: 0.334\n",
      "test accuracy on epoch 3697: 0.769\n",
      "train loss on epoch 3698 : 0.162\n",
      "train accuracy on epoch 3698: 0.889\n",
      "test loss on epoch 3698: 0.322\n",
      "test accuracy on epoch 3698: 0.769\n",
      "train loss on epoch 3699 : 0.135\n",
      "train accuracy on epoch 3699: 0.944\n",
      "test loss on epoch 3699: 0.321\n",
      "test accuracy on epoch 3699: 0.846\n",
      "train loss on epoch 3700 : 0.279\n",
      "train accuracy on epoch 3700: 0.889\n",
      "test loss on epoch 3700: 0.322\n",
      "test accuracy on epoch 3700: 0.846\n",
      "train loss on epoch 3701 : 0.164\n",
      "train accuracy on epoch 3701: 0.944\n",
      "test loss on epoch 3701: 0.330\n",
      "test accuracy on epoch 3701: 0.769\n",
      "train loss on epoch 3702 : 0.378\n",
      "train accuracy on epoch 3702: 0.889\n",
      "test loss on epoch 3702: 0.327\n",
      "test accuracy on epoch 3702: 0.769\n",
      "train loss on epoch 3703 : 0.106\n",
      "train accuracy on epoch 3703: 0.944\n",
      "test loss on epoch 3703: 0.329\n",
      "test accuracy on epoch 3703: 0.769\n",
      "train loss on epoch 3704 : 0.229\n",
      "train accuracy on epoch 3704: 0.833\n",
      "test loss on epoch 3704: 0.335\n",
      "test accuracy on epoch 3704: 0.769\n",
      "train loss on epoch 3705 : 0.055\n",
      "train accuracy on epoch 3705: 1.000\n",
      "test loss on epoch 3705: 0.332\n",
      "test accuracy on epoch 3705: 0.769\n",
      "train loss on epoch 3706 : 0.106\n",
      "train accuracy on epoch 3706: 0.944\n",
      "test loss on epoch 3706: 0.338\n",
      "test accuracy on epoch 3706: 0.769\n",
      "train loss on epoch 3707 : 0.171\n",
      "train accuracy on epoch 3707: 0.889\n",
      "test loss on epoch 3707: 0.345\n",
      "test accuracy on epoch 3707: 0.769\n",
      "train loss on epoch 3708 : 0.254\n",
      "train accuracy on epoch 3708: 0.889\n",
      "test loss on epoch 3708: 0.354\n",
      "test accuracy on epoch 3708: 0.769\n",
      "train loss on epoch 3709 : 0.210\n",
      "train accuracy on epoch 3709: 0.944\n",
      "test loss on epoch 3709: 0.360\n",
      "test accuracy on epoch 3709: 0.769\n",
      "train loss on epoch 3710 : 0.331\n",
      "train accuracy on epoch 3710: 0.889\n",
      "test loss on epoch 3710: 0.356\n",
      "test accuracy on epoch 3710: 0.769\n",
      "train loss on epoch 3711 : 0.064\n",
      "train accuracy on epoch 3711: 1.000\n",
      "test loss on epoch 3711: 0.353\n",
      "test accuracy on epoch 3711: 0.769\n",
      "train loss on epoch 3712 : 0.326\n",
      "train accuracy on epoch 3712: 0.889\n",
      "test loss on epoch 3712: 0.348\n",
      "test accuracy on epoch 3712: 0.769\n",
      "train loss on epoch 3713 : 0.046\n",
      "train accuracy on epoch 3713: 1.000\n",
      "test loss on epoch 3713: 0.345\n",
      "test accuracy on epoch 3713: 0.769\n",
      "train loss on epoch 3714 : 0.100\n",
      "train accuracy on epoch 3714: 1.000\n",
      "test loss on epoch 3714: 0.351\n",
      "test accuracy on epoch 3714: 0.769\n",
      "train loss on epoch 3715 : 0.198\n",
      "train accuracy on epoch 3715: 0.889\n",
      "test loss on epoch 3715: 0.351\n",
      "test accuracy on epoch 3715: 0.769\n",
      "train loss on epoch 3716 : 0.249\n",
      "train accuracy on epoch 3716: 0.889\n",
      "test loss on epoch 3716: 0.343\n",
      "test accuracy on epoch 3716: 0.769\n",
      "train loss on epoch 3717 : 0.051\n",
      "train accuracy on epoch 3717: 1.000\n",
      "test loss on epoch 3717: 0.350\n",
      "test accuracy on epoch 3717: 0.769\n",
      "train loss on epoch 3718 : 0.273\n",
      "train accuracy on epoch 3718: 0.889\n",
      "test loss on epoch 3718: 0.339\n",
      "test accuracy on epoch 3718: 0.769\n",
      "train loss on epoch 3719 : 0.303\n",
      "train accuracy on epoch 3719: 0.889\n",
      "test loss on epoch 3719: 0.341\n",
      "test accuracy on epoch 3719: 0.769\n",
      "train loss on epoch 3720 : 0.103\n",
      "train accuracy on epoch 3720: 0.944\n",
      "test loss on epoch 3720: 0.347\n",
      "test accuracy on epoch 3720: 0.769\n",
      "train loss on epoch 3721 : 0.186\n",
      "train accuracy on epoch 3721: 0.889\n",
      "test loss on epoch 3721: 0.347\n",
      "test accuracy on epoch 3721: 0.769\n",
      "train loss on epoch 3722 : 0.251\n",
      "train accuracy on epoch 3722: 0.889\n",
      "test loss on epoch 3722: 0.346\n",
      "test accuracy on epoch 3722: 0.769\n",
      "train loss on epoch 3723 : 0.104\n",
      "train accuracy on epoch 3723: 0.944\n",
      "test loss on epoch 3723: 0.346\n",
      "test accuracy on epoch 3723: 0.769\n",
      "train loss on epoch 3724 : 0.142\n",
      "train accuracy on epoch 3724: 0.944\n",
      "test loss on epoch 3724: 0.353\n",
      "test accuracy on epoch 3724: 0.769\n",
      "train loss on epoch 3725 : 0.365\n",
      "train accuracy on epoch 3725: 0.833\n",
      "test loss on epoch 3725: 0.354\n",
      "test accuracy on epoch 3725: 0.769\n",
      "train loss on epoch 3726 : 0.181\n",
      "train accuracy on epoch 3726: 0.889\n",
      "test loss on epoch 3726: 0.353\n",
      "test accuracy on epoch 3726: 0.769\n",
      "train loss on epoch 3727 : 0.222\n",
      "train accuracy on epoch 3727: 0.889\n",
      "test loss on epoch 3727: 0.348\n",
      "test accuracy on epoch 3727: 0.769\n",
      "train loss on epoch 3728 : 0.232\n",
      "train accuracy on epoch 3728: 0.889\n",
      "test loss on epoch 3728: 0.358\n",
      "test accuracy on epoch 3728: 0.769\n",
      "train loss on epoch 3729 : 0.316\n",
      "train accuracy on epoch 3729: 0.889\n",
      "test loss on epoch 3729: 0.351\n",
      "test accuracy on epoch 3729: 0.769\n",
      "train loss on epoch 3730 : 0.212\n",
      "train accuracy on epoch 3730: 0.833\n",
      "test loss on epoch 3730: 0.348\n",
      "test accuracy on epoch 3730: 0.769\n",
      "train loss on epoch 3731 : 0.225\n",
      "train accuracy on epoch 3731: 0.889\n",
      "test loss on epoch 3731: 0.346\n",
      "test accuracy on epoch 3731: 0.769\n",
      "train loss on epoch 3732 : 0.128\n",
      "train accuracy on epoch 3732: 0.944\n",
      "test loss on epoch 3732: 0.347\n",
      "test accuracy on epoch 3732: 0.769\n",
      "train loss on epoch 3733 : 0.324\n",
      "train accuracy on epoch 3733: 0.889\n",
      "test loss on epoch 3733: 0.342\n",
      "test accuracy on epoch 3733: 0.769\n",
      "train loss on epoch 3734 : 0.184\n",
      "train accuracy on epoch 3734: 0.944\n",
      "test loss on epoch 3734: 0.344\n",
      "test accuracy on epoch 3734: 0.769\n",
      "train loss on epoch 3735 : 0.190\n",
      "train accuracy on epoch 3735: 0.944\n",
      "test loss on epoch 3735: 0.358\n",
      "test accuracy on epoch 3735: 0.769\n",
      "train loss on epoch 3736 : 0.243\n",
      "train accuracy on epoch 3736: 0.889\n",
      "test loss on epoch 3736: 0.362\n",
      "test accuracy on epoch 3736: 0.769\n",
      "train loss on epoch 3737 : 0.202\n",
      "train accuracy on epoch 3737: 0.889\n",
      "test loss on epoch 3737: 0.370\n",
      "test accuracy on epoch 3737: 0.769\n",
      "train loss on epoch 3738 : 0.238\n",
      "train accuracy on epoch 3738: 0.889\n",
      "test loss on epoch 3738: 0.375\n",
      "test accuracy on epoch 3738: 0.769\n",
      "train loss on epoch 3739 : 0.040\n",
      "train accuracy on epoch 3739: 1.000\n",
      "test loss on epoch 3739: 0.376\n",
      "test accuracy on epoch 3739: 0.769\n",
      "train loss on epoch 3740 : 0.317\n",
      "train accuracy on epoch 3740: 0.833\n",
      "test loss on epoch 3740: 0.376\n",
      "test accuracy on epoch 3740: 0.769\n",
      "train loss on epoch 3741 : 0.224\n",
      "train accuracy on epoch 3741: 0.889\n",
      "test loss on epoch 3741: 0.377\n",
      "test accuracy on epoch 3741: 0.769\n",
      "train loss on epoch 3742 : 0.219\n",
      "train accuracy on epoch 3742: 0.944\n",
      "test loss on epoch 3742: 0.366\n",
      "test accuracy on epoch 3742: 0.769\n",
      "train loss on epoch 3743 : 0.112\n",
      "train accuracy on epoch 3743: 0.944\n",
      "test loss on epoch 3743: 0.360\n",
      "test accuracy on epoch 3743: 0.769\n",
      "train loss on epoch 3744 : 0.120\n",
      "train accuracy on epoch 3744: 0.944\n",
      "test loss on epoch 3744: 0.354\n",
      "test accuracy on epoch 3744: 0.769\n",
      "train loss on epoch 3745 : 0.147\n",
      "train accuracy on epoch 3745: 0.944\n",
      "test loss on epoch 3745: 0.342\n",
      "test accuracy on epoch 3745: 0.769\n",
      "train loss on epoch 3746 : 0.114\n",
      "train accuracy on epoch 3746: 1.000\n",
      "test loss on epoch 3746: 0.340\n",
      "test accuracy on epoch 3746: 0.769\n",
      "train loss on epoch 3747 : 0.224\n",
      "train accuracy on epoch 3747: 0.889\n",
      "test loss on epoch 3747: 0.344\n",
      "test accuracy on epoch 3747: 0.769\n",
      "train loss on epoch 3748 : 0.343\n",
      "train accuracy on epoch 3748: 0.778\n",
      "test loss on epoch 3748: 0.352\n",
      "test accuracy on epoch 3748: 0.769\n",
      "train loss on epoch 3749 : 0.185\n",
      "train accuracy on epoch 3749: 0.889\n",
      "test loss on epoch 3749: 0.355\n",
      "test accuracy on epoch 3749: 0.769\n",
      "train loss on epoch 3750 : 0.271\n",
      "train accuracy on epoch 3750: 0.889\n",
      "test loss on epoch 3750: 0.348\n",
      "test accuracy on epoch 3750: 0.769\n",
      "train loss on epoch 3751 : 0.419\n",
      "train accuracy on epoch 3751: 0.778\n",
      "test loss on epoch 3751: 0.355\n",
      "test accuracy on epoch 3751: 0.769\n",
      "train loss on epoch 3752 : 0.095\n",
      "train accuracy on epoch 3752: 1.000\n",
      "test loss on epoch 3752: 0.349\n",
      "test accuracy on epoch 3752: 0.769\n",
      "train loss on epoch 3753 : 0.233\n",
      "train accuracy on epoch 3753: 0.889\n",
      "test loss on epoch 3753: 0.350\n",
      "test accuracy on epoch 3753: 0.769\n",
      "train loss on epoch 3754 : 0.169\n",
      "train accuracy on epoch 3754: 0.889\n",
      "test loss on epoch 3754: 0.347\n",
      "test accuracy on epoch 3754: 0.769\n",
      "train loss on epoch 3755 : 0.304\n",
      "train accuracy on epoch 3755: 0.889\n",
      "test loss on epoch 3755: 0.342\n",
      "test accuracy on epoch 3755: 0.769\n",
      "train loss on epoch 3756 : 0.240\n",
      "train accuracy on epoch 3756: 0.833\n",
      "test loss on epoch 3756: 0.336\n",
      "test accuracy on epoch 3756: 0.769\n",
      "train loss on epoch 3757 : 0.144\n",
      "train accuracy on epoch 3757: 0.944\n",
      "test loss on epoch 3757: 0.335\n",
      "test accuracy on epoch 3757: 0.769\n",
      "train loss on epoch 3758 : 0.152\n",
      "train accuracy on epoch 3758: 0.889\n",
      "test loss on epoch 3758: 0.337\n",
      "test accuracy on epoch 3758: 0.769\n",
      "train loss on epoch 3759 : 0.069\n",
      "train accuracy on epoch 3759: 0.944\n",
      "test loss on epoch 3759: 0.331\n",
      "test accuracy on epoch 3759: 0.769\n",
      "train loss on epoch 3760 : 0.135\n",
      "train accuracy on epoch 3760: 0.944\n",
      "test loss on epoch 3760: 0.339\n",
      "test accuracy on epoch 3760: 0.769\n",
      "train loss on epoch 3761 : 0.259\n",
      "train accuracy on epoch 3761: 0.889\n",
      "test loss on epoch 3761: 0.338\n",
      "test accuracy on epoch 3761: 0.769\n",
      "train loss on epoch 3762 : 0.204\n",
      "train accuracy on epoch 3762: 0.889\n",
      "test loss on epoch 3762: 0.342\n",
      "test accuracy on epoch 3762: 0.769\n",
      "train loss on epoch 3763 : 0.058\n",
      "train accuracy on epoch 3763: 1.000\n",
      "test loss on epoch 3763: 0.341\n",
      "test accuracy on epoch 3763: 0.769\n",
      "train loss on epoch 3764 : 0.121\n",
      "train accuracy on epoch 3764: 0.944\n",
      "test loss on epoch 3764: 0.340\n",
      "test accuracy on epoch 3764: 0.769\n",
      "train loss on epoch 3765 : 0.295\n",
      "train accuracy on epoch 3765: 0.833\n",
      "test loss on epoch 3765: 0.339\n",
      "test accuracy on epoch 3765: 0.769\n",
      "train loss on epoch 3766 : 0.103\n",
      "train accuracy on epoch 3766: 0.944\n",
      "test loss on epoch 3766: 0.342\n",
      "test accuracy on epoch 3766: 0.769\n",
      "train loss on epoch 3767 : 0.051\n",
      "train accuracy on epoch 3767: 1.000\n",
      "test loss on epoch 3767: 0.342\n",
      "test accuracy on epoch 3767: 0.769\n",
      "train loss on epoch 3768 : 0.322\n",
      "train accuracy on epoch 3768: 0.944\n",
      "test loss on epoch 3768: 0.345\n",
      "test accuracy on epoch 3768: 0.769\n",
      "train loss on epoch 3769 : 0.166\n",
      "train accuracy on epoch 3769: 0.944\n",
      "test loss on epoch 3769: 0.350\n",
      "test accuracy on epoch 3769: 0.769\n",
      "train loss on epoch 3770 : 0.120\n",
      "train accuracy on epoch 3770: 0.944\n",
      "test loss on epoch 3770: 0.348\n",
      "test accuracy on epoch 3770: 0.769\n",
      "train loss on epoch 3771 : 0.107\n",
      "train accuracy on epoch 3771: 1.000\n",
      "test loss on epoch 3771: 0.353\n",
      "test accuracy on epoch 3771: 0.769\n",
      "train loss on epoch 3772 : 0.328\n",
      "train accuracy on epoch 3772: 0.944\n",
      "test loss on epoch 3772: 0.360\n",
      "test accuracy on epoch 3772: 0.769\n",
      "train loss on epoch 3773 : 0.075\n",
      "train accuracy on epoch 3773: 1.000\n",
      "test loss on epoch 3773: 0.360\n",
      "test accuracy on epoch 3773: 0.769\n",
      "train loss on epoch 3774 : 0.134\n",
      "train accuracy on epoch 3774: 0.944\n",
      "test loss on epoch 3774: 0.363\n",
      "test accuracy on epoch 3774: 0.769\n",
      "train loss on epoch 3775 : 0.122\n",
      "train accuracy on epoch 3775: 0.944\n",
      "test loss on epoch 3775: 0.365\n",
      "test accuracy on epoch 3775: 0.769\n",
      "train loss on epoch 3776 : 0.161\n",
      "train accuracy on epoch 3776: 0.944\n",
      "test loss on epoch 3776: 0.362\n",
      "test accuracy on epoch 3776: 0.769\n",
      "train loss on epoch 3777 : 0.415\n",
      "train accuracy on epoch 3777: 0.833\n",
      "test loss on epoch 3777: 0.354\n",
      "test accuracy on epoch 3777: 0.769\n",
      "train loss on epoch 3778 : 0.179\n",
      "train accuracy on epoch 3778: 0.944\n",
      "test loss on epoch 3778: 0.349\n",
      "test accuracy on epoch 3778: 0.769\n",
      "train loss on epoch 3779 : 0.052\n",
      "train accuracy on epoch 3779: 1.000\n",
      "test loss on epoch 3779: 0.342\n",
      "test accuracy on epoch 3779: 0.769\n",
      "train loss on epoch 3780 : 0.154\n",
      "train accuracy on epoch 3780: 0.944\n",
      "test loss on epoch 3780: 0.339\n",
      "test accuracy on epoch 3780: 0.769\n",
      "train loss on epoch 3781 : 0.266\n",
      "train accuracy on epoch 3781: 0.944\n",
      "test loss on epoch 3781: 0.337\n",
      "test accuracy on epoch 3781: 0.769\n",
      "train loss on epoch 3782 : 0.114\n",
      "train accuracy on epoch 3782: 0.944\n",
      "test loss on epoch 3782: 0.340\n",
      "test accuracy on epoch 3782: 0.769\n",
      "train loss on epoch 3783 : 0.336\n",
      "train accuracy on epoch 3783: 0.889\n",
      "test loss on epoch 3783: 0.339\n",
      "test accuracy on epoch 3783: 0.769\n",
      "train loss on epoch 3784 : 0.378\n",
      "train accuracy on epoch 3784: 0.944\n",
      "test loss on epoch 3784: 0.336\n",
      "test accuracy on epoch 3784: 0.769\n",
      "train loss on epoch 3785 : 0.041\n",
      "train accuracy on epoch 3785: 1.000\n",
      "test loss on epoch 3785: 0.341\n",
      "test accuracy on epoch 3785: 0.769\n",
      "train loss on epoch 3786 : 0.087\n",
      "train accuracy on epoch 3786: 0.944\n",
      "test loss on epoch 3786: 0.349\n",
      "test accuracy on epoch 3786: 0.769\n",
      "train loss on epoch 3787 : 0.154\n",
      "train accuracy on epoch 3787: 0.833\n",
      "test loss on epoch 3787: 0.353\n",
      "test accuracy on epoch 3787: 0.769\n",
      "train loss on epoch 3788 : 0.071\n",
      "train accuracy on epoch 3788: 1.000\n",
      "test loss on epoch 3788: 0.356\n",
      "test accuracy on epoch 3788: 0.769\n",
      "train loss on epoch 3789 : 0.122\n",
      "train accuracy on epoch 3789: 0.944\n",
      "test loss on epoch 3789: 0.355\n",
      "test accuracy on epoch 3789: 0.769\n",
      "train loss on epoch 3790 : 0.058\n",
      "train accuracy on epoch 3790: 1.000\n",
      "test loss on epoch 3790: 0.353\n",
      "test accuracy on epoch 3790: 0.769\n",
      "train loss on epoch 3791 : 0.084\n",
      "train accuracy on epoch 3791: 1.000\n",
      "test loss on epoch 3791: 0.352\n",
      "test accuracy on epoch 3791: 0.769\n",
      "train loss on epoch 3792 : 0.251\n",
      "train accuracy on epoch 3792: 0.889\n",
      "test loss on epoch 3792: 0.347\n",
      "test accuracy on epoch 3792: 0.769\n",
      "train loss on epoch 3793 : 0.186\n",
      "train accuracy on epoch 3793: 0.944\n",
      "test loss on epoch 3793: 0.347\n",
      "test accuracy on epoch 3793: 0.769\n",
      "train loss on epoch 3794 : 0.197\n",
      "train accuracy on epoch 3794: 0.889\n",
      "test loss on epoch 3794: 0.352\n",
      "test accuracy on epoch 3794: 0.769\n",
      "train loss on epoch 3795 : 0.201\n",
      "train accuracy on epoch 3795: 0.889\n",
      "test loss on epoch 3795: 0.349\n",
      "test accuracy on epoch 3795: 0.769\n",
      "train loss on epoch 3796 : 0.194\n",
      "train accuracy on epoch 3796: 0.944\n",
      "test loss on epoch 3796: 0.348\n",
      "test accuracy on epoch 3796: 0.769\n",
      "train loss on epoch 3797 : 0.216\n",
      "train accuracy on epoch 3797: 0.944\n",
      "test loss on epoch 3797: 0.346\n",
      "test accuracy on epoch 3797: 0.769\n",
      "train loss on epoch 3798 : 0.159\n",
      "train accuracy on epoch 3798: 0.944\n",
      "test loss on epoch 3798: 0.346\n",
      "test accuracy on epoch 3798: 0.769\n",
      "train loss on epoch 3799 : 0.075\n",
      "train accuracy on epoch 3799: 1.000\n",
      "test loss on epoch 3799: 0.345\n",
      "test accuracy on epoch 3799: 0.769\n",
      "train loss on epoch 3800 : 0.185\n",
      "train accuracy on epoch 3800: 0.944\n",
      "test loss on epoch 3800: 0.346\n",
      "test accuracy on epoch 3800: 0.769\n",
      "train loss on epoch 3801 : 0.214\n",
      "train accuracy on epoch 3801: 0.944\n",
      "test loss on epoch 3801: 0.351\n",
      "test accuracy on epoch 3801: 0.769\n",
      "train loss on epoch 3802 : 0.345\n",
      "train accuracy on epoch 3802: 0.889\n",
      "test loss on epoch 3802: 0.361\n",
      "test accuracy on epoch 3802: 0.769\n",
      "train loss on epoch 3803 : 0.323\n",
      "train accuracy on epoch 3803: 0.889\n",
      "test loss on epoch 3803: 0.367\n",
      "test accuracy on epoch 3803: 0.769\n",
      "train loss on epoch 3804 : 0.295\n",
      "train accuracy on epoch 3804: 0.889\n",
      "test loss on epoch 3804: 0.373\n",
      "test accuracy on epoch 3804: 0.769\n",
      "train loss on epoch 3805 : 0.115\n",
      "train accuracy on epoch 3805: 0.944\n",
      "test loss on epoch 3805: 0.369\n",
      "test accuracy on epoch 3805: 0.769\n",
      "train loss on epoch 3806 : 0.097\n",
      "train accuracy on epoch 3806: 0.944\n",
      "test loss on epoch 3806: 0.365\n",
      "test accuracy on epoch 3806: 0.769\n",
      "train loss on epoch 3807 : 0.434\n",
      "train accuracy on epoch 3807: 0.889\n",
      "test loss on epoch 3807: 0.356\n",
      "test accuracy on epoch 3807: 0.769\n",
      "train loss on epoch 3808 : 0.065\n",
      "train accuracy on epoch 3808: 1.000\n",
      "test loss on epoch 3808: 0.353\n",
      "test accuracy on epoch 3808: 0.769\n",
      "train loss on epoch 3809 : 0.248\n",
      "train accuracy on epoch 3809: 0.833\n",
      "test loss on epoch 3809: 0.348\n",
      "test accuracy on epoch 3809: 0.769\n",
      "train loss on epoch 3810 : 0.346\n",
      "train accuracy on epoch 3810: 0.833\n",
      "test loss on epoch 3810: 0.342\n",
      "test accuracy on epoch 3810: 0.769\n",
      "train loss on epoch 3811 : 0.299\n",
      "train accuracy on epoch 3811: 0.889\n",
      "test loss on epoch 3811: 0.342\n",
      "test accuracy on epoch 3811: 0.769\n",
      "train loss on epoch 3812 : 0.138\n",
      "train accuracy on epoch 3812: 0.944\n",
      "test loss on epoch 3812: 0.340\n",
      "test accuracy on epoch 3812: 0.769\n",
      "train loss on epoch 3813 : 0.201\n",
      "train accuracy on epoch 3813: 0.833\n",
      "test loss on epoch 3813: 0.348\n",
      "test accuracy on epoch 3813: 0.769\n",
      "train loss on epoch 3814 : 0.057\n",
      "train accuracy on epoch 3814: 1.000\n",
      "test loss on epoch 3814: 0.344\n",
      "test accuracy on epoch 3814: 0.769\n",
      "train loss on epoch 3815 : 0.213\n",
      "train accuracy on epoch 3815: 0.944\n",
      "test loss on epoch 3815: 0.345\n",
      "test accuracy on epoch 3815: 0.769\n",
      "train loss on epoch 3816 : 0.090\n",
      "train accuracy on epoch 3816: 1.000\n",
      "test loss on epoch 3816: 0.344\n",
      "test accuracy on epoch 3816: 0.769\n",
      "train loss on epoch 3817 : 0.039\n",
      "train accuracy on epoch 3817: 1.000\n",
      "test loss on epoch 3817: 0.341\n",
      "test accuracy on epoch 3817: 0.769\n",
      "train loss on epoch 3818 : 0.113\n",
      "train accuracy on epoch 3818: 0.944\n",
      "test loss on epoch 3818: 0.345\n",
      "test accuracy on epoch 3818: 0.769\n",
      "train loss on epoch 3819 : 0.153\n",
      "train accuracy on epoch 3819: 0.889\n",
      "test loss on epoch 3819: 0.345\n",
      "test accuracy on epoch 3819: 0.769\n",
      "train loss on epoch 3820 : 0.131\n",
      "train accuracy on epoch 3820: 0.944\n",
      "test loss on epoch 3820: 0.350\n",
      "test accuracy on epoch 3820: 0.769\n",
      "train loss on epoch 3821 : 0.277\n",
      "train accuracy on epoch 3821: 0.944\n",
      "test loss on epoch 3821: 0.349\n",
      "test accuracy on epoch 3821: 0.769\n",
      "train loss on epoch 3822 : 0.140\n",
      "train accuracy on epoch 3822: 0.944\n",
      "test loss on epoch 3822: 0.345\n",
      "test accuracy on epoch 3822: 0.769\n",
      "train loss on epoch 3823 : 0.127\n",
      "train accuracy on epoch 3823: 0.944\n",
      "test loss on epoch 3823: 0.351\n",
      "test accuracy on epoch 3823: 0.769\n",
      "train loss on epoch 3824 : 0.295\n",
      "train accuracy on epoch 3824: 0.889\n",
      "test loss on epoch 3824: 0.351\n",
      "test accuracy on epoch 3824: 0.769\n",
      "train loss on epoch 3825 : 0.303\n",
      "train accuracy on epoch 3825: 0.833\n",
      "test loss on epoch 3825: 0.349\n",
      "test accuracy on epoch 3825: 0.769\n",
      "train loss on epoch 3826 : 0.123\n",
      "train accuracy on epoch 3826: 0.944\n",
      "test loss on epoch 3826: 0.346\n",
      "test accuracy on epoch 3826: 0.769\n",
      "train loss on epoch 3827 : 0.055\n",
      "train accuracy on epoch 3827: 1.000\n",
      "test loss on epoch 3827: 0.341\n",
      "test accuracy on epoch 3827: 0.769\n",
      "train loss on epoch 3828 : 0.059\n",
      "train accuracy on epoch 3828: 1.000\n",
      "test loss on epoch 3828: 0.344\n",
      "test accuracy on epoch 3828: 0.769\n",
      "train loss on epoch 3829 : 0.082\n",
      "train accuracy on epoch 3829: 1.000\n",
      "test loss on epoch 3829: 0.342\n",
      "test accuracy on epoch 3829: 0.769\n",
      "train loss on epoch 3830 : 0.126\n",
      "train accuracy on epoch 3830: 0.944\n",
      "test loss on epoch 3830: 0.345\n",
      "test accuracy on epoch 3830: 0.769\n",
      "train loss on epoch 3831 : 0.351\n",
      "train accuracy on epoch 3831: 0.889\n",
      "test loss on epoch 3831: 0.344\n",
      "test accuracy on epoch 3831: 0.769\n",
      "train loss on epoch 3832 : 0.360\n",
      "train accuracy on epoch 3832: 0.833\n",
      "test loss on epoch 3832: 0.350\n",
      "test accuracy on epoch 3832: 0.769\n",
      "train loss on epoch 3833 : 0.407\n",
      "train accuracy on epoch 3833: 0.889\n",
      "test loss on epoch 3833: 0.346\n",
      "test accuracy on epoch 3833: 0.769\n",
      "train loss on epoch 3834 : 0.064\n",
      "train accuracy on epoch 3834: 1.000\n",
      "test loss on epoch 3834: 0.341\n",
      "test accuracy on epoch 3834: 0.769\n",
      "train loss on epoch 3835 : 0.334\n",
      "train accuracy on epoch 3835: 0.889\n",
      "test loss on epoch 3835: 0.343\n",
      "test accuracy on epoch 3835: 0.769\n",
      "train loss on epoch 3836 : 0.280\n",
      "train accuracy on epoch 3836: 0.889\n",
      "test loss on epoch 3836: 0.335\n",
      "test accuracy on epoch 3836: 0.769\n",
      "train loss on epoch 3837 : 0.088\n",
      "train accuracy on epoch 3837: 1.000\n",
      "test loss on epoch 3837: 0.336\n",
      "test accuracy on epoch 3837: 0.769\n",
      "train loss on epoch 3838 : 0.267\n",
      "train accuracy on epoch 3838: 0.944\n",
      "test loss on epoch 3838: 0.342\n",
      "test accuracy on epoch 3838: 0.769\n",
      "train loss on epoch 3839 : 0.071\n",
      "train accuracy on epoch 3839: 0.944\n",
      "test loss on epoch 3839: 0.342\n",
      "test accuracy on epoch 3839: 0.769\n",
      "train loss on epoch 3840 : 0.146\n",
      "train accuracy on epoch 3840: 0.944\n",
      "test loss on epoch 3840: 0.343\n",
      "test accuracy on epoch 3840: 0.769\n",
      "train loss on epoch 3841 : 0.188\n",
      "train accuracy on epoch 3841: 0.944\n",
      "test loss on epoch 3841: 0.339\n",
      "test accuracy on epoch 3841: 0.769\n",
      "train loss on epoch 3842 : 0.130\n",
      "train accuracy on epoch 3842: 0.944\n",
      "test loss on epoch 3842: 0.345\n",
      "test accuracy on epoch 3842: 0.769\n",
      "train loss on epoch 3843 : 0.244\n",
      "train accuracy on epoch 3843: 0.944\n",
      "test loss on epoch 3843: 0.344\n",
      "test accuracy on epoch 3843: 0.769\n",
      "train loss on epoch 3844 : 0.162\n",
      "train accuracy on epoch 3844: 0.889\n",
      "test loss on epoch 3844: 0.349\n",
      "test accuracy on epoch 3844: 0.769\n",
      "train loss on epoch 3845 : 0.206\n",
      "train accuracy on epoch 3845: 0.889\n",
      "test loss on epoch 3845: 0.344\n",
      "test accuracy on epoch 3845: 0.692\n",
      "train loss on epoch 3846 : 0.084\n",
      "train accuracy on epoch 3846: 1.000\n",
      "test loss on epoch 3846: 0.333\n",
      "test accuracy on epoch 3846: 0.846\n",
      "train loss on epoch 3847 : 0.255\n",
      "train accuracy on epoch 3847: 0.889\n",
      "test loss on epoch 3847: 0.345\n",
      "test accuracy on epoch 3847: 0.769\n",
      "train loss on epoch 3848 : 0.138\n",
      "train accuracy on epoch 3848: 0.944\n",
      "test loss on epoch 3848: 0.345\n",
      "test accuracy on epoch 3848: 0.769\n",
      "train loss on epoch 3849 : 0.157\n",
      "train accuracy on epoch 3849: 0.944\n",
      "test loss on epoch 3849: 0.350\n",
      "test accuracy on epoch 3849: 0.692\n",
      "train loss on epoch 3850 : 0.200\n",
      "train accuracy on epoch 3850: 0.889\n",
      "test loss on epoch 3850: 0.351\n",
      "test accuracy on epoch 3850: 0.692\n",
      "train loss on epoch 3851 : 0.279\n",
      "train accuracy on epoch 3851: 0.889\n",
      "test loss on epoch 3851: 0.338\n",
      "test accuracy on epoch 3851: 0.769\n",
      "train loss on epoch 3852 : 0.142\n",
      "train accuracy on epoch 3852: 0.944\n",
      "test loss on epoch 3852: 0.339\n",
      "test accuracy on epoch 3852: 0.769\n",
      "train loss on epoch 3853 : 0.151\n",
      "train accuracy on epoch 3853: 0.944\n",
      "test loss on epoch 3853: 0.346\n",
      "test accuracy on epoch 3853: 0.769\n",
      "train loss on epoch 3854 : 0.077\n",
      "train accuracy on epoch 3854: 1.000\n",
      "test loss on epoch 3854: 0.352\n",
      "test accuracy on epoch 3854: 0.769\n",
      "train loss on epoch 3855 : 0.174\n",
      "train accuracy on epoch 3855: 0.944\n",
      "test loss on epoch 3855: 0.363\n",
      "test accuracy on epoch 3855: 0.769\n",
      "train loss on epoch 3856 : 0.108\n",
      "train accuracy on epoch 3856: 1.000\n",
      "test loss on epoch 3856: 0.375\n",
      "test accuracy on epoch 3856: 0.769\n",
      "train loss on epoch 3857 : 0.128\n",
      "train accuracy on epoch 3857: 0.944\n",
      "test loss on epoch 3857: 0.380\n",
      "test accuracy on epoch 3857: 0.769\n",
      "train loss on epoch 3858 : 0.301\n",
      "train accuracy on epoch 3858: 0.889\n",
      "test loss on epoch 3858: 0.375\n",
      "test accuracy on epoch 3858: 0.769\n",
      "train loss on epoch 3859 : 0.314\n",
      "train accuracy on epoch 3859: 0.944\n",
      "test loss on epoch 3859: 0.363\n",
      "test accuracy on epoch 3859: 0.769\n",
      "train loss on epoch 3860 : 0.175\n",
      "train accuracy on epoch 3860: 0.944\n",
      "test loss on epoch 3860: 0.353\n",
      "test accuracy on epoch 3860: 0.769\n",
      "train loss on epoch 3861 : 0.172\n",
      "train accuracy on epoch 3861: 0.944\n",
      "test loss on epoch 3861: 0.353\n",
      "test accuracy on epoch 3861: 0.769\n",
      "train loss on epoch 3862 : 0.113\n",
      "train accuracy on epoch 3862: 1.000\n",
      "test loss on epoch 3862: 0.349\n",
      "test accuracy on epoch 3862: 0.769\n",
      "train loss on epoch 3863 : 0.093\n",
      "train accuracy on epoch 3863: 1.000\n",
      "test loss on epoch 3863: 0.348\n",
      "test accuracy on epoch 3863: 0.769\n",
      "train loss on epoch 3864 : 0.158\n",
      "train accuracy on epoch 3864: 0.889\n",
      "test loss on epoch 3864: 0.343\n",
      "test accuracy on epoch 3864: 0.769\n",
      "train loss on epoch 3865 : 0.241\n",
      "train accuracy on epoch 3865: 0.889\n",
      "test loss on epoch 3865: 0.350\n",
      "test accuracy on epoch 3865: 0.769\n",
      "train loss on epoch 3866 : 0.061\n",
      "train accuracy on epoch 3866: 0.944\n",
      "test loss on epoch 3866: 0.352\n",
      "test accuracy on epoch 3866: 0.769\n",
      "train loss on epoch 3867 : 0.613\n",
      "train accuracy on epoch 3867: 0.833\n",
      "test loss on epoch 3867: 0.351\n",
      "test accuracy on epoch 3867: 0.769\n",
      "train loss on epoch 3868 : 0.116\n",
      "train accuracy on epoch 3868: 0.889\n",
      "test loss on epoch 3868: 0.345\n",
      "test accuracy on epoch 3868: 0.769\n",
      "train loss on epoch 3869 : 0.422\n",
      "train accuracy on epoch 3869: 0.889\n",
      "test loss on epoch 3869: 0.338\n",
      "test accuracy on epoch 3869: 0.769\n",
      "train loss on epoch 3870 : 0.191\n",
      "train accuracy on epoch 3870: 0.944\n",
      "test loss on epoch 3870: 0.337\n",
      "test accuracy on epoch 3870: 0.769\n",
      "train loss on epoch 3871 : 0.092\n",
      "train accuracy on epoch 3871: 0.944\n",
      "test loss on epoch 3871: 0.342\n",
      "test accuracy on epoch 3871: 0.769\n",
      "train loss on epoch 3872 : 0.228\n",
      "train accuracy on epoch 3872: 0.833\n",
      "test loss on epoch 3872: 0.342\n",
      "test accuracy on epoch 3872: 0.769\n",
      "train loss on epoch 3873 : 0.065\n",
      "train accuracy on epoch 3873: 0.944\n",
      "test loss on epoch 3873: 0.346\n",
      "test accuracy on epoch 3873: 0.769\n",
      "train loss on epoch 3874 : 0.281\n",
      "train accuracy on epoch 3874: 0.889\n",
      "test loss on epoch 3874: 0.351\n",
      "test accuracy on epoch 3874: 0.769\n",
      "train loss on epoch 3875 : 0.200\n",
      "train accuracy on epoch 3875: 0.889\n",
      "test loss on epoch 3875: 0.346\n",
      "test accuracy on epoch 3875: 0.769\n",
      "train loss on epoch 3876 : 0.056\n",
      "train accuracy on epoch 3876: 1.000\n",
      "test loss on epoch 3876: 0.347\n",
      "test accuracy on epoch 3876: 0.769\n",
      "train loss on epoch 3877 : 0.251\n",
      "train accuracy on epoch 3877: 0.833\n",
      "test loss on epoch 3877: 0.346\n",
      "test accuracy on epoch 3877: 0.769\n",
      "train loss on epoch 3878 : 0.139\n",
      "train accuracy on epoch 3878: 0.944\n",
      "test loss on epoch 3878: 0.338\n",
      "test accuracy on epoch 3878: 0.769\n",
      "train loss on epoch 3879 : 0.214\n",
      "train accuracy on epoch 3879: 0.889\n",
      "test loss on epoch 3879: 0.343\n",
      "test accuracy on epoch 3879: 0.769\n",
      "train loss on epoch 3880 : 0.048\n",
      "train accuracy on epoch 3880: 1.000\n",
      "test loss on epoch 3880: 0.341\n",
      "test accuracy on epoch 3880: 0.769\n",
      "train loss on epoch 3881 : 0.303\n",
      "train accuracy on epoch 3881: 0.889\n",
      "test loss on epoch 3881: 0.333\n",
      "test accuracy on epoch 3881: 0.769\n",
      "train loss on epoch 3882 : 0.298\n",
      "train accuracy on epoch 3882: 0.944\n",
      "test loss on epoch 3882: 0.333\n",
      "test accuracy on epoch 3882: 0.769\n",
      "train loss on epoch 3883 : 0.131\n",
      "train accuracy on epoch 3883: 0.944\n",
      "test loss on epoch 3883: 0.334\n",
      "test accuracy on epoch 3883: 0.692\n",
      "train loss on epoch 3884 : 0.218\n",
      "train accuracy on epoch 3884: 0.833\n",
      "test loss on epoch 3884: 0.326\n",
      "test accuracy on epoch 3884: 0.846\n",
      "train loss on epoch 3885 : 0.203\n",
      "train accuracy on epoch 3885: 0.944\n",
      "test loss on epoch 3885: 0.327\n",
      "test accuracy on epoch 3885: 0.846\n",
      "train loss on epoch 3886 : 0.121\n",
      "train accuracy on epoch 3886: 0.944\n",
      "test loss on epoch 3886: 0.331\n",
      "test accuracy on epoch 3886: 0.769\n",
      "train loss on epoch 3887 : 0.258\n",
      "train accuracy on epoch 3887: 0.833\n",
      "test loss on epoch 3887: 0.336\n",
      "test accuracy on epoch 3887: 0.769\n",
      "train loss on epoch 3888 : 0.128\n",
      "train accuracy on epoch 3888: 0.944\n",
      "test loss on epoch 3888: 0.336\n",
      "test accuracy on epoch 3888: 0.769\n",
      "train loss on epoch 3889 : 0.121\n",
      "train accuracy on epoch 3889: 0.889\n",
      "test loss on epoch 3889: 0.337\n",
      "test accuracy on epoch 3889: 0.769\n",
      "train loss on epoch 3890 : 0.232\n",
      "train accuracy on epoch 3890: 0.889\n",
      "test loss on epoch 3890: 0.335\n",
      "test accuracy on epoch 3890: 0.769\n",
      "train loss on epoch 3891 : 0.041\n",
      "train accuracy on epoch 3891: 1.000\n",
      "test loss on epoch 3891: 0.331\n",
      "test accuracy on epoch 3891: 0.769\n",
      "train loss on epoch 3892 : 0.483\n",
      "train accuracy on epoch 3892: 0.833\n",
      "test loss on epoch 3892: 0.337\n",
      "test accuracy on epoch 3892: 0.769\n",
      "train loss on epoch 3893 : 0.253\n",
      "train accuracy on epoch 3893: 0.889\n",
      "test loss on epoch 3893: 0.340\n",
      "test accuracy on epoch 3893: 0.769\n",
      "train loss on epoch 3894 : 0.106\n",
      "train accuracy on epoch 3894: 0.944\n",
      "test loss on epoch 3894: 0.339\n",
      "test accuracy on epoch 3894: 0.769\n",
      "train loss on epoch 3895 : 0.153\n",
      "train accuracy on epoch 3895: 0.889\n",
      "test loss on epoch 3895: 0.326\n",
      "test accuracy on epoch 3895: 0.846\n",
      "train loss on epoch 3896 : 0.269\n",
      "train accuracy on epoch 3896: 0.889\n",
      "test loss on epoch 3896: 0.326\n",
      "test accuracy on epoch 3896: 0.846\n",
      "train loss on epoch 3897 : 0.276\n",
      "train accuracy on epoch 3897: 0.889\n",
      "test loss on epoch 3897: 0.332\n",
      "test accuracy on epoch 3897: 0.769\n",
      "train loss on epoch 3898 : 0.196\n",
      "train accuracy on epoch 3898: 0.944\n",
      "test loss on epoch 3898: 0.329\n",
      "test accuracy on epoch 3898: 0.769\n",
      "train loss on epoch 3899 : 0.077\n",
      "train accuracy on epoch 3899: 0.944\n",
      "test loss on epoch 3899: 0.329\n",
      "test accuracy on epoch 3899: 0.769\n",
      "train loss on epoch 3900 : 0.336\n",
      "train accuracy on epoch 3900: 0.889\n",
      "test loss on epoch 3900: 0.338\n",
      "test accuracy on epoch 3900: 0.769\n",
      "train loss on epoch 3901 : 0.235\n",
      "train accuracy on epoch 3901: 0.833\n",
      "test loss on epoch 3901: 0.331\n",
      "test accuracy on epoch 3901: 0.769\n",
      "train loss on epoch 3902 : 0.371\n",
      "train accuracy on epoch 3902: 0.889\n",
      "test loss on epoch 3902: 0.332\n",
      "test accuracy on epoch 3902: 0.769\n",
      "train loss on epoch 3903 : 0.037\n",
      "train accuracy on epoch 3903: 1.000\n",
      "test loss on epoch 3903: 0.332\n",
      "test accuracy on epoch 3903: 0.769\n",
      "train loss on epoch 3904 : 0.162\n",
      "train accuracy on epoch 3904: 0.889\n",
      "test loss on epoch 3904: 0.337\n",
      "test accuracy on epoch 3904: 0.769\n",
      "train loss on epoch 3905 : 0.451\n",
      "train accuracy on epoch 3905: 0.833\n",
      "test loss on epoch 3905: 0.336\n",
      "test accuracy on epoch 3905: 0.769\n",
      "train loss on epoch 3906 : 0.196\n",
      "train accuracy on epoch 3906: 0.944\n",
      "test loss on epoch 3906: 0.340\n",
      "test accuracy on epoch 3906: 0.769\n",
      "train loss on epoch 3907 : 0.164\n",
      "train accuracy on epoch 3907: 0.944\n",
      "test loss on epoch 3907: 0.336\n",
      "test accuracy on epoch 3907: 0.769\n",
      "train loss on epoch 3908 : 0.057\n",
      "train accuracy on epoch 3908: 1.000\n",
      "test loss on epoch 3908: 0.335\n",
      "test accuracy on epoch 3908: 0.692\n",
      "train loss on epoch 3909 : 0.285\n",
      "train accuracy on epoch 3909: 0.889\n",
      "test loss on epoch 3909: 0.325\n",
      "test accuracy on epoch 3909: 0.769\n",
      "train loss on epoch 3910 : 0.223\n",
      "train accuracy on epoch 3910: 0.889\n",
      "test loss on epoch 3910: 0.325\n",
      "test accuracy on epoch 3910: 0.769\n",
      "train loss on epoch 3911 : 0.225\n",
      "train accuracy on epoch 3911: 0.889\n",
      "test loss on epoch 3911: 0.336\n",
      "test accuracy on epoch 3911: 0.692\n",
      "train loss on epoch 3912 : 0.241\n",
      "train accuracy on epoch 3912: 0.944\n",
      "test loss on epoch 3912: 0.323\n",
      "test accuracy on epoch 3912: 0.769\n",
      "train loss on epoch 3913 : 0.144\n",
      "train accuracy on epoch 3913: 0.944\n",
      "test loss on epoch 3913: 0.327\n",
      "test accuracy on epoch 3913: 0.769\n",
      "train loss on epoch 3914 : 0.367\n",
      "train accuracy on epoch 3914: 0.889\n",
      "test loss on epoch 3914: 0.332\n",
      "test accuracy on epoch 3914: 0.769\n",
      "train loss on epoch 3915 : 0.153\n",
      "train accuracy on epoch 3915: 0.889\n",
      "test loss on epoch 3915: 0.331\n",
      "test accuracy on epoch 3915: 0.769\n",
      "train loss on epoch 3916 : 0.074\n",
      "train accuracy on epoch 3916: 1.000\n",
      "test loss on epoch 3916: 0.329\n",
      "test accuracy on epoch 3916: 0.769\n",
      "train loss on epoch 3917 : 0.115\n",
      "train accuracy on epoch 3917: 0.944\n",
      "test loss on epoch 3917: 0.331\n",
      "test accuracy on epoch 3917: 0.769\n",
      "train loss on epoch 3918 : 0.059\n",
      "train accuracy on epoch 3918: 1.000\n",
      "test loss on epoch 3918: 0.337\n",
      "test accuracy on epoch 3918: 0.769\n",
      "train loss on epoch 3919 : 0.081\n",
      "train accuracy on epoch 3919: 1.000\n",
      "test loss on epoch 3919: 0.334\n",
      "test accuracy on epoch 3919: 0.769\n",
      "train loss on epoch 3920 : 0.032\n",
      "train accuracy on epoch 3920: 1.000\n",
      "test loss on epoch 3920: 0.326\n",
      "test accuracy on epoch 3920: 0.769\n",
      "train loss on epoch 3921 : 0.064\n",
      "train accuracy on epoch 3921: 1.000\n",
      "test loss on epoch 3921: 0.329\n",
      "test accuracy on epoch 3921: 0.769\n",
      "train loss on epoch 3922 : 0.077\n",
      "train accuracy on epoch 3922: 0.944\n",
      "test loss on epoch 3922: 0.333\n",
      "test accuracy on epoch 3922: 0.769\n",
      "train loss on epoch 3923 : 0.099\n",
      "train accuracy on epoch 3923: 1.000\n",
      "test loss on epoch 3923: 0.333\n",
      "test accuracy on epoch 3923: 0.769\n",
      "train loss on epoch 3924 : 0.087\n",
      "train accuracy on epoch 3924: 0.944\n",
      "test loss on epoch 3924: 0.328\n",
      "test accuracy on epoch 3924: 0.769\n",
      "train loss on epoch 3925 : 0.217\n",
      "train accuracy on epoch 3925: 0.944\n",
      "test loss on epoch 3925: 0.324\n",
      "test accuracy on epoch 3925: 0.769\n",
      "train loss on epoch 3926 : 0.163\n",
      "train accuracy on epoch 3926: 0.944\n",
      "test loss on epoch 3926: 0.322\n",
      "test accuracy on epoch 3926: 0.769\n",
      "train loss on epoch 3927 : 0.215\n",
      "train accuracy on epoch 3927: 0.833\n",
      "test loss on epoch 3927: 0.332\n",
      "test accuracy on epoch 3927: 0.769\n",
      "train loss on epoch 3928 : 0.191\n",
      "train accuracy on epoch 3928: 0.889\n",
      "test loss on epoch 3928: 0.327\n",
      "test accuracy on epoch 3928: 0.769\n",
      "train loss on epoch 3929 : 0.226\n",
      "train accuracy on epoch 3929: 0.889\n",
      "test loss on epoch 3929: 0.340\n",
      "test accuracy on epoch 3929: 0.769\n",
      "train loss on epoch 3930 : 0.340\n",
      "train accuracy on epoch 3930: 0.889\n",
      "test loss on epoch 3930: 0.342\n",
      "test accuracy on epoch 3930: 0.769\n",
      "train loss on epoch 3931 : 0.167\n",
      "train accuracy on epoch 3931: 0.944\n",
      "test loss on epoch 3931: 0.342\n",
      "test accuracy on epoch 3931: 0.769\n",
      "train loss on epoch 3932 : 0.259\n",
      "train accuracy on epoch 3932: 0.833\n",
      "test loss on epoch 3932: 0.336\n",
      "test accuracy on epoch 3932: 0.769\n",
      "train loss on epoch 3933 : 0.161\n",
      "train accuracy on epoch 3933: 0.889\n",
      "test loss on epoch 3933: 0.335\n",
      "test accuracy on epoch 3933: 0.769\n",
      "train loss on epoch 3934 : 0.272\n",
      "train accuracy on epoch 3934: 0.889\n",
      "test loss on epoch 3934: 0.327\n",
      "test accuracy on epoch 3934: 0.769\n",
      "train loss on epoch 3935 : 0.143\n",
      "train accuracy on epoch 3935: 0.944\n",
      "test loss on epoch 3935: 0.328\n",
      "test accuracy on epoch 3935: 0.769\n",
      "train loss on epoch 3936 : 0.141\n",
      "train accuracy on epoch 3936: 0.889\n",
      "test loss on epoch 3936: 0.329\n",
      "test accuracy on epoch 3936: 0.769\n",
      "train loss on epoch 3937 : 0.094\n",
      "train accuracy on epoch 3937: 0.944\n",
      "test loss on epoch 3937: 0.332\n",
      "test accuracy on epoch 3937: 0.769\n",
      "train loss on epoch 3938 : 0.215\n",
      "train accuracy on epoch 3938: 0.944\n",
      "test loss on epoch 3938: 0.331\n",
      "test accuracy on epoch 3938: 0.769\n",
      "train loss on epoch 3939 : 0.191\n",
      "train accuracy on epoch 3939: 0.944\n",
      "test loss on epoch 3939: 0.326\n",
      "test accuracy on epoch 3939: 0.769\n",
      "train loss on epoch 3940 : 0.108\n",
      "train accuracy on epoch 3940: 0.944\n",
      "test loss on epoch 3940: 0.327\n",
      "test accuracy on epoch 3940: 0.769\n",
      "train loss on epoch 3941 : 0.142\n",
      "train accuracy on epoch 3941: 0.944\n",
      "test loss on epoch 3941: 0.323\n",
      "test accuracy on epoch 3941: 0.769\n",
      "train loss on epoch 3942 : 0.185\n",
      "train accuracy on epoch 3942: 0.889\n",
      "test loss on epoch 3942: 0.325\n",
      "test accuracy on epoch 3942: 0.769\n",
      "train loss on epoch 3943 : 0.077\n",
      "train accuracy on epoch 3943: 1.000\n",
      "test loss on epoch 3943: 0.320\n",
      "test accuracy on epoch 3943: 0.769\n",
      "train loss on epoch 3944 : 0.204\n",
      "train accuracy on epoch 3944: 0.833\n",
      "test loss on epoch 3944: 0.329\n",
      "test accuracy on epoch 3944: 0.769\n",
      "train loss on epoch 3945 : 0.076\n",
      "train accuracy on epoch 3945: 1.000\n",
      "test loss on epoch 3945: 0.323\n",
      "test accuracy on epoch 3945: 0.769\n",
      "train loss on epoch 3946 : 0.127\n",
      "train accuracy on epoch 3946: 1.000\n",
      "test loss on epoch 3946: 0.329\n",
      "test accuracy on epoch 3946: 0.769\n",
      "train loss on epoch 3947 : 0.338\n",
      "train accuracy on epoch 3947: 0.889\n",
      "test loss on epoch 3947: 0.333\n",
      "test accuracy on epoch 3947: 0.769\n",
      "train loss on epoch 3948 : 0.200\n",
      "train accuracy on epoch 3948: 0.889\n",
      "test loss on epoch 3948: 0.328\n",
      "test accuracy on epoch 3948: 0.769\n",
      "train loss on epoch 3949 : 0.094\n",
      "train accuracy on epoch 3949: 0.944\n",
      "test loss on epoch 3949: 0.324\n",
      "test accuracy on epoch 3949: 0.769\n",
      "train loss on epoch 3950 : 0.094\n",
      "train accuracy on epoch 3950: 0.944\n",
      "test loss on epoch 3950: 0.319\n",
      "test accuracy on epoch 3950: 0.769\n",
      "train loss on epoch 3951 : 0.170\n",
      "train accuracy on epoch 3951: 0.889\n",
      "test loss on epoch 3951: 0.320\n",
      "test accuracy on epoch 3951: 0.846\n",
      "train loss on epoch 3952 : 0.209\n",
      "train accuracy on epoch 3952: 0.944\n",
      "test loss on epoch 3952: 0.316\n",
      "test accuracy on epoch 3952: 0.769\n",
      "train loss on epoch 3953 : 0.157\n",
      "train accuracy on epoch 3953: 0.889\n",
      "test loss on epoch 3953: 0.329\n",
      "test accuracy on epoch 3953: 0.769\n",
      "train loss on epoch 3954 : 0.272\n",
      "train accuracy on epoch 3954: 0.944\n",
      "test loss on epoch 3954: 0.323\n",
      "test accuracy on epoch 3954: 0.769\n",
      "train loss on epoch 3955 : 0.110\n",
      "train accuracy on epoch 3955: 0.944\n",
      "test loss on epoch 3955: 0.324\n",
      "test accuracy on epoch 3955: 0.769\n",
      "train loss on epoch 3956 : 0.237\n",
      "train accuracy on epoch 3956: 0.833\n",
      "test loss on epoch 3956: 0.320\n",
      "test accuracy on epoch 3956: 0.769\n",
      "train loss on epoch 3957 : 0.187\n",
      "train accuracy on epoch 3957: 0.889\n",
      "test loss on epoch 3957: 0.321\n",
      "test accuracy on epoch 3957: 0.769\n",
      "train loss on epoch 3958 : 0.122\n",
      "train accuracy on epoch 3958: 0.944\n",
      "test loss on epoch 3958: 0.330\n",
      "test accuracy on epoch 3958: 0.769\n",
      "train loss on epoch 3959 : 0.232\n",
      "train accuracy on epoch 3959: 0.889\n",
      "test loss on epoch 3959: 0.331\n",
      "test accuracy on epoch 3959: 0.769\n",
      "train loss on epoch 3960 : 0.212\n",
      "train accuracy on epoch 3960: 0.889\n",
      "test loss on epoch 3960: 0.330\n",
      "test accuracy on epoch 3960: 0.769\n",
      "train loss on epoch 3961 : 0.199\n",
      "train accuracy on epoch 3961: 0.889\n",
      "test loss on epoch 3961: 0.331\n",
      "test accuracy on epoch 3961: 0.769\n",
      "train loss on epoch 3962 : 0.144\n",
      "train accuracy on epoch 3962: 0.944\n",
      "test loss on epoch 3962: 0.332\n",
      "test accuracy on epoch 3962: 0.769\n",
      "train loss on epoch 3963 : 0.073\n",
      "train accuracy on epoch 3963: 0.944\n",
      "test loss on epoch 3963: 0.324\n",
      "test accuracy on epoch 3963: 0.769\n",
      "train loss on epoch 3964 : 0.292\n",
      "train accuracy on epoch 3964: 0.889\n",
      "test loss on epoch 3964: 0.325\n",
      "test accuracy on epoch 3964: 0.769\n",
      "train loss on epoch 3965 : 0.051\n",
      "train accuracy on epoch 3965: 1.000\n",
      "test loss on epoch 3965: 0.330\n",
      "test accuracy on epoch 3965: 0.769\n",
      "train loss on epoch 3966 : 0.201\n",
      "train accuracy on epoch 3966: 0.889\n",
      "test loss on epoch 3966: 0.330\n",
      "test accuracy on epoch 3966: 0.769\n",
      "train loss on epoch 3967 : 0.298\n",
      "train accuracy on epoch 3967: 0.889\n",
      "test loss on epoch 3967: 0.327\n",
      "test accuracy on epoch 3967: 0.769\n",
      "train loss on epoch 3968 : 0.042\n",
      "train accuracy on epoch 3968: 1.000\n",
      "test loss on epoch 3968: 0.328\n",
      "test accuracy on epoch 3968: 0.769\n",
      "train loss on epoch 3969 : 0.155\n",
      "train accuracy on epoch 3969: 0.944\n",
      "test loss on epoch 3969: 0.333\n",
      "test accuracy on epoch 3969: 0.769\n",
      "train loss on epoch 3970 : 0.174\n",
      "train accuracy on epoch 3970: 0.889\n",
      "test loss on epoch 3970: 0.332\n",
      "test accuracy on epoch 3970: 0.769\n",
      "train loss on epoch 3971 : 0.365\n",
      "train accuracy on epoch 3971: 0.889\n",
      "test loss on epoch 3971: 0.330\n",
      "test accuracy on epoch 3971: 0.769\n",
      "train loss on epoch 3972 : 0.176\n",
      "train accuracy on epoch 3972: 0.944\n",
      "test loss on epoch 3972: 0.323\n",
      "test accuracy on epoch 3972: 0.769\n",
      "train loss on epoch 3973 : 0.304\n",
      "train accuracy on epoch 3973: 0.889\n",
      "test loss on epoch 3973: 0.328\n",
      "test accuracy on epoch 3973: 0.769\n",
      "train loss on epoch 3974 : 0.159\n",
      "train accuracy on epoch 3974: 0.889\n",
      "test loss on epoch 3974: 0.325\n",
      "test accuracy on epoch 3974: 0.769\n",
      "train loss on epoch 3975 : 0.041\n",
      "train accuracy on epoch 3975: 1.000\n",
      "test loss on epoch 3975: 0.331\n",
      "test accuracy on epoch 3975: 0.692\n",
      "train loss on epoch 3976 : 0.208\n",
      "train accuracy on epoch 3976: 0.944\n",
      "test loss on epoch 3976: 0.321\n",
      "test accuracy on epoch 3976: 0.846\n",
      "train loss on epoch 3977 : 0.357\n",
      "train accuracy on epoch 3977: 0.889\n",
      "test loss on epoch 3977: 0.331\n",
      "test accuracy on epoch 3977: 0.769\n",
      "train loss on epoch 3978 : 0.329\n",
      "train accuracy on epoch 3978: 0.889\n",
      "test loss on epoch 3978: 0.332\n",
      "test accuracy on epoch 3978: 0.769\n",
      "train loss on epoch 3979 : 0.267\n",
      "train accuracy on epoch 3979: 0.889\n",
      "test loss on epoch 3979: 0.340\n",
      "test accuracy on epoch 3979: 0.769\n",
      "train loss on epoch 3980 : 0.169\n",
      "train accuracy on epoch 3980: 0.944\n",
      "test loss on epoch 3980: 0.348\n",
      "test accuracy on epoch 3980: 0.769\n",
      "train loss on epoch 3981 : 0.136\n",
      "train accuracy on epoch 3981: 0.889\n",
      "test loss on epoch 3981: 0.352\n",
      "test accuracy on epoch 3981: 0.769\n",
      "train loss on epoch 3982 : 0.087\n",
      "train accuracy on epoch 3982: 0.944\n",
      "test loss on epoch 3982: 0.355\n",
      "test accuracy on epoch 3982: 0.769\n",
      "train loss on epoch 3983 : 0.233\n",
      "train accuracy on epoch 3983: 0.889\n",
      "test loss on epoch 3983: 0.355\n",
      "test accuracy on epoch 3983: 0.769\n",
      "train loss on epoch 3984 : 0.127\n",
      "train accuracy on epoch 3984: 0.944\n",
      "test loss on epoch 3984: 0.348\n",
      "test accuracy on epoch 3984: 0.769\n",
      "train loss on epoch 3985 : 0.067\n",
      "train accuracy on epoch 3985: 0.944\n",
      "test loss on epoch 3985: 0.347\n",
      "test accuracy on epoch 3985: 0.769\n",
      "train loss on epoch 3986 : 0.124\n",
      "train accuracy on epoch 3986: 0.944\n",
      "test loss on epoch 3986: 0.344\n",
      "test accuracy on epoch 3986: 0.769\n",
      "train loss on epoch 3987 : 0.136\n",
      "train accuracy on epoch 3987: 0.944\n",
      "test loss on epoch 3987: 0.335\n",
      "test accuracy on epoch 3987: 0.769\n",
      "train loss on epoch 3988 : 0.057\n",
      "train accuracy on epoch 3988: 1.000\n",
      "test loss on epoch 3988: 0.332\n",
      "test accuracy on epoch 3988: 0.769\n",
      "train loss on epoch 3989 : 0.285\n",
      "train accuracy on epoch 3989: 0.889\n",
      "test loss on epoch 3989: 0.338\n",
      "test accuracy on epoch 3989: 0.769\n",
      "train loss on epoch 3990 : 0.029\n",
      "train accuracy on epoch 3990: 1.000\n",
      "test loss on epoch 3990: 0.336\n",
      "test accuracy on epoch 3990: 0.769\n",
      "train loss on epoch 3991 : 0.097\n",
      "train accuracy on epoch 3991: 0.944\n",
      "test loss on epoch 3991: 0.333\n",
      "test accuracy on epoch 3991: 0.769\n",
      "train loss on epoch 3992 : 0.054\n",
      "train accuracy on epoch 3992: 0.944\n",
      "test loss on epoch 3992: 0.332\n",
      "test accuracy on epoch 3992: 0.769\n",
      "train loss on epoch 3993 : 0.101\n",
      "train accuracy on epoch 3993: 0.944\n",
      "test loss on epoch 3993: 0.332\n",
      "test accuracy on epoch 3993: 0.769\n",
      "train loss on epoch 3994 : 0.051\n",
      "train accuracy on epoch 3994: 1.000\n",
      "test loss on epoch 3994: 0.328\n",
      "test accuracy on epoch 3994: 0.769\n",
      "train loss on epoch 3995 : 0.105\n",
      "train accuracy on epoch 3995: 1.000\n",
      "test loss on epoch 3995: 0.329\n",
      "test accuracy on epoch 3995: 0.769\n",
      "train loss on epoch 3996 : 0.290\n",
      "train accuracy on epoch 3996: 0.944\n",
      "test loss on epoch 3996: 0.334\n",
      "test accuracy on epoch 3996: 0.769\n",
      "train loss on epoch 3997 : 0.389\n",
      "train accuracy on epoch 3997: 0.833\n",
      "test loss on epoch 3997: 0.339\n",
      "test accuracy on epoch 3997: 0.769\n",
      "train loss on epoch 3998 : 0.117\n",
      "train accuracy on epoch 3998: 0.944\n",
      "test loss on epoch 3998: 0.349\n",
      "test accuracy on epoch 3998: 0.769\n",
      "train loss on epoch 3999 : 0.150\n",
      "train accuracy on epoch 3999: 0.944\n",
      "test loss on epoch 3999: 0.355\n",
      "test accuracy on epoch 3999: 0.769\n",
      "train loss on epoch 4000 : 0.146\n",
      "train accuracy on epoch 4000: 0.944\n",
      "test loss on epoch 4000: 0.363\n",
      "test accuracy on epoch 4000: 0.769\n",
      "train loss on epoch 4001 : 0.146\n",
      "train accuracy on epoch 4001: 0.944\n",
      "test loss on epoch 4001: 0.368\n",
      "test accuracy on epoch 4001: 0.769\n",
      "train loss on epoch 4002 : 0.314\n",
      "train accuracy on epoch 4002: 0.889\n",
      "test loss on epoch 4002: 0.364\n",
      "test accuracy on epoch 4002: 0.769\n",
      "train loss on epoch 4003 : 0.276\n",
      "train accuracy on epoch 4003: 0.944\n",
      "test loss on epoch 4003: 0.357\n",
      "test accuracy on epoch 4003: 0.769\n",
      "train loss on epoch 4004 : 0.086\n",
      "train accuracy on epoch 4004: 1.000\n",
      "test loss on epoch 4004: 0.352\n",
      "test accuracy on epoch 4004: 0.769\n",
      "train loss on epoch 4005 : 0.263\n",
      "train accuracy on epoch 4005: 0.889\n",
      "test loss on epoch 4005: 0.342\n",
      "test accuracy on epoch 4005: 0.769\n",
      "train loss on epoch 4006 : 0.184\n",
      "train accuracy on epoch 4006: 0.889\n",
      "test loss on epoch 4006: 0.340\n",
      "test accuracy on epoch 4006: 0.769\n",
      "train loss on epoch 4007 : 0.165\n",
      "train accuracy on epoch 4007: 0.944\n",
      "test loss on epoch 4007: 0.340\n",
      "test accuracy on epoch 4007: 0.769\n",
      "train loss on epoch 4008 : 0.215\n",
      "train accuracy on epoch 4008: 0.778\n",
      "test loss on epoch 4008: 0.341\n",
      "test accuracy on epoch 4008: 0.769\n",
      "train loss on epoch 4009 : 0.301\n",
      "train accuracy on epoch 4009: 0.889\n",
      "test loss on epoch 4009: 0.340\n",
      "test accuracy on epoch 4009: 0.769\n",
      "train loss on epoch 4010 : 0.160\n",
      "train accuracy on epoch 4010: 0.944\n",
      "test loss on epoch 4010: 0.341\n",
      "test accuracy on epoch 4010: 0.769\n",
      "train loss on epoch 4011 : 0.081\n",
      "train accuracy on epoch 4011: 0.944\n",
      "test loss on epoch 4011: 0.347\n",
      "test accuracy on epoch 4011: 0.769\n",
      "train loss on epoch 4012 : 0.364\n",
      "train accuracy on epoch 4012: 0.833\n",
      "test loss on epoch 4012: 0.343\n",
      "test accuracy on epoch 4012: 0.769\n",
      "train loss on epoch 4013 : 0.404\n",
      "train accuracy on epoch 4013: 0.889\n",
      "test loss on epoch 4013: 0.345\n",
      "test accuracy on epoch 4013: 0.769\n",
      "train loss on epoch 4014 : 0.120\n",
      "train accuracy on epoch 4014: 0.889\n",
      "test loss on epoch 4014: 0.343\n",
      "test accuracy on epoch 4014: 0.769\n",
      "train loss on epoch 4015 : 0.424\n",
      "train accuracy on epoch 4015: 0.889\n",
      "test loss on epoch 4015: 0.336\n",
      "test accuracy on epoch 4015: 0.769\n",
      "train loss on epoch 4016 : 0.080\n",
      "train accuracy on epoch 4016: 0.944\n",
      "test loss on epoch 4016: 0.332\n",
      "test accuracy on epoch 4016: 0.769\n",
      "train loss on epoch 4017 : 0.179\n",
      "train accuracy on epoch 4017: 0.944\n",
      "test loss on epoch 4017: 0.333\n",
      "test accuracy on epoch 4017: 0.769\n",
      "train loss on epoch 4018 : 0.092\n",
      "train accuracy on epoch 4018: 0.944\n",
      "test loss on epoch 4018: 0.333\n",
      "test accuracy on epoch 4018: 0.769\n",
      "train loss on epoch 4019 : 0.085\n",
      "train accuracy on epoch 4019: 1.000\n",
      "test loss on epoch 4019: 0.338\n",
      "test accuracy on epoch 4019: 0.769\n",
      "train loss on epoch 4020 : 0.202\n",
      "train accuracy on epoch 4020: 0.833\n",
      "test loss on epoch 4020: 0.334\n",
      "test accuracy on epoch 4020: 0.769\n",
      "train loss on epoch 4021 : 0.173\n",
      "train accuracy on epoch 4021: 0.889\n",
      "test loss on epoch 4021: 0.339\n",
      "test accuracy on epoch 4021: 0.769\n",
      "train loss on epoch 4022 : 0.050\n",
      "train accuracy on epoch 4022: 1.000\n",
      "test loss on epoch 4022: 0.339\n",
      "test accuracy on epoch 4022: 0.769\n",
      "train loss on epoch 4023 : 0.130\n",
      "train accuracy on epoch 4023: 0.944\n",
      "test loss on epoch 4023: 0.336\n",
      "test accuracy on epoch 4023: 0.769\n",
      "train loss on epoch 4024 : 0.123\n",
      "train accuracy on epoch 4024: 0.944\n",
      "test loss on epoch 4024: 0.341\n",
      "test accuracy on epoch 4024: 0.769\n",
      "train loss on epoch 4025 : 0.239\n",
      "train accuracy on epoch 4025: 0.889\n",
      "test loss on epoch 4025: 0.341\n",
      "test accuracy on epoch 4025: 0.769\n",
      "train loss on epoch 4026 : 0.116\n",
      "train accuracy on epoch 4026: 1.000\n",
      "test loss on epoch 4026: 0.335\n",
      "test accuracy on epoch 4026: 0.769\n",
      "train loss on epoch 4027 : 0.067\n",
      "train accuracy on epoch 4027: 1.000\n",
      "test loss on epoch 4027: 0.335\n",
      "test accuracy on epoch 4027: 0.769\n",
      "train loss on epoch 4028 : 0.173\n",
      "train accuracy on epoch 4028: 0.889\n",
      "test loss on epoch 4028: 0.329\n",
      "test accuracy on epoch 4028: 0.769\n",
      "train loss on epoch 4029 : 0.077\n",
      "train accuracy on epoch 4029: 1.000\n",
      "test loss on epoch 4029: 0.326\n",
      "test accuracy on epoch 4029: 0.769\n",
      "train loss on epoch 4030 : 0.261\n",
      "train accuracy on epoch 4030: 0.889\n",
      "test loss on epoch 4030: 0.330\n",
      "test accuracy on epoch 4030: 0.769\n",
      "train loss on epoch 4031 : 0.256\n",
      "train accuracy on epoch 4031: 0.889\n",
      "test loss on epoch 4031: 0.323\n",
      "test accuracy on epoch 4031: 0.769\n",
      "train loss on epoch 4032 : 0.191\n",
      "train accuracy on epoch 4032: 0.889\n",
      "test loss on epoch 4032: 0.333\n",
      "test accuracy on epoch 4032: 0.769\n",
      "train loss on epoch 4033 : 0.206\n",
      "train accuracy on epoch 4033: 0.944\n",
      "test loss on epoch 4033: 0.325\n",
      "test accuracy on epoch 4033: 0.769\n",
      "train loss on epoch 4034 : 0.230\n",
      "train accuracy on epoch 4034: 0.944\n",
      "test loss on epoch 4034: 0.332\n",
      "test accuracy on epoch 4034: 0.692\n",
      "train loss on epoch 4035 : 0.135\n",
      "train accuracy on epoch 4035: 0.889\n",
      "test loss on epoch 4035: 0.326\n",
      "test accuracy on epoch 4035: 0.769\n",
      "train loss on epoch 4036 : 0.155\n",
      "train accuracy on epoch 4036: 0.944\n",
      "test loss on epoch 4036: 0.331\n",
      "test accuracy on epoch 4036: 0.769\n",
      "train loss on epoch 4037 : 0.102\n",
      "train accuracy on epoch 4037: 1.000\n",
      "test loss on epoch 4037: 0.328\n",
      "test accuracy on epoch 4037: 0.769\n",
      "train loss on epoch 4038 : 0.267\n",
      "train accuracy on epoch 4038: 0.833\n",
      "test loss on epoch 4038: 0.333\n",
      "test accuracy on epoch 4038: 0.769\n",
      "train loss on epoch 4039 : 0.141\n",
      "train accuracy on epoch 4039: 0.944\n",
      "test loss on epoch 4039: 0.332\n",
      "test accuracy on epoch 4039: 0.769\n",
      "train loss on epoch 4040 : 0.130\n",
      "train accuracy on epoch 4040: 0.889\n",
      "test loss on epoch 4040: 0.339\n",
      "test accuracy on epoch 4040: 0.769\n",
      "train loss on epoch 4041 : 0.207\n",
      "train accuracy on epoch 4041: 0.944\n",
      "test loss on epoch 4041: 0.319\n",
      "test accuracy on epoch 4041: 0.769\n",
      "train loss on epoch 4042 : 0.233\n",
      "train accuracy on epoch 4042: 0.944\n",
      "test loss on epoch 4042: 0.323\n",
      "test accuracy on epoch 4042: 0.769\n",
      "train loss on epoch 4043 : 0.332\n",
      "train accuracy on epoch 4043: 0.889\n",
      "test loss on epoch 4043: 0.326\n",
      "test accuracy on epoch 4043: 0.692\n",
      "train loss on epoch 4044 : 0.118\n",
      "train accuracy on epoch 4044: 0.944\n",
      "test loss on epoch 4044: 0.331\n",
      "test accuracy on epoch 4044: 0.769\n",
      "train loss on epoch 4045 : 0.229\n",
      "train accuracy on epoch 4045: 0.889\n",
      "test loss on epoch 4045: 0.323\n",
      "test accuracy on epoch 4045: 0.769\n",
      "train loss on epoch 4046 : 0.141\n",
      "train accuracy on epoch 4046: 0.944\n",
      "test loss on epoch 4046: 0.320\n",
      "test accuracy on epoch 4046: 0.769\n",
      "train loss on epoch 4047 : 0.112\n",
      "train accuracy on epoch 4047: 1.000\n",
      "test loss on epoch 4047: 0.330\n",
      "test accuracy on epoch 4047: 0.769\n",
      "train loss on epoch 4048 : 0.094\n",
      "train accuracy on epoch 4048: 0.944\n",
      "test loss on epoch 4048: 0.324\n",
      "test accuracy on epoch 4048: 0.769\n",
      "train loss on epoch 4049 : 0.180\n",
      "train accuracy on epoch 4049: 0.944\n",
      "test loss on epoch 4049: 0.331\n",
      "test accuracy on epoch 4049: 0.769\n",
      "train loss on epoch 4050 : 0.169\n",
      "train accuracy on epoch 4050: 0.944\n",
      "test loss on epoch 4050: 0.328\n",
      "test accuracy on epoch 4050: 0.769\n",
      "train loss on epoch 4051 : 0.194\n",
      "train accuracy on epoch 4051: 0.889\n",
      "test loss on epoch 4051: 0.331\n",
      "test accuracy on epoch 4051: 0.769\n",
      "train loss on epoch 4052 : 0.302\n",
      "train accuracy on epoch 4052: 0.944\n",
      "test loss on epoch 4052: 0.327\n",
      "test accuracy on epoch 4052: 0.769\n",
      "train loss on epoch 4053 : 0.117\n",
      "train accuracy on epoch 4053: 0.889\n",
      "test loss on epoch 4053: 0.330\n",
      "test accuracy on epoch 4053: 0.769\n",
      "train loss on epoch 4054 : 0.143\n",
      "train accuracy on epoch 4054: 0.889\n",
      "test loss on epoch 4054: 0.325\n",
      "test accuracy on epoch 4054: 0.769\n",
      "train loss on epoch 4055 : 0.280\n",
      "train accuracy on epoch 4055: 0.944\n",
      "test loss on epoch 4055: 0.327\n",
      "test accuracy on epoch 4055: 0.769\n",
      "train loss on epoch 4056 : 0.302\n",
      "train accuracy on epoch 4056: 0.889\n",
      "test loss on epoch 4056: 0.329\n",
      "test accuracy on epoch 4056: 0.769\n",
      "train loss on epoch 4057 : 0.153\n",
      "train accuracy on epoch 4057: 0.944\n",
      "test loss on epoch 4057: 0.341\n",
      "test accuracy on epoch 4057: 0.769\n",
      "train loss on epoch 4058 : 0.102\n",
      "train accuracy on epoch 4058: 0.944\n",
      "test loss on epoch 4058: 0.349\n",
      "test accuracy on epoch 4058: 0.769\n",
      "train loss on epoch 4059 : 0.190\n",
      "train accuracy on epoch 4059: 0.944\n",
      "test loss on epoch 4059: 0.347\n",
      "test accuracy on epoch 4059: 0.769\n",
      "train loss on epoch 4060 : 0.074\n",
      "train accuracy on epoch 4060: 1.000\n",
      "test loss on epoch 4060: 0.355\n",
      "test accuracy on epoch 4060: 0.769\n",
      "train loss on epoch 4061 : 0.043\n",
      "train accuracy on epoch 4061: 1.000\n",
      "test loss on epoch 4061: 0.355\n",
      "test accuracy on epoch 4061: 0.769\n",
      "train loss on epoch 4062 : 0.268\n",
      "train accuracy on epoch 4062: 0.889\n",
      "test loss on epoch 4062: 0.355\n",
      "test accuracy on epoch 4062: 0.769\n",
      "train loss on epoch 4063 : 0.169\n",
      "train accuracy on epoch 4063: 0.944\n",
      "test loss on epoch 4063: 0.349\n",
      "test accuracy on epoch 4063: 0.769\n",
      "train loss on epoch 4064 : 0.200\n",
      "train accuracy on epoch 4064: 0.944\n",
      "test loss on epoch 4064: 0.348\n",
      "test accuracy on epoch 4064: 0.769\n",
      "train loss on epoch 4065 : 0.069\n",
      "train accuracy on epoch 4065: 1.000\n",
      "test loss on epoch 4065: 0.344\n",
      "test accuracy on epoch 4065: 0.769\n",
      "train loss on epoch 4066 : 0.066\n",
      "train accuracy on epoch 4066: 0.944\n",
      "test loss on epoch 4066: 0.341\n",
      "test accuracy on epoch 4066: 0.769\n",
      "train loss on epoch 4067 : 0.077\n",
      "train accuracy on epoch 4067: 1.000\n",
      "test loss on epoch 4067: 0.340\n",
      "test accuracy on epoch 4067: 0.769\n",
      "train loss on epoch 4068 : 0.266\n",
      "train accuracy on epoch 4068: 0.889\n",
      "test loss on epoch 4068: 0.336\n",
      "test accuracy on epoch 4068: 0.769\n",
      "train loss on epoch 4069 : 0.200\n",
      "train accuracy on epoch 4069: 0.944\n",
      "test loss on epoch 4069: 0.338\n",
      "test accuracy on epoch 4069: 0.769\n",
      "train loss on epoch 4070 : 0.202\n",
      "train accuracy on epoch 4070: 0.944\n",
      "test loss on epoch 4070: 0.330\n",
      "test accuracy on epoch 4070: 0.769\n",
      "train loss on epoch 4071 : 0.165\n",
      "train accuracy on epoch 4071: 0.944\n",
      "test loss on epoch 4071: 0.328\n",
      "test accuracy on epoch 4071: 0.769\n",
      "train loss on epoch 4072 : 0.110\n",
      "train accuracy on epoch 4072: 0.944\n",
      "test loss on epoch 4072: 0.327\n",
      "test accuracy on epoch 4072: 0.769\n",
      "train loss on epoch 4073 : 0.179\n",
      "train accuracy on epoch 4073: 0.889\n",
      "test loss on epoch 4073: 0.328\n",
      "test accuracy on epoch 4073: 0.769\n",
      "train loss on epoch 4074 : 0.196\n",
      "train accuracy on epoch 4074: 0.833\n",
      "test loss on epoch 4074: 0.327\n",
      "test accuracy on epoch 4074: 0.769\n",
      "train loss on epoch 4075 : 0.268\n",
      "train accuracy on epoch 4075: 0.833\n",
      "test loss on epoch 4075: 0.331\n",
      "test accuracy on epoch 4075: 0.769\n",
      "train loss on epoch 4076 : 0.142\n",
      "train accuracy on epoch 4076: 1.000\n",
      "test loss on epoch 4076: 0.338\n",
      "test accuracy on epoch 4076: 0.769\n",
      "train loss on epoch 4077 : 0.240\n",
      "train accuracy on epoch 4077: 0.889\n",
      "test loss on epoch 4077: 0.345\n",
      "test accuracy on epoch 4077: 0.769\n",
      "train loss on epoch 4078 : 0.086\n",
      "train accuracy on epoch 4078: 1.000\n",
      "test loss on epoch 4078: 0.351\n",
      "test accuracy on epoch 4078: 0.769\n",
      "train loss on epoch 4079 : 0.111\n",
      "train accuracy on epoch 4079: 0.944\n",
      "test loss on epoch 4079: 0.346\n",
      "test accuracy on epoch 4079: 0.769\n",
      "train loss on epoch 4080 : 0.135\n",
      "train accuracy on epoch 4080: 0.944\n",
      "test loss on epoch 4080: 0.345\n",
      "test accuracy on epoch 4080: 0.769\n",
      "train loss on epoch 4081 : 0.184\n",
      "train accuracy on epoch 4081: 0.889\n",
      "test loss on epoch 4081: 0.351\n",
      "test accuracy on epoch 4081: 0.769\n",
      "train loss on epoch 4082 : 0.303\n",
      "train accuracy on epoch 4082: 0.889\n",
      "test loss on epoch 4082: 0.338\n",
      "test accuracy on epoch 4082: 0.769\n",
      "train loss on epoch 4083 : 0.161\n",
      "train accuracy on epoch 4083: 0.889\n",
      "test loss on epoch 4083: 0.334\n",
      "test accuracy on epoch 4083: 0.769\n",
      "train loss on epoch 4084 : 0.137\n",
      "train accuracy on epoch 4084: 0.944\n",
      "test loss on epoch 4084: 0.331\n",
      "test accuracy on epoch 4084: 0.769\n",
      "train loss on epoch 4085 : 0.215\n",
      "train accuracy on epoch 4085: 0.944\n",
      "test loss on epoch 4085: 0.327\n",
      "test accuracy on epoch 4085: 0.769\n",
      "train loss on epoch 4086 : 0.220\n",
      "train accuracy on epoch 4086: 0.833\n",
      "test loss on epoch 4086: 0.326\n",
      "test accuracy on epoch 4086: 0.769\n",
      "train loss on epoch 4087 : 0.219\n",
      "train accuracy on epoch 4087: 0.889\n",
      "test loss on epoch 4087: 0.329\n",
      "test accuracy on epoch 4087: 0.769\n",
      "train loss on epoch 4088 : 0.150\n",
      "train accuracy on epoch 4088: 1.000\n",
      "test loss on epoch 4088: 0.326\n",
      "test accuracy on epoch 4088: 0.769\n",
      "train loss on epoch 4089 : 0.090\n",
      "train accuracy on epoch 4089: 1.000\n",
      "test loss on epoch 4089: 0.326\n",
      "test accuracy on epoch 4089: 0.769\n",
      "train loss on epoch 4090 : 0.167\n",
      "train accuracy on epoch 4090: 0.944\n",
      "test loss on epoch 4090: 0.330\n",
      "test accuracy on epoch 4090: 0.769\n",
      "train loss on epoch 4091 : 0.219\n",
      "train accuracy on epoch 4091: 0.944\n",
      "test loss on epoch 4091: 0.333\n",
      "test accuracy on epoch 4091: 0.769\n",
      "train loss on epoch 4092 : 0.305\n",
      "train accuracy on epoch 4092: 0.889\n",
      "test loss on epoch 4092: 0.333\n",
      "test accuracy on epoch 4092: 0.769\n",
      "train loss on epoch 4093 : 0.080\n",
      "train accuracy on epoch 4093: 1.000\n",
      "test loss on epoch 4093: 0.333\n",
      "test accuracy on epoch 4093: 0.769\n",
      "train loss on epoch 4094 : 0.176\n",
      "train accuracy on epoch 4094: 0.944\n",
      "test loss on epoch 4094: 0.326\n",
      "test accuracy on epoch 4094: 0.769\n",
      "train loss on epoch 4095 : 0.344\n",
      "train accuracy on epoch 4095: 0.833\n",
      "test loss on epoch 4095: 0.328\n",
      "test accuracy on epoch 4095: 0.769\n",
      "train loss on epoch 4096 : 0.148\n",
      "train accuracy on epoch 4096: 0.889\n",
      "test loss on epoch 4096: 0.325\n",
      "test accuracy on epoch 4096: 0.769\n",
      "train loss on epoch 4097 : 0.195\n",
      "train accuracy on epoch 4097: 0.944\n",
      "test loss on epoch 4097: 0.325\n",
      "test accuracy on epoch 4097: 0.769\n",
      "train loss on epoch 4098 : 0.031\n",
      "train accuracy on epoch 4098: 1.000\n",
      "test loss on epoch 4098: 0.321\n",
      "test accuracy on epoch 4098: 0.769\n",
      "train loss on epoch 4099 : 0.384\n",
      "train accuracy on epoch 4099: 0.833\n",
      "test loss on epoch 4099: 0.322\n",
      "test accuracy on epoch 4099: 0.769\n",
      "train loss on epoch 4100 : 0.067\n",
      "train accuracy on epoch 4100: 0.944\n",
      "test loss on epoch 4100: 0.321\n",
      "test accuracy on epoch 4100: 0.769\n",
      "train loss on epoch 4101 : 0.182\n",
      "train accuracy on epoch 4101: 0.944\n",
      "test loss on epoch 4101: 0.319\n",
      "test accuracy on epoch 4101: 0.769\n",
      "train loss on epoch 4102 : 0.288\n",
      "train accuracy on epoch 4102: 0.889\n",
      "test loss on epoch 4102: 0.317\n",
      "test accuracy on epoch 4102: 0.769\n",
      "train loss on epoch 4103 : 0.259\n",
      "train accuracy on epoch 4103: 0.944\n",
      "test loss on epoch 4103: 0.320\n",
      "test accuracy on epoch 4103: 0.769\n",
      "train loss on epoch 4104 : 0.119\n",
      "train accuracy on epoch 4104: 0.889\n",
      "test loss on epoch 4104: 0.321\n",
      "test accuracy on epoch 4104: 0.769\n",
      "train loss on epoch 4105 : 0.158\n",
      "train accuracy on epoch 4105: 0.944\n",
      "test loss on epoch 4105: 0.324\n",
      "test accuracy on epoch 4105: 0.769\n",
      "train loss on epoch 4106 : 0.062\n",
      "train accuracy on epoch 4106: 1.000\n",
      "test loss on epoch 4106: 0.325\n",
      "test accuracy on epoch 4106: 0.769\n",
      "train loss on epoch 4107 : 0.188\n",
      "train accuracy on epoch 4107: 0.944\n",
      "test loss on epoch 4107: 0.325\n",
      "test accuracy on epoch 4107: 0.769\n",
      "train loss on epoch 4108 : 0.156\n",
      "train accuracy on epoch 4108: 0.833\n",
      "test loss on epoch 4108: 0.328\n",
      "test accuracy on epoch 4108: 0.769\n",
      "train loss on epoch 4109 : 0.155\n",
      "train accuracy on epoch 4109: 0.944\n",
      "test loss on epoch 4109: 0.324\n",
      "test accuracy on epoch 4109: 0.769\n",
      "train loss on epoch 4110 : 0.092\n",
      "train accuracy on epoch 4110: 0.944\n",
      "test loss on epoch 4110: 0.322\n",
      "test accuracy on epoch 4110: 0.769\n",
      "train loss on epoch 4111 : 0.137\n",
      "train accuracy on epoch 4111: 0.944\n",
      "test loss on epoch 4111: 0.327\n",
      "test accuracy on epoch 4111: 0.769\n",
      "train loss on epoch 4112 : 0.084\n",
      "train accuracy on epoch 4112: 0.944\n",
      "test loss on epoch 4112: 0.330\n",
      "test accuracy on epoch 4112: 0.769\n",
      "train loss on epoch 4113 : 0.173\n",
      "train accuracy on epoch 4113: 0.944\n",
      "test loss on epoch 4113: 0.331\n",
      "test accuracy on epoch 4113: 0.769\n",
      "train loss on epoch 4114 : 0.207\n",
      "train accuracy on epoch 4114: 0.944\n",
      "test loss on epoch 4114: 0.343\n",
      "test accuracy on epoch 4114: 0.769\n",
      "train loss on epoch 4115 : 0.452\n",
      "train accuracy on epoch 4115: 0.889\n",
      "test loss on epoch 4115: 0.345\n",
      "test accuracy on epoch 4115: 0.769\n",
      "train loss on epoch 4116 : 0.059\n",
      "train accuracy on epoch 4116: 1.000\n",
      "test loss on epoch 4116: 0.345\n",
      "test accuracy on epoch 4116: 0.769\n",
      "train loss on epoch 4117 : 0.337\n",
      "train accuracy on epoch 4117: 0.889\n",
      "test loss on epoch 4117: 0.334\n",
      "test accuracy on epoch 4117: 0.769\n",
      "train loss on epoch 4118 : 0.124\n",
      "train accuracy on epoch 4118: 0.944\n",
      "test loss on epoch 4118: 0.339\n",
      "test accuracy on epoch 4118: 0.769\n",
      "train loss on epoch 4119 : 0.281\n",
      "train accuracy on epoch 4119: 0.889\n",
      "test loss on epoch 4119: 0.327\n",
      "test accuracy on epoch 4119: 0.769\n",
      "train loss on epoch 4120 : 0.071\n",
      "train accuracy on epoch 4120: 1.000\n",
      "test loss on epoch 4120: 0.334\n",
      "test accuracy on epoch 4120: 0.769\n",
      "train loss on epoch 4121 : 0.225\n",
      "train accuracy on epoch 4121: 0.944\n",
      "test loss on epoch 4121: 0.320\n",
      "test accuracy on epoch 4121: 0.769\n",
      "train loss on epoch 4122 : 0.282\n",
      "train accuracy on epoch 4122: 0.833\n",
      "test loss on epoch 4122: 0.323\n",
      "test accuracy on epoch 4122: 0.769\n",
      "train loss on epoch 4123 : 0.405\n",
      "train accuracy on epoch 4123: 0.944\n",
      "test loss on epoch 4123: 0.313\n",
      "test accuracy on epoch 4123: 0.769\n",
      "train loss on epoch 4124 : 0.093\n",
      "train accuracy on epoch 4124: 0.944\n",
      "test loss on epoch 4124: 0.312\n",
      "test accuracy on epoch 4124: 0.769\n",
      "train loss on epoch 4125 : 0.252\n",
      "train accuracy on epoch 4125: 0.889\n",
      "test loss on epoch 4125: 0.311\n",
      "test accuracy on epoch 4125: 0.769\n",
      "train loss on epoch 4126 : 0.150\n",
      "train accuracy on epoch 4126: 0.944\n",
      "test loss on epoch 4126: 0.306\n",
      "test accuracy on epoch 4126: 0.769\n",
      "train loss on epoch 4127 : 0.386\n",
      "train accuracy on epoch 4127: 0.889\n",
      "test loss on epoch 4127: 0.308\n",
      "test accuracy on epoch 4127: 0.769\n",
      "train loss on epoch 4128 : 0.132\n",
      "train accuracy on epoch 4128: 0.944\n",
      "test loss on epoch 4128: 0.314\n",
      "test accuracy on epoch 4128: 0.769\n",
      "train loss on epoch 4129 : 0.280\n",
      "train accuracy on epoch 4129: 0.944\n",
      "test loss on epoch 4129: 0.312\n",
      "test accuracy on epoch 4129: 0.769\n",
      "train loss on epoch 4130 : 0.088\n",
      "train accuracy on epoch 4130: 1.000\n",
      "test loss on epoch 4130: 0.312\n",
      "test accuracy on epoch 4130: 0.769\n",
      "train loss on epoch 4131 : 0.351\n",
      "train accuracy on epoch 4131: 0.889\n",
      "test loss on epoch 4131: 0.312\n",
      "test accuracy on epoch 4131: 0.769\n",
      "train loss on epoch 4132 : 0.029\n",
      "train accuracy on epoch 4132: 1.000\n",
      "test loss on epoch 4132: 0.310\n",
      "test accuracy on epoch 4132: 0.769\n",
      "train loss on epoch 4133 : 0.295\n",
      "train accuracy on epoch 4133: 0.833\n",
      "test loss on epoch 4133: 0.318\n",
      "test accuracy on epoch 4133: 0.769\n",
      "train loss on epoch 4134 : 0.168\n",
      "train accuracy on epoch 4134: 0.944\n",
      "test loss on epoch 4134: 0.315\n",
      "test accuracy on epoch 4134: 0.846\n",
      "train loss on epoch 4135 : 0.090\n",
      "train accuracy on epoch 4135: 0.944\n",
      "test loss on epoch 4135: 0.319\n",
      "test accuracy on epoch 4135: 0.769\n",
      "train loss on epoch 4136 : 0.232\n",
      "train accuracy on epoch 4136: 0.944\n",
      "test loss on epoch 4136: 0.317\n",
      "test accuracy on epoch 4136: 0.769\n",
      "train loss on epoch 4137 : 0.212\n",
      "train accuracy on epoch 4137: 0.833\n",
      "test loss on epoch 4137: 0.319\n",
      "test accuracy on epoch 4137: 0.769\n",
      "train loss on epoch 4138 : 0.325\n",
      "train accuracy on epoch 4138: 0.889\n",
      "test loss on epoch 4138: 0.315\n",
      "test accuracy on epoch 4138: 0.769\n",
      "train loss on epoch 4139 : 0.139\n",
      "train accuracy on epoch 4139: 0.944\n",
      "test loss on epoch 4139: 0.327\n",
      "test accuracy on epoch 4139: 0.769\n",
      "train loss on epoch 4140 : 0.270\n",
      "train accuracy on epoch 4140: 0.944\n",
      "test loss on epoch 4140: 0.333\n",
      "test accuracy on epoch 4140: 0.769\n",
      "train loss on epoch 4141 : 0.138\n",
      "train accuracy on epoch 4141: 0.944\n",
      "test loss on epoch 4141: 0.336\n",
      "test accuracy on epoch 4141: 0.769\n",
      "train loss on epoch 4142 : 0.130\n",
      "train accuracy on epoch 4142: 0.889\n",
      "test loss on epoch 4142: 0.319\n",
      "test accuracy on epoch 4142: 0.769\n",
      "train loss on epoch 4143 : 0.141\n",
      "train accuracy on epoch 4143: 0.944\n",
      "test loss on epoch 4143: 0.328\n",
      "test accuracy on epoch 4143: 0.769\n",
      "train loss on epoch 4144 : 0.196\n",
      "train accuracy on epoch 4144: 0.889\n",
      "test loss on epoch 4144: 0.331\n",
      "test accuracy on epoch 4144: 0.769\n",
      "train loss on epoch 4145 : 0.373\n",
      "train accuracy on epoch 4145: 0.889\n",
      "test loss on epoch 4145: 0.324\n",
      "test accuracy on epoch 4145: 0.769\n",
      "train loss on epoch 4146 : 0.104\n",
      "train accuracy on epoch 4146: 1.000\n",
      "test loss on epoch 4146: 0.326\n",
      "test accuracy on epoch 4146: 0.769\n",
      "train loss on epoch 4147 : 0.129\n",
      "train accuracy on epoch 4147: 0.944\n",
      "test loss on epoch 4147: 0.327\n",
      "test accuracy on epoch 4147: 0.769\n",
      "train loss on epoch 4148 : 0.191\n",
      "train accuracy on epoch 4148: 0.944\n",
      "test loss on epoch 4148: 0.329\n",
      "test accuracy on epoch 4148: 0.769\n",
      "train loss on epoch 4149 : 0.194\n",
      "train accuracy on epoch 4149: 0.889\n",
      "test loss on epoch 4149: 0.321\n",
      "test accuracy on epoch 4149: 0.769\n",
      "train loss on epoch 4150 : 0.129\n",
      "train accuracy on epoch 4150: 1.000\n",
      "test loss on epoch 4150: 0.316\n",
      "test accuracy on epoch 4150: 0.769\n",
      "train loss on epoch 4151 : 0.248\n",
      "train accuracy on epoch 4151: 0.889\n",
      "test loss on epoch 4151: 0.320\n",
      "test accuracy on epoch 4151: 0.769\n",
      "train loss on epoch 4152 : 0.190\n",
      "train accuracy on epoch 4152: 0.889\n",
      "test loss on epoch 4152: 0.324\n",
      "test accuracy on epoch 4152: 0.769\n",
      "train loss on epoch 4153 : 0.410\n",
      "train accuracy on epoch 4153: 0.889\n",
      "test loss on epoch 4153: 0.322\n",
      "test accuracy on epoch 4153: 0.769\n",
      "train loss on epoch 4154 : 0.231\n",
      "train accuracy on epoch 4154: 0.889\n",
      "test loss on epoch 4154: 0.320\n",
      "test accuracy on epoch 4154: 0.769\n",
      "train loss on epoch 4155 : 0.360\n",
      "train accuracy on epoch 4155: 0.889\n",
      "test loss on epoch 4155: 0.318\n",
      "test accuracy on epoch 4155: 0.769\n",
      "train loss on epoch 4156 : 0.064\n",
      "train accuracy on epoch 4156: 1.000\n",
      "test loss on epoch 4156: 0.319\n",
      "test accuracy on epoch 4156: 0.769\n",
      "train loss on epoch 4157 : 0.121\n",
      "train accuracy on epoch 4157: 1.000\n",
      "test loss on epoch 4157: 0.329\n",
      "test accuracy on epoch 4157: 0.769\n",
      "train loss on epoch 4158 : 0.068\n",
      "train accuracy on epoch 4158: 0.944\n",
      "test loss on epoch 4158: 0.322\n",
      "test accuracy on epoch 4158: 0.769\n",
      "train loss on epoch 4159 : 0.138\n",
      "train accuracy on epoch 4159: 0.944\n",
      "test loss on epoch 4159: 0.325\n",
      "test accuracy on epoch 4159: 0.769\n",
      "train loss on epoch 4160 : 0.065\n",
      "train accuracy on epoch 4160: 0.944\n",
      "test loss on epoch 4160: 0.328\n",
      "test accuracy on epoch 4160: 0.769\n",
      "train loss on epoch 4161 : 0.148\n",
      "train accuracy on epoch 4161: 0.889\n",
      "test loss on epoch 4161: 0.339\n",
      "test accuracy on epoch 4161: 0.769\n",
      "train loss on epoch 4162 : 0.275\n",
      "train accuracy on epoch 4162: 0.944\n",
      "test loss on epoch 4162: 0.334\n",
      "test accuracy on epoch 4162: 0.769\n",
      "train loss on epoch 4163 : 0.364\n",
      "train accuracy on epoch 4163: 0.778\n",
      "test loss on epoch 4163: 0.328\n",
      "test accuracy on epoch 4163: 0.769\n",
      "train loss on epoch 4164 : 0.116\n",
      "train accuracy on epoch 4164: 0.944\n",
      "test loss on epoch 4164: 0.330\n",
      "test accuracy on epoch 4164: 0.769\n",
      "train loss on epoch 4165 : 0.155\n",
      "train accuracy on epoch 4165: 0.944\n",
      "test loss on epoch 4165: 0.332\n",
      "test accuracy on epoch 4165: 0.769\n",
      "train loss on epoch 4166 : 0.061\n",
      "train accuracy on epoch 4166: 1.000\n",
      "test loss on epoch 4166: 0.323\n",
      "test accuracy on epoch 4166: 0.769\n",
      "train loss on epoch 4167 : 0.195\n",
      "train accuracy on epoch 4167: 0.944\n",
      "test loss on epoch 4167: 0.326\n",
      "test accuracy on epoch 4167: 0.769\n",
      "train loss on epoch 4168 : 0.241\n",
      "train accuracy on epoch 4168: 0.889\n",
      "test loss on epoch 4168: 0.336\n",
      "test accuracy on epoch 4168: 0.769\n",
      "train loss on epoch 4169 : 0.291\n",
      "train accuracy on epoch 4169: 0.944\n",
      "test loss on epoch 4169: 0.331\n",
      "test accuracy on epoch 4169: 0.769\n",
      "train loss on epoch 4170 : 0.323\n",
      "train accuracy on epoch 4170: 0.944\n",
      "test loss on epoch 4170: 0.330\n",
      "test accuracy on epoch 4170: 0.769\n",
      "train loss on epoch 4171 : 0.454\n",
      "train accuracy on epoch 4171: 0.889\n",
      "test loss on epoch 4171: 0.317\n",
      "test accuracy on epoch 4171: 0.769\n",
      "train loss on epoch 4172 : 0.453\n",
      "train accuracy on epoch 4172: 0.833\n",
      "test loss on epoch 4172: 0.326\n",
      "test accuracy on epoch 4172: 0.769\n",
      "train loss on epoch 4173 : 0.077\n",
      "train accuracy on epoch 4173: 1.000\n",
      "test loss on epoch 4173: 0.326\n",
      "test accuracy on epoch 4173: 0.769\n",
      "train loss on epoch 4174 : 0.257\n",
      "train accuracy on epoch 4174: 0.944\n",
      "test loss on epoch 4174: 0.324\n",
      "test accuracy on epoch 4174: 0.769\n",
      "train loss on epoch 4175 : 0.091\n",
      "train accuracy on epoch 4175: 0.944\n",
      "test loss on epoch 4175: 0.325\n",
      "test accuracy on epoch 4175: 0.769\n",
      "train loss on epoch 4176 : 0.219\n",
      "train accuracy on epoch 4176: 0.889\n",
      "test loss on epoch 4176: 0.321\n",
      "test accuracy on epoch 4176: 0.769\n",
      "train loss on epoch 4177 : 0.132\n",
      "train accuracy on epoch 4177: 0.889\n",
      "test loss on epoch 4177: 0.316\n",
      "test accuracy on epoch 4177: 0.769\n",
      "train loss on epoch 4178 : 0.070\n",
      "train accuracy on epoch 4178: 1.000\n",
      "test loss on epoch 4178: 0.319\n",
      "test accuracy on epoch 4178: 0.769\n",
      "train loss on epoch 4179 : 0.168\n",
      "train accuracy on epoch 4179: 1.000\n",
      "test loss on epoch 4179: 0.316\n",
      "test accuracy on epoch 4179: 0.769\n",
      "train loss on epoch 4180 : 0.178\n",
      "train accuracy on epoch 4180: 0.944\n",
      "test loss on epoch 4180: 0.320\n",
      "test accuracy on epoch 4180: 0.769\n",
      "train loss on epoch 4181 : 0.064\n",
      "train accuracy on epoch 4181: 1.000\n",
      "test loss on epoch 4181: 0.322\n",
      "test accuracy on epoch 4181: 0.769\n",
      "train loss on epoch 4182 : 0.124\n",
      "train accuracy on epoch 4182: 0.944\n",
      "test loss on epoch 4182: 0.326\n",
      "test accuracy on epoch 4182: 0.769\n",
      "train loss on epoch 4183 : 0.226\n",
      "train accuracy on epoch 4183: 0.833\n",
      "test loss on epoch 4183: 0.328\n",
      "test accuracy on epoch 4183: 0.769\n",
      "train loss on epoch 4184 : 0.071\n",
      "train accuracy on epoch 4184: 0.944\n",
      "test loss on epoch 4184: 0.323\n",
      "test accuracy on epoch 4184: 0.769\n",
      "train loss on epoch 4185 : 0.194\n",
      "train accuracy on epoch 4185: 0.889\n",
      "test loss on epoch 4185: 0.319\n",
      "test accuracy on epoch 4185: 0.769\n",
      "train loss on epoch 4186 : 0.215\n",
      "train accuracy on epoch 4186: 0.833\n",
      "test loss on epoch 4186: 0.325\n",
      "test accuracy on epoch 4186: 0.769\n",
      "train loss on epoch 4187 : 0.318\n",
      "train accuracy on epoch 4187: 0.889\n",
      "test loss on epoch 4187: 0.327\n",
      "test accuracy on epoch 4187: 0.769\n",
      "train loss on epoch 4188 : 0.135\n",
      "train accuracy on epoch 4188: 0.944\n",
      "test loss on epoch 4188: 0.327\n",
      "test accuracy on epoch 4188: 0.769\n",
      "train loss on epoch 4189 : 0.072\n",
      "train accuracy on epoch 4189: 1.000\n",
      "test loss on epoch 4189: 0.329\n",
      "test accuracy on epoch 4189: 0.769\n",
      "train loss on epoch 4190 : 0.251\n",
      "train accuracy on epoch 4190: 0.889\n",
      "test loss on epoch 4190: 0.325\n",
      "test accuracy on epoch 4190: 0.769\n",
      "train loss on epoch 4191 : 0.175\n",
      "train accuracy on epoch 4191: 0.944\n",
      "test loss on epoch 4191: 0.327\n",
      "test accuracy on epoch 4191: 0.769\n",
      "train loss on epoch 4192 : 0.163\n",
      "train accuracy on epoch 4192: 0.944\n",
      "test loss on epoch 4192: 0.322\n",
      "test accuracy on epoch 4192: 0.769\n",
      "train loss on epoch 4193 : 0.268\n",
      "train accuracy on epoch 4193: 0.944\n",
      "test loss on epoch 4193: 0.328\n",
      "test accuracy on epoch 4193: 0.769\n",
      "train loss on epoch 4194 : 0.144\n",
      "train accuracy on epoch 4194: 0.944\n",
      "test loss on epoch 4194: 0.334\n",
      "test accuracy on epoch 4194: 0.769\n",
      "train loss on epoch 4195 : 0.093\n",
      "train accuracy on epoch 4195: 1.000\n",
      "test loss on epoch 4195: 0.334\n",
      "test accuracy on epoch 4195: 0.769\n",
      "train loss on epoch 4196 : 0.351\n",
      "train accuracy on epoch 4196: 0.778\n",
      "test loss on epoch 4196: 0.344\n",
      "test accuracy on epoch 4196: 0.769\n",
      "train loss on epoch 4197 : 0.052\n",
      "train accuracy on epoch 4197: 1.000\n",
      "test loss on epoch 4197: 0.348\n",
      "test accuracy on epoch 4197: 0.769\n",
      "train loss on epoch 4198 : 0.164\n",
      "train accuracy on epoch 4198: 0.944\n",
      "test loss on epoch 4198: 0.338\n",
      "test accuracy on epoch 4198: 0.769\n",
      "train loss on epoch 4199 : 0.128\n",
      "train accuracy on epoch 4199: 0.944\n",
      "test loss on epoch 4199: 0.336\n",
      "test accuracy on epoch 4199: 0.769\n",
      "train loss on epoch 4200 : 0.252\n",
      "train accuracy on epoch 4200: 0.944\n",
      "test loss on epoch 4200: 0.331\n",
      "test accuracy on epoch 4200: 0.769\n",
      "train loss on epoch 4201 : 0.266\n",
      "train accuracy on epoch 4201: 0.889\n",
      "test loss on epoch 4201: 0.321\n",
      "test accuracy on epoch 4201: 0.769\n",
      "train loss on epoch 4202 : 0.327\n",
      "train accuracy on epoch 4202: 0.889\n",
      "test loss on epoch 4202: 0.321\n",
      "test accuracy on epoch 4202: 0.769\n",
      "train loss on epoch 4203 : 0.239\n",
      "train accuracy on epoch 4203: 0.889\n",
      "test loss on epoch 4203: 0.317\n",
      "test accuracy on epoch 4203: 0.769\n",
      "train loss on epoch 4204 : 0.232\n",
      "train accuracy on epoch 4204: 0.889\n",
      "test loss on epoch 4204: 0.315\n",
      "test accuracy on epoch 4204: 0.769\n",
      "train loss on epoch 4205 : 0.217\n",
      "train accuracy on epoch 4205: 0.944\n",
      "test loss on epoch 4205: 0.321\n",
      "test accuracy on epoch 4205: 0.769\n",
      "train loss on epoch 4206 : 0.228\n",
      "train accuracy on epoch 4206: 0.889\n",
      "test loss on epoch 4206: 0.318\n",
      "test accuracy on epoch 4206: 0.769\n",
      "train loss on epoch 4207 : 0.345\n",
      "train accuracy on epoch 4207: 0.833\n",
      "test loss on epoch 4207: 0.323\n",
      "test accuracy on epoch 4207: 0.769\n",
      "train loss on epoch 4208 : 0.148\n",
      "train accuracy on epoch 4208: 0.944\n",
      "test loss on epoch 4208: 0.320\n",
      "test accuracy on epoch 4208: 0.769\n",
      "train loss on epoch 4209 : 0.281\n",
      "train accuracy on epoch 4209: 0.889\n",
      "test loss on epoch 4209: 0.313\n",
      "test accuracy on epoch 4209: 0.846\n",
      "train loss on epoch 4210 : 0.245\n",
      "train accuracy on epoch 4210: 0.944\n",
      "test loss on epoch 4210: 0.314\n",
      "test accuracy on epoch 4210: 0.846\n",
      "train loss on epoch 4211 : 0.466\n",
      "train accuracy on epoch 4211: 0.778\n",
      "test loss on epoch 4211: 0.323\n",
      "test accuracy on epoch 4211: 0.769\n",
      "train loss on epoch 4212 : 0.163\n",
      "train accuracy on epoch 4212: 0.944\n",
      "test loss on epoch 4212: 0.321\n",
      "test accuracy on epoch 4212: 0.769\n",
      "train loss on epoch 4213 : 0.113\n",
      "train accuracy on epoch 4213: 0.944\n",
      "test loss on epoch 4213: 0.323\n",
      "test accuracy on epoch 4213: 0.769\n",
      "train loss on epoch 4214 : 0.186\n",
      "train accuracy on epoch 4214: 0.944\n",
      "test loss on epoch 4214: 0.326\n",
      "test accuracy on epoch 4214: 0.769\n",
      "train loss on epoch 4215 : 0.099\n",
      "train accuracy on epoch 4215: 0.889\n",
      "test loss on epoch 4215: 0.327\n",
      "test accuracy on epoch 4215: 0.769\n",
      "train loss on epoch 4216 : 0.604\n",
      "train accuracy on epoch 4216: 0.889\n",
      "test loss on epoch 4216: 0.329\n",
      "test accuracy on epoch 4216: 0.769\n",
      "train loss on epoch 4217 : 0.167\n",
      "train accuracy on epoch 4217: 0.944\n",
      "test loss on epoch 4217: 0.330\n",
      "test accuracy on epoch 4217: 0.769\n",
      "train loss on epoch 4218 : 0.131\n",
      "train accuracy on epoch 4218: 0.944\n",
      "test loss on epoch 4218: 0.331\n",
      "test accuracy on epoch 4218: 0.769\n",
      "train loss on epoch 4219 : 0.037\n",
      "train accuracy on epoch 4219: 1.000\n",
      "test loss on epoch 4219: 0.333\n",
      "test accuracy on epoch 4219: 0.769\n",
      "train loss on epoch 4220 : 0.179\n",
      "train accuracy on epoch 4220: 0.889\n",
      "test loss on epoch 4220: 0.328\n",
      "test accuracy on epoch 4220: 0.769\n",
      "train loss on epoch 4221 : 0.186\n",
      "train accuracy on epoch 4221: 0.944\n",
      "test loss on epoch 4221: 0.331\n",
      "test accuracy on epoch 4221: 0.769\n",
      "train loss on epoch 4222 : 0.184\n",
      "train accuracy on epoch 4222: 0.889\n",
      "test loss on epoch 4222: 0.326\n",
      "test accuracy on epoch 4222: 0.769\n",
      "train loss on epoch 4223 : 0.137\n",
      "train accuracy on epoch 4223: 0.944\n",
      "test loss on epoch 4223: 0.322\n",
      "test accuracy on epoch 4223: 0.769\n",
      "train loss on epoch 4224 : 0.294\n",
      "train accuracy on epoch 4224: 0.889\n",
      "test loss on epoch 4224: 0.321\n",
      "test accuracy on epoch 4224: 0.769\n",
      "train loss on epoch 4225 : 0.045\n",
      "train accuracy on epoch 4225: 1.000\n",
      "test loss on epoch 4225: 0.321\n",
      "test accuracy on epoch 4225: 0.769\n",
      "train loss on epoch 4226 : 0.390\n",
      "train accuracy on epoch 4226: 0.833\n",
      "test loss on epoch 4226: 0.325\n",
      "test accuracy on epoch 4226: 0.769\n",
      "train loss on epoch 4227 : 0.152\n",
      "train accuracy on epoch 4227: 0.944\n",
      "test loss on epoch 4227: 0.321\n",
      "test accuracy on epoch 4227: 0.769\n",
      "train loss on epoch 4228 : 0.200\n",
      "train accuracy on epoch 4228: 0.889\n",
      "test loss on epoch 4228: 0.333\n",
      "test accuracy on epoch 4228: 0.769\n",
      "train loss on epoch 4229 : 0.467\n",
      "train accuracy on epoch 4229: 0.889\n",
      "test loss on epoch 4229: 0.329\n",
      "test accuracy on epoch 4229: 0.769\n",
      "train loss on epoch 4230 : 0.242\n",
      "train accuracy on epoch 4230: 0.944\n",
      "test loss on epoch 4230: 0.342\n",
      "test accuracy on epoch 4230: 0.769\n",
      "train loss on epoch 4231 : 0.050\n",
      "train accuracy on epoch 4231: 1.000\n",
      "test loss on epoch 4231: 0.328\n",
      "test accuracy on epoch 4231: 0.769\n",
      "train loss on epoch 4232 : 0.346\n",
      "train accuracy on epoch 4232: 0.889\n",
      "test loss on epoch 4232: 0.332\n",
      "test accuracy on epoch 4232: 0.769\n",
      "train loss on epoch 4233 : 0.181\n",
      "train accuracy on epoch 4233: 0.889\n",
      "test loss on epoch 4233: 0.338\n",
      "test accuracy on epoch 4233: 0.769\n",
      "train loss on epoch 4234 : 0.326\n",
      "train accuracy on epoch 4234: 0.889\n",
      "test loss on epoch 4234: 0.332\n",
      "test accuracy on epoch 4234: 0.769\n",
      "train loss on epoch 4235 : 0.366\n",
      "train accuracy on epoch 4235: 0.889\n",
      "test loss on epoch 4235: 0.340\n",
      "test accuracy on epoch 4235: 0.769\n",
      "train loss on epoch 4236 : 0.412\n",
      "train accuracy on epoch 4236: 0.778\n",
      "test loss on epoch 4236: 0.329\n",
      "test accuracy on epoch 4236: 0.769\n",
      "train loss on epoch 4237 : 0.077\n",
      "train accuracy on epoch 4237: 0.944\n",
      "test loss on epoch 4237: 0.331\n",
      "test accuracy on epoch 4237: 0.769\n",
      "train loss on epoch 4238 : 0.174\n",
      "train accuracy on epoch 4238: 0.889\n",
      "test loss on epoch 4238: 0.345\n",
      "test accuracy on epoch 4238: 0.769\n",
      "train loss on epoch 4239 : 0.152\n",
      "train accuracy on epoch 4239: 0.889\n",
      "test loss on epoch 4239: 0.327\n",
      "test accuracy on epoch 4239: 0.769\n",
      "train loss on epoch 4240 : 0.398\n",
      "train accuracy on epoch 4240: 0.833\n",
      "test loss on epoch 4240: 0.343\n",
      "test accuracy on epoch 4240: 0.692\n",
      "train loss on epoch 4241 : 0.294\n",
      "train accuracy on epoch 4241: 0.889\n",
      "test loss on epoch 4241: 0.336\n",
      "test accuracy on epoch 4241: 0.769\n",
      "train loss on epoch 4242 : 0.090\n",
      "train accuracy on epoch 4242: 1.000\n",
      "test loss on epoch 4242: 0.337\n",
      "test accuracy on epoch 4242: 0.769\n",
      "train loss on epoch 4243 : 0.186\n",
      "train accuracy on epoch 4243: 0.944\n",
      "test loss on epoch 4243: 0.340\n",
      "test accuracy on epoch 4243: 0.769\n",
      "train loss on epoch 4244 : 0.258\n",
      "train accuracy on epoch 4244: 0.833\n",
      "test loss on epoch 4244: 0.344\n",
      "test accuracy on epoch 4244: 0.769\n",
      "train loss on epoch 4245 : 0.148\n",
      "train accuracy on epoch 4245: 0.944\n",
      "test loss on epoch 4245: 0.335\n",
      "test accuracy on epoch 4245: 0.769\n",
      "train loss on epoch 4246 : 0.256\n",
      "train accuracy on epoch 4246: 0.833\n",
      "test loss on epoch 4246: 0.343\n",
      "test accuracy on epoch 4246: 0.769\n",
      "train loss on epoch 4247 : 0.094\n",
      "train accuracy on epoch 4247: 0.944\n",
      "test loss on epoch 4247: 0.341\n",
      "test accuracy on epoch 4247: 0.769\n",
      "train loss on epoch 4248 : 0.112\n",
      "train accuracy on epoch 4248: 1.000\n",
      "test loss on epoch 4248: 0.336\n",
      "test accuracy on epoch 4248: 0.769\n",
      "train loss on epoch 4249 : 0.209\n",
      "train accuracy on epoch 4249: 0.889\n",
      "test loss on epoch 4249: 0.332\n",
      "test accuracy on epoch 4249: 0.769\n",
      "train loss on epoch 4250 : 0.262\n",
      "train accuracy on epoch 4250: 0.944\n",
      "test loss on epoch 4250: 0.331\n",
      "test accuracy on epoch 4250: 0.769\n",
      "train loss on epoch 4251 : 0.101\n",
      "train accuracy on epoch 4251: 1.000\n",
      "test loss on epoch 4251: 0.326\n",
      "test accuracy on epoch 4251: 0.769\n",
      "train loss on epoch 4252 : 0.365\n",
      "train accuracy on epoch 4252: 0.889\n",
      "test loss on epoch 4252: 0.328\n",
      "test accuracy on epoch 4252: 0.769\n",
      "train loss on epoch 4253 : 0.150\n",
      "train accuracy on epoch 4253: 0.944\n",
      "test loss on epoch 4253: 0.331\n",
      "test accuracy on epoch 4253: 0.769\n",
      "train loss on epoch 4254 : 0.137\n",
      "train accuracy on epoch 4254: 0.944\n",
      "test loss on epoch 4254: 0.318\n",
      "test accuracy on epoch 4254: 0.769\n",
      "train loss on epoch 4255 : 0.277\n",
      "train accuracy on epoch 4255: 0.889\n",
      "test loss on epoch 4255: 0.334\n",
      "test accuracy on epoch 4255: 0.692\n",
      "train loss on epoch 4256 : 0.152\n",
      "train accuracy on epoch 4256: 0.944\n",
      "test loss on epoch 4256: 0.325\n",
      "test accuracy on epoch 4256: 0.769\n",
      "train loss on epoch 4257 : 0.078\n",
      "train accuracy on epoch 4257: 1.000\n",
      "test loss on epoch 4257: 0.339\n",
      "test accuracy on epoch 4257: 0.769\n",
      "train loss on epoch 4258 : 0.180\n",
      "train accuracy on epoch 4258: 0.889\n",
      "test loss on epoch 4258: 0.338\n",
      "test accuracy on epoch 4258: 0.769\n",
      "train loss on epoch 4259 : 0.160\n",
      "train accuracy on epoch 4259: 0.944\n",
      "test loss on epoch 4259: 0.349\n",
      "test accuracy on epoch 4259: 0.769\n",
      "train loss on epoch 4260 : 0.123\n",
      "train accuracy on epoch 4260: 0.944\n",
      "test loss on epoch 4260: 0.339\n",
      "test accuracy on epoch 4260: 0.769\n",
      "train loss on epoch 4261 : 0.054\n",
      "train accuracy on epoch 4261: 1.000\n",
      "test loss on epoch 4261: 0.345\n",
      "test accuracy on epoch 4261: 0.769\n",
      "train loss on epoch 4262 : 0.234\n",
      "train accuracy on epoch 4262: 0.889\n",
      "test loss on epoch 4262: 0.349\n",
      "test accuracy on epoch 4262: 0.769\n",
      "train loss on epoch 4263 : 0.234\n",
      "train accuracy on epoch 4263: 0.889\n",
      "test loss on epoch 4263: 0.342\n",
      "test accuracy on epoch 4263: 0.769\n",
      "train loss on epoch 4264 : 0.253\n",
      "train accuracy on epoch 4264: 0.833\n",
      "test loss on epoch 4264: 0.335\n",
      "test accuracy on epoch 4264: 0.769\n",
      "train loss on epoch 4265 : 0.149\n",
      "train accuracy on epoch 4265: 0.944\n",
      "test loss on epoch 4265: 0.335\n",
      "test accuracy on epoch 4265: 0.769\n",
      "train loss on epoch 4266 : 0.156\n",
      "train accuracy on epoch 4266: 0.944\n",
      "test loss on epoch 4266: 0.332\n",
      "test accuracy on epoch 4266: 0.769\n",
      "train loss on epoch 4267 : 0.147\n",
      "train accuracy on epoch 4267: 0.889\n",
      "test loss on epoch 4267: 0.340\n",
      "test accuracy on epoch 4267: 0.769\n",
      "train loss on epoch 4268 : 0.166\n",
      "train accuracy on epoch 4268: 0.889\n",
      "test loss on epoch 4268: 0.335\n",
      "test accuracy on epoch 4268: 0.769\n",
      "train loss on epoch 4269 : 0.094\n",
      "train accuracy on epoch 4269: 1.000\n",
      "test loss on epoch 4269: 0.338\n",
      "test accuracy on epoch 4269: 0.769\n",
      "train loss on epoch 4270 : 0.057\n",
      "train accuracy on epoch 4270: 1.000\n",
      "test loss on epoch 4270: 0.335\n",
      "test accuracy on epoch 4270: 0.769\n",
      "train loss on epoch 4271 : 0.163\n",
      "train accuracy on epoch 4271: 0.889\n",
      "test loss on epoch 4271: 0.332\n",
      "test accuracy on epoch 4271: 0.769\n",
      "train loss on epoch 4272 : 0.238\n",
      "train accuracy on epoch 4272: 0.889\n",
      "test loss on epoch 4272: 0.331\n",
      "test accuracy on epoch 4272: 0.769\n",
      "train loss on epoch 4273 : 0.175\n",
      "train accuracy on epoch 4273: 0.833\n",
      "test loss on epoch 4273: 0.329\n",
      "test accuracy on epoch 4273: 0.769\n",
      "train loss on epoch 4274 : 0.296\n",
      "train accuracy on epoch 4274: 0.889\n",
      "test loss on epoch 4274: 0.325\n",
      "test accuracy on epoch 4274: 0.769\n",
      "train loss on epoch 4275 : 0.254\n",
      "train accuracy on epoch 4275: 0.889\n",
      "test loss on epoch 4275: 0.325\n",
      "test accuracy on epoch 4275: 0.769\n",
      "train loss on epoch 4276 : 0.076\n",
      "train accuracy on epoch 4276: 1.000\n",
      "test loss on epoch 4276: 0.323\n",
      "test accuracy on epoch 4276: 0.769\n",
      "train loss on epoch 4277 : 0.334\n",
      "train accuracy on epoch 4277: 0.889\n",
      "test loss on epoch 4277: 0.314\n",
      "test accuracy on epoch 4277: 0.846\n",
      "train loss on epoch 4278 : 0.248\n",
      "train accuracy on epoch 4278: 0.889\n",
      "test loss on epoch 4278: 0.331\n",
      "test accuracy on epoch 4278: 0.769\n",
      "train loss on epoch 4279 : 0.233\n",
      "train accuracy on epoch 4279: 0.889\n",
      "test loss on epoch 4279: 0.315\n",
      "test accuracy on epoch 4279: 0.769\n",
      "train loss on epoch 4280 : 0.112\n",
      "train accuracy on epoch 4280: 0.944\n",
      "test loss on epoch 4280: 0.319\n",
      "test accuracy on epoch 4280: 0.769\n",
      "train loss on epoch 4281 : 0.185\n",
      "train accuracy on epoch 4281: 0.944\n",
      "test loss on epoch 4281: 0.335\n",
      "test accuracy on epoch 4281: 0.769\n",
      "train loss on epoch 4282 : 0.273\n",
      "train accuracy on epoch 4282: 0.889\n",
      "test loss on epoch 4282: 0.328\n",
      "test accuracy on epoch 4282: 0.769\n",
      "train loss on epoch 4283 : 0.034\n",
      "train accuracy on epoch 4283: 1.000\n",
      "test loss on epoch 4283: 0.317\n",
      "test accuracy on epoch 4283: 0.769\n",
      "train loss on epoch 4284 : 0.184\n",
      "train accuracy on epoch 4284: 0.944\n",
      "test loss on epoch 4284: 0.317\n",
      "test accuracy on epoch 4284: 0.769\n",
      "train loss on epoch 4285 : 0.262\n",
      "train accuracy on epoch 4285: 0.833\n",
      "test loss on epoch 4285: 0.316\n",
      "test accuracy on epoch 4285: 0.769\n",
      "train loss on epoch 4286 : 0.204\n",
      "train accuracy on epoch 4286: 0.889\n",
      "test loss on epoch 4286: 0.315\n",
      "test accuracy on epoch 4286: 0.769\n",
      "train loss on epoch 4287 : 0.304\n",
      "train accuracy on epoch 4287: 0.944\n",
      "test loss on epoch 4287: 0.327\n",
      "test accuracy on epoch 4287: 0.769\n",
      "train loss on epoch 4288 : 0.245\n",
      "train accuracy on epoch 4288: 0.833\n",
      "test loss on epoch 4288: 0.329\n",
      "test accuracy on epoch 4288: 0.692\n",
      "train loss on epoch 4289 : 0.112\n",
      "train accuracy on epoch 4289: 0.944\n",
      "test loss on epoch 4289: 0.330\n",
      "test accuracy on epoch 4289: 0.692\n",
      "train loss on epoch 4290 : 0.135\n",
      "train accuracy on epoch 4290: 0.944\n",
      "test loss on epoch 4290: 0.329\n",
      "test accuracy on epoch 4290: 0.769\n",
      "train loss on epoch 4291 : 0.156\n",
      "train accuracy on epoch 4291: 0.944\n",
      "test loss on epoch 4291: 0.329\n",
      "test accuracy on epoch 4291: 0.692\n",
      "train loss on epoch 4292 : 0.153\n",
      "train accuracy on epoch 4292: 0.944\n",
      "test loss on epoch 4292: 0.329\n",
      "test accuracy on epoch 4292: 0.692\n",
      "train loss on epoch 4293 : 0.062\n",
      "train accuracy on epoch 4293: 1.000\n",
      "test loss on epoch 4293: 0.316\n",
      "test accuracy on epoch 4293: 0.846\n",
      "train loss on epoch 4294 : 0.049\n",
      "train accuracy on epoch 4294: 1.000\n",
      "test loss on epoch 4294: 0.328\n",
      "test accuracy on epoch 4294: 0.692\n",
      "train loss on epoch 4295 : 0.180\n",
      "train accuracy on epoch 4295: 0.944\n",
      "test loss on epoch 4295: 0.327\n",
      "test accuracy on epoch 4295: 0.692\n",
      "train loss on epoch 4296 : 0.180\n",
      "train accuracy on epoch 4296: 0.889\n",
      "test loss on epoch 4296: 0.328\n",
      "test accuracy on epoch 4296: 0.692\n",
      "train loss on epoch 4297 : 0.115\n",
      "train accuracy on epoch 4297: 0.944\n",
      "test loss on epoch 4297: 0.313\n",
      "test accuracy on epoch 4297: 0.769\n",
      "train loss on epoch 4298 : 0.216\n",
      "train accuracy on epoch 4298: 0.944\n",
      "test loss on epoch 4298: 0.319\n",
      "test accuracy on epoch 4298: 0.769\n",
      "train loss on epoch 4299 : 0.206\n",
      "train accuracy on epoch 4299: 0.944\n",
      "test loss on epoch 4299: 0.317\n",
      "test accuracy on epoch 4299: 0.769\n",
      "train loss on epoch 4300 : 0.055\n",
      "train accuracy on epoch 4300: 1.000\n",
      "test loss on epoch 4300: 0.330\n",
      "test accuracy on epoch 4300: 0.769\n",
      "train loss on epoch 4301 : 0.182\n",
      "train accuracy on epoch 4301: 0.889\n",
      "test loss on epoch 4301: 0.329\n",
      "test accuracy on epoch 4301: 0.769\n",
      "train loss on epoch 4302 : 0.283\n",
      "train accuracy on epoch 4302: 0.889\n",
      "test loss on epoch 4302: 0.331\n",
      "test accuracy on epoch 4302: 0.769\n",
      "train loss on epoch 4303 : 0.084\n",
      "train accuracy on epoch 4303: 1.000\n",
      "test loss on epoch 4303: 0.332\n",
      "test accuracy on epoch 4303: 0.769\n",
      "train loss on epoch 4304 : 0.368\n",
      "train accuracy on epoch 4304: 0.889\n",
      "test loss on epoch 4304: 0.337\n",
      "test accuracy on epoch 4304: 0.769\n",
      "train loss on epoch 4305 : 0.088\n",
      "train accuracy on epoch 4305: 1.000\n",
      "test loss on epoch 4305: 0.335\n",
      "test accuracy on epoch 4305: 0.769\n",
      "train loss on epoch 4306 : 0.078\n",
      "train accuracy on epoch 4306: 1.000\n",
      "test loss on epoch 4306: 0.333\n",
      "test accuracy on epoch 4306: 0.769\n",
      "train loss on epoch 4307 : 0.108\n",
      "train accuracy on epoch 4307: 0.944\n",
      "test loss on epoch 4307: 0.330\n",
      "test accuracy on epoch 4307: 0.769\n",
      "train loss on epoch 4308 : 0.067\n",
      "train accuracy on epoch 4308: 0.944\n",
      "test loss on epoch 4308: 0.330\n",
      "test accuracy on epoch 4308: 0.769\n",
      "train loss on epoch 4309 : 0.142\n",
      "train accuracy on epoch 4309: 0.944\n",
      "test loss on epoch 4309: 0.327\n",
      "test accuracy on epoch 4309: 0.769\n",
      "train loss on epoch 4310 : 0.246\n",
      "train accuracy on epoch 4310: 0.944\n",
      "test loss on epoch 4310: 0.328\n",
      "test accuracy on epoch 4310: 0.769\n",
      "train loss on epoch 4311 : 0.108\n",
      "train accuracy on epoch 4311: 1.000\n",
      "test loss on epoch 4311: 0.323\n",
      "test accuracy on epoch 4311: 0.769\n",
      "train loss on epoch 4312 : 0.272\n",
      "train accuracy on epoch 4312: 0.889\n",
      "test loss on epoch 4312: 0.323\n",
      "test accuracy on epoch 4312: 0.769\n",
      "train loss on epoch 4313 : 0.157\n",
      "train accuracy on epoch 4313: 0.889\n",
      "test loss on epoch 4313: 0.324\n",
      "test accuracy on epoch 4313: 0.769\n",
      "train loss on epoch 4314 : 0.161\n",
      "train accuracy on epoch 4314: 0.889\n",
      "test loss on epoch 4314: 0.325\n",
      "test accuracy on epoch 4314: 0.769\n",
      "train loss on epoch 4315 : 0.106\n",
      "train accuracy on epoch 4315: 0.944\n",
      "test loss on epoch 4315: 0.329\n",
      "test accuracy on epoch 4315: 0.769\n",
      "train loss on epoch 4316 : 0.093\n",
      "train accuracy on epoch 4316: 0.944\n",
      "test loss on epoch 4316: 0.327\n",
      "test accuracy on epoch 4316: 0.769\n",
      "train loss on epoch 4317 : 0.238\n",
      "train accuracy on epoch 4317: 0.944\n",
      "test loss on epoch 4317: 0.330\n",
      "test accuracy on epoch 4317: 0.769\n",
      "train loss on epoch 4318 : 0.398\n",
      "train accuracy on epoch 4318: 0.889\n",
      "test loss on epoch 4318: 0.331\n",
      "test accuracy on epoch 4318: 0.769\n",
      "train loss on epoch 4319 : 0.293\n",
      "train accuracy on epoch 4319: 0.833\n",
      "test loss on epoch 4319: 0.329\n",
      "test accuracy on epoch 4319: 0.769\n",
      "train loss on epoch 4320 : 0.392\n",
      "train accuracy on epoch 4320: 0.889\n",
      "test loss on epoch 4320: 0.332\n",
      "test accuracy on epoch 4320: 0.769\n",
      "train loss on epoch 4321 : 0.092\n",
      "train accuracy on epoch 4321: 1.000\n",
      "test loss on epoch 4321: 0.336\n",
      "test accuracy on epoch 4321: 0.769\n",
      "train loss on epoch 4322 : 0.363\n",
      "train accuracy on epoch 4322: 0.889\n",
      "test loss on epoch 4322: 0.340\n",
      "test accuracy on epoch 4322: 0.769\n",
      "train loss on epoch 4323 : 0.111\n",
      "train accuracy on epoch 4323: 1.000\n",
      "test loss on epoch 4323: 0.334\n",
      "test accuracy on epoch 4323: 0.769\n",
      "train loss on epoch 4324 : 0.124\n",
      "train accuracy on epoch 4324: 0.944\n",
      "test loss on epoch 4324: 0.336\n",
      "test accuracy on epoch 4324: 0.769\n",
      "train loss on epoch 4325 : 0.061\n",
      "train accuracy on epoch 4325: 1.000\n",
      "test loss on epoch 4325: 0.337\n",
      "test accuracy on epoch 4325: 0.769\n",
      "train loss on epoch 4326 : 0.116\n",
      "train accuracy on epoch 4326: 0.889\n",
      "test loss on epoch 4326: 0.334\n",
      "test accuracy on epoch 4326: 0.769\n",
      "train loss on epoch 4327 : 0.313\n",
      "train accuracy on epoch 4327: 0.778\n",
      "test loss on epoch 4327: 0.329\n",
      "test accuracy on epoch 4327: 0.769\n",
      "train loss on epoch 4328 : 0.316\n",
      "train accuracy on epoch 4328: 0.889\n",
      "test loss on epoch 4328: 0.323\n",
      "test accuracy on epoch 4328: 0.769\n",
      "train loss on epoch 4329 : 0.180\n",
      "train accuracy on epoch 4329: 0.944\n",
      "test loss on epoch 4329: 0.328\n",
      "test accuracy on epoch 4329: 0.769\n",
      "train loss on epoch 4330 : 0.153\n",
      "train accuracy on epoch 4330: 0.944\n",
      "test loss on epoch 4330: 0.326\n",
      "test accuracy on epoch 4330: 0.769\n",
      "train loss on epoch 4331 : 0.187\n",
      "train accuracy on epoch 4331: 0.889\n",
      "test loss on epoch 4331: 0.322\n",
      "test accuracy on epoch 4331: 0.769\n",
      "train loss on epoch 4332 : 0.047\n",
      "train accuracy on epoch 4332: 1.000\n",
      "test loss on epoch 4332: 0.319\n",
      "test accuracy on epoch 4332: 0.769\n",
      "train loss on epoch 4333 : 0.070\n",
      "train accuracy on epoch 4333: 0.944\n",
      "test loss on epoch 4333: 0.321\n",
      "test accuracy on epoch 4333: 0.769\n",
      "train loss on epoch 4334 : 0.031\n",
      "train accuracy on epoch 4334: 1.000\n",
      "test loss on epoch 4334: 0.318\n",
      "test accuracy on epoch 4334: 0.769\n",
      "train loss on epoch 4335 : 0.075\n",
      "train accuracy on epoch 4335: 1.000\n",
      "test loss on epoch 4335: 0.324\n",
      "test accuracy on epoch 4335: 0.769\n",
      "train loss on epoch 4336 : 0.391\n",
      "train accuracy on epoch 4336: 0.889\n",
      "test loss on epoch 4336: 0.326\n",
      "test accuracy on epoch 4336: 0.769\n",
      "train loss on epoch 4337 : 0.119\n",
      "train accuracy on epoch 4337: 0.944\n",
      "test loss on epoch 4337: 0.319\n",
      "test accuracy on epoch 4337: 0.769\n",
      "train loss on epoch 4338 : 0.156\n",
      "train accuracy on epoch 4338: 1.000\n",
      "test loss on epoch 4338: 0.321\n",
      "test accuracy on epoch 4338: 0.769\n",
      "train loss on epoch 4339 : 0.360\n",
      "train accuracy on epoch 4339: 0.889\n",
      "test loss on epoch 4339: 0.324\n",
      "test accuracy on epoch 4339: 0.769\n",
      "train loss on epoch 4340 : 0.145\n",
      "train accuracy on epoch 4340: 0.944\n",
      "test loss on epoch 4340: 0.322\n",
      "test accuracy on epoch 4340: 0.769\n",
      "train loss on epoch 4341 : 0.534\n",
      "train accuracy on epoch 4341: 0.889\n",
      "test loss on epoch 4341: 0.329\n",
      "test accuracy on epoch 4341: 0.769\n",
      "train loss on epoch 4342 : 0.104\n",
      "train accuracy on epoch 4342: 0.889\n",
      "test loss on epoch 4342: 0.318\n",
      "test accuracy on epoch 4342: 0.769\n",
      "train loss on epoch 4343 : 0.291\n",
      "train accuracy on epoch 4343: 0.944\n",
      "test loss on epoch 4343: 0.314\n",
      "test accuracy on epoch 4343: 0.769\n",
      "train loss on epoch 4344 : 0.254\n",
      "train accuracy on epoch 4344: 0.944\n",
      "test loss on epoch 4344: 0.324\n",
      "test accuracy on epoch 4344: 0.769\n",
      "train loss on epoch 4345 : 0.358\n",
      "train accuracy on epoch 4345: 0.889\n",
      "test loss on epoch 4345: 0.316\n",
      "test accuracy on epoch 4345: 0.846\n",
      "train loss on epoch 4346 : 0.239\n",
      "train accuracy on epoch 4346: 0.889\n",
      "test loss on epoch 4346: 0.312\n",
      "test accuracy on epoch 4346: 0.846\n",
      "train loss on epoch 4347 : 0.139\n",
      "train accuracy on epoch 4347: 0.889\n",
      "test loss on epoch 4347: 0.320\n",
      "test accuracy on epoch 4347: 0.769\n",
      "train loss on epoch 4348 : 0.095\n",
      "train accuracy on epoch 4348: 0.944\n",
      "test loss on epoch 4348: 0.317\n",
      "test accuracy on epoch 4348: 0.769\n",
      "train loss on epoch 4349 : 0.174\n",
      "train accuracy on epoch 4349: 0.944\n",
      "test loss on epoch 4349: 0.326\n",
      "test accuracy on epoch 4349: 0.769\n",
      "train loss on epoch 4350 : 0.047\n",
      "train accuracy on epoch 4350: 1.000\n",
      "test loss on epoch 4350: 0.314\n",
      "test accuracy on epoch 4350: 0.769\n",
      "train loss on epoch 4351 : 0.188\n",
      "train accuracy on epoch 4351: 0.889\n",
      "test loss on epoch 4351: 0.313\n",
      "test accuracy on epoch 4351: 0.769\n",
      "train loss on epoch 4352 : 0.373\n",
      "train accuracy on epoch 4352: 0.833\n",
      "test loss on epoch 4352: 0.319\n",
      "test accuracy on epoch 4352: 0.769\n",
      "train loss on epoch 4353 : 0.180\n",
      "train accuracy on epoch 4353: 0.944\n",
      "test loss on epoch 4353: 0.317\n",
      "test accuracy on epoch 4353: 0.769\n",
      "train loss on epoch 4354 : 0.092\n",
      "train accuracy on epoch 4354: 0.944\n",
      "test loss on epoch 4354: 0.312\n",
      "test accuracy on epoch 4354: 0.769\n",
      "train loss on epoch 4355 : 0.064\n",
      "train accuracy on epoch 4355: 1.000\n",
      "test loss on epoch 4355: 0.318\n",
      "test accuracy on epoch 4355: 0.769\n",
      "train loss on epoch 4356 : 0.178\n",
      "train accuracy on epoch 4356: 0.889\n",
      "test loss on epoch 4356: 0.319\n",
      "test accuracy on epoch 4356: 0.769\n",
      "train loss on epoch 4357 : 0.220\n",
      "train accuracy on epoch 4357: 0.889\n",
      "test loss on epoch 4357: 0.315\n",
      "test accuracy on epoch 4357: 0.846\n",
      "train loss on epoch 4358 : 0.140\n",
      "train accuracy on epoch 4358: 0.944\n",
      "test loss on epoch 4358: 0.322\n",
      "test accuracy on epoch 4358: 0.769\n",
      "train loss on epoch 4359 : 0.154\n",
      "train accuracy on epoch 4359: 0.889\n",
      "test loss on epoch 4359: 0.324\n",
      "test accuracy on epoch 4359: 0.769\n",
      "train loss on epoch 4360 : 0.112\n",
      "train accuracy on epoch 4360: 0.944\n",
      "test loss on epoch 4360: 0.328\n",
      "test accuracy on epoch 4360: 0.769\n",
      "train loss on epoch 4361 : 0.266\n",
      "train accuracy on epoch 4361: 0.833\n",
      "test loss on epoch 4361: 0.331\n",
      "test accuracy on epoch 4361: 0.769\n",
      "train loss on epoch 4362 : 0.233\n",
      "train accuracy on epoch 4362: 0.889\n",
      "test loss on epoch 4362: 0.333\n",
      "test accuracy on epoch 4362: 0.769\n",
      "train loss on epoch 4363 : 0.057\n",
      "train accuracy on epoch 4363: 1.000\n",
      "test loss on epoch 4363: 0.332\n",
      "test accuracy on epoch 4363: 0.769\n",
      "train loss on epoch 4364 : 0.064\n",
      "train accuracy on epoch 4364: 1.000\n",
      "test loss on epoch 4364: 0.325\n",
      "test accuracy on epoch 4364: 0.769\n",
      "train loss on epoch 4365 : 0.105\n",
      "train accuracy on epoch 4365: 0.944\n",
      "test loss on epoch 4365: 0.329\n",
      "test accuracy on epoch 4365: 0.769\n",
      "train loss on epoch 4366 : 0.183\n",
      "train accuracy on epoch 4366: 0.833\n",
      "test loss on epoch 4366: 0.327\n",
      "test accuracy on epoch 4366: 0.769\n",
      "train loss on epoch 4367 : 0.113\n",
      "train accuracy on epoch 4367: 0.944\n",
      "test loss on epoch 4367: 0.324\n",
      "test accuracy on epoch 4367: 0.769\n",
      "train loss on epoch 4368 : 0.079\n",
      "train accuracy on epoch 4368: 0.944\n",
      "test loss on epoch 4368: 0.325\n",
      "test accuracy on epoch 4368: 0.769\n",
      "train loss on epoch 4369 : 0.146\n",
      "train accuracy on epoch 4369: 0.944\n",
      "test loss on epoch 4369: 0.316\n",
      "test accuracy on epoch 4369: 0.846\n",
      "train loss on epoch 4370 : 0.228\n",
      "train accuracy on epoch 4370: 0.889\n",
      "test loss on epoch 4370: 0.327\n",
      "test accuracy on epoch 4370: 0.692\n",
      "train loss on epoch 4371 : 0.026\n",
      "train accuracy on epoch 4371: 1.000\n",
      "test loss on epoch 4371: 0.325\n",
      "test accuracy on epoch 4371: 0.692\n",
      "train loss on epoch 4372 : 0.129\n",
      "train accuracy on epoch 4372: 0.889\n",
      "test loss on epoch 4372: 0.325\n",
      "test accuracy on epoch 4372: 0.769\n",
      "train loss on epoch 4373 : 0.173\n",
      "train accuracy on epoch 4373: 0.889\n",
      "test loss on epoch 4373: 0.324\n",
      "test accuracy on epoch 4373: 0.769\n",
      "train loss on epoch 4374 : 0.154\n",
      "train accuracy on epoch 4374: 0.944\n",
      "test loss on epoch 4374: 0.325\n",
      "test accuracy on epoch 4374: 0.769\n",
      "train loss on epoch 4375 : 0.156\n",
      "train accuracy on epoch 4375: 0.944\n",
      "test loss on epoch 4375: 0.320\n",
      "test accuracy on epoch 4375: 0.769\n",
      "train loss on epoch 4376 : 0.128\n",
      "train accuracy on epoch 4376: 1.000\n",
      "test loss on epoch 4376: 0.320\n",
      "test accuracy on epoch 4376: 0.846\n",
      "train loss on epoch 4377 : 0.114\n",
      "train accuracy on epoch 4377: 0.944\n",
      "test loss on epoch 4377: 0.325\n",
      "test accuracy on epoch 4377: 0.769\n",
      "train loss on epoch 4378 : 0.310\n",
      "train accuracy on epoch 4378: 0.889\n",
      "test loss on epoch 4378: 0.330\n",
      "test accuracy on epoch 4378: 0.769\n",
      "train loss on epoch 4379 : 0.393\n",
      "train accuracy on epoch 4379: 0.778\n",
      "test loss on epoch 4379: 0.330\n",
      "test accuracy on epoch 4379: 0.769\n",
      "train loss on epoch 4380 : 0.059\n",
      "train accuracy on epoch 4380: 1.000\n",
      "test loss on epoch 4380: 0.330\n",
      "test accuracy on epoch 4380: 0.769\n",
      "train loss on epoch 4381 : 0.134\n",
      "train accuracy on epoch 4381: 1.000\n",
      "test loss on epoch 4381: 0.324\n",
      "test accuracy on epoch 4381: 0.769\n",
      "train loss on epoch 4382 : 0.193\n",
      "train accuracy on epoch 4382: 0.833\n",
      "test loss on epoch 4382: 0.329\n",
      "test accuracy on epoch 4382: 0.769\n",
      "train loss on epoch 4383 : 0.134\n",
      "train accuracy on epoch 4383: 0.944\n",
      "test loss on epoch 4383: 0.324\n",
      "test accuracy on epoch 4383: 0.769\n",
      "train loss on epoch 4384 : 0.231\n",
      "train accuracy on epoch 4384: 0.889\n",
      "test loss on epoch 4384: 0.325\n",
      "test accuracy on epoch 4384: 0.769\n",
      "train loss on epoch 4385 : 0.230\n",
      "train accuracy on epoch 4385: 0.944\n",
      "test loss on epoch 4385: 0.328\n",
      "test accuracy on epoch 4385: 0.769\n",
      "train loss on epoch 4386 : 0.155\n",
      "train accuracy on epoch 4386: 0.944\n",
      "test loss on epoch 4386: 0.326\n",
      "test accuracy on epoch 4386: 0.769\n",
      "train loss on epoch 4387 : 0.085\n",
      "train accuracy on epoch 4387: 0.944\n",
      "test loss on epoch 4387: 0.324\n",
      "test accuracy on epoch 4387: 0.769\n",
      "train loss on epoch 4388 : 0.196\n",
      "train accuracy on epoch 4388: 0.944\n",
      "test loss on epoch 4388: 0.323\n",
      "test accuracy on epoch 4388: 0.769\n",
      "train loss on epoch 4389 : 0.398\n",
      "train accuracy on epoch 4389: 0.889\n",
      "test loss on epoch 4389: 0.321\n",
      "test accuracy on epoch 4389: 0.769\n",
      "train loss on epoch 4390 : 0.192\n",
      "train accuracy on epoch 4390: 0.889\n",
      "test loss on epoch 4390: 0.311\n",
      "test accuracy on epoch 4390: 0.846\n",
      "train loss on epoch 4391 : 0.093\n",
      "train accuracy on epoch 4391: 0.944\n",
      "test loss on epoch 4391: 0.325\n",
      "test accuracy on epoch 4391: 0.692\n",
      "train loss on epoch 4392 : 0.201\n",
      "train accuracy on epoch 4392: 0.944\n",
      "test loss on epoch 4392: 0.313\n",
      "test accuracy on epoch 4392: 0.846\n",
      "train loss on epoch 4393 : 0.308\n",
      "train accuracy on epoch 4393: 0.889\n",
      "test loss on epoch 4393: 0.311\n",
      "test accuracy on epoch 4393: 0.846\n",
      "train loss on epoch 4394 : 0.225\n",
      "train accuracy on epoch 4394: 0.889\n",
      "test loss on epoch 4394: 0.317\n",
      "test accuracy on epoch 4394: 0.769\n",
      "train loss on epoch 4395 : 0.115\n",
      "train accuracy on epoch 4395: 0.944\n",
      "test loss on epoch 4395: 0.310\n",
      "test accuracy on epoch 4395: 0.846\n",
      "train loss on epoch 4396 : 0.134\n",
      "train accuracy on epoch 4396: 0.944\n",
      "test loss on epoch 4396: 0.321\n",
      "test accuracy on epoch 4396: 0.692\n",
      "train loss on epoch 4397 : 0.132\n",
      "train accuracy on epoch 4397: 0.944\n",
      "test loss on epoch 4397: 0.312\n",
      "test accuracy on epoch 4397: 0.846\n",
      "train loss on epoch 4398 : 0.056\n",
      "train accuracy on epoch 4398: 1.000\n",
      "test loss on epoch 4398: 0.310\n",
      "test accuracy on epoch 4398: 0.769\n",
      "train loss on epoch 4399 : 0.305\n",
      "train accuracy on epoch 4399: 0.889\n",
      "test loss on epoch 4399: 0.319\n",
      "test accuracy on epoch 4399: 0.769\n",
      "train loss on epoch 4400 : 0.091\n",
      "train accuracy on epoch 4400: 0.944\n",
      "test loss on epoch 4400: 0.312\n",
      "test accuracy on epoch 4400: 0.846\n",
      "train loss on epoch 4401 : 0.068\n",
      "train accuracy on epoch 4401: 1.000\n",
      "test loss on epoch 4401: 0.314\n",
      "test accuracy on epoch 4401: 0.769\n",
      "train loss on epoch 4402 : 0.143\n",
      "train accuracy on epoch 4402: 0.944\n",
      "test loss on epoch 4402: 0.320\n",
      "test accuracy on epoch 4402: 0.769\n",
      "train loss on epoch 4403 : 0.117\n",
      "train accuracy on epoch 4403: 0.944\n",
      "test loss on epoch 4403: 0.314\n",
      "test accuracy on epoch 4403: 0.769\n",
      "train loss on epoch 4404 : 0.151\n",
      "train accuracy on epoch 4404: 0.944\n",
      "test loss on epoch 4404: 0.315\n",
      "test accuracy on epoch 4404: 0.769\n",
      "train loss on epoch 4405 : 0.169\n",
      "train accuracy on epoch 4405: 0.889\n",
      "test loss on epoch 4405: 0.318\n",
      "test accuracy on epoch 4405: 0.692\n",
      "train loss on epoch 4406 : 0.114\n",
      "train accuracy on epoch 4406: 0.944\n",
      "test loss on epoch 4406: 0.315\n",
      "test accuracy on epoch 4406: 0.769\n",
      "train loss on epoch 4407 : 0.501\n",
      "train accuracy on epoch 4407: 0.889\n",
      "test loss on epoch 4407: 0.309\n",
      "test accuracy on epoch 4407: 0.846\n",
      "train loss on epoch 4408 : 0.275\n",
      "train accuracy on epoch 4408: 0.889\n",
      "test loss on epoch 4408: 0.311\n",
      "test accuracy on epoch 4408: 0.846\n",
      "train loss on epoch 4409 : 0.514\n",
      "train accuracy on epoch 4409: 0.889\n",
      "test loss on epoch 4409: 0.319\n",
      "test accuracy on epoch 4409: 0.692\n",
      "train loss on epoch 4410 : 0.184\n",
      "train accuracy on epoch 4410: 0.944\n",
      "test loss on epoch 4410: 0.322\n",
      "test accuracy on epoch 4410: 0.769\n",
      "train loss on epoch 4411 : 0.116\n",
      "train accuracy on epoch 4411: 0.944\n",
      "test loss on epoch 4411: 0.312\n",
      "test accuracy on epoch 4411: 0.769\n",
      "train loss on epoch 4412 : 0.276\n",
      "train accuracy on epoch 4412: 0.889\n",
      "test loss on epoch 4412: 0.306\n",
      "test accuracy on epoch 4412: 0.769\n",
      "train loss on epoch 4413 : 0.145\n",
      "train accuracy on epoch 4413: 0.944\n",
      "test loss on epoch 4413: 0.310\n",
      "test accuracy on epoch 4413: 0.769\n",
      "train loss on epoch 4414 : 0.273\n",
      "train accuracy on epoch 4414: 0.889\n",
      "test loss on epoch 4414: 0.322\n",
      "test accuracy on epoch 4414: 0.769\n",
      "train loss on epoch 4415 : 0.075\n",
      "train accuracy on epoch 4415: 0.944\n",
      "test loss on epoch 4415: 0.311\n",
      "test accuracy on epoch 4415: 0.769\n",
      "train loss on epoch 4416 : 0.059\n",
      "train accuracy on epoch 4416: 1.000\n",
      "test loss on epoch 4416: 0.319\n",
      "test accuracy on epoch 4416: 0.769\n",
      "train loss on epoch 4417 : 0.175\n",
      "train accuracy on epoch 4417: 0.889\n",
      "test loss on epoch 4417: 0.321\n",
      "test accuracy on epoch 4417: 0.769\n",
      "train loss on epoch 4418 : 0.093\n",
      "train accuracy on epoch 4418: 0.944\n",
      "test loss on epoch 4418: 0.321\n",
      "test accuracy on epoch 4418: 0.769\n",
      "train loss on epoch 4419 : 0.301\n",
      "train accuracy on epoch 4419: 0.889\n",
      "test loss on epoch 4419: 0.305\n",
      "test accuracy on epoch 4419: 0.769\n",
      "train loss on epoch 4420 : 0.118\n",
      "train accuracy on epoch 4420: 0.889\n",
      "test loss on epoch 4420: 0.308\n",
      "test accuracy on epoch 4420: 0.769\n",
      "train loss on epoch 4421 : 0.150\n",
      "train accuracy on epoch 4421: 0.944\n",
      "test loss on epoch 4421: 0.325\n",
      "test accuracy on epoch 4421: 0.769\n",
      "train loss on epoch 4422 : 0.258\n",
      "train accuracy on epoch 4422: 0.889\n",
      "test loss on epoch 4422: 0.309\n",
      "test accuracy on epoch 4422: 0.769\n",
      "train loss on epoch 4423 : 0.178\n",
      "train accuracy on epoch 4423: 0.944\n",
      "test loss on epoch 4423: 0.308\n",
      "test accuracy on epoch 4423: 0.769\n",
      "train loss on epoch 4424 : 0.166\n",
      "train accuracy on epoch 4424: 0.889\n",
      "test loss on epoch 4424: 0.315\n",
      "test accuracy on epoch 4424: 0.769\n",
      "train loss on epoch 4425 : 0.444\n",
      "train accuracy on epoch 4425: 0.778\n",
      "test loss on epoch 4425: 0.315\n",
      "test accuracy on epoch 4425: 0.769\n",
      "train loss on epoch 4426 : 0.125\n",
      "train accuracy on epoch 4426: 0.889\n",
      "test loss on epoch 4426: 0.328\n",
      "test accuracy on epoch 4426: 0.769\n",
      "train loss on epoch 4427 : 0.206\n",
      "train accuracy on epoch 4427: 0.944\n",
      "test loss on epoch 4427: 0.310\n",
      "test accuracy on epoch 4427: 0.769\n",
      "train loss on epoch 4428 : 0.075\n",
      "train accuracy on epoch 4428: 1.000\n",
      "test loss on epoch 4428: 0.327\n",
      "test accuracy on epoch 4428: 0.769\n",
      "train loss on epoch 4429 : 0.201\n",
      "train accuracy on epoch 4429: 0.889\n",
      "test loss on epoch 4429: 0.330\n",
      "test accuracy on epoch 4429: 0.769\n",
      "train loss on epoch 4430 : 0.142\n",
      "train accuracy on epoch 4430: 0.944\n",
      "test loss on epoch 4430: 0.315\n",
      "test accuracy on epoch 4430: 0.769\n",
      "train loss on epoch 4431 : 0.476\n",
      "train accuracy on epoch 4431: 0.833\n",
      "test loss on epoch 4431: 0.316\n",
      "test accuracy on epoch 4431: 0.769\n",
      "train loss on epoch 4432 : 0.077\n",
      "train accuracy on epoch 4432: 0.944\n",
      "test loss on epoch 4432: 0.317\n",
      "test accuracy on epoch 4432: 0.769\n",
      "train loss on epoch 4433 : 0.166\n",
      "train accuracy on epoch 4433: 0.889\n",
      "test loss on epoch 4433: 0.310\n",
      "test accuracy on epoch 4433: 0.769\n",
      "train loss on epoch 4434 : 0.179\n",
      "train accuracy on epoch 4434: 0.944\n",
      "test loss on epoch 4434: 0.315\n",
      "test accuracy on epoch 4434: 0.769\n",
      "train loss on epoch 4435 : 0.120\n",
      "train accuracy on epoch 4435: 0.944\n",
      "test loss on epoch 4435: 0.326\n",
      "test accuracy on epoch 4435: 0.769\n",
      "train loss on epoch 4436 : 0.255\n",
      "train accuracy on epoch 4436: 0.889\n",
      "test loss on epoch 4436: 0.331\n",
      "test accuracy on epoch 4436: 0.769\n",
      "train loss on epoch 4437 : 0.192\n",
      "train accuracy on epoch 4437: 0.889\n",
      "test loss on epoch 4437: 0.310\n",
      "test accuracy on epoch 4437: 0.769\n",
      "train loss on epoch 4438 : 0.139\n",
      "train accuracy on epoch 4438: 0.944\n",
      "test loss on epoch 4438: 0.326\n",
      "test accuracy on epoch 4438: 0.769\n",
      "train loss on epoch 4439 : 0.535\n",
      "train accuracy on epoch 4439: 0.778\n",
      "test loss on epoch 4439: 0.310\n",
      "test accuracy on epoch 4439: 0.846\n",
      "train loss on epoch 4440 : 0.215\n",
      "train accuracy on epoch 4440: 0.944\n",
      "test loss on epoch 4440: 0.325\n",
      "test accuracy on epoch 4440: 0.692\n",
      "train loss on epoch 4441 : 0.046\n",
      "train accuracy on epoch 4441: 1.000\n",
      "test loss on epoch 4441: 0.316\n",
      "test accuracy on epoch 4441: 0.769\n",
      "train loss on epoch 4442 : 0.231\n",
      "train accuracy on epoch 4442: 0.944\n",
      "test loss on epoch 4442: 0.318\n",
      "test accuracy on epoch 4442: 0.769\n",
      "train loss on epoch 4443 : 0.207\n",
      "train accuracy on epoch 4443: 0.944\n",
      "test loss on epoch 4443: 0.321\n",
      "test accuracy on epoch 4443: 0.769\n",
      "train loss on epoch 4444 : 0.194\n",
      "train accuracy on epoch 4444: 0.944\n",
      "test loss on epoch 4444: 0.325\n",
      "test accuracy on epoch 4444: 0.769\n",
      "train loss on epoch 4445 : 0.108\n",
      "train accuracy on epoch 4445: 0.944\n",
      "test loss on epoch 4445: 0.330\n",
      "test accuracy on epoch 4445: 0.769\n",
      "train loss on epoch 4446 : 0.202\n",
      "train accuracy on epoch 4446: 0.944\n",
      "test loss on epoch 4446: 0.322\n",
      "test accuracy on epoch 4446: 0.769\n",
      "train loss on epoch 4447 : 0.168\n",
      "train accuracy on epoch 4447: 0.889\n",
      "test loss on epoch 4447: 0.323\n",
      "test accuracy on epoch 4447: 0.692\n",
      "train loss on epoch 4448 : 0.098\n",
      "train accuracy on epoch 4448: 0.944\n",
      "test loss on epoch 4448: 0.313\n",
      "test accuracy on epoch 4448: 0.769\n",
      "train loss on epoch 4449 : 0.098\n",
      "train accuracy on epoch 4449: 0.944\n",
      "test loss on epoch 4449: 0.305\n",
      "test accuracy on epoch 4449: 0.769\n",
      "train loss on epoch 4450 : 0.213\n",
      "train accuracy on epoch 4450: 0.944\n",
      "test loss on epoch 4450: 0.320\n",
      "test accuracy on epoch 4450: 0.769\n",
      "train loss on epoch 4451 : 0.257\n",
      "train accuracy on epoch 4451: 0.833\n",
      "test loss on epoch 4451: 0.328\n",
      "test accuracy on epoch 4451: 0.769\n",
      "train loss on epoch 4452 : 0.365\n",
      "train accuracy on epoch 4452: 0.889\n",
      "test loss on epoch 4452: 0.310\n",
      "test accuracy on epoch 4452: 0.769\n",
      "train loss on epoch 4453 : 0.131\n",
      "train accuracy on epoch 4453: 0.944\n",
      "test loss on epoch 4453: 0.309\n",
      "test accuracy on epoch 4453: 0.769\n",
      "train loss on epoch 4454 : 0.243\n",
      "train accuracy on epoch 4454: 0.833\n",
      "test loss on epoch 4454: 0.311\n",
      "test accuracy on epoch 4454: 0.769\n",
      "train loss on epoch 4455 : 0.164\n",
      "train accuracy on epoch 4455: 0.944\n",
      "test loss on epoch 4455: 0.325\n",
      "test accuracy on epoch 4455: 0.769\n",
      "train loss on epoch 4456 : 0.225\n",
      "train accuracy on epoch 4456: 0.944\n",
      "test loss on epoch 4456: 0.320\n",
      "test accuracy on epoch 4456: 0.769\n",
      "train loss on epoch 4457 : 0.094\n",
      "train accuracy on epoch 4457: 1.000\n",
      "test loss on epoch 4457: 0.313\n",
      "test accuracy on epoch 4457: 0.769\n",
      "train loss on epoch 4458 : 0.427\n",
      "train accuracy on epoch 4458: 0.889\n",
      "test loss on epoch 4458: 0.315\n",
      "test accuracy on epoch 4458: 0.769\n",
      "train loss on epoch 4459 : 0.221\n",
      "train accuracy on epoch 4459: 0.944\n",
      "test loss on epoch 4459: 0.309\n",
      "test accuracy on epoch 4459: 0.769\n",
      "train loss on epoch 4460 : 0.135\n",
      "train accuracy on epoch 4460: 0.944\n",
      "test loss on epoch 4460: 0.306\n",
      "test accuracy on epoch 4460: 0.769\n",
      "train loss on epoch 4461 : 0.202\n",
      "train accuracy on epoch 4461: 0.889\n",
      "test loss on epoch 4461: 0.324\n",
      "test accuracy on epoch 4461: 0.769\n",
      "train loss on epoch 4462 : 0.190\n",
      "train accuracy on epoch 4462: 0.944\n",
      "test loss on epoch 4462: 0.311\n",
      "test accuracy on epoch 4462: 0.769\n",
      "train loss on epoch 4463 : 0.123\n",
      "train accuracy on epoch 4463: 0.944\n",
      "test loss on epoch 4463: 0.323\n",
      "test accuracy on epoch 4463: 0.769\n",
      "train loss on epoch 4464 : 0.133\n",
      "train accuracy on epoch 4464: 0.944\n",
      "test loss on epoch 4464: 0.309\n",
      "test accuracy on epoch 4464: 0.769\n",
      "train loss on epoch 4465 : 0.317\n",
      "train accuracy on epoch 4465: 0.944\n",
      "test loss on epoch 4465: 0.327\n",
      "test accuracy on epoch 4465: 0.769\n",
      "train loss on epoch 4466 : 0.130\n",
      "train accuracy on epoch 4466: 0.944\n",
      "test loss on epoch 4466: 0.326\n",
      "test accuracy on epoch 4466: 0.769\n",
      "train loss on epoch 4467 : 0.209\n",
      "train accuracy on epoch 4467: 0.944\n",
      "test loss on epoch 4467: 0.317\n",
      "test accuracy on epoch 4467: 0.769\n",
      "train loss on epoch 4468 : 0.109\n",
      "train accuracy on epoch 4468: 0.944\n",
      "test loss on epoch 4468: 0.323\n",
      "test accuracy on epoch 4468: 0.769\n",
      "train loss on epoch 4469 : 0.098\n",
      "train accuracy on epoch 4469: 0.944\n",
      "test loss on epoch 4469: 0.323\n",
      "test accuracy on epoch 4469: 0.769\n",
      "train loss on epoch 4470 : 0.151\n",
      "train accuracy on epoch 4470: 0.944\n",
      "test loss on epoch 4470: 0.316\n",
      "test accuracy on epoch 4470: 0.769\n",
      "train loss on epoch 4471 : 0.201\n",
      "train accuracy on epoch 4471: 0.889\n",
      "test loss on epoch 4471: 0.322\n",
      "test accuracy on epoch 4471: 0.769\n",
      "train loss on epoch 4472 : 0.142\n",
      "train accuracy on epoch 4472: 0.944\n",
      "test loss on epoch 4472: 0.322\n",
      "test accuracy on epoch 4472: 0.769\n",
      "train loss on epoch 4473 : 0.223\n",
      "train accuracy on epoch 4473: 0.944\n",
      "test loss on epoch 4473: 0.327\n",
      "test accuracy on epoch 4473: 0.769\n",
      "train loss on epoch 4474 : 0.173\n",
      "train accuracy on epoch 4474: 0.889\n",
      "test loss on epoch 4474: 0.321\n",
      "test accuracy on epoch 4474: 0.769\n",
      "train loss on epoch 4475 : 0.065\n",
      "train accuracy on epoch 4475: 1.000\n",
      "test loss on epoch 4475: 0.321\n",
      "test accuracy on epoch 4475: 0.769\n",
      "train loss on epoch 4476 : 0.335\n",
      "train accuracy on epoch 4476: 0.889\n",
      "test loss on epoch 4476: 0.314\n",
      "test accuracy on epoch 4476: 0.846\n",
      "train loss on epoch 4477 : 0.297\n",
      "train accuracy on epoch 4477: 0.889\n",
      "test loss on epoch 4477: 0.319\n",
      "test accuracy on epoch 4477: 0.769\n",
      "train loss on epoch 4478 : 0.333\n",
      "train accuracy on epoch 4478: 0.889\n",
      "test loss on epoch 4478: 0.312\n",
      "test accuracy on epoch 4478: 0.846\n",
      "train loss on epoch 4479 : 0.137\n",
      "train accuracy on epoch 4479: 0.944\n",
      "test loss on epoch 4479: 0.319\n",
      "test accuracy on epoch 4479: 0.769\n",
      "train loss on epoch 4480 : 0.068\n",
      "train accuracy on epoch 4480: 0.944\n",
      "test loss on epoch 4480: 0.312\n",
      "test accuracy on epoch 4480: 0.769\n",
      "train loss on epoch 4481 : 0.415\n",
      "train accuracy on epoch 4481: 0.833\n",
      "test loss on epoch 4481: 0.317\n",
      "test accuracy on epoch 4481: 0.769\n",
      "train loss on epoch 4482 : 0.174\n",
      "train accuracy on epoch 4482: 0.944\n",
      "test loss on epoch 4482: 0.322\n",
      "test accuracy on epoch 4482: 0.769\n",
      "train loss on epoch 4483 : 0.230\n",
      "train accuracy on epoch 4483: 0.944\n",
      "test loss on epoch 4483: 0.323\n",
      "test accuracy on epoch 4483: 0.769\n",
      "train loss on epoch 4484 : 0.142\n",
      "train accuracy on epoch 4484: 0.944\n",
      "test loss on epoch 4484: 0.327\n",
      "test accuracy on epoch 4484: 0.769\n",
      "train loss on epoch 4485 : 0.277\n",
      "train accuracy on epoch 4485: 0.889\n",
      "test loss on epoch 4485: 0.329\n",
      "test accuracy on epoch 4485: 0.769\n",
      "train loss on epoch 4486 : 0.122\n",
      "train accuracy on epoch 4486: 0.944\n",
      "test loss on epoch 4486: 0.334\n",
      "test accuracy on epoch 4486: 0.769\n",
      "train loss on epoch 4487 : 0.159\n",
      "train accuracy on epoch 4487: 0.889\n",
      "test loss on epoch 4487: 0.334\n",
      "test accuracy on epoch 4487: 0.769\n",
      "train loss on epoch 4488 : 0.197\n",
      "train accuracy on epoch 4488: 0.889\n",
      "test loss on epoch 4488: 0.326\n",
      "test accuracy on epoch 4488: 0.769\n",
      "train loss on epoch 4489 : 0.241\n",
      "train accuracy on epoch 4489: 0.833\n",
      "test loss on epoch 4489: 0.322\n",
      "test accuracy on epoch 4489: 0.769\n",
      "train loss on epoch 4490 : 0.267\n",
      "train accuracy on epoch 4490: 0.833\n",
      "test loss on epoch 4490: 0.314\n",
      "test accuracy on epoch 4490: 0.769\n",
      "train loss on epoch 4491 : 0.150\n",
      "train accuracy on epoch 4491: 0.889\n",
      "test loss on epoch 4491: 0.313\n",
      "test accuracy on epoch 4491: 0.769\n",
      "train loss on epoch 4492 : 0.080\n",
      "train accuracy on epoch 4492: 0.944\n",
      "test loss on epoch 4492: 0.313\n",
      "test accuracy on epoch 4492: 0.769\n",
      "train loss on epoch 4493 : 0.220\n",
      "train accuracy on epoch 4493: 0.944\n",
      "test loss on epoch 4493: 0.303\n",
      "test accuracy on epoch 4493: 0.769\n",
      "train loss on epoch 4494 : 0.029\n",
      "train accuracy on epoch 4494: 1.000\n",
      "test loss on epoch 4494: 0.305\n",
      "test accuracy on epoch 4494: 0.769\n",
      "train loss on epoch 4495 : 0.231\n",
      "train accuracy on epoch 4495: 0.944\n",
      "test loss on epoch 4495: 0.316\n",
      "test accuracy on epoch 4495: 0.769\n",
      "train loss on epoch 4496 : 0.187\n",
      "train accuracy on epoch 4496: 0.944\n",
      "test loss on epoch 4496: 0.318\n",
      "test accuracy on epoch 4496: 0.769\n",
      "train loss on epoch 4497 : 0.301\n",
      "train accuracy on epoch 4497: 0.889\n",
      "test loss on epoch 4497: 0.307\n",
      "test accuracy on epoch 4497: 0.769\n",
      "train loss on epoch 4498 : 0.083\n",
      "train accuracy on epoch 4498: 1.000\n",
      "test loss on epoch 4498: 0.317\n",
      "test accuracy on epoch 4498: 0.769\n",
      "train loss on epoch 4499 : 0.085\n",
      "train accuracy on epoch 4499: 1.000\n",
      "test loss on epoch 4499: 0.313\n",
      "test accuracy on epoch 4499: 0.769\n",
      "train loss on epoch 4500 : 0.230\n",
      "train accuracy on epoch 4500: 0.944\n",
      "test loss on epoch 4500: 0.305\n",
      "test accuracy on epoch 4500: 0.769\n",
      "train loss on epoch 4501 : 0.240\n",
      "train accuracy on epoch 4501: 0.944\n",
      "test loss on epoch 4501: 0.317\n",
      "test accuracy on epoch 4501: 0.769\n",
      "train loss on epoch 4502 : 0.151\n",
      "train accuracy on epoch 4502: 0.944\n",
      "test loss on epoch 4502: 0.307\n",
      "test accuracy on epoch 4502: 0.769\n",
      "train loss on epoch 4503 : 0.426\n",
      "train accuracy on epoch 4503: 0.833\n",
      "test loss on epoch 4503: 0.323\n",
      "test accuracy on epoch 4503: 0.769\n",
      "train loss on epoch 4504 : 0.150\n",
      "train accuracy on epoch 4504: 0.944\n",
      "test loss on epoch 4504: 0.305\n",
      "test accuracy on epoch 4504: 0.769\n",
      "train loss on epoch 4505 : 0.129\n",
      "train accuracy on epoch 4505: 0.944\n",
      "test loss on epoch 4505: 0.308\n",
      "test accuracy on epoch 4505: 0.769\n",
      "train loss on epoch 4506 : 0.251\n",
      "train accuracy on epoch 4506: 0.889\n",
      "test loss on epoch 4506: 0.304\n",
      "test accuracy on epoch 4506: 0.769\n",
      "train loss on epoch 4507 : 0.232\n",
      "train accuracy on epoch 4507: 0.944\n",
      "test loss on epoch 4507: 0.305\n",
      "test accuracy on epoch 4507: 0.692\n",
      "train loss on epoch 4508 : 0.273\n",
      "train accuracy on epoch 4508: 0.944\n",
      "test loss on epoch 4508: 0.307\n",
      "test accuracy on epoch 4508: 0.692\n",
      "train loss on epoch 4509 : 0.147\n",
      "train accuracy on epoch 4509: 0.889\n",
      "test loss on epoch 4509: 0.305\n",
      "test accuracy on epoch 4509: 0.769\n",
      "train loss on epoch 4510 : 0.316\n",
      "train accuracy on epoch 4510: 0.944\n",
      "test loss on epoch 4510: 0.303\n",
      "test accuracy on epoch 4510: 0.846\n",
      "train loss on epoch 4511 : 0.317\n",
      "train accuracy on epoch 4511: 0.889\n",
      "test loss on epoch 4511: 0.307\n",
      "test accuracy on epoch 4511: 0.769\n",
      "train loss on epoch 4512 : 0.138\n",
      "train accuracy on epoch 4512: 0.944\n",
      "test loss on epoch 4512: 0.302\n",
      "test accuracy on epoch 4512: 0.769\n",
      "train loss on epoch 4513 : 0.132\n",
      "train accuracy on epoch 4513: 0.944\n",
      "test loss on epoch 4513: 0.304\n",
      "test accuracy on epoch 4513: 0.769\n",
      "train loss on epoch 4514 : 0.217\n",
      "train accuracy on epoch 4514: 0.889\n",
      "test loss on epoch 4514: 0.312\n",
      "test accuracy on epoch 4514: 0.769\n",
      "train loss on epoch 4515 : 0.163\n",
      "train accuracy on epoch 4515: 0.889\n",
      "test loss on epoch 4515: 0.311\n",
      "test accuracy on epoch 4515: 0.769\n",
      "train loss on epoch 4516 : 0.170\n",
      "train accuracy on epoch 4516: 0.944\n",
      "test loss on epoch 4516: 0.310\n",
      "test accuracy on epoch 4516: 0.769\n",
      "train loss on epoch 4517 : 0.177\n",
      "train accuracy on epoch 4517: 0.889\n",
      "test loss on epoch 4517: 0.308\n",
      "test accuracy on epoch 4517: 0.769\n",
      "train loss on epoch 4518 : 0.317\n",
      "train accuracy on epoch 4518: 0.889\n",
      "test loss on epoch 4518: 0.305\n",
      "test accuracy on epoch 4518: 0.769\n",
      "train loss on epoch 4519 : 0.430\n",
      "train accuracy on epoch 4519: 0.833\n",
      "test loss on epoch 4519: 0.307\n",
      "test accuracy on epoch 4519: 0.769\n",
      "train loss on epoch 4520 : 0.176\n",
      "train accuracy on epoch 4520: 0.889\n",
      "test loss on epoch 4520: 0.314\n",
      "test accuracy on epoch 4520: 0.769\n",
      "train loss on epoch 4521 : 0.151\n",
      "train accuracy on epoch 4521: 0.944\n",
      "test loss on epoch 4521: 0.313\n",
      "test accuracy on epoch 4521: 0.769\n",
      "train loss on epoch 4522 : 0.137\n",
      "train accuracy on epoch 4522: 0.944\n",
      "test loss on epoch 4522: 0.315\n",
      "test accuracy on epoch 4522: 0.769\n",
      "train loss on epoch 4523 : 0.226\n",
      "train accuracy on epoch 4523: 0.889\n",
      "test loss on epoch 4523: 0.311\n",
      "test accuracy on epoch 4523: 0.769\n",
      "train loss on epoch 4524 : 0.193\n",
      "train accuracy on epoch 4524: 0.833\n",
      "test loss on epoch 4524: 0.308\n",
      "test accuracy on epoch 4524: 0.769\n",
      "train loss on epoch 4525 : 0.101\n",
      "train accuracy on epoch 4525: 0.944\n",
      "test loss on epoch 4525: 0.299\n",
      "test accuracy on epoch 4525: 0.769\n",
      "train loss on epoch 4526 : 0.196\n",
      "train accuracy on epoch 4526: 0.944\n",
      "test loss on epoch 4526: 0.305\n",
      "test accuracy on epoch 4526: 0.769\n",
      "train loss on epoch 4527 : 0.073\n",
      "train accuracy on epoch 4527: 1.000\n",
      "test loss on epoch 4527: 0.309\n",
      "test accuracy on epoch 4527: 0.769\n",
      "train loss on epoch 4528 : 0.185\n",
      "train accuracy on epoch 4528: 0.944\n",
      "test loss on epoch 4528: 0.312\n",
      "test accuracy on epoch 4528: 0.769\n",
      "train loss on epoch 4529 : 0.142\n",
      "train accuracy on epoch 4529: 0.944\n",
      "test loss on epoch 4529: 0.312\n",
      "test accuracy on epoch 4529: 0.769\n",
      "train loss on epoch 4530 : 0.264\n",
      "train accuracy on epoch 4530: 0.944\n",
      "test loss on epoch 4530: 0.307\n",
      "test accuracy on epoch 4530: 0.769\n",
      "train loss on epoch 4531 : 0.090\n",
      "train accuracy on epoch 4531: 1.000\n",
      "test loss on epoch 4531: 0.315\n",
      "test accuracy on epoch 4531: 0.769\n",
      "train loss on epoch 4532 : 0.166\n",
      "train accuracy on epoch 4532: 0.889\n",
      "test loss on epoch 4532: 0.306\n",
      "test accuracy on epoch 4532: 0.769\n",
      "train loss on epoch 4533 : 0.137\n",
      "train accuracy on epoch 4533: 0.944\n",
      "test loss on epoch 4533: 0.302\n",
      "test accuracy on epoch 4533: 0.769\n",
      "train loss on epoch 4534 : 0.116\n",
      "train accuracy on epoch 4534: 0.889\n",
      "test loss on epoch 4534: 0.304\n",
      "test accuracy on epoch 4534: 0.769\n",
      "train loss on epoch 4535 : 0.291\n",
      "train accuracy on epoch 4535: 0.944\n",
      "test loss on epoch 4535: 0.313\n",
      "test accuracy on epoch 4535: 0.769\n",
      "train loss on epoch 4536 : 0.108\n",
      "train accuracy on epoch 4536: 0.944\n",
      "test loss on epoch 4536: 0.304\n",
      "test accuracy on epoch 4536: 0.769\n",
      "train loss on epoch 4537 : 0.167\n",
      "train accuracy on epoch 4537: 0.944\n",
      "test loss on epoch 4537: 0.310\n",
      "test accuracy on epoch 4537: 0.769\n",
      "train loss on epoch 4538 : 0.128\n",
      "train accuracy on epoch 4538: 0.944\n",
      "test loss on epoch 4538: 0.306\n",
      "test accuracy on epoch 4538: 0.769\n",
      "train loss on epoch 4539 : 0.220\n",
      "train accuracy on epoch 4539: 0.944\n",
      "test loss on epoch 4539: 0.302\n",
      "test accuracy on epoch 4539: 0.769\n",
      "train loss on epoch 4540 : 0.237\n",
      "train accuracy on epoch 4540: 0.944\n",
      "test loss on epoch 4540: 0.309\n",
      "test accuracy on epoch 4540: 0.769\n",
      "train loss on epoch 4541 : 0.199\n",
      "train accuracy on epoch 4541: 0.944\n",
      "test loss on epoch 4541: 0.310\n",
      "test accuracy on epoch 4541: 0.769\n",
      "train loss on epoch 4542 : 0.076\n",
      "train accuracy on epoch 4542: 0.944\n",
      "test loss on epoch 4542: 0.311\n",
      "test accuracy on epoch 4542: 0.769\n",
      "train loss on epoch 4543 : 0.107\n",
      "train accuracy on epoch 4543: 0.944\n",
      "test loss on epoch 4543: 0.315\n",
      "test accuracy on epoch 4543: 0.769\n",
      "train loss on epoch 4544 : 0.180\n",
      "train accuracy on epoch 4544: 0.889\n",
      "test loss on epoch 4544: 0.314\n",
      "test accuracy on epoch 4544: 0.769\n",
      "train loss on epoch 4545 : 0.261\n",
      "train accuracy on epoch 4545: 0.889\n",
      "test loss on epoch 4545: 0.312\n",
      "test accuracy on epoch 4545: 0.769\n",
      "train loss on epoch 4546 : 0.135\n",
      "train accuracy on epoch 4546: 0.944\n",
      "test loss on epoch 4546: 0.314\n",
      "test accuracy on epoch 4546: 0.769\n",
      "train loss on epoch 4547 : 0.062\n",
      "train accuracy on epoch 4547: 1.000\n",
      "test loss on epoch 4547: 0.315\n",
      "test accuracy on epoch 4547: 0.769\n",
      "train loss on epoch 4548 : 0.078\n",
      "train accuracy on epoch 4548: 1.000\n",
      "test loss on epoch 4548: 0.319\n",
      "test accuracy on epoch 4548: 0.769\n",
      "train loss on epoch 4549 : 0.117\n",
      "train accuracy on epoch 4549: 0.944\n",
      "test loss on epoch 4549: 0.313\n",
      "test accuracy on epoch 4549: 0.769\n",
      "train loss on epoch 4550 : 0.174\n",
      "train accuracy on epoch 4550: 0.944\n",
      "test loss on epoch 4550: 0.311\n",
      "test accuracy on epoch 4550: 0.769\n",
      "train loss on epoch 4551 : 0.133\n",
      "train accuracy on epoch 4551: 0.944\n",
      "test loss on epoch 4551: 0.307\n",
      "test accuracy on epoch 4551: 0.769\n",
      "train loss on epoch 4552 : 0.250\n",
      "train accuracy on epoch 4552: 0.833\n",
      "test loss on epoch 4552: 0.310\n",
      "test accuracy on epoch 4552: 0.769\n",
      "train loss on epoch 4553 : 0.074\n",
      "train accuracy on epoch 4553: 1.000\n",
      "test loss on epoch 4553: 0.311\n",
      "test accuracy on epoch 4553: 0.846\n",
      "train loss on epoch 4554 : 0.059\n",
      "train accuracy on epoch 4554: 1.000\n",
      "test loss on epoch 4554: 0.308\n",
      "test accuracy on epoch 4554: 0.769\n",
      "train loss on epoch 4555 : 0.282\n",
      "train accuracy on epoch 4555: 0.889\n",
      "test loss on epoch 4555: 0.309\n",
      "test accuracy on epoch 4555: 0.769\n",
      "train loss on epoch 4556 : 0.215\n",
      "train accuracy on epoch 4556: 0.889\n",
      "test loss on epoch 4556: 0.308\n",
      "test accuracy on epoch 4556: 0.769\n",
      "train loss on epoch 4557 : 0.086\n",
      "train accuracy on epoch 4557: 0.944\n",
      "test loss on epoch 4557: 0.305\n",
      "test accuracy on epoch 4557: 0.769\n",
      "train loss on epoch 4558 : 0.316\n",
      "train accuracy on epoch 4558: 0.944\n",
      "test loss on epoch 4558: 0.311\n",
      "test accuracy on epoch 4558: 0.769\n",
      "train loss on epoch 4559 : 0.162\n",
      "train accuracy on epoch 4559: 0.889\n",
      "test loss on epoch 4559: 0.314\n",
      "test accuracy on epoch 4559: 0.769\n",
      "train loss on epoch 4560 : 0.133\n",
      "train accuracy on epoch 4560: 0.944\n",
      "test loss on epoch 4560: 0.314\n",
      "test accuracy on epoch 4560: 0.769\n",
      "train loss on epoch 4561 : 0.163\n",
      "train accuracy on epoch 4561: 0.944\n",
      "test loss on epoch 4561: 0.308\n",
      "test accuracy on epoch 4561: 0.769\n",
      "train loss on epoch 4562 : 0.197\n",
      "train accuracy on epoch 4562: 0.944\n",
      "test loss on epoch 4562: 0.307\n",
      "test accuracy on epoch 4562: 0.769\n",
      "train loss on epoch 4563 : 0.168\n",
      "train accuracy on epoch 4563: 0.889\n",
      "test loss on epoch 4563: 0.312\n",
      "test accuracy on epoch 4563: 0.769\n",
      "train loss on epoch 4564 : 0.163\n",
      "train accuracy on epoch 4564: 0.889\n",
      "test loss on epoch 4564: 0.308\n",
      "test accuracy on epoch 4564: 0.769\n",
      "train loss on epoch 4565 : 0.114\n",
      "train accuracy on epoch 4565: 0.889\n",
      "test loss on epoch 4565: 0.309\n",
      "test accuracy on epoch 4565: 0.769\n",
      "train loss on epoch 4566 : 0.105\n",
      "train accuracy on epoch 4566: 1.000\n",
      "test loss on epoch 4566: 0.314\n",
      "test accuracy on epoch 4566: 0.769\n",
      "train loss on epoch 4567 : 0.331\n",
      "train accuracy on epoch 4567: 0.889\n",
      "test loss on epoch 4567: 0.313\n",
      "test accuracy on epoch 4567: 0.769\n",
      "train loss on epoch 4568 : 0.169\n",
      "train accuracy on epoch 4568: 0.889\n",
      "test loss on epoch 4568: 0.308\n",
      "test accuracy on epoch 4568: 0.692\n",
      "train loss on epoch 4569 : 0.157\n",
      "train accuracy on epoch 4569: 0.889\n",
      "test loss on epoch 4569: 0.305\n",
      "test accuracy on epoch 4569: 0.769\n",
      "train loss on epoch 4570 : 0.092\n",
      "train accuracy on epoch 4570: 0.944\n",
      "test loss on epoch 4570: 0.306\n",
      "test accuracy on epoch 4570: 0.769\n",
      "train loss on epoch 4571 : 0.052\n",
      "train accuracy on epoch 4571: 0.944\n",
      "test loss on epoch 4571: 0.305\n",
      "test accuracy on epoch 4571: 0.769\n",
      "train loss on epoch 4572 : 0.122\n",
      "train accuracy on epoch 4572: 0.944\n",
      "test loss on epoch 4572: 0.304\n",
      "test accuracy on epoch 4572: 0.692\n",
      "train loss on epoch 4573 : 0.141\n",
      "train accuracy on epoch 4573: 0.944\n",
      "test loss on epoch 4573: 0.305\n",
      "test accuracy on epoch 4573: 0.769\n",
      "train loss on epoch 4574 : 0.130\n",
      "train accuracy on epoch 4574: 0.944\n",
      "test loss on epoch 4574: 0.311\n",
      "test accuracy on epoch 4574: 0.769\n",
      "train loss on epoch 4575 : 0.139\n",
      "train accuracy on epoch 4575: 0.944\n",
      "test loss on epoch 4575: 0.303\n",
      "test accuracy on epoch 4575: 0.769\n",
      "train loss on epoch 4576 : 0.275\n",
      "train accuracy on epoch 4576: 0.889\n",
      "test loss on epoch 4576: 0.308\n",
      "test accuracy on epoch 4576: 0.769\n",
      "train loss on epoch 4577 : 0.316\n",
      "train accuracy on epoch 4577: 0.889\n",
      "test loss on epoch 4577: 0.312\n",
      "test accuracy on epoch 4577: 0.692\n",
      "train loss on epoch 4578 : 0.191\n",
      "train accuracy on epoch 4578: 0.944\n",
      "test loss on epoch 4578: 0.303\n",
      "test accuracy on epoch 4578: 0.846\n",
      "train loss on epoch 4579 : 0.190\n",
      "train accuracy on epoch 4579: 0.889\n",
      "test loss on epoch 4579: 0.312\n",
      "test accuracy on epoch 4579: 0.769\n",
      "train loss on epoch 4580 : 0.112\n",
      "train accuracy on epoch 4580: 0.944\n",
      "test loss on epoch 4580: 0.304\n",
      "test accuracy on epoch 4580: 0.769\n",
      "train loss on epoch 4581 : 0.267\n",
      "train accuracy on epoch 4581: 0.889\n",
      "test loss on epoch 4581: 0.304\n",
      "test accuracy on epoch 4581: 0.769\n",
      "train loss on epoch 4582 : 0.071\n",
      "train accuracy on epoch 4582: 1.000\n",
      "test loss on epoch 4582: 0.306\n",
      "test accuracy on epoch 4582: 0.769\n",
      "train loss on epoch 4583 : 0.382\n",
      "train accuracy on epoch 4583: 0.778\n",
      "test loss on epoch 4583: 0.313\n",
      "test accuracy on epoch 4583: 0.769\n",
      "train loss on epoch 4584 : 0.225\n",
      "train accuracy on epoch 4584: 0.833\n",
      "test loss on epoch 4584: 0.307\n",
      "test accuracy on epoch 4584: 0.769\n",
      "train loss on epoch 4585 : 0.093\n",
      "train accuracy on epoch 4585: 0.944\n",
      "test loss on epoch 4585: 0.312\n",
      "test accuracy on epoch 4585: 0.769\n",
      "train loss on epoch 4586 : 0.191\n",
      "train accuracy on epoch 4586: 0.944\n",
      "test loss on epoch 4586: 0.311\n",
      "test accuracy on epoch 4586: 0.769\n",
      "train loss on epoch 4587 : 0.110\n",
      "train accuracy on epoch 4587: 0.944\n",
      "test loss on epoch 4587: 0.305\n",
      "test accuracy on epoch 4587: 0.769\n",
      "train loss on epoch 4588 : 0.073\n",
      "train accuracy on epoch 4588: 1.000\n",
      "test loss on epoch 4588: 0.313\n",
      "test accuracy on epoch 4588: 0.769\n",
      "train loss on epoch 4589 : 0.347\n",
      "train accuracy on epoch 4589: 0.944\n",
      "test loss on epoch 4589: 0.307\n",
      "test accuracy on epoch 4589: 0.846\n",
      "train loss on epoch 4590 : 0.277\n",
      "train accuracy on epoch 4590: 0.889\n",
      "test loss on epoch 4590: 0.311\n",
      "test accuracy on epoch 4590: 0.769\n",
      "train loss on epoch 4591 : 0.194\n",
      "train accuracy on epoch 4591: 0.944\n",
      "test loss on epoch 4591: 0.312\n",
      "test accuracy on epoch 4591: 0.769\n",
      "train loss on epoch 4592 : 0.088\n",
      "train accuracy on epoch 4592: 1.000\n",
      "test loss on epoch 4592: 0.310\n",
      "test accuracy on epoch 4592: 0.769\n",
      "train loss on epoch 4593 : 0.154\n",
      "train accuracy on epoch 4593: 0.889\n",
      "test loss on epoch 4593: 0.311\n",
      "test accuracy on epoch 4593: 0.692\n",
      "train loss on epoch 4594 : 0.099\n",
      "train accuracy on epoch 4594: 0.944\n",
      "test loss on epoch 4594: 0.306\n",
      "test accuracy on epoch 4594: 0.846\n",
      "train loss on epoch 4595 : 0.171\n",
      "train accuracy on epoch 4595: 0.944\n",
      "test loss on epoch 4595: 0.304\n",
      "test accuracy on epoch 4595: 0.769\n",
      "train loss on epoch 4596 : 0.392\n",
      "train accuracy on epoch 4596: 0.833\n",
      "test loss on epoch 4596: 0.309\n",
      "test accuracy on epoch 4596: 0.692\n",
      "train loss on epoch 4597 : 0.235\n",
      "train accuracy on epoch 4597: 0.889\n",
      "test loss on epoch 4597: 0.310\n",
      "test accuracy on epoch 4597: 0.769\n",
      "train loss on epoch 4598 : 0.232\n",
      "train accuracy on epoch 4598: 0.889\n",
      "test loss on epoch 4598: 0.309\n",
      "test accuracy on epoch 4598: 0.769\n",
      "train loss on epoch 4599 : 0.053\n",
      "train accuracy on epoch 4599: 1.000\n",
      "test loss on epoch 4599: 0.307\n",
      "test accuracy on epoch 4599: 0.769\n",
      "train loss on epoch 4600 : 0.195\n",
      "train accuracy on epoch 4600: 0.944\n",
      "test loss on epoch 4600: 0.310\n",
      "test accuracy on epoch 4600: 0.769\n",
      "train loss on epoch 4601 : 0.158\n",
      "train accuracy on epoch 4601: 0.944\n",
      "test loss on epoch 4601: 0.314\n",
      "test accuracy on epoch 4601: 0.769\n",
      "train loss on epoch 4602 : 0.330\n",
      "train accuracy on epoch 4602: 0.833\n",
      "test loss on epoch 4602: 0.314\n",
      "test accuracy on epoch 4602: 0.769\n",
      "train loss on epoch 4603 : 0.112\n",
      "train accuracy on epoch 4603: 0.944\n",
      "test loss on epoch 4603: 0.318\n",
      "test accuracy on epoch 4603: 0.769\n",
      "train loss on epoch 4604 : 0.131\n",
      "train accuracy on epoch 4604: 0.889\n",
      "test loss on epoch 4604: 0.316\n",
      "test accuracy on epoch 4604: 0.769\n",
      "train loss on epoch 4605 : 0.136\n",
      "train accuracy on epoch 4605: 0.889\n",
      "test loss on epoch 4605: 0.316\n",
      "test accuracy on epoch 4605: 0.769\n",
      "train loss on epoch 4606 : 0.129\n",
      "train accuracy on epoch 4606: 0.944\n",
      "test loss on epoch 4606: 0.313\n",
      "test accuracy on epoch 4606: 0.769\n",
      "train loss on epoch 4607 : 0.220\n",
      "train accuracy on epoch 4607: 0.889\n",
      "test loss on epoch 4607: 0.317\n",
      "test accuracy on epoch 4607: 0.769\n",
      "train loss on epoch 4608 : 0.293\n",
      "train accuracy on epoch 4608: 0.889\n",
      "test loss on epoch 4608: 0.315\n",
      "test accuracy on epoch 4608: 0.769\n",
      "train loss on epoch 4609 : 0.187\n",
      "train accuracy on epoch 4609: 0.944\n",
      "test loss on epoch 4609: 0.310\n",
      "test accuracy on epoch 4609: 0.769\n",
      "train loss on epoch 4610 : 0.158\n",
      "train accuracy on epoch 4610: 0.889\n",
      "test loss on epoch 4610: 0.307\n",
      "test accuracy on epoch 4610: 0.769\n",
      "train loss on epoch 4611 : 0.048\n",
      "train accuracy on epoch 4611: 1.000\n",
      "test loss on epoch 4611: 0.305\n",
      "test accuracy on epoch 4611: 0.769\n",
      "train loss on epoch 4612 : 0.208\n",
      "train accuracy on epoch 4612: 0.833\n",
      "test loss on epoch 4612: 0.304\n",
      "test accuracy on epoch 4612: 0.846\n",
      "train loss on epoch 4613 : 0.091\n",
      "train accuracy on epoch 4613: 0.944\n",
      "test loss on epoch 4613: 0.304\n",
      "test accuracy on epoch 4613: 0.769\n",
      "train loss on epoch 4614 : 0.068\n",
      "train accuracy on epoch 4614: 0.944\n",
      "test loss on epoch 4614: 0.303\n",
      "test accuracy on epoch 4614: 0.769\n",
      "train loss on epoch 4615 : 0.321\n",
      "train accuracy on epoch 4615: 0.833\n",
      "test loss on epoch 4615: 0.310\n",
      "test accuracy on epoch 4615: 0.769\n",
      "train loss on epoch 4616 : 0.220\n",
      "train accuracy on epoch 4616: 0.889\n",
      "test loss on epoch 4616: 0.307\n",
      "test accuracy on epoch 4616: 0.769\n",
      "train loss on epoch 4617 : 0.071\n",
      "train accuracy on epoch 4617: 1.000\n",
      "test loss on epoch 4617: 0.307\n",
      "test accuracy on epoch 4617: 0.769\n",
      "train loss on epoch 4618 : 0.144\n",
      "train accuracy on epoch 4618: 0.944\n",
      "test loss on epoch 4618: 0.314\n",
      "test accuracy on epoch 4618: 0.769\n",
      "train loss on epoch 4619 : 0.085\n",
      "train accuracy on epoch 4619: 1.000\n",
      "test loss on epoch 4619: 0.304\n",
      "test accuracy on epoch 4619: 0.769\n",
      "train loss on epoch 4620 : 0.147\n",
      "train accuracy on epoch 4620: 0.944\n",
      "test loss on epoch 4620: 0.302\n",
      "test accuracy on epoch 4620: 0.769\n",
      "train loss on epoch 4621 : 0.179\n",
      "train accuracy on epoch 4621: 0.944\n",
      "test loss on epoch 4621: 0.314\n",
      "test accuracy on epoch 4621: 0.769\n",
      "train loss on epoch 4622 : 0.126\n",
      "train accuracy on epoch 4622: 0.944\n",
      "test loss on epoch 4622: 0.302\n",
      "test accuracy on epoch 4622: 0.769\n",
      "train loss on epoch 4623 : 0.218\n",
      "train accuracy on epoch 4623: 0.944\n",
      "test loss on epoch 4623: 0.308\n",
      "test accuracy on epoch 4623: 0.769\n",
      "train loss on epoch 4624 : 0.031\n",
      "train accuracy on epoch 4624: 1.000\n",
      "test loss on epoch 4624: 0.303\n",
      "test accuracy on epoch 4624: 0.769\n",
      "train loss on epoch 4625 : 0.084\n",
      "train accuracy on epoch 4625: 1.000\n",
      "test loss on epoch 4625: 0.309\n",
      "test accuracy on epoch 4625: 0.769\n",
      "train loss on epoch 4626 : 0.428\n",
      "train accuracy on epoch 4626: 0.889\n",
      "test loss on epoch 4626: 0.310\n",
      "test accuracy on epoch 4626: 0.769\n",
      "train loss on epoch 4627 : 0.064\n",
      "train accuracy on epoch 4627: 0.944\n",
      "test loss on epoch 4627: 0.307\n",
      "test accuracy on epoch 4627: 0.769\n",
      "train loss on epoch 4628 : 0.158\n",
      "train accuracy on epoch 4628: 0.889\n",
      "test loss on epoch 4628: 0.314\n",
      "test accuracy on epoch 4628: 0.769\n",
      "train loss on epoch 4629 : 0.101\n",
      "train accuracy on epoch 4629: 1.000\n",
      "test loss on epoch 4629: 0.306\n",
      "test accuracy on epoch 4629: 0.769\n",
      "train loss on epoch 4630 : 0.067\n",
      "train accuracy on epoch 4630: 1.000\n",
      "test loss on epoch 4630: 0.316\n",
      "test accuracy on epoch 4630: 0.769\n",
      "train loss on epoch 4631 : 0.079\n",
      "train accuracy on epoch 4631: 0.944\n",
      "test loss on epoch 4631: 0.307\n",
      "test accuracy on epoch 4631: 0.769\n",
      "train loss on epoch 4632 : 0.097\n",
      "train accuracy on epoch 4632: 0.944\n",
      "test loss on epoch 4632: 0.313\n",
      "test accuracy on epoch 4632: 0.769\n",
      "train loss on epoch 4633 : 0.050\n",
      "train accuracy on epoch 4633: 1.000\n",
      "test loss on epoch 4633: 0.307\n",
      "test accuracy on epoch 4633: 0.769\n",
      "train loss on epoch 4634 : 0.164\n",
      "train accuracy on epoch 4634: 0.944\n",
      "test loss on epoch 4634: 0.315\n",
      "test accuracy on epoch 4634: 0.769\n",
      "train loss on epoch 4635 : 0.175\n",
      "train accuracy on epoch 4635: 0.944\n",
      "test loss on epoch 4635: 0.308\n",
      "test accuracy on epoch 4635: 0.846\n",
      "train loss on epoch 4636 : 0.250\n",
      "train accuracy on epoch 4636: 0.889\n",
      "test loss on epoch 4636: 0.316\n",
      "test accuracy on epoch 4636: 0.692\n",
      "train loss on epoch 4637 : 0.112\n",
      "train accuracy on epoch 4637: 0.944\n",
      "test loss on epoch 4637: 0.304\n",
      "test accuracy on epoch 4637: 0.846\n",
      "train loss on epoch 4638 : 0.248\n",
      "train accuracy on epoch 4638: 0.889\n",
      "test loss on epoch 4638: 0.304\n",
      "test accuracy on epoch 4638: 0.846\n",
      "train loss on epoch 4639 : 0.176\n",
      "train accuracy on epoch 4639: 0.944\n",
      "test loss on epoch 4639: 0.309\n",
      "test accuracy on epoch 4639: 0.769\n",
      "train loss on epoch 4640 : 0.184\n",
      "train accuracy on epoch 4640: 0.944\n",
      "test loss on epoch 4640: 0.315\n",
      "test accuracy on epoch 4640: 0.769\n",
      "train loss on epoch 4641 : 0.112\n",
      "train accuracy on epoch 4641: 0.944\n",
      "test loss on epoch 4641: 0.318\n",
      "test accuracy on epoch 4641: 0.769\n",
      "train loss on epoch 4642 : 0.218\n",
      "train accuracy on epoch 4642: 0.833\n",
      "test loss on epoch 4642: 0.315\n",
      "test accuracy on epoch 4642: 0.769\n",
      "train loss on epoch 4643 : 0.184\n",
      "train accuracy on epoch 4643: 0.889\n",
      "test loss on epoch 4643: 0.306\n",
      "test accuracy on epoch 4643: 0.846\n",
      "train loss on epoch 4644 : 0.063\n",
      "train accuracy on epoch 4644: 1.000\n",
      "test loss on epoch 4644: 0.306\n",
      "test accuracy on epoch 4644: 0.769\n",
      "train loss on epoch 4645 : 0.250\n",
      "train accuracy on epoch 4645: 0.889\n",
      "test loss on epoch 4645: 0.312\n",
      "test accuracy on epoch 4645: 0.769\n",
      "train loss on epoch 4646 : 0.189\n",
      "train accuracy on epoch 4646: 0.944\n",
      "test loss on epoch 4646: 0.315\n",
      "test accuracy on epoch 4646: 0.769\n",
      "train loss on epoch 4647 : 0.197\n",
      "train accuracy on epoch 4647: 0.889\n",
      "test loss on epoch 4647: 0.319\n",
      "test accuracy on epoch 4647: 0.769\n",
      "train loss on epoch 4648 : 0.230\n",
      "train accuracy on epoch 4648: 0.889\n",
      "test loss on epoch 4648: 0.309\n",
      "test accuracy on epoch 4648: 0.769\n",
      "train loss on epoch 4649 : 0.445\n",
      "train accuracy on epoch 4649: 0.833\n",
      "test loss on epoch 4649: 0.318\n",
      "test accuracy on epoch 4649: 0.769\n",
      "train loss on epoch 4650 : 0.271\n",
      "train accuracy on epoch 4650: 0.889\n",
      "test loss on epoch 4650: 0.311\n",
      "test accuracy on epoch 4650: 0.769\n",
      "train loss on epoch 4651 : 0.043\n",
      "train accuracy on epoch 4651: 1.000\n",
      "test loss on epoch 4651: 0.319\n",
      "test accuracy on epoch 4651: 0.769\n",
      "train loss on epoch 4652 : 0.135\n",
      "train accuracy on epoch 4652: 0.944\n",
      "test loss on epoch 4652: 0.310\n",
      "test accuracy on epoch 4652: 0.769\n",
      "train loss on epoch 4653 : 0.160\n",
      "train accuracy on epoch 4653: 0.944\n",
      "test loss on epoch 4653: 0.314\n",
      "test accuracy on epoch 4653: 0.692\n",
      "train loss on epoch 4654 : 0.190\n",
      "train accuracy on epoch 4654: 0.889\n",
      "test loss on epoch 4654: 0.317\n",
      "test accuracy on epoch 4654: 0.769\n",
      "train loss on epoch 4655 : 0.035\n",
      "train accuracy on epoch 4655: 1.000\n",
      "test loss on epoch 4655: 0.315\n",
      "test accuracy on epoch 4655: 0.769\n",
      "train loss on epoch 4656 : 0.364\n",
      "train accuracy on epoch 4656: 0.889\n",
      "test loss on epoch 4656: 0.319\n",
      "test accuracy on epoch 4656: 0.769\n",
      "train loss on epoch 4657 : 0.225\n",
      "train accuracy on epoch 4657: 0.944\n",
      "test loss on epoch 4657: 0.323\n",
      "test accuracy on epoch 4657: 0.769\n",
      "train loss on epoch 4658 : 0.200\n",
      "train accuracy on epoch 4658: 0.944\n",
      "test loss on epoch 4658: 0.320\n",
      "test accuracy on epoch 4658: 0.769\n",
      "train loss on epoch 4659 : 0.203\n",
      "train accuracy on epoch 4659: 0.944\n",
      "test loss on epoch 4659: 0.320\n",
      "test accuracy on epoch 4659: 0.769\n",
      "train loss on epoch 4660 : 0.104\n",
      "train accuracy on epoch 4660: 1.000\n",
      "test loss on epoch 4660: 0.319\n",
      "test accuracy on epoch 4660: 0.769\n",
      "train loss on epoch 4661 : 0.167\n",
      "train accuracy on epoch 4661: 0.944\n",
      "test loss on epoch 4661: 0.315\n",
      "test accuracy on epoch 4661: 0.769\n",
      "train loss on epoch 4662 : 0.215\n",
      "train accuracy on epoch 4662: 0.889\n",
      "test loss on epoch 4662: 0.315\n",
      "test accuracy on epoch 4662: 0.769\n",
      "train loss on epoch 4663 : 0.077\n",
      "train accuracy on epoch 4663: 1.000\n",
      "test loss on epoch 4663: 0.313\n",
      "test accuracy on epoch 4663: 0.769\n",
      "train loss on epoch 4664 : 0.272\n",
      "train accuracy on epoch 4664: 0.833\n",
      "test loss on epoch 4664: 0.311\n",
      "test accuracy on epoch 4664: 0.769\n",
      "train loss on epoch 4665 : 0.130\n",
      "train accuracy on epoch 4665: 0.944\n",
      "test loss on epoch 4665: 0.308\n",
      "test accuracy on epoch 4665: 0.769\n",
      "train loss on epoch 4666 : 0.172\n",
      "train accuracy on epoch 4666: 0.889\n",
      "test loss on epoch 4666: 0.302\n",
      "test accuracy on epoch 4666: 0.769\n",
      "train loss on epoch 4667 : 0.209\n",
      "train accuracy on epoch 4667: 0.889\n",
      "test loss on epoch 4667: 0.310\n",
      "test accuracy on epoch 4667: 0.769\n",
      "train loss on epoch 4668 : 0.180\n",
      "train accuracy on epoch 4668: 0.944\n",
      "test loss on epoch 4668: 0.313\n",
      "test accuracy on epoch 4668: 0.769\n",
      "train loss on epoch 4669 : 0.257\n",
      "train accuracy on epoch 4669: 0.889\n",
      "test loss on epoch 4669: 0.308\n",
      "test accuracy on epoch 4669: 0.769\n",
      "train loss on epoch 4670 : 0.218\n",
      "train accuracy on epoch 4670: 0.944\n",
      "test loss on epoch 4670: 0.309\n",
      "test accuracy on epoch 4670: 0.769\n",
      "train loss on epoch 4671 : 0.046\n",
      "train accuracy on epoch 4671: 1.000\n",
      "test loss on epoch 4671: 0.305\n",
      "test accuracy on epoch 4671: 0.769\n",
      "train loss on epoch 4672 : 0.143\n",
      "train accuracy on epoch 4672: 0.944\n",
      "test loss on epoch 4672: 0.303\n",
      "test accuracy on epoch 4672: 0.769\n",
      "train loss on epoch 4673 : 0.331\n",
      "train accuracy on epoch 4673: 0.944\n",
      "test loss on epoch 4673: 0.306\n",
      "test accuracy on epoch 4673: 0.846\n",
      "train loss on epoch 4674 : 0.259\n",
      "train accuracy on epoch 4674: 0.889\n",
      "test loss on epoch 4674: 0.312\n",
      "test accuracy on epoch 4674: 0.769\n",
      "train loss on epoch 4675 : 0.122\n",
      "train accuracy on epoch 4675: 0.944\n",
      "test loss on epoch 4675: 0.315\n",
      "test accuracy on epoch 4675: 0.769\n",
      "train loss on epoch 4676 : 0.168\n",
      "train accuracy on epoch 4676: 0.889\n",
      "test loss on epoch 4676: 0.307\n",
      "test accuracy on epoch 4676: 0.769\n",
      "train loss on epoch 4677 : 0.378\n",
      "train accuracy on epoch 4677: 0.833\n",
      "test loss on epoch 4677: 0.316\n",
      "test accuracy on epoch 4677: 0.769\n",
      "train loss on epoch 4678 : 0.193\n",
      "train accuracy on epoch 4678: 0.889\n",
      "test loss on epoch 4678: 0.310\n",
      "test accuracy on epoch 4678: 0.769\n",
      "train loss on epoch 4679 : 0.147\n",
      "train accuracy on epoch 4679: 0.944\n",
      "test loss on epoch 4679: 0.309\n",
      "test accuracy on epoch 4679: 0.769\n",
      "train loss on epoch 4680 : 0.140\n",
      "train accuracy on epoch 4680: 0.889\n",
      "test loss on epoch 4680: 0.319\n",
      "test accuracy on epoch 4680: 0.769\n",
      "train loss on epoch 4681 : 0.073\n",
      "train accuracy on epoch 4681: 1.000\n",
      "test loss on epoch 4681: 0.315\n",
      "test accuracy on epoch 4681: 0.769\n",
      "train loss on epoch 4682 : 0.243\n",
      "train accuracy on epoch 4682: 0.889\n",
      "test loss on epoch 4682: 0.321\n",
      "test accuracy on epoch 4682: 0.769\n",
      "train loss on epoch 4683 : 0.384\n",
      "train accuracy on epoch 4683: 0.833\n",
      "test loss on epoch 4683: 0.320\n",
      "test accuracy on epoch 4683: 0.769\n",
      "train loss on epoch 4684 : 0.177\n",
      "train accuracy on epoch 4684: 0.944\n",
      "test loss on epoch 4684: 0.318\n",
      "test accuracy on epoch 4684: 0.769\n",
      "train loss on epoch 4685 : 0.055\n",
      "train accuracy on epoch 4685: 1.000\n",
      "test loss on epoch 4685: 0.316\n",
      "test accuracy on epoch 4685: 0.769\n",
      "train loss on epoch 4686 : 0.099\n",
      "train accuracy on epoch 4686: 0.944\n",
      "test loss on epoch 4686: 0.319\n",
      "test accuracy on epoch 4686: 0.769\n",
      "train loss on epoch 4687 : 0.179\n",
      "train accuracy on epoch 4687: 0.944\n",
      "test loss on epoch 4687: 0.319\n",
      "test accuracy on epoch 4687: 0.769\n",
      "train loss on epoch 4688 : 0.319\n",
      "train accuracy on epoch 4688: 0.889\n",
      "test loss on epoch 4688: 0.318\n",
      "test accuracy on epoch 4688: 0.769\n",
      "train loss on epoch 4689 : 0.056\n",
      "train accuracy on epoch 4689: 0.944\n",
      "test loss on epoch 4689: 0.317\n",
      "test accuracy on epoch 4689: 0.769\n",
      "train loss on epoch 4690 : 0.144\n",
      "train accuracy on epoch 4690: 0.889\n",
      "test loss on epoch 4690: 0.314\n",
      "test accuracy on epoch 4690: 0.769\n",
      "train loss on epoch 4691 : 0.220\n",
      "train accuracy on epoch 4691: 0.889\n",
      "test loss on epoch 4691: 0.309\n",
      "test accuracy on epoch 4691: 0.769\n",
      "train loss on epoch 4692 : 0.447\n",
      "train accuracy on epoch 4692: 0.889\n",
      "test loss on epoch 4692: 0.314\n",
      "test accuracy on epoch 4692: 0.769\n",
      "train loss on epoch 4693 : 0.101\n",
      "train accuracy on epoch 4693: 0.944\n",
      "test loss on epoch 4693: 0.313\n",
      "test accuracy on epoch 4693: 0.769\n",
      "train loss on epoch 4694 : 0.100\n",
      "train accuracy on epoch 4694: 0.944\n",
      "test loss on epoch 4694: 0.311\n",
      "test accuracy on epoch 4694: 0.769\n",
      "train loss on epoch 4695 : 0.093\n",
      "train accuracy on epoch 4695: 1.000\n",
      "test loss on epoch 4695: 0.315\n",
      "test accuracy on epoch 4695: 0.769\n",
      "train loss on epoch 4696 : 0.063\n",
      "train accuracy on epoch 4696: 1.000\n",
      "test loss on epoch 4696: 0.315\n",
      "test accuracy on epoch 4696: 0.769\n",
      "train loss on epoch 4697 : 0.262\n",
      "train accuracy on epoch 4697: 0.833\n",
      "test loss on epoch 4697: 0.316\n",
      "test accuracy on epoch 4697: 0.769\n",
      "train loss on epoch 4698 : 0.148\n",
      "train accuracy on epoch 4698: 0.944\n",
      "test loss on epoch 4698: 0.316\n",
      "test accuracy on epoch 4698: 0.769\n",
      "train loss on epoch 4699 : 0.055\n",
      "train accuracy on epoch 4699: 1.000\n",
      "test loss on epoch 4699: 0.314\n",
      "test accuracy on epoch 4699: 0.769\n",
      "train loss on epoch 4700 : 0.074\n",
      "train accuracy on epoch 4700: 1.000\n",
      "test loss on epoch 4700: 0.314\n",
      "test accuracy on epoch 4700: 0.769\n",
      "train loss on epoch 4701 : 0.295\n",
      "train accuracy on epoch 4701: 0.889\n",
      "test loss on epoch 4701: 0.321\n",
      "test accuracy on epoch 4701: 0.769\n",
      "train loss on epoch 4702 : 0.098\n",
      "train accuracy on epoch 4702: 0.944\n",
      "test loss on epoch 4702: 0.321\n",
      "test accuracy on epoch 4702: 0.769\n",
      "train loss on epoch 4703 : 0.164\n",
      "train accuracy on epoch 4703: 0.944\n",
      "test loss on epoch 4703: 0.314\n",
      "test accuracy on epoch 4703: 0.769\n",
      "train loss on epoch 4704 : 0.130\n",
      "train accuracy on epoch 4704: 1.000\n",
      "test loss on epoch 4704: 0.313\n",
      "test accuracy on epoch 4704: 0.769\n",
      "train loss on epoch 4705 : 0.030\n",
      "train accuracy on epoch 4705: 1.000\n",
      "test loss on epoch 4705: 0.312\n",
      "test accuracy on epoch 4705: 0.769\n",
      "train loss on epoch 4706 : 0.291\n",
      "train accuracy on epoch 4706: 0.833\n",
      "test loss on epoch 4706: 0.322\n",
      "test accuracy on epoch 4706: 0.769\n",
      "train loss on epoch 4707 : 0.213\n",
      "train accuracy on epoch 4707: 0.889\n",
      "test loss on epoch 4707: 0.324\n",
      "test accuracy on epoch 4707: 0.769\n",
      "train loss on epoch 4708 : 0.062\n",
      "train accuracy on epoch 4708: 0.944\n",
      "test loss on epoch 4708: 0.317\n",
      "test accuracy on epoch 4708: 0.769\n",
      "train loss on epoch 4709 : 0.130\n",
      "train accuracy on epoch 4709: 0.944\n",
      "test loss on epoch 4709: 0.315\n",
      "test accuracy on epoch 4709: 0.769\n",
      "train loss on epoch 4710 : 0.248\n",
      "train accuracy on epoch 4710: 0.833\n",
      "test loss on epoch 4710: 0.318\n",
      "test accuracy on epoch 4710: 0.769\n",
      "train loss on epoch 4711 : 0.183\n",
      "train accuracy on epoch 4711: 0.944\n",
      "test loss on epoch 4711: 0.313\n",
      "test accuracy on epoch 4711: 0.769\n",
      "train loss on epoch 4712 : 0.322\n",
      "train accuracy on epoch 4712: 0.889\n",
      "test loss on epoch 4712: 0.315\n",
      "test accuracy on epoch 4712: 0.769\n",
      "train loss on epoch 4713 : 0.413\n",
      "train accuracy on epoch 4713: 0.889\n",
      "test loss on epoch 4713: 0.318\n",
      "test accuracy on epoch 4713: 0.769\n",
      "train loss on epoch 4714 : 0.395\n",
      "train accuracy on epoch 4714: 0.889\n",
      "test loss on epoch 4714: 0.316\n",
      "test accuracy on epoch 4714: 0.769\n",
      "train loss on epoch 4715 : 0.077\n",
      "train accuracy on epoch 4715: 1.000\n",
      "test loss on epoch 4715: 0.313\n",
      "test accuracy on epoch 4715: 0.769\n",
      "train loss on epoch 4716 : 0.382\n",
      "train accuracy on epoch 4716: 0.944\n",
      "test loss on epoch 4716: 0.317\n",
      "test accuracy on epoch 4716: 0.769\n",
      "train loss on epoch 4717 : 0.052\n",
      "train accuracy on epoch 4717: 1.000\n",
      "test loss on epoch 4717: 0.323\n",
      "test accuracy on epoch 4717: 0.769\n",
      "train loss on epoch 4718 : 0.218\n",
      "train accuracy on epoch 4718: 0.889\n",
      "test loss on epoch 4718: 0.313\n",
      "test accuracy on epoch 4718: 0.769\n",
      "train loss on epoch 4719 : 0.212\n",
      "train accuracy on epoch 4719: 0.889\n",
      "test loss on epoch 4719: 0.320\n",
      "test accuracy on epoch 4719: 0.769\n",
      "train loss on epoch 4720 : 0.079\n",
      "train accuracy on epoch 4720: 0.944\n",
      "test loss on epoch 4720: 0.317\n",
      "test accuracy on epoch 4720: 0.769\n",
      "train loss on epoch 4721 : 0.059\n",
      "train accuracy on epoch 4721: 1.000\n",
      "test loss on epoch 4721: 0.319\n",
      "test accuracy on epoch 4721: 0.769\n",
      "train loss on epoch 4722 : 0.132\n",
      "train accuracy on epoch 4722: 0.944\n",
      "test loss on epoch 4722: 0.320\n",
      "test accuracy on epoch 4722: 0.769\n",
      "train loss on epoch 4723 : 0.211\n",
      "train accuracy on epoch 4723: 0.889\n",
      "test loss on epoch 4723: 0.319\n",
      "test accuracy on epoch 4723: 0.769\n",
      "train loss on epoch 4724 : 0.164\n",
      "train accuracy on epoch 4724: 0.944\n",
      "test loss on epoch 4724: 0.320\n",
      "test accuracy on epoch 4724: 0.769\n",
      "train loss on epoch 4725 : 0.357\n",
      "train accuracy on epoch 4725: 0.944\n",
      "test loss on epoch 4725: 0.319\n",
      "test accuracy on epoch 4725: 0.769\n",
      "train loss on epoch 4726 : 0.232\n",
      "train accuracy on epoch 4726: 0.944\n",
      "test loss on epoch 4726: 0.317\n",
      "test accuracy on epoch 4726: 0.769\n",
      "train loss on epoch 4727 : 0.333\n",
      "train accuracy on epoch 4727: 0.889\n",
      "test loss on epoch 4727: 0.315\n",
      "test accuracy on epoch 4727: 0.769\n",
      "train loss on epoch 4728 : 0.163\n",
      "train accuracy on epoch 4728: 0.944\n",
      "test loss on epoch 4728: 0.324\n",
      "test accuracy on epoch 4728: 0.769\n",
      "train loss on epoch 4729 : 0.111\n",
      "train accuracy on epoch 4729: 0.944\n",
      "test loss on epoch 4729: 0.320\n",
      "test accuracy on epoch 4729: 0.769\n",
      "train loss on epoch 4730 : 0.157\n",
      "train accuracy on epoch 4730: 0.944\n",
      "test loss on epoch 4730: 0.319\n",
      "test accuracy on epoch 4730: 0.769\n",
      "train loss on epoch 4731 : 0.338\n",
      "train accuracy on epoch 4731: 0.833\n",
      "test loss on epoch 4731: 0.317\n",
      "test accuracy on epoch 4731: 0.769\n",
      "train loss on epoch 4732 : 0.145\n",
      "train accuracy on epoch 4732: 0.944\n",
      "test loss on epoch 4732: 0.316\n",
      "test accuracy on epoch 4732: 0.769\n",
      "train loss on epoch 4733 : 0.156\n",
      "train accuracy on epoch 4733: 0.889\n",
      "test loss on epoch 4733: 0.311\n",
      "test accuracy on epoch 4733: 0.769\n",
      "train loss on epoch 4734 : 0.203\n",
      "train accuracy on epoch 4734: 0.944\n",
      "test loss on epoch 4734: 0.321\n",
      "test accuracy on epoch 4734: 0.769\n",
      "train loss on epoch 4735 : 0.153\n",
      "train accuracy on epoch 4735: 0.944\n",
      "test loss on epoch 4735: 0.327\n",
      "test accuracy on epoch 4735: 0.769\n",
      "train loss on epoch 4736 : 0.313\n",
      "train accuracy on epoch 4736: 0.889\n",
      "test loss on epoch 4736: 0.317\n",
      "test accuracy on epoch 4736: 0.769\n",
      "train loss on epoch 4737 : 0.180\n",
      "train accuracy on epoch 4737: 0.889\n",
      "test loss on epoch 4737: 0.312\n",
      "test accuracy on epoch 4737: 0.769\n",
      "train loss on epoch 4738 : 0.316\n",
      "train accuracy on epoch 4738: 0.833\n",
      "test loss on epoch 4738: 0.324\n",
      "test accuracy on epoch 4738: 0.769\n",
      "train loss on epoch 4739 : 0.313\n",
      "train accuracy on epoch 4739: 0.889\n",
      "test loss on epoch 4739: 0.323\n",
      "test accuracy on epoch 4739: 0.769\n",
      "train loss on epoch 4740 : 0.202\n",
      "train accuracy on epoch 4740: 0.889\n",
      "test loss on epoch 4740: 0.314\n",
      "test accuracy on epoch 4740: 0.769\n",
      "train loss on epoch 4741 : 0.143\n",
      "train accuracy on epoch 4741: 0.944\n",
      "test loss on epoch 4741: 0.315\n",
      "test accuracy on epoch 4741: 0.846\n",
      "train loss on epoch 4742 : 0.087\n",
      "train accuracy on epoch 4742: 0.944\n",
      "test loss on epoch 4742: 0.316\n",
      "test accuracy on epoch 4742: 0.846\n",
      "train loss on epoch 4743 : 0.111\n",
      "train accuracy on epoch 4743: 0.889\n",
      "test loss on epoch 4743: 0.317\n",
      "test accuracy on epoch 4743: 0.769\n",
      "train loss on epoch 4744 : 0.152\n",
      "train accuracy on epoch 4744: 0.944\n",
      "test loss on epoch 4744: 0.317\n",
      "test accuracy on epoch 4744: 0.769\n",
      "train loss on epoch 4745 : 0.313\n",
      "train accuracy on epoch 4745: 0.889\n",
      "test loss on epoch 4745: 0.316\n",
      "test accuracy on epoch 4745: 0.769\n",
      "train loss on epoch 4746 : 0.197\n",
      "train accuracy on epoch 4746: 0.889\n",
      "test loss on epoch 4746: 0.317\n",
      "test accuracy on epoch 4746: 0.769\n",
      "train loss on epoch 4747 : 0.051\n",
      "train accuracy on epoch 4747: 1.000\n",
      "test loss on epoch 4747: 0.317\n",
      "test accuracy on epoch 4747: 0.769\n",
      "train loss on epoch 4748 : 0.211\n",
      "train accuracy on epoch 4748: 0.944\n",
      "test loss on epoch 4748: 0.316\n",
      "test accuracy on epoch 4748: 0.769\n",
      "train loss on epoch 4749 : 0.138\n",
      "train accuracy on epoch 4749: 0.889\n",
      "test loss on epoch 4749: 0.315\n",
      "test accuracy on epoch 4749: 0.769\n",
      "train loss on epoch 4750 : 0.013\n",
      "train accuracy on epoch 4750: 1.000\n",
      "test loss on epoch 4750: 0.320\n",
      "test accuracy on epoch 4750: 0.692\n",
      "train loss on epoch 4751 : 0.324\n",
      "train accuracy on epoch 4751: 0.944\n",
      "test loss on epoch 4751: 0.315\n",
      "test accuracy on epoch 4751: 0.769\n",
      "train loss on epoch 4752 : 0.183\n",
      "train accuracy on epoch 4752: 0.944\n",
      "test loss on epoch 4752: 0.316\n",
      "test accuracy on epoch 4752: 0.769\n",
      "train loss on epoch 4753 : 0.089\n",
      "train accuracy on epoch 4753: 1.000\n",
      "test loss on epoch 4753: 0.326\n",
      "test accuracy on epoch 4753: 0.769\n",
      "train loss on epoch 4754 : 0.122\n",
      "train accuracy on epoch 4754: 0.944\n",
      "test loss on epoch 4754: 0.322\n",
      "test accuracy on epoch 4754: 0.769\n",
      "train loss on epoch 4755 : 0.255\n",
      "train accuracy on epoch 4755: 0.889\n",
      "test loss on epoch 4755: 0.326\n",
      "test accuracy on epoch 4755: 0.769\n",
      "train loss on epoch 4756 : 0.119\n",
      "train accuracy on epoch 4756: 0.944\n",
      "test loss on epoch 4756: 0.325\n",
      "test accuracy on epoch 4756: 0.769\n",
      "train loss on epoch 4757 : 0.269\n",
      "train accuracy on epoch 4757: 0.833\n",
      "test loss on epoch 4757: 0.320\n",
      "test accuracy on epoch 4757: 0.769\n",
      "train loss on epoch 4758 : 0.222\n",
      "train accuracy on epoch 4758: 0.889\n",
      "test loss on epoch 4758: 0.320\n",
      "test accuracy on epoch 4758: 0.769\n",
      "train loss on epoch 4759 : 0.110\n",
      "train accuracy on epoch 4759: 0.944\n",
      "test loss on epoch 4759: 0.321\n",
      "test accuracy on epoch 4759: 0.769\n",
      "train loss on epoch 4760 : 0.050\n",
      "train accuracy on epoch 4760: 1.000\n",
      "test loss on epoch 4760: 0.325\n",
      "test accuracy on epoch 4760: 0.769\n",
      "train loss on epoch 4761 : 0.155\n",
      "train accuracy on epoch 4761: 0.944\n",
      "test loss on epoch 4761: 0.314\n",
      "test accuracy on epoch 4761: 0.769\n",
      "train loss on epoch 4762 : 0.087\n",
      "train accuracy on epoch 4762: 1.000\n",
      "test loss on epoch 4762: 0.323\n",
      "test accuracy on epoch 4762: 0.692\n",
      "train loss on epoch 4763 : 0.174\n",
      "train accuracy on epoch 4763: 0.944\n",
      "test loss on epoch 4763: 0.312\n",
      "test accuracy on epoch 4763: 0.769\n",
      "train loss on epoch 4764 : 0.101\n",
      "train accuracy on epoch 4764: 0.889\n",
      "test loss on epoch 4764: 0.315\n",
      "test accuracy on epoch 4764: 0.769\n",
      "train loss on epoch 4765 : 0.056\n",
      "train accuracy on epoch 4765: 1.000\n",
      "test loss on epoch 4765: 0.322\n",
      "test accuracy on epoch 4765: 0.769\n",
      "train loss on epoch 4766 : 0.128\n",
      "train accuracy on epoch 4766: 0.944\n",
      "test loss on epoch 4766: 0.320\n",
      "test accuracy on epoch 4766: 0.769\n",
      "train loss on epoch 4767 : 0.111\n",
      "train accuracy on epoch 4767: 0.944\n",
      "test loss on epoch 4767: 0.312\n",
      "test accuracy on epoch 4767: 0.769\n",
      "train loss on epoch 4768 : 0.241\n",
      "train accuracy on epoch 4768: 0.889\n",
      "test loss on epoch 4768: 0.312\n",
      "test accuracy on epoch 4768: 0.769\n",
      "train loss on epoch 4769 : 0.156\n",
      "train accuracy on epoch 4769: 0.889\n",
      "test loss on epoch 4769: 0.312\n",
      "test accuracy on epoch 4769: 0.846\n",
      "train loss on epoch 4770 : 0.054\n",
      "train accuracy on epoch 4770: 1.000\n",
      "test loss on epoch 4770: 0.320\n",
      "test accuracy on epoch 4770: 0.769\n",
      "train loss on epoch 4771 : 0.268\n",
      "train accuracy on epoch 4771: 0.889\n",
      "test loss on epoch 4771: 0.316\n",
      "test accuracy on epoch 4771: 0.769\n",
      "train loss on epoch 4772 : 0.096\n",
      "train accuracy on epoch 4772: 1.000\n",
      "test loss on epoch 4772: 0.312\n",
      "test accuracy on epoch 4772: 0.769\n",
      "train loss on epoch 4773 : 0.224\n",
      "train accuracy on epoch 4773: 0.944\n",
      "test loss on epoch 4773: 0.311\n",
      "test accuracy on epoch 4773: 0.769\n",
      "train loss on epoch 4774 : 0.272\n",
      "train accuracy on epoch 4774: 0.944\n",
      "test loss on epoch 4774: 0.310\n",
      "test accuracy on epoch 4774: 0.769\n",
      "train loss on epoch 4775 : 0.218\n",
      "train accuracy on epoch 4775: 0.889\n",
      "test loss on epoch 4775: 0.309\n",
      "test accuracy on epoch 4775: 0.769\n",
      "train loss on epoch 4776 : 0.086\n",
      "train accuracy on epoch 4776: 0.944\n",
      "test loss on epoch 4776: 0.309\n",
      "test accuracy on epoch 4776: 0.769\n",
      "train loss on epoch 4777 : 0.459\n",
      "train accuracy on epoch 4777: 0.889\n",
      "test loss on epoch 4777: 0.317\n",
      "test accuracy on epoch 4777: 0.769\n",
      "train loss on epoch 4778 : 0.197\n",
      "train accuracy on epoch 4778: 0.889\n",
      "test loss on epoch 4778: 0.310\n",
      "test accuracy on epoch 4778: 0.769\n",
      "train loss on epoch 4779 : 0.224\n",
      "train accuracy on epoch 4779: 0.889\n",
      "test loss on epoch 4779: 0.329\n",
      "test accuracy on epoch 4779: 0.769\n",
      "train loss on epoch 4780 : 0.137\n",
      "train accuracy on epoch 4780: 0.944\n",
      "test loss on epoch 4780: 0.310\n",
      "test accuracy on epoch 4780: 0.769\n",
      "train loss on epoch 4781 : 0.176\n",
      "train accuracy on epoch 4781: 0.889\n",
      "test loss on epoch 4781: 0.328\n",
      "test accuracy on epoch 4781: 0.769\n",
      "train loss on epoch 4782 : 0.210\n",
      "train accuracy on epoch 4782: 0.944\n",
      "test loss on epoch 4782: 0.329\n",
      "test accuracy on epoch 4782: 0.769\n",
      "train loss on epoch 4783 : 0.246\n",
      "train accuracy on epoch 4783: 0.833\n",
      "test loss on epoch 4783: 0.328\n",
      "test accuracy on epoch 4783: 0.769\n",
      "train loss on epoch 4784 : 0.423\n",
      "train accuracy on epoch 4784: 0.833\n",
      "test loss on epoch 4784: 0.315\n",
      "test accuracy on epoch 4784: 0.769\n",
      "train loss on epoch 4785 : 0.091\n",
      "train accuracy on epoch 4785: 0.944\n",
      "test loss on epoch 4785: 0.315\n",
      "test accuracy on epoch 4785: 0.769\n",
      "train loss on epoch 4786 : 0.111\n",
      "train accuracy on epoch 4786: 0.944\n",
      "test loss on epoch 4786: 0.334\n",
      "test accuracy on epoch 4786: 0.769\n",
      "train loss on epoch 4787 : 0.037\n",
      "train accuracy on epoch 4787: 1.000\n",
      "test loss on epoch 4787: 0.320\n",
      "test accuracy on epoch 4787: 0.769\n",
      "train loss on epoch 4788 : 0.234\n",
      "train accuracy on epoch 4788: 0.944\n",
      "test loss on epoch 4788: 0.331\n",
      "test accuracy on epoch 4788: 0.769\n",
      "train loss on epoch 4789 : 0.199\n",
      "train accuracy on epoch 4789: 0.944\n",
      "test loss on epoch 4789: 0.324\n",
      "test accuracy on epoch 4789: 0.769\n",
      "train loss on epoch 4790 : 0.055\n",
      "train accuracy on epoch 4790: 1.000\n",
      "test loss on epoch 4790: 0.313\n",
      "test accuracy on epoch 4790: 0.769\n",
      "train loss on epoch 4791 : 0.211\n",
      "train accuracy on epoch 4791: 0.889\n",
      "test loss on epoch 4791: 0.315\n",
      "test accuracy on epoch 4791: 0.769\n",
      "train loss on epoch 4792 : 0.218\n",
      "train accuracy on epoch 4792: 0.889\n",
      "test loss on epoch 4792: 0.329\n",
      "test accuracy on epoch 4792: 0.769\n",
      "train loss on epoch 4793 : 0.299\n",
      "train accuracy on epoch 4793: 0.833\n",
      "test loss on epoch 4793: 0.317\n",
      "test accuracy on epoch 4793: 0.769\n",
      "train loss on epoch 4794 : 0.295\n",
      "train accuracy on epoch 4794: 0.833\n",
      "test loss on epoch 4794: 0.324\n",
      "test accuracy on epoch 4794: 0.769\n",
      "train loss on epoch 4795 : 0.257\n",
      "train accuracy on epoch 4795: 0.944\n",
      "test loss on epoch 4795: 0.333\n",
      "test accuracy on epoch 4795: 0.769\n",
      "train loss on epoch 4796 : 0.267\n",
      "train accuracy on epoch 4796: 0.833\n",
      "test loss on epoch 4796: 0.317\n",
      "test accuracy on epoch 4796: 0.846\n",
      "train loss on epoch 4797 : 0.294\n",
      "train accuracy on epoch 4797: 0.944\n",
      "test loss on epoch 4797: 0.334\n",
      "test accuracy on epoch 4797: 0.692\n",
      "train loss on epoch 4798 : 0.154\n",
      "train accuracy on epoch 4798: 0.944\n",
      "test loss on epoch 4798: 0.332\n",
      "test accuracy on epoch 4798: 0.692\n",
      "train loss on epoch 4799 : 0.275\n",
      "train accuracy on epoch 4799: 0.833\n",
      "test loss on epoch 4799: 0.315\n",
      "test accuracy on epoch 4799: 0.769\n",
      "train loss on epoch 4800 : 0.389\n",
      "train accuracy on epoch 4800: 0.833\n",
      "test loss on epoch 4800: 0.313\n",
      "test accuracy on epoch 4800: 0.846\n",
      "train loss on epoch 4801 : 0.055\n",
      "train accuracy on epoch 4801: 1.000\n",
      "test loss on epoch 4801: 0.316\n",
      "test accuracy on epoch 4801: 0.846\n",
      "train loss on epoch 4802 : 0.194\n",
      "train accuracy on epoch 4802: 0.944\n",
      "test loss on epoch 4802: 0.327\n",
      "test accuracy on epoch 4802: 0.769\n",
      "train loss on epoch 4803 : 0.142\n",
      "train accuracy on epoch 4803: 1.000\n",
      "test loss on epoch 4803: 0.326\n",
      "test accuracy on epoch 4803: 0.769\n",
      "train loss on epoch 4804 : 0.126\n",
      "train accuracy on epoch 4804: 0.944\n",
      "test loss on epoch 4804: 0.335\n",
      "test accuracy on epoch 4804: 0.769\n",
      "train loss on epoch 4805 : 0.139\n",
      "train accuracy on epoch 4805: 0.944\n",
      "test loss on epoch 4805: 0.315\n",
      "test accuracy on epoch 4805: 0.769\n",
      "train loss on epoch 4806 : 0.172\n",
      "train accuracy on epoch 4806: 0.889\n",
      "test loss on epoch 4806: 0.328\n",
      "test accuracy on epoch 4806: 0.769\n",
      "train loss on epoch 4807 : 0.112\n",
      "train accuracy on epoch 4807: 0.944\n",
      "test loss on epoch 4807: 0.330\n",
      "test accuracy on epoch 4807: 0.769\n",
      "train loss on epoch 4808 : 0.239\n",
      "train accuracy on epoch 4808: 0.944\n",
      "test loss on epoch 4808: 0.315\n",
      "test accuracy on epoch 4808: 0.769\n",
      "train loss on epoch 4809 : 0.186\n",
      "train accuracy on epoch 4809: 0.889\n",
      "test loss on epoch 4809: 0.312\n",
      "test accuracy on epoch 4809: 0.846\n",
      "train loss on epoch 4810 : 0.066\n",
      "train accuracy on epoch 4810: 1.000\n",
      "test loss on epoch 4810: 0.322\n",
      "test accuracy on epoch 4810: 0.692\n",
      "train loss on epoch 4811 : 0.214\n",
      "train accuracy on epoch 4811: 0.889\n",
      "test loss on epoch 4811: 0.310\n",
      "test accuracy on epoch 4811: 0.769\n",
      "train loss on epoch 4812 : 0.192\n",
      "train accuracy on epoch 4812: 0.889\n",
      "test loss on epoch 4812: 0.313\n",
      "test accuracy on epoch 4812: 0.769\n",
      "train loss on epoch 4813 : 0.174\n",
      "train accuracy on epoch 4813: 0.944\n",
      "test loss on epoch 4813: 0.316\n",
      "test accuracy on epoch 4813: 0.769\n",
      "train loss on epoch 4814 : 0.211\n",
      "train accuracy on epoch 4814: 0.944\n",
      "test loss on epoch 4814: 0.325\n",
      "test accuracy on epoch 4814: 0.769\n",
      "train loss on epoch 4815 : 0.229\n",
      "train accuracy on epoch 4815: 0.944\n",
      "test loss on epoch 4815: 0.315\n",
      "test accuracy on epoch 4815: 0.769\n",
      "train loss on epoch 4816 : 0.238\n",
      "train accuracy on epoch 4816: 0.944\n",
      "test loss on epoch 4816: 0.322\n",
      "test accuracy on epoch 4816: 0.692\n",
      "train loss on epoch 4817 : 0.083\n",
      "train accuracy on epoch 4817: 0.944\n",
      "test loss on epoch 4817: 0.310\n",
      "test accuracy on epoch 4817: 0.769\n",
      "train loss on epoch 4818 : 0.156\n",
      "train accuracy on epoch 4818: 0.944\n",
      "test loss on epoch 4818: 0.305\n",
      "test accuracy on epoch 4818: 0.769\n",
      "train loss on epoch 4819 : 0.126\n",
      "train accuracy on epoch 4819: 0.889\n",
      "test loss on epoch 4819: 0.306\n",
      "test accuracy on epoch 4819: 0.769\n",
      "train loss on epoch 4820 : 0.162\n",
      "train accuracy on epoch 4820: 0.889\n",
      "test loss on epoch 4820: 0.312\n",
      "test accuracy on epoch 4820: 0.769\n",
      "train loss on epoch 4821 : 0.324\n",
      "train accuracy on epoch 4821: 0.889\n",
      "test loss on epoch 4821: 0.323\n",
      "test accuracy on epoch 4821: 0.769\n",
      "train loss on epoch 4822 : 0.086\n",
      "train accuracy on epoch 4822: 1.000\n",
      "test loss on epoch 4822: 0.318\n",
      "test accuracy on epoch 4822: 0.769\n",
      "train loss on epoch 4823 : 0.166\n",
      "train accuracy on epoch 4823: 0.944\n",
      "test loss on epoch 4823: 0.325\n",
      "test accuracy on epoch 4823: 0.769\n",
      "train loss on epoch 4824 : 0.051\n",
      "train accuracy on epoch 4824: 1.000\n",
      "test loss on epoch 4824: 0.317\n",
      "test accuracy on epoch 4824: 0.769\n",
      "train loss on epoch 4825 : 0.231\n",
      "train accuracy on epoch 4825: 0.889\n",
      "test loss on epoch 4825: 0.316\n",
      "test accuracy on epoch 4825: 0.769\n",
      "train loss on epoch 4826 : 0.233\n",
      "train accuracy on epoch 4826: 0.833\n",
      "test loss on epoch 4826: 0.321\n",
      "test accuracy on epoch 4826: 0.692\n",
      "train loss on epoch 4827 : 0.125\n",
      "train accuracy on epoch 4827: 0.944\n",
      "test loss on epoch 4827: 0.321\n",
      "test accuracy on epoch 4827: 0.769\n",
      "train loss on epoch 4828 : 0.211\n",
      "train accuracy on epoch 4828: 0.833\n",
      "test loss on epoch 4828: 0.324\n",
      "test accuracy on epoch 4828: 0.769\n",
      "train loss on epoch 4829 : 0.075\n",
      "train accuracy on epoch 4829: 0.944\n",
      "test loss on epoch 4829: 0.305\n",
      "test accuracy on epoch 4829: 0.769\n",
      "train loss on epoch 4830 : 0.279\n",
      "train accuracy on epoch 4830: 0.944\n",
      "test loss on epoch 4830: 0.303\n",
      "test accuracy on epoch 4830: 0.769\n",
      "train loss on epoch 4831 : 0.210\n",
      "train accuracy on epoch 4831: 0.889\n",
      "test loss on epoch 4831: 0.318\n",
      "test accuracy on epoch 4831: 0.769\n",
      "train loss on epoch 4832 : 0.125\n",
      "train accuracy on epoch 4832: 0.944\n",
      "test loss on epoch 4832: 0.303\n",
      "test accuracy on epoch 4832: 0.769\n",
      "train loss on epoch 4833 : 0.291\n",
      "train accuracy on epoch 4833: 0.889\n",
      "test loss on epoch 4833: 0.319\n",
      "test accuracy on epoch 4833: 0.769\n",
      "train loss on epoch 4834 : 0.064\n",
      "train accuracy on epoch 4834: 1.000\n",
      "test loss on epoch 4834: 0.308\n",
      "test accuracy on epoch 4834: 0.769\n",
      "train loss on epoch 4835 : 0.080\n",
      "train accuracy on epoch 4835: 1.000\n",
      "test loss on epoch 4835: 0.321\n",
      "test accuracy on epoch 4835: 0.769\n",
      "train loss on epoch 4836 : 0.051\n",
      "train accuracy on epoch 4836: 1.000\n",
      "test loss on epoch 4836: 0.301\n",
      "test accuracy on epoch 4836: 0.769\n",
      "train loss on epoch 4837 : 0.350\n",
      "train accuracy on epoch 4837: 0.944\n",
      "test loss on epoch 4837: 0.313\n",
      "test accuracy on epoch 4837: 0.769\n",
      "train loss on epoch 4838 : 0.058\n",
      "train accuracy on epoch 4838: 1.000\n",
      "test loss on epoch 4838: 0.319\n",
      "test accuracy on epoch 4838: 0.769\n",
      "train loss on epoch 4839 : 0.271\n",
      "train accuracy on epoch 4839: 0.833\n",
      "test loss on epoch 4839: 0.302\n",
      "test accuracy on epoch 4839: 0.769\n",
      "train loss on epoch 4840 : 0.232\n",
      "train accuracy on epoch 4840: 0.944\n",
      "test loss on epoch 4840: 0.318\n",
      "test accuracy on epoch 4840: 0.769\n",
      "train loss on epoch 4841 : 0.183\n",
      "train accuracy on epoch 4841: 0.889\n",
      "test loss on epoch 4841: 0.308\n",
      "test accuracy on epoch 4841: 0.769\n",
      "train loss on epoch 4842 : 0.295\n",
      "train accuracy on epoch 4842: 0.833\n",
      "test loss on epoch 4842: 0.310\n",
      "test accuracy on epoch 4842: 0.769\n",
      "train loss on epoch 4843 : 0.109\n",
      "train accuracy on epoch 4843: 0.944\n",
      "test loss on epoch 4843: 0.310\n",
      "test accuracy on epoch 4843: 0.769\n",
      "train loss on epoch 4844 : 0.247\n",
      "train accuracy on epoch 4844: 0.944\n",
      "test loss on epoch 4844: 0.307\n",
      "test accuracy on epoch 4844: 0.769\n",
      "train loss on epoch 4845 : 0.178\n",
      "train accuracy on epoch 4845: 0.833\n",
      "test loss on epoch 4845: 0.306\n",
      "test accuracy on epoch 4845: 0.769\n",
      "train loss on epoch 4846 : 0.240\n",
      "train accuracy on epoch 4846: 0.944\n",
      "test loss on epoch 4846: 0.314\n",
      "test accuracy on epoch 4846: 0.769\n",
      "train loss on epoch 4847 : 0.064\n",
      "train accuracy on epoch 4847: 1.000\n",
      "test loss on epoch 4847: 0.315\n",
      "test accuracy on epoch 4847: 0.769\n",
      "train loss on epoch 4848 : 0.088\n",
      "train accuracy on epoch 4848: 0.944\n",
      "test loss on epoch 4848: 0.318\n",
      "test accuracy on epoch 4848: 0.769\n",
      "train loss on epoch 4849 : 0.296\n",
      "train accuracy on epoch 4849: 0.833\n",
      "test loss on epoch 4849: 0.322\n",
      "test accuracy on epoch 4849: 0.769\n",
      "train loss on epoch 4850 : 0.296\n",
      "train accuracy on epoch 4850: 0.889\n",
      "test loss on epoch 4850: 0.322\n",
      "test accuracy on epoch 4850: 0.769\n",
      "train loss on epoch 4851 : 0.061\n",
      "train accuracy on epoch 4851: 1.000\n",
      "test loss on epoch 4851: 0.321\n",
      "test accuracy on epoch 4851: 0.769\n",
      "train loss on epoch 4852 : 0.343\n",
      "train accuracy on epoch 4852: 0.833\n",
      "test loss on epoch 4852: 0.315\n",
      "test accuracy on epoch 4852: 0.769\n",
      "train loss on epoch 4853 : 0.110\n",
      "train accuracy on epoch 4853: 0.944\n",
      "test loss on epoch 4853: 0.324\n",
      "test accuracy on epoch 4853: 0.769\n",
      "train loss on epoch 4854 : 0.206\n",
      "train accuracy on epoch 4854: 0.944\n",
      "test loss on epoch 4854: 0.318\n",
      "test accuracy on epoch 4854: 0.769\n",
      "train loss on epoch 4855 : 0.173\n",
      "train accuracy on epoch 4855: 0.944\n",
      "test loss on epoch 4855: 0.307\n",
      "test accuracy on epoch 4855: 0.769\n",
      "train loss on epoch 4856 : 0.247\n",
      "train accuracy on epoch 4856: 0.889\n",
      "test loss on epoch 4856: 0.318\n",
      "test accuracy on epoch 4856: 0.692\n",
      "train loss on epoch 4857 : 0.103\n",
      "train accuracy on epoch 4857: 0.944\n",
      "test loss on epoch 4857: 0.318\n",
      "test accuracy on epoch 4857: 0.769\n",
      "train loss on epoch 4858 : 0.166\n",
      "train accuracy on epoch 4858: 0.889\n",
      "test loss on epoch 4858: 0.307\n",
      "test accuracy on epoch 4858: 0.769\n",
      "train loss on epoch 4859 : 0.204\n",
      "train accuracy on epoch 4859: 0.889\n",
      "test loss on epoch 4859: 0.315\n",
      "test accuracy on epoch 4859: 0.769\n",
      "train loss on epoch 4860 : 0.154\n",
      "train accuracy on epoch 4860: 0.944\n",
      "test loss on epoch 4860: 0.320\n",
      "test accuracy on epoch 4860: 0.769\n",
      "train loss on epoch 4861 : 0.334\n",
      "train accuracy on epoch 4861: 0.889\n",
      "test loss on epoch 4861: 0.315\n",
      "test accuracy on epoch 4861: 0.769\n",
      "train loss on epoch 4862 : 0.259\n",
      "train accuracy on epoch 4862: 0.889\n",
      "test loss on epoch 4862: 0.320\n",
      "test accuracy on epoch 4862: 0.769\n",
      "train loss on epoch 4863 : 0.128\n",
      "train accuracy on epoch 4863: 0.944\n",
      "test loss on epoch 4863: 0.303\n",
      "test accuracy on epoch 4863: 0.769\n",
      "train loss on epoch 4864 : 0.028\n",
      "train accuracy on epoch 4864: 1.000\n",
      "test loss on epoch 4864: 0.320\n",
      "test accuracy on epoch 4864: 0.769\n",
      "train loss on epoch 4865 : 0.198\n",
      "train accuracy on epoch 4865: 0.944\n",
      "test loss on epoch 4865: 0.307\n",
      "test accuracy on epoch 4865: 0.769\n",
      "train loss on epoch 4866 : 0.310\n",
      "train accuracy on epoch 4866: 0.944\n",
      "test loss on epoch 4866: 0.302\n",
      "test accuracy on epoch 4866: 0.769\n",
      "train loss on epoch 4867 : 0.088\n",
      "train accuracy on epoch 4867: 0.944\n",
      "test loss on epoch 4867: 0.311\n",
      "test accuracy on epoch 4867: 0.769\n",
      "train loss on epoch 4868 : 0.210\n",
      "train accuracy on epoch 4868: 0.889\n",
      "test loss on epoch 4868: 0.315\n",
      "test accuracy on epoch 4868: 0.769\n",
      "train loss on epoch 4869 : 0.222\n",
      "train accuracy on epoch 4869: 0.944\n",
      "test loss on epoch 4869: 0.306\n",
      "test accuracy on epoch 4869: 0.769\n",
      "train loss on epoch 4870 : 0.145\n",
      "train accuracy on epoch 4870: 0.889\n",
      "test loss on epoch 4870: 0.318\n",
      "test accuracy on epoch 4870: 0.692\n",
      "train loss on epoch 4871 : 0.107\n",
      "train accuracy on epoch 4871: 0.944\n",
      "test loss on epoch 4871: 0.321\n",
      "test accuracy on epoch 4871: 0.769\n",
      "train loss on epoch 4872 : 0.203\n",
      "train accuracy on epoch 4872: 0.889\n",
      "test loss on epoch 4872: 0.303\n",
      "test accuracy on epoch 4872: 0.769\n",
      "train loss on epoch 4873 : 0.184\n",
      "train accuracy on epoch 4873: 0.889\n",
      "test loss on epoch 4873: 0.305\n",
      "test accuracy on epoch 4873: 0.769\n",
      "train loss on epoch 4874 : 0.066\n",
      "train accuracy on epoch 4874: 1.000\n",
      "test loss on epoch 4874: 0.310\n",
      "test accuracy on epoch 4874: 0.846\n",
      "train loss on epoch 4875 : 0.106\n",
      "train accuracy on epoch 4875: 0.944\n",
      "test loss on epoch 4875: 0.327\n",
      "test accuracy on epoch 4875: 0.769\n",
      "train loss on epoch 4876 : 0.156\n",
      "train accuracy on epoch 4876: 0.944\n",
      "test loss on epoch 4876: 0.322\n",
      "test accuracy on epoch 4876: 0.769\n",
      "train loss on epoch 4877 : 0.279\n",
      "train accuracy on epoch 4877: 0.889\n",
      "test loss on epoch 4877: 0.316\n",
      "test accuracy on epoch 4877: 0.769\n",
      "train loss on epoch 4878 : 0.230\n",
      "train accuracy on epoch 4878: 0.889\n",
      "test loss on epoch 4878: 0.307\n",
      "test accuracy on epoch 4878: 0.846\n",
      "train loss on epoch 4879 : 0.139\n",
      "train accuracy on epoch 4879: 0.944\n",
      "test loss on epoch 4879: 0.312\n",
      "test accuracy on epoch 4879: 0.769\n",
      "train loss on epoch 4880 : 0.232\n",
      "train accuracy on epoch 4880: 0.889\n",
      "test loss on epoch 4880: 0.305\n",
      "test accuracy on epoch 4880: 0.769\n",
      "train loss on epoch 4881 : 0.270\n",
      "train accuracy on epoch 4881: 0.833\n",
      "test loss on epoch 4881: 0.312\n",
      "test accuracy on epoch 4881: 0.769\n",
      "train loss on epoch 4882 : 0.279\n",
      "train accuracy on epoch 4882: 0.944\n",
      "test loss on epoch 4882: 0.316\n",
      "test accuracy on epoch 4882: 0.769\n",
      "train loss on epoch 4883 : 0.091\n",
      "train accuracy on epoch 4883: 0.944\n",
      "test loss on epoch 4883: 0.324\n",
      "test accuracy on epoch 4883: 0.769\n",
      "train loss on epoch 4884 : 0.142\n",
      "train accuracy on epoch 4884: 0.889\n",
      "test loss on epoch 4884: 0.325\n",
      "test accuracy on epoch 4884: 0.769\n",
      "train loss on epoch 4885 : 0.066\n",
      "train accuracy on epoch 4885: 1.000\n",
      "test loss on epoch 4885: 0.321\n",
      "test accuracy on epoch 4885: 0.769\n",
      "train loss on epoch 4886 : 0.179\n",
      "train accuracy on epoch 4886: 0.944\n",
      "test loss on epoch 4886: 0.309\n",
      "test accuracy on epoch 4886: 0.769\n",
      "train loss on epoch 4887 : 0.083\n",
      "train accuracy on epoch 4887: 1.000\n",
      "test loss on epoch 4887: 0.314\n",
      "test accuracy on epoch 4887: 0.769\n",
      "train loss on epoch 4888 : 0.144\n",
      "train accuracy on epoch 4888: 0.889\n",
      "test loss on epoch 4888: 0.321\n",
      "test accuracy on epoch 4888: 0.692\n",
      "train loss on epoch 4889 : 0.196\n",
      "train accuracy on epoch 4889: 0.944\n",
      "test loss on epoch 4889: 0.315\n",
      "test accuracy on epoch 4889: 0.769\n",
      "train loss on epoch 4890 : 0.258\n",
      "train accuracy on epoch 4890: 0.889\n",
      "test loss on epoch 4890: 0.304\n",
      "test accuracy on epoch 4890: 0.769\n",
      "train loss on epoch 4891 : 0.069\n",
      "train accuracy on epoch 4891: 1.000\n",
      "test loss on epoch 4891: 0.321\n",
      "test accuracy on epoch 4891: 0.769\n",
      "train loss on epoch 4892 : 0.070\n",
      "train accuracy on epoch 4892: 0.944\n",
      "test loss on epoch 4892: 0.308\n",
      "test accuracy on epoch 4892: 0.769\n",
      "train loss on epoch 4893 : 0.417\n",
      "train accuracy on epoch 4893: 0.833\n",
      "test loss on epoch 4893: 0.324\n",
      "test accuracy on epoch 4893: 0.769\n",
      "train loss on epoch 4894 : 0.085\n",
      "train accuracy on epoch 4894: 1.000\n",
      "test loss on epoch 4894: 0.323\n",
      "test accuracy on epoch 4894: 0.769\n",
      "train loss on epoch 4895 : 0.020\n",
      "train accuracy on epoch 4895: 1.000\n",
      "test loss on epoch 4895: 0.325\n",
      "test accuracy on epoch 4895: 0.769\n",
      "train loss on epoch 4896 : 0.170\n",
      "train accuracy on epoch 4896: 0.889\n",
      "test loss on epoch 4896: 0.305\n",
      "test accuracy on epoch 4896: 0.769\n",
      "train loss on epoch 4897 : 0.136\n",
      "train accuracy on epoch 4897: 0.944\n",
      "test loss on epoch 4897: 0.318\n",
      "test accuracy on epoch 4897: 0.769\n",
      "train loss on epoch 4898 : 0.405\n",
      "train accuracy on epoch 4898: 0.944\n",
      "test loss on epoch 4898: 0.323\n",
      "test accuracy on epoch 4898: 0.769\n",
      "train loss on epoch 4899 : 0.140\n",
      "train accuracy on epoch 4899: 0.889\n",
      "test loss on epoch 4899: 0.312\n",
      "test accuracy on epoch 4899: 0.769\n",
      "train loss on epoch 4900 : 0.339\n",
      "train accuracy on epoch 4900: 0.889\n",
      "test loss on epoch 4900: 0.325\n",
      "test accuracy on epoch 4900: 0.769\n",
      "train loss on epoch 4901 : 0.225\n",
      "train accuracy on epoch 4901: 0.944\n",
      "test loss on epoch 4901: 0.311\n",
      "test accuracy on epoch 4901: 0.769\n",
      "train loss on epoch 4902 : 0.055\n",
      "train accuracy on epoch 4902: 1.000\n",
      "test loss on epoch 4902: 0.321\n",
      "test accuracy on epoch 4902: 0.769\n",
      "train loss on epoch 4903 : 0.122\n",
      "train accuracy on epoch 4903: 0.944\n",
      "test loss on epoch 4903: 0.328\n",
      "test accuracy on epoch 4903: 0.692\n",
      "train loss on epoch 4904 : 0.446\n",
      "train accuracy on epoch 4904: 0.833\n",
      "test loss on epoch 4904: 0.318\n",
      "test accuracy on epoch 4904: 0.769\n",
      "train loss on epoch 4905 : 0.148\n",
      "train accuracy on epoch 4905: 0.944\n",
      "test loss on epoch 4905: 0.328\n",
      "test accuracy on epoch 4905: 0.769\n",
      "train loss on epoch 4906 : 0.210\n",
      "train accuracy on epoch 4906: 0.889\n",
      "test loss on epoch 4906: 0.321\n",
      "test accuracy on epoch 4906: 0.769\n",
      "train loss on epoch 4907 : 0.296\n",
      "train accuracy on epoch 4907: 0.889\n",
      "test loss on epoch 4907: 0.328\n",
      "test accuracy on epoch 4907: 0.769\n",
      "train loss on epoch 4908 : 0.231\n",
      "train accuracy on epoch 4908: 0.944\n",
      "test loss on epoch 4908: 0.330\n",
      "test accuracy on epoch 4908: 0.769\n",
      "train loss on epoch 4909 : 0.120\n",
      "train accuracy on epoch 4909: 0.944\n",
      "test loss on epoch 4909: 0.333\n",
      "test accuracy on epoch 4909: 0.769\n",
      "train loss on epoch 4910 : 0.153\n",
      "train accuracy on epoch 4910: 0.944\n",
      "test loss on epoch 4910: 0.327\n",
      "test accuracy on epoch 4910: 0.769\n",
      "train loss on epoch 4911 : 0.500\n",
      "train accuracy on epoch 4911: 0.889\n",
      "test loss on epoch 4911: 0.331\n",
      "test accuracy on epoch 4911: 0.769\n",
      "train loss on epoch 4912 : 0.337\n",
      "train accuracy on epoch 4912: 0.889\n",
      "test loss on epoch 4912: 0.316\n",
      "test accuracy on epoch 4912: 0.769\n",
      "train loss on epoch 4913 : 0.099\n",
      "train accuracy on epoch 4913: 0.944\n",
      "test loss on epoch 4913: 0.320\n",
      "test accuracy on epoch 4913: 0.769\n",
      "train loss on epoch 4914 : 0.248\n",
      "train accuracy on epoch 4914: 0.889\n",
      "test loss on epoch 4914: 0.316\n",
      "test accuracy on epoch 4914: 0.769\n",
      "train loss on epoch 4915 : 0.031\n",
      "train accuracy on epoch 4915: 1.000\n",
      "test loss on epoch 4915: 0.305\n",
      "test accuracy on epoch 4915: 0.769\n",
      "train loss on epoch 4916 : 0.084\n",
      "train accuracy on epoch 4916: 0.944\n",
      "test loss on epoch 4916: 0.311\n",
      "test accuracy on epoch 4916: 0.769\n",
      "train loss on epoch 4917 : 0.146\n",
      "train accuracy on epoch 4917: 0.944\n",
      "test loss on epoch 4917: 0.320\n",
      "test accuracy on epoch 4917: 0.769\n",
      "train loss on epoch 4918 : 0.080\n",
      "train accuracy on epoch 4918: 0.944\n",
      "test loss on epoch 4918: 0.317\n",
      "test accuracy on epoch 4918: 0.769\n",
      "train loss on epoch 4919 : 0.270\n",
      "train accuracy on epoch 4919: 0.944\n",
      "test loss on epoch 4919: 0.316\n",
      "test accuracy on epoch 4919: 0.769\n",
      "train loss on epoch 4920 : 0.164\n",
      "train accuracy on epoch 4920: 0.889\n",
      "test loss on epoch 4920: 0.302\n",
      "test accuracy on epoch 4920: 0.769\n",
      "train loss on epoch 4921 : 0.315\n",
      "train accuracy on epoch 4921: 0.889\n",
      "test loss on epoch 4921: 0.303\n",
      "test accuracy on epoch 4921: 0.769\n",
      "train loss on epoch 4922 : 0.143\n",
      "train accuracy on epoch 4922: 0.944\n",
      "test loss on epoch 4922: 0.305\n",
      "test accuracy on epoch 4922: 0.769\n",
      "train loss on epoch 4923 : 0.147\n",
      "train accuracy on epoch 4923: 0.944\n",
      "test loss on epoch 4923: 0.312\n",
      "test accuracy on epoch 4923: 0.769\n",
      "train loss on epoch 4924 : 0.064\n",
      "train accuracy on epoch 4924: 1.000\n",
      "test loss on epoch 4924: 0.316\n",
      "test accuracy on epoch 4924: 0.769\n",
      "train loss on epoch 4925 : 0.343\n",
      "train accuracy on epoch 4925: 0.889\n",
      "test loss on epoch 4925: 0.308\n",
      "test accuracy on epoch 4925: 0.846\n",
      "train loss on epoch 4926 : 0.097\n",
      "train accuracy on epoch 4926: 0.944\n",
      "test loss on epoch 4926: 0.326\n",
      "test accuracy on epoch 4926: 0.692\n",
      "train loss on epoch 4927 : 0.502\n",
      "train accuracy on epoch 4927: 0.889\n",
      "test loss on epoch 4927: 0.325\n",
      "test accuracy on epoch 4927: 0.692\n",
      "train loss on epoch 4928 : 0.089\n",
      "train accuracy on epoch 4928: 1.000\n",
      "test loss on epoch 4928: 0.328\n",
      "test accuracy on epoch 4928: 0.692\n",
      "train loss on epoch 4929 : 0.134\n",
      "train accuracy on epoch 4929: 0.944\n",
      "test loss on epoch 4929: 0.307\n",
      "test accuracy on epoch 4929: 0.769\n",
      "train loss on epoch 4930 : 0.130\n",
      "train accuracy on epoch 4930: 0.944\n",
      "test loss on epoch 4930: 0.319\n",
      "test accuracy on epoch 4930: 0.769\n",
      "train loss on epoch 4931 : 0.077\n",
      "train accuracy on epoch 4931: 0.944\n",
      "test loss on epoch 4931: 0.327\n",
      "test accuracy on epoch 4931: 0.769\n",
      "train loss on epoch 4932 : 0.266\n",
      "train accuracy on epoch 4932: 0.833\n",
      "test loss on epoch 4932: 0.310\n",
      "test accuracy on epoch 4932: 0.846\n",
      "train loss on epoch 4933 : 0.433\n",
      "train accuracy on epoch 4933: 0.889\n",
      "test loss on epoch 4933: 0.319\n",
      "test accuracy on epoch 4933: 0.769\n",
      "train loss on epoch 4934 : 0.088\n",
      "train accuracy on epoch 4934: 0.944\n",
      "test loss on epoch 4934: 0.320\n",
      "test accuracy on epoch 4934: 0.769\n",
      "train loss on epoch 4935 : 0.106\n",
      "train accuracy on epoch 4935: 0.944\n",
      "test loss on epoch 4935: 0.318\n",
      "test accuracy on epoch 4935: 0.769\n",
      "train loss on epoch 4936 : 0.176\n",
      "train accuracy on epoch 4936: 0.889\n",
      "test loss on epoch 4936: 0.310\n",
      "test accuracy on epoch 4936: 0.846\n",
      "train loss on epoch 4937 : 0.058\n",
      "train accuracy on epoch 4937: 1.000\n",
      "test loss on epoch 4937: 0.328\n",
      "test accuracy on epoch 4937: 0.692\n",
      "train loss on epoch 4938 : 0.132\n",
      "train accuracy on epoch 4938: 0.944\n",
      "test loss on epoch 4938: 0.329\n",
      "test accuracy on epoch 4938: 0.769\n",
      "train loss on epoch 4939 : 0.213\n",
      "train accuracy on epoch 4939: 0.944\n",
      "test loss on epoch 4939: 0.329\n",
      "test accuracy on epoch 4939: 0.769\n",
      "train loss on epoch 4940 : 0.215\n",
      "train accuracy on epoch 4940: 0.889\n",
      "test loss on epoch 4940: 0.311\n",
      "test accuracy on epoch 4940: 0.769\n",
      "train loss on epoch 4941 : 0.452\n",
      "train accuracy on epoch 4941: 0.889\n",
      "test loss on epoch 4941: 0.329\n",
      "test accuracy on epoch 4941: 0.769\n",
      "train loss on epoch 4942 : 0.173\n",
      "train accuracy on epoch 4942: 0.944\n",
      "test loss on epoch 4942: 0.329\n",
      "test accuracy on epoch 4942: 0.769\n",
      "train loss on epoch 4943 : 0.075\n",
      "train accuracy on epoch 4943: 1.000\n",
      "test loss on epoch 4943: 0.315\n",
      "test accuracy on epoch 4943: 0.846\n",
      "train loss on epoch 4944 : 0.064\n",
      "train accuracy on epoch 4944: 1.000\n",
      "test loss on epoch 4944: 0.321\n",
      "test accuracy on epoch 4944: 0.769\n",
      "train loss on epoch 4945 : 0.085\n",
      "train accuracy on epoch 4945: 1.000\n",
      "test loss on epoch 4945: 0.331\n",
      "test accuracy on epoch 4945: 0.692\n",
      "train loss on epoch 4946 : 0.323\n",
      "train accuracy on epoch 4946: 0.944\n",
      "test loss on epoch 4946: 0.311\n",
      "test accuracy on epoch 4946: 0.769\n",
      "train loss on epoch 4947 : 0.253\n",
      "train accuracy on epoch 4947: 0.889\n",
      "test loss on epoch 4947: 0.318\n",
      "test accuracy on epoch 4947: 0.769\n",
      "train loss on epoch 4948 : 0.153\n",
      "train accuracy on epoch 4948: 0.944\n",
      "test loss on epoch 4948: 0.319\n",
      "test accuracy on epoch 4948: 0.769\n",
      "train loss on epoch 4949 : 0.103\n",
      "train accuracy on epoch 4949: 1.000\n",
      "test loss on epoch 4949: 0.312\n",
      "test accuracy on epoch 4949: 0.769\n",
      "train loss on epoch 4950 : 0.175\n",
      "train accuracy on epoch 4950: 0.889\n",
      "test loss on epoch 4950: 0.335\n",
      "test accuracy on epoch 4950: 0.769\n",
      "train loss on epoch 4951 : 0.193\n",
      "train accuracy on epoch 4951: 0.944\n",
      "test loss on epoch 4951: 0.311\n",
      "test accuracy on epoch 4951: 0.769\n",
      "train loss on epoch 4952 : 0.144\n",
      "train accuracy on epoch 4952: 0.944\n",
      "test loss on epoch 4952: 0.330\n",
      "test accuracy on epoch 4952: 0.769\n",
      "train loss on epoch 4953 : 0.081\n",
      "train accuracy on epoch 4953: 1.000\n",
      "test loss on epoch 4953: 0.341\n",
      "test accuracy on epoch 4953: 0.769\n",
      "train loss on epoch 4954 : 0.031\n",
      "train accuracy on epoch 4954: 1.000\n",
      "test loss on epoch 4954: 0.311\n",
      "test accuracy on epoch 4954: 0.769\n",
      "train loss on epoch 4955 : 0.172\n",
      "train accuracy on epoch 4955: 0.889\n",
      "test loss on epoch 4955: 0.342\n",
      "test accuracy on epoch 4955: 0.769\n",
      "train loss on epoch 4956 : 0.169\n",
      "train accuracy on epoch 4956: 0.944\n",
      "test loss on epoch 4956: 0.341\n",
      "test accuracy on epoch 4956: 0.769\n",
      "train loss on epoch 4957 : 0.156\n",
      "train accuracy on epoch 4957: 0.944\n",
      "test loss on epoch 4957: 0.333\n",
      "test accuracy on epoch 4957: 0.769\n",
      "train loss on epoch 4958 : 0.112\n",
      "train accuracy on epoch 4958: 0.944\n",
      "test loss on epoch 4958: 0.331\n",
      "test accuracy on epoch 4958: 0.769\n",
      "train loss on epoch 4959 : 0.069\n",
      "train accuracy on epoch 4959: 1.000\n",
      "test loss on epoch 4959: 0.315\n",
      "test accuracy on epoch 4959: 0.769\n",
      "train loss on epoch 4960 : 0.042\n",
      "train accuracy on epoch 4960: 1.000\n",
      "test loss on epoch 4960: 0.336\n",
      "test accuracy on epoch 4960: 0.769\n",
      "train loss on epoch 4961 : 0.093\n",
      "train accuracy on epoch 4961: 1.000\n",
      "test loss on epoch 4961: 0.337\n",
      "test accuracy on epoch 4961: 0.692\n",
      "train loss on epoch 4962 : 0.468\n",
      "train accuracy on epoch 4962: 0.889\n",
      "test loss on epoch 4962: 0.318\n",
      "test accuracy on epoch 4962: 0.846\n",
      "train loss on epoch 4963 : 0.387\n",
      "train accuracy on epoch 4963: 0.889\n",
      "test loss on epoch 4963: 0.320\n",
      "test accuracy on epoch 4963: 0.846\n",
      "train loss on epoch 4964 : 0.067\n",
      "train accuracy on epoch 4964: 1.000\n",
      "test loss on epoch 4964: 0.332\n",
      "test accuracy on epoch 4964: 0.769\n",
      "train loss on epoch 4965 : 0.086\n",
      "train accuracy on epoch 4965: 1.000\n",
      "test loss on epoch 4965: 0.323\n",
      "test accuracy on epoch 4965: 0.846\n",
      "train loss on epoch 4966 : 0.133\n",
      "train accuracy on epoch 4966: 0.944\n",
      "test loss on epoch 4966: 0.329\n",
      "test accuracy on epoch 4966: 0.769\n",
      "train loss on epoch 4967 : 0.258\n",
      "train accuracy on epoch 4967: 0.944\n",
      "test loss on epoch 4967: 0.339\n",
      "test accuracy on epoch 4967: 0.769\n",
      "train loss on epoch 4968 : 0.080\n",
      "train accuracy on epoch 4968: 1.000\n",
      "test loss on epoch 4968: 0.315\n",
      "test accuracy on epoch 4968: 0.769\n",
      "train loss on epoch 4969 : 0.211\n",
      "train accuracy on epoch 4969: 0.944\n",
      "test loss on epoch 4969: 0.327\n",
      "test accuracy on epoch 4969: 0.769\n",
      "train loss on epoch 4970 : 0.114\n",
      "train accuracy on epoch 4970: 0.944\n",
      "test loss on epoch 4970: 0.343\n",
      "test accuracy on epoch 4970: 0.769\n",
      "train loss on epoch 4971 : 0.135\n",
      "train accuracy on epoch 4971: 0.944\n",
      "test loss on epoch 4971: 0.319\n",
      "test accuracy on epoch 4971: 0.769\n",
      "train loss on epoch 4972 : 0.048\n",
      "train accuracy on epoch 4972: 1.000\n",
      "test loss on epoch 4972: 0.316\n",
      "test accuracy on epoch 4972: 0.769\n",
      "train loss on epoch 4973 : 0.187\n",
      "train accuracy on epoch 4973: 0.944\n",
      "test loss on epoch 4973: 0.326\n",
      "test accuracy on epoch 4973: 0.769\n",
      "train loss on epoch 4974 : 0.096\n",
      "train accuracy on epoch 4974: 0.944\n",
      "test loss on epoch 4974: 0.346\n",
      "test accuracy on epoch 4974: 0.769\n",
      "train loss on epoch 4975 : 0.167\n",
      "train accuracy on epoch 4975: 0.889\n",
      "test loss on epoch 4975: 0.317\n",
      "test accuracy on epoch 4975: 0.769\n",
      "train loss on epoch 4976 : 0.144\n",
      "train accuracy on epoch 4976: 0.944\n",
      "test loss on epoch 4976: 0.334\n",
      "test accuracy on epoch 4976: 0.769\n",
      "train loss on epoch 4977 : 0.508\n",
      "train accuracy on epoch 4977: 0.889\n",
      "test loss on epoch 4977: 0.342\n",
      "test accuracy on epoch 4977: 0.692\n",
      "train loss on epoch 4978 : 0.306\n",
      "train accuracy on epoch 4978: 0.944\n",
      "test loss on epoch 4978: 0.334\n",
      "test accuracy on epoch 4978: 0.769\n",
      "train loss on epoch 4979 : 0.049\n",
      "train accuracy on epoch 4979: 1.000\n",
      "test loss on epoch 4979: 0.330\n",
      "test accuracy on epoch 4979: 0.769\n",
      "train loss on epoch 4980 : 0.045\n",
      "train accuracy on epoch 4980: 1.000\n",
      "test loss on epoch 4980: 0.333\n",
      "test accuracy on epoch 4980: 0.769\n",
      "train loss on epoch 4981 : 0.385\n",
      "train accuracy on epoch 4981: 0.889\n",
      "test loss on epoch 4981: 0.344\n",
      "test accuracy on epoch 4981: 0.692\n",
      "train loss on epoch 4982 : 0.265\n",
      "train accuracy on epoch 4982: 0.889\n",
      "test loss on epoch 4982: 0.341\n",
      "test accuracy on epoch 4982: 0.769\n",
      "train loss on epoch 4983 : 0.303\n",
      "train accuracy on epoch 4983: 0.889\n",
      "test loss on epoch 4983: 0.335\n",
      "test accuracy on epoch 4983: 0.769\n",
      "train loss on epoch 4984 : 0.248\n",
      "train accuracy on epoch 4984: 0.944\n",
      "test loss on epoch 4984: 0.342\n",
      "test accuracy on epoch 4984: 0.769\n",
      "train loss on epoch 4985 : 0.188\n",
      "train accuracy on epoch 4985: 0.889\n",
      "test loss on epoch 4985: 0.337\n",
      "test accuracy on epoch 4985: 0.769\n",
      "train loss on epoch 4986 : 0.368\n",
      "train accuracy on epoch 4986: 0.889\n",
      "test loss on epoch 4986: 0.337\n",
      "test accuracy on epoch 4986: 0.769\n",
      "train loss on epoch 4987 : 0.284\n",
      "train accuracy on epoch 4987: 0.889\n",
      "test loss on epoch 4987: 0.344\n",
      "test accuracy on epoch 4987: 0.769\n",
      "train loss on epoch 4988 : 0.081\n",
      "train accuracy on epoch 4988: 1.000\n",
      "test loss on epoch 4988: 0.344\n",
      "test accuracy on epoch 4988: 0.769\n",
      "train loss on epoch 4989 : 0.263\n",
      "train accuracy on epoch 4989: 0.833\n",
      "test loss on epoch 4989: 0.348\n",
      "test accuracy on epoch 4989: 0.769\n",
      "train loss on epoch 4990 : 0.160\n",
      "train accuracy on epoch 4990: 0.944\n",
      "test loss on epoch 4990: 0.347\n",
      "test accuracy on epoch 4990: 0.769\n",
      "train loss on epoch 4991 : 0.027\n",
      "train accuracy on epoch 4991: 1.000\n",
      "test loss on epoch 4991: 0.348\n",
      "test accuracy on epoch 4991: 0.769\n",
      "train loss on epoch 4992 : 0.074\n",
      "train accuracy on epoch 4992: 1.000\n",
      "test loss on epoch 4992: 0.352\n",
      "test accuracy on epoch 4992: 0.769\n",
      "train loss on epoch 4993 : 0.241\n",
      "train accuracy on epoch 4993: 0.889\n",
      "test loss on epoch 4993: 0.351\n",
      "test accuracy on epoch 4993: 0.769\n",
      "train loss on epoch 4994 : 0.237\n",
      "train accuracy on epoch 4994: 0.889\n",
      "test loss on epoch 4994: 0.329\n",
      "test accuracy on epoch 4994: 0.769\n",
      "train loss on epoch 4995 : 0.083\n",
      "train accuracy on epoch 4995: 0.944\n",
      "test loss on epoch 4995: 0.330\n",
      "test accuracy on epoch 4995: 0.769\n",
      "train loss on epoch 4996 : 0.222\n",
      "train accuracy on epoch 4996: 0.944\n",
      "test loss on epoch 4996: 0.336\n",
      "test accuracy on epoch 4996: 0.769\n",
      "train loss on epoch 4997 : 0.124\n",
      "train accuracy on epoch 4997: 0.944\n",
      "test loss on epoch 4997: 0.352\n",
      "test accuracy on epoch 4997: 0.769\n",
      "train loss on epoch 4998 : 0.090\n",
      "train accuracy on epoch 4998: 0.944\n",
      "test loss on epoch 4998: 0.353\n",
      "test accuracy on epoch 4998: 0.769\n",
      "train loss on epoch 4999 : 0.195\n",
      "train accuracy on epoch 4999: 0.889\n",
      "test loss on epoch 4999: 0.358\n",
      "test accuracy on epoch 4999: 0.769\n"
     ]
    }
   ],
   "source": [
    "\n",
    "net = MyModel(pad_idx+1, 50, pad_idx,dropout=0.5)\n",
    "weights=torch.concat((torch.tensor(embeds.vectors), torch.zeros((1,embeds.vectors.shape[-1]))),0)\n",
    "\n",
    "net.embedding.weight.data=weights   # on charge les pre-train \n",
    "net.embedding.weight.requires_grad=False  # on freeze\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(net.parameters(), lr=0.01,weight_decay=0.0)\n",
    "epochs = 5000\n",
    "\n",
    "train_test(train_loader, test_loader, net, loss_function, optimizer, epochs,clip=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714f4f78",
   "metadata": {},
   "source": [
    "#### Méthode 2 : avec la méthode nn.Embedding.from_pretrained()\n",
    "\n",
    "nn.Embedding permet d'importer directement une matrice de poids (embeddings pré-entraînés) grâce à la méthode from_pretrained() :  \n",
    "```\n",
    "weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]])\n",
    "embedding = nn.Embedding.from_pretrained(weight)\n",
    "```\n",
    "Plus particulièrement, la matrice de poids correspond à notre élément vocab.vectors qui va permettre d'initialiser la matrice d'embeddings.\n",
    "\n",
    "Par défaut, les embeddings sont \"gelés\" : ils ne sont pas modifiés avec la backpropagation, mais il est possible de les modifier avec le paramètre freeze=False. Cela revient à \"fine-tuner\" les embeddings sur la tâche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50cfc2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss on epoch 0 : 0.697\n",
      "train accuracy on epoch 0: 0.556\n",
      "test loss on epoch 0: 0.652\n",
      "test accuracy on epoch 0: 0.615\n",
      "train loss on epoch 1 : 0.617\n",
      "train accuracy on epoch 1: 0.778\n",
      "test loss on epoch 1: 0.615\n",
      "test accuracy on epoch 1: 0.692\n",
      "train loss on epoch 2 : 0.614\n",
      "train accuracy on epoch 2: 0.722\n",
      "test loss on epoch 2: 0.574\n",
      "test accuracy on epoch 2: 0.692\n",
      "train loss on epoch 3 : 0.545\n",
      "train accuracy on epoch 3: 0.778\n",
      "test loss on epoch 3: 0.561\n",
      "test accuracy on epoch 3: 0.769\n",
      "train loss on epoch 4 : 0.478\n",
      "train accuracy on epoch 4: 0.944\n",
      "test loss on epoch 4: 0.535\n",
      "test accuracy on epoch 4: 0.769\n",
      "train loss on epoch 5 : 0.487\n",
      "train accuracy on epoch 5: 0.833\n",
      "test loss on epoch 5: 0.503\n",
      "test accuracy on epoch 5: 0.846\n",
      "train loss on epoch 6 : 0.445\n",
      "train accuracy on epoch 6: 0.889\n",
      "test loss on epoch 6: 0.481\n",
      "test accuracy on epoch 6: 0.846\n",
      "train loss on epoch 7 : 0.408\n",
      "train accuracy on epoch 7: 0.889\n",
      "test loss on epoch 7: 0.475\n",
      "test accuracy on epoch 7: 0.846\n",
      "train loss on epoch 8 : 0.362\n",
      "train accuracy on epoch 8: 0.889\n",
      "test loss on epoch 8: 0.459\n",
      "test accuracy on epoch 8: 0.846\n",
      "train loss on epoch 9 : 0.339\n",
      "train accuracy on epoch 9: 0.944\n",
      "test loss on epoch 9: 0.443\n",
      "test accuracy on epoch 9: 0.846\n",
      "train loss on epoch 10 : 0.367\n",
      "train accuracy on epoch 10: 0.944\n",
      "test loss on epoch 10: 0.436\n",
      "test accuracy on epoch 10: 0.846\n",
      "train loss on epoch 11 : 0.314\n",
      "train accuracy on epoch 11: 0.944\n",
      "test loss on epoch 11: 0.423\n",
      "test accuracy on epoch 11: 0.846\n",
      "train loss on epoch 12 : 0.395\n",
      "train accuracy on epoch 12: 0.833\n",
      "test loss on epoch 12: 0.410\n",
      "test accuracy on epoch 12: 0.846\n",
      "train loss on epoch 13 : 0.343\n",
      "train accuracy on epoch 13: 0.889\n",
      "test loss on epoch 13: 0.411\n",
      "test accuracy on epoch 13: 0.846\n",
      "train loss on epoch 14 : 0.353\n",
      "train accuracy on epoch 14: 0.833\n",
      "test loss on epoch 14: 0.396\n",
      "test accuracy on epoch 14: 0.846\n",
      "train loss on epoch 15 : 0.279\n",
      "train accuracy on epoch 15: 0.889\n",
      "test loss on epoch 15: 0.391\n",
      "test accuracy on epoch 15: 0.846\n",
      "train loss on epoch 16 : 0.347\n",
      "train accuracy on epoch 16: 0.833\n",
      "test loss on epoch 16: 0.382\n",
      "test accuracy on epoch 16: 0.846\n",
      "train loss on epoch 17 : 0.289\n",
      "train accuracy on epoch 17: 0.944\n",
      "test loss on epoch 17: 0.393\n",
      "test accuracy on epoch 17: 0.846\n",
      "train loss on epoch 18 : 0.400\n",
      "train accuracy on epoch 18: 0.778\n",
      "test loss on epoch 18: 0.375\n",
      "test accuracy on epoch 18: 0.846\n",
      "train loss on epoch 19 : 0.312\n",
      "train accuracy on epoch 19: 0.944\n",
      "test loss on epoch 19: 0.387\n",
      "test accuracy on epoch 19: 0.846\n",
      "train loss on epoch 20 : 0.303\n",
      "train accuracy on epoch 20: 0.889\n",
      "test loss on epoch 20: 0.378\n",
      "test accuracy on epoch 20: 0.846\n",
      "train loss on epoch 21 : 0.224\n",
      "train accuracy on epoch 21: 0.944\n",
      "test loss on epoch 21: 0.381\n",
      "test accuracy on epoch 21: 0.769\n",
      "train loss on epoch 22 : 0.289\n",
      "train accuracy on epoch 22: 0.889\n",
      "test loss on epoch 22: 0.365\n",
      "test accuracy on epoch 22: 0.769\n",
      "train loss on epoch 23 : 0.252\n",
      "train accuracy on epoch 23: 0.889\n",
      "test loss on epoch 23: 0.370\n",
      "test accuracy on epoch 23: 0.769\n",
      "train loss on epoch 24 : 0.211\n",
      "train accuracy on epoch 24: 1.000\n",
      "test loss on epoch 24: 0.365\n",
      "test accuracy on epoch 24: 0.692\n",
      "train loss on epoch 25 : 0.226\n",
      "train accuracy on epoch 25: 0.944\n",
      "test loss on epoch 25: 0.366\n",
      "test accuracy on epoch 25: 0.769\n",
      "train loss on epoch 26 : 0.274\n",
      "train accuracy on epoch 26: 0.944\n",
      "test loss on epoch 26: 0.363\n",
      "test accuracy on epoch 26: 0.769\n",
      "train loss on epoch 27 : 0.281\n",
      "train accuracy on epoch 27: 0.944\n",
      "test loss on epoch 27: 0.359\n",
      "test accuracy on epoch 27: 0.769\n",
      "train loss on epoch 28 : 0.193\n",
      "train accuracy on epoch 28: 0.944\n",
      "test loss on epoch 28: 0.354\n",
      "test accuracy on epoch 28: 0.769\n",
      "train loss on epoch 29 : 0.308\n",
      "train accuracy on epoch 29: 0.833\n",
      "test loss on epoch 29: 0.351\n",
      "test accuracy on epoch 29: 0.769\n",
      "train loss on epoch 30 : 0.213\n",
      "train accuracy on epoch 30: 0.944\n",
      "test loss on epoch 30: 0.351\n",
      "test accuracy on epoch 30: 0.769\n",
      "train loss on epoch 31 : 0.230\n",
      "train accuracy on epoch 31: 0.889\n",
      "test loss on epoch 31: 0.355\n",
      "test accuracy on epoch 31: 0.769\n",
      "train loss on epoch 32 : 0.211\n",
      "train accuracy on epoch 32: 0.944\n",
      "test loss on epoch 32: 0.352\n",
      "test accuracy on epoch 32: 0.769\n",
      "train loss on epoch 33 : 0.245\n",
      "train accuracy on epoch 33: 0.944\n",
      "test loss on epoch 33: 0.356\n",
      "test accuracy on epoch 33: 0.769\n",
      "train loss on epoch 34 : 0.111\n",
      "train accuracy on epoch 34: 1.000\n",
      "test loss on epoch 34: 0.350\n",
      "test accuracy on epoch 34: 0.769\n",
      "train loss on epoch 35 : 0.186\n",
      "train accuracy on epoch 35: 0.889\n",
      "test loss on epoch 35: 0.342\n",
      "test accuracy on epoch 35: 0.769\n",
      "train loss on epoch 36 : 0.258\n",
      "train accuracy on epoch 36: 0.889\n",
      "test loss on epoch 36: 0.349\n",
      "test accuracy on epoch 36: 0.769\n",
      "train loss on epoch 37 : 0.302\n",
      "train accuracy on epoch 37: 0.833\n",
      "test loss on epoch 37: 0.350\n",
      "test accuracy on epoch 37: 0.769\n",
      "train loss on epoch 38 : 0.416\n",
      "train accuracy on epoch 38: 0.944\n",
      "test loss on epoch 38: 0.344\n",
      "test accuracy on epoch 38: 0.769\n",
      "train loss on epoch 39 : 0.198\n",
      "train accuracy on epoch 39: 0.889\n",
      "test loss on epoch 39: 0.341\n",
      "test accuracy on epoch 39: 0.769\n",
      "train loss on epoch 40 : 0.139\n",
      "train accuracy on epoch 40: 1.000\n",
      "test loss on epoch 40: 0.343\n",
      "test accuracy on epoch 40: 0.769\n",
      "train loss on epoch 41 : 0.271\n",
      "train accuracy on epoch 41: 0.944\n",
      "test loss on epoch 41: 0.345\n",
      "test accuracy on epoch 41: 0.769\n",
      "train loss on epoch 42 : 0.214\n",
      "train accuracy on epoch 42: 1.000\n",
      "test loss on epoch 42: 0.335\n",
      "test accuracy on epoch 42: 0.769\n",
      "train loss on epoch 43 : 0.152\n",
      "train accuracy on epoch 43: 0.944\n",
      "test loss on epoch 43: 0.339\n",
      "test accuracy on epoch 43: 0.769\n",
      "train loss on epoch 44 : 0.300\n",
      "train accuracy on epoch 44: 0.944\n",
      "test loss on epoch 44: 0.337\n",
      "test accuracy on epoch 44: 0.769\n",
      "train loss on epoch 45 : 0.190\n",
      "train accuracy on epoch 45: 0.944\n",
      "test loss on epoch 45: 0.340\n",
      "test accuracy on epoch 45: 0.769\n",
      "train loss on epoch 46 : 0.281\n",
      "train accuracy on epoch 46: 0.944\n",
      "test loss on epoch 46: 0.338\n",
      "test accuracy on epoch 46: 0.769\n",
      "train loss on epoch 47 : 0.266\n",
      "train accuracy on epoch 47: 0.889\n",
      "test loss on epoch 47: 0.342\n",
      "test accuracy on epoch 47: 0.769\n",
      "train loss on epoch 48 : 0.241\n",
      "train accuracy on epoch 48: 0.944\n",
      "test loss on epoch 48: 0.333\n",
      "test accuracy on epoch 48: 0.769\n",
      "train loss on epoch 49 : 0.304\n",
      "train accuracy on epoch 49: 0.833\n",
      "test loss on epoch 49: 0.331\n",
      "test accuracy on epoch 49: 0.769\n",
      "train loss on epoch 50 : 0.205\n",
      "train accuracy on epoch 50: 0.944\n",
      "test loss on epoch 50: 0.330\n",
      "test accuracy on epoch 50: 0.769\n",
      "train loss on epoch 51 : 0.215\n",
      "train accuracy on epoch 51: 0.944\n",
      "test loss on epoch 51: 0.330\n",
      "test accuracy on epoch 51: 0.769\n",
      "train loss on epoch 52 : 0.197\n",
      "train accuracy on epoch 52: 0.944\n",
      "test loss on epoch 52: 0.335\n",
      "test accuracy on epoch 52: 0.769\n",
      "train loss on epoch 53 : 0.292\n",
      "train accuracy on epoch 53: 0.833\n",
      "test loss on epoch 53: 0.335\n",
      "test accuracy on epoch 53: 0.769\n",
      "train loss on epoch 54 : 0.222\n",
      "train accuracy on epoch 54: 0.944\n",
      "test loss on epoch 54: 0.325\n",
      "test accuracy on epoch 54: 0.769\n",
      "train loss on epoch 55 : 0.158\n",
      "train accuracy on epoch 55: 0.944\n",
      "test loss on epoch 55: 0.326\n",
      "test accuracy on epoch 55: 0.769\n",
      "train loss on epoch 56 : 0.157\n",
      "train accuracy on epoch 56: 0.944\n",
      "test loss on epoch 56: 0.333\n",
      "test accuracy on epoch 56: 0.769\n",
      "train loss on epoch 57 : 0.186\n",
      "train accuracy on epoch 57: 0.944\n",
      "test loss on epoch 57: 0.329\n",
      "test accuracy on epoch 57: 0.769\n",
      "train loss on epoch 58 : 0.241\n",
      "train accuracy on epoch 58: 0.944\n",
      "test loss on epoch 58: 0.325\n",
      "test accuracy on epoch 58: 0.769\n",
      "train loss on epoch 59 : 0.100\n",
      "train accuracy on epoch 59: 1.000\n",
      "test loss on epoch 59: 0.338\n",
      "test accuracy on epoch 59: 0.769\n",
      "train loss on epoch 60 : 0.204\n",
      "train accuracy on epoch 60: 0.889\n",
      "test loss on epoch 60: 0.337\n",
      "test accuracy on epoch 60: 0.769\n",
      "train loss on epoch 61 : 0.205\n",
      "train accuracy on epoch 61: 0.944\n",
      "test loss on epoch 61: 0.327\n",
      "test accuracy on epoch 61: 0.769\n",
      "train loss on epoch 62 : 0.211\n",
      "train accuracy on epoch 62: 0.833\n",
      "test loss on epoch 62: 0.338\n",
      "test accuracy on epoch 62: 0.769\n",
      "train loss on epoch 63 : 0.137\n",
      "train accuracy on epoch 63: 0.944\n",
      "test loss on epoch 63: 0.329\n",
      "test accuracy on epoch 63: 0.769\n",
      "train loss on epoch 64 : 0.232\n",
      "train accuracy on epoch 64: 0.889\n",
      "test loss on epoch 64: 0.332\n",
      "test accuracy on epoch 64: 0.769\n",
      "train loss on epoch 65 : 0.195\n",
      "train accuracy on epoch 65: 0.944\n",
      "test loss on epoch 65: 0.323\n",
      "test accuracy on epoch 65: 0.769\n",
      "train loss on epoch 66 : 0.255\n",
      "train accuracy on epoch 66: 0.833\n",
      "test loss on epoch 66: 0.323\n",
      "test accuracy on epoch 66: 0.769\n",
      "train loss on epoch 67 : 0.211\n",
      "train accuracy on epoch 67: 0.944\n",
      "test loss on epoch 67: 0.317\n",
      "test accuracy on epoch 67: 0.769\n",
      "train loss on epoch 68 : 0.205\n",
      "train accuracy on epoch 68: 0.944\n",
      "test loss on epoch 68: 0.318\n",
      "test accuracy on epoch 68: 0.769\n",
      "train loss on epoch 69 : 0.147\n",
      "train accuracy on epoch 69: 0.944\n",
      "test loss on epoch 69: 0.317\n",
      "test accuracy on epoch 69: 0.769\n",
      "train loss on epoch 70 : 0.080\n",
      "train accuracy on epoch 70: 1.000\n",
      "test loss on epoch 70: 0.317\n",
      "test accuracy on epoch 70: 0.769\n",
      "train loss on epoch 71 : 0.175\n",
      "train accuracy on epoch 71: 0.889\n",
      "test loss on epoch 71: 0.330\n",
      "test accuracy on epoch 71: 0.769\n",
      "train loss on epoch 72 : 0.144\n",
      "train accuracy on epoch 72: 1.000\n",
      "test loss on epoch 72: 0.322\n",
      "test accuracy on epoch 72: 0.769\n",
      "train loss on epoch 73 : 0.296\n",
      "train accuracy on epoch 73: 0.889\n",
      "test loss on epoch 73: 0.326\n",
      "test accuracy on epoch 73: 0.769\n",
      "train loss on epoch 74 : 0.275\n",
      "train accuracy on epoch 74: 0.833\n",
      "test loss on epoch 74: 0.317\n",
      "test accuracy on epoch 74: 0.769\n",
      "train loss on epoch 75 : 0.245\n",
      "train accuracy on epoch 75: 0.944\n",
      "test loss on epoch 75: 0.328\n",
      "test accuracy on epoch 75: 0.769\n",
      "train loss on epoch 76 : 0.138\n",
      "train accuracy on epoch 76: 0.944\n",
      "test loss on epoch 76: 0.328\n",
      "test accuracy on epoch 76: 0.769\n",
      "train loss on epoch 77 : 0.237\n",
      "train accuracy on epoch 77: 0.944\n",
      "test loss on epoch 77: 0.330\n",
      "test accuracy on epoch 77: 0.769\n",
      "train loss on epoch 78 : 0.258\n",
      "train accuracy on epoch 78: 0.889\n",
      "test loss on epoch 78: 0.320\n",
      "test accuracy on epoch 78: 0.769\n",
      "train loss on epoch 79 : 0.186\n",
      "train accuracy on epoch 79: 0.944\n",
      "test loss on epoch 79: 0.325\n",
      "test accuracy on epoch 79: 0.769\n",
      "train loss on epoch 80 : 0.208\n",
      "train accuracy on epoch 80: 0.889\n",
      "test loss on epoch 80: 0.320\n",
      "test accuracy on epoch 80: 0.769\n",
      "train loss on epoch 81 : 0.136\n",
      "train accuracy on epoch 81: 1.000\n",
      "test loss on epoch 81: 0.313\n",
      "test accuracy on epoch 81: 0.769\n",
      "train loss on epoch 82 : 0.142\n",
      "train accuracy on epoch 82: 0.889\n",
      "test loss on epoch 82: 0.325\n",
      "test accuracy on epoch 82: 0.769\n",
      "train loss on epoch 83 : 0.253\n",
      "train accuracy on epoch 83: 0.889\n",
      "test loss on epoch 83: 0.320\n",
      "test accuracy on epoch 83: 0.769\n",
      "train loss on epoch 84 : 0.259\n",
      "train accuracy on epoch 84: 0.944\n",
      "test loss on epoch 84: 0.314\n",
      "test accuracy on epoch 84: 0.769\n",
      "train loss on epoch 85 : 0.343\n",
      "train accuracy on epoch 85: 0.944\n",
      "test loss on epoch 85: 0.313\n",
      "test accuracy on epoch 85: 0.769\n",
      "train loss on epoch 86 : 0.184\n",
      "train accuracy on epoch 86: 0.889\n",
      "test loss on epoch 86: 0.323\n",
      "test accuracy on epoch 86: 0.769\n",
      "train loss on epoch 87 : 0.253\n",
      "train accuracy on epoch 87: 0.889\n",
      "test loss on epoch 87: 0.315\n",
      "test accuracy on epoch 87: 0.769\n",
      "train loss on epoch 88 : 0.098\n",
      "train accuracy on epoch 88: 1.000\n",
      "test loss on epoch 88: 0.316\n",
      "test accuracy on epoch 88: 0.769\n",
      "train loss on epoch 89 : 0.318\n",
      "train accuracy on epoch 89: 0.889\n",
      "test loss on epoch 89: 0.317\n",
      "test accuracy on epoch 89: 0.769\n",
      "train loss on epoch 90 : 0.243\n",
      "train accuracy on epoch 90: 0.889\n",
      "test loss on epoch 90: 0.313\n",
      "test accuracy on epoch 90: 0.769\n",
      "train loss on epoch 91 : 0.191\n",
      "train accuracy on epoch 91: 0.944\n",
      "test loss on epoch 91: 0.315\n",
      "test accuracy on epoch 91: 0.769\n",
      "train loss on epoch 92 : 0.210\n",
      "train accuracy on epoch 92: 0.889\n",
      "test loss on epoch 92: 0.315\n",
      "test accuracy on epoch 92: 0.769\n",
      "train loss on epoch 93 : 0.248\n",
      "train accuracy on epoch 93: 0.944\n",
      "test loss on epoch 93: 0.317\n",
      "test accuracy on epoch 93: 0.769\n",
      "train loss on epoch 94 : 0.100\n",
      "train accuracy on epoch 94: 1.000\n",
      "test loss on epoch 94: 0.312\n",
      "test accuracy on epoch 94: 0.769\n",
      "train loss on epoch 95 : 0.131\n",
      "train accuracy on epoch 95: 0.944\n",
      "test loss on epoch 95: 0.309\n",
      "test accuracy on epoch 95: 0.769\n",
      "train loss on epoch 96 : 0.297\n",
      "train accuracy on epoch 96: 0.889\n",
      "test loss on epoch 96: 0.313\n",
      "test accuracy on epoch 96: 0.769\n",
      "train loss on epoch 97 : 0.081\n",
      "train accuracy on epoch 97: 1.000\n",
      "test loss on epoch 97: 0.308\n",
      "test accuracy on epoch 97: 0.769\n",
      "train loss on epoch 98 : 0.304\n",
      "train accuracy on epoch 98: 0.889\n",
      "test loss on epoch 98: 0.309\n",
      "test accuracy on epoch 98: 0.769\n",
      "train loss on epoch 99 : 0.132\n",
      "train accuracy on epoch 99: 0.944\n",
      "test loss on epoch 99: 0.305\n",
      "test accuracy on epoch 99: 0.769\n",
      "train loss on epoch 100 : 0.166\n",
      "train accuracy on epoch 100: 0.944\n",
      "test loss on epoch 100: 0.316\n",
      "test accuracy on epoch 100: 0.769\n",
      "train loss on epoch 101 : 0.199\n",
      "train accuracy on epoch 101: 0.889\n",
      "test loss on epoch 101: 0.319\n",
      "test accuracy on epoch 101: 0.769\n",
      "train loss on epoch 102 : 0.183\n",
      "train accuracy on epoch 102: 0.889\n",
      "test loss on epoch 102: 0.312\n",
      "test accuracy on epoch 102: 0.769\n",
      "train loss on epoch 103 : 0.171\n",
      "train accuracy on epoch 103: 0.944\n",
      "test loss on epoch 103: 0.310\n",
      "test accuracy on epoch 103: 0.769\n",
      "train loss on epoch 104 : 0.095\n",
      "train accuracy on epoch 104: 1.000\n",
      "test loss on epoch 104: 0.316\n",
      "test accuracy on epoch 104: 0.769\n",
      "train loss on epoch 105 : 0.207\n",
      "train accuracy on epoch 105: 0.889\n",
      "test loss on epoch 105: 0.308\n",
      "test accuracy on epoch 105: 0.769\n",
      "train loss on epoch 106 : 0.216\n",
      "train accuracy on epoch 106: 0.889\n",
      "test loss on epoch 106: 0.306\n",
      "test accuracy on epoch 106: 0.769\n",
      "train loss on epoch 107 : 0.311\n",
      "train accuracy on epoch 107: 0.944\n",
      "test loss on epoch 107: 0.312\n",
      "test accuracy on epoch 107: 0.769\n",
      "train loss on epoch 108 : 0.196\n",
      "train accuracy on epoch 108: 0.944\n",
      "test loss on epoch 108: 0.312\n",
      "test accuracy on epoch 108: 0.769\n",
      "train loss on epoch 109 : 0.199\n",
      "train accuracy on epoch 109: 0.944\n",
      "test loss on epoch 109: 0.319\n",
      "test accuracy on epoch 109: 0.769\n",
      "train loss on epoch 110 : 0.176\n",
      "train accuracy on epoch 110: 0.889\n",
      "test loss on epoch 110: 0.321\n",
      "test accuracy on epoch 110: 0.769\n",
      "train loss on epoch 111 : 0.247\n",
      "train accuracy on epoch 111: 0.833\n",
      "test loss on epoch 111: 0.312\n",
      "test accuracy on epoch 111: 0.769\n",
      "train loss on epoch 112 : 0.105\n",
      "train accuracy on epoch 112: 0.944\n",
      "test loss on epoch 112: 0.315\n",
      "test accuracy on epoch 112: 0.769\n",
      "train loss on epoch 113 : 0.306\n",
      "train accuracy on epoch 113: 0.833\n",
      "test loss on epoch 113: 0.306\n",
      "test accuracy on epoch 113: 0.769\n",
      "train loss on epoch 114 : 0.140\n",
      "train accuracy on epoch 114: 0.889\n",
      "test loss on epoch 114: 0.311\n",
      "test accuracy on epoch 114: 0.769\n",
      "train loss on epoch 115 : 0.250\n",
      "train accuracy on epoch 115: 0.889\n",
      "test loss on epoch 115: 0.314\n",
      "test accuracy on epoch 115: 0.769\n",
      "train loss on epoch 116 : 0.258\n",
      "train accuracy on epoch 116: 0.889\n",
      "test loss on epoch 116: 0.313\n",
      "test accuracy on epoch 116: 0.769\n",
      "train loss on epoch 117 : 0.165\n",
      "train accuracy on epoch 117: 0.889\n",
      "test loss on epoch 117: 0.312\n",
      "test accuracy on epoch 117: 0.769\n",
      "train loss on epoch 118 : 0.247\n",
      "train accuracy on epoch 118: 0.889\n",
      "test loss on epoch 118: 0.304\n",
      "test accuracy on epoch 118: 0.769\n",
      "train loss on epoch 119 : 0.106\n",
      "train accuracy on epoch 119: 1.000\n",
      "test loss on epoch 119: 0.304\n",
      "test accuracy on epoch 119: 0.769\n",
      "train loss on epoch 120 : 0.145\n",
      "train accuracy on epoch 120: 0.944\n",
      "test loss on epoch 120: 0.307\n",
      "test accuracy on epoch 120: 0.769\n",
      "train loss on epoch 121 : 0.293\n",
      "train accuracy on epoch 121: 0.889\n",
      "test loss on epoch 121: 0.315\n",
      "test accuracy on epoch 121: 0.769\n",
      "train loss on epoch 122 : 0.138\n",
      "train accuracy on epoch 122: 0.944\n",
      "test loss on epoch 122: 0.312\n",
      "test accuracy on epoch 122: 0.769\n",
      "train loss on epoch 123 : 0.130\n",
      "train accuracy on epoch 123: 1.000\n",
      "test loss on epoch 123: 0.309\n",
      "test accuracy on epoch 123: 0.769\n",
      "train loss on epoch 124 : 0.099\n",
      "train accuracy on epoch 124: 1.000\n",
      "test loss on epoch 124: 0.307\n",
      "test accuracy on epoch 124: 0.769\n",
      "train loss on epoch 125 : 0.075\n",
      "train accuracy on epoch 125: 1.000\n",
      "test loss on epoch 125: 0.301\n",
      "test accuracy on epoch 125: 0.769\n",
      "train loss on epoch 126 : 0.142\n",
      "train accuracy on epoch 126: 1.000\n",
      "test loss on epoch 126: 0.309\n",
      "test accuracy on epoch 126: 0.769\n",
      "train loss on epoch 127 : 0.275\n",
      "train accuracy on epoch 127: 0.944\n",
      "test loss on epoch 127: 0.309\n",
      "test accuracy on epoch 127: 0.769\n",
      "train loss on epoch 128 : 0.156\n",
      "train accuracy on epoch 128: 0.944\n",
      "test loss on epoch 128: 0.305\n",
      "test accuracy on epoch 128: 0.769\n",
      "train loss on epoch 129 : 0.153\n",
      "train accuracy on epoch 129: 0.944\n",
      "test loss on epoch 129: 0.305\n",
      "test accuracy on epoch 129: 0.769\n",
      "train loss on epoch 130 : 0.113\n",
      "train accuracy on epoch 130: 0.944\n",
      "test loss on epoch 130: 0.306\n",
      "test accuracy on epoch 130: 0.769\n",
      "train loss on epoch 131 : 0.150\n",
      "train accuracy on epoch 131: 0.944\n",
      "test loss on epoch 131: 0.309\n",
      "test accuracy on epoch 131: 0.769\n",
      "train loss on epoch 132 : 0.250\n",
      "train accuracy on epoch 132: 0.889\n",
      "test loss on epoch 132: 0.312\n",
      "test accuracy on epoch 132: 0.769\n",
      "train loss on epoch 133 : 0.052\n",
      "train accuracy on epoch 133: 1.000\n",
      "test loss on epoch 133: 0.311\n",
      "test accuracy on epoch 133: 0.769\n",
      "train loss on epoch 134 : 0.170\n",
      "train accuracy on epoch 134: 1.000\n",
      "test loss on epoch 134: 0.313\n",
      "test accuracy on epoch 134: 0.769\n",
      "train loss on epoch 135 : 0.211\n",
      "train accuracy on epoch 135: 0.889\n",
      "test loss on epoch 135: 0.312\n",
      "test accuracy on epoch 135: 0.769\n",
      "train loss on epoch 136 : 0.079\n",
      "train accuracy on epoch 136: 1.000\n",
      "test loss on epoch 136: 0.313\n",
      "test accuracy on epoch 136: 0.769\n",
      "train loss on epoch 137 : 0.204\n",
      "train accuracy on epoch 137: 0.889\n",
      "test loss on epoch 137: 0.320\n",
      "test accuracy on epoch 137: 0.769\n",
      "train loss on epoch 138 : 0.181\n",
      "train accuracy on epoch 138: 0.889\n",
      "test loss on epoch 138: 0.312\n",
      "test accuracy on epoch 138: 0.769\n",
      "train loss on epoch 139 : 0.132\n",
      "train accuracy on epoch 139: 0.944\n",
      "test loss on epoch 139: 0.317\n",
      "test accuracy on epoch 139: 0.769\n",
      "train loss on epoch 140 : 0.065\n",
      "train accuracy on epoch 140: 0.944\n",
      "test loss on epoch 140: 0.307\n",
      "test accuracy on epoch 140: 0.769\n",
      "train loss on epoch 141 : 0.233\n",
      "train accuracy on epoch 141: 0.889\n",
      "test loss on epoch 141: 0.306\n",
      "test accuracy on epoch 141: 0.769\n",
      "train loss on epoch 142 : 0.238\n",
      "train accuracy on epoch 142: 0.889\n",
      "test loss on epoch 142: 0.304\n",
      "test accuracy on epoch 142: 0.769\n",
      "train loss on epoch 143 : 0.216\n",
      "train accuracy on epoch 143: 0.944\n",
      "test loss on epoch 143: 0.303\n",
      "test accuracy on epoch 143: 0.769\n",
      "train loss on epoch 144 : 0.152\n",
      "train accuracy on epoch 144: 0.889\n",
      "test loss on epoch 144: 0.305\n",
      "test accuracy on epoch 144: 0.769\n",
      "train loss on epoch 145 : 0.211\n",
      "train accuracy on epoch 145: 0.944\n",
      "test loss on epoch 145: 0.303\n",
      "test accuracy on epoch 145: 0.769\n",
      "train loss on epoch 146 : 0.215\n",
      "train accuracy on epoch 146: 0.833\n",
      "test loss on epoch 146: 0.302\n",
      "test accuracy on epoch 146: 0.769\n",
      "train loss on epoch 147 : 0.075\n",
      "train accuracy on epoch 147: 1.000\n",
      "test loss on epoch 147: 0.302\n",
      "test accuracy on epoch 147: 0.769\n",
      "train loss on epoch 148 : 0.299\n",
      "train accuracy on epoch 148: 0.889\n",
      "test loss on epoch 148: 0.299\n",
      "test accuracy on epoch 148: 0.769\n",
      "train loss on epoch 149 : 0.206\n",
      "train accuracy on epoch 149: 0.944\n",
      "test loss on epoch 149: 0.304\n",
      "test accuracy on epoch 149: 0.769\n",
      "train loss on epoch 150 : 0.239\n",
      "train accuracy on epoch 150: 0.833\n",
      "test loss on epoch 150: 0.305\n",
      "test accuracy on epoch 150: 0.769\n",
      "train loss on epoch 151 : 0.257\n",
      "train accuracy on epoch 151: 0.889\n",
      "test loss on epoch 151: 0.309\n",
      "test accuracy on epoch 151: 0.769\n",
      "train loss on epoch 152 : 0.154\n",
      "train accuracy on epoch 152: 1.000\n",
      "test loss on epoch 152: 0.305\n",
      "test accuracy on epoch 152: 0.769\n",
      "train loss on epoch 153 : 0.249\n",
      "train accuracy on epoch 153: 0.833\n",
      "test loss on epoch 153: 0.301\n",
      "test accuracy on epoch 153: 0.769\n",
      "train loss on epoch 154 : 0.148\n",
      "train accuracy on epoch 154: 0.944\n",
      "test loss on epoch 154: 0.295\n",
      "test accuracy on epoch 154: 0.769\n",
      "train loss on epoch 155 : 0.113\n",
      "train accuracy on epoch 155: 0.944\n",
      "test loss on epoch 155: 0.303\n",
      "test accuracy on epoch 155: 0.769\n",
      "train loss on epoch 156 : 0.103\n",
      "train accuracy on epoch 156: 1.000\n",
      "test loss on epoch 156: 0.295\n",
      "test accuracy on epoch 156: 0.769\n",
      "train loss on epoch 157 : 0.113\n",
      "train accuracy on epoch 157: 1.000\n",
      "test loss on epoch 157: 0.303\n",
      "test accuracy on epoch 157: 0.769\n",
      "train loss on epoch 158 : 0.196\n",
      "train accuracy on epoch 158: 0.944\n",
      "test loss on epoch 158: 0.299\n",
      "test accuracy on epoch 158: 0.769\n",
      "train loss on epoch 159 : 0.191\n",
      "train accuracy on epoch 159: 0.944\n",
      "test loss on epoch 159: 0.299\n",
      "test accuracy on epoch 159: 0.769\n",
      "train loss on epoch 160 : 0.161\n",
      "train accuracy on epoch 160: 0.944\n",
      "test loss on epoch 160: 0.303\n",
      "test accuracy on epoch 160: 0.769\n",
      "train loss on epoch 161 : 0.130\n",
      "train accuracy on epoch 161: 1.000\n",
      "test loss on epoch 161: 0.296\n",
      "test accuracy on epoch 161: 0.769\n",
      "train loss on epoch 162 : 0.307\n",
      "train accuracy on epoch 162: 0.944\n",
      "test loss on epoch 162: 0.303\n",
      "test accuracy on epoch 162: 0.769\n",
      "train loss on epoch 163 : 0.104\n",
      "train accuracy on epoch 163: 1.000\n",
      "test loss on epoch 163: 0.297\n",
      "test accuracy on epoch 163: 0.769\n",
      "train loss on epoch 164 : 0.186\n",
      "train accuracy on epoch 164: 0.889\n",
      "test loss on epoch 164: 0.298\n",
      "test accuracy on epoch 164: 0.769\n",
      "train loss on epoch 165 : 0.172\n",
      "train accuracy on epoch 165: 0.944\n",
      "test loss on epoch 165: 0.301\n",
      "test accuracy on epoch 165: 0.769\n",
      "train loss on epoch 166 : 0.057\n",
      "train accuracy on epoch 166: 1.000\n",
      "test loss on epoch 166: 0.302\n",
      "test accuracy on epoch 166: 0.769\n",
      "train loss on epoch 167 : 0.144\n",
      "train accuracy on epoch 167: 0.944\n",
      "test loss on epoch 167: 0.305\n",
      "test accuracy on epoch 167: 0.769\n",
      "train loss on epoch 168 : 0.162\n",
      "train accuracy on epoch 168: 0.944\n",
      "test loss on epoch 168: 0.298\n",
      "test accuracy on epoch 168: 0.769\n",
      "train loss on epoch 169 : 0.153\n",
      "train accuracy on epoch 169: 0.944\n",
      "test loss on epoch 169: 0.301\n",
      "test accuracy on epoch 169: 0.769\n",
      "train loss on epoch 170 : 0.057\n",
      "train accuracy on epoch 170: 1.000\n",
      "test loss on epoch 170: 0.300\n",
      "test accuracy on epoch 170: 0.769\n",
      "train loss on epoch 171 : 0.158\n",
      "train accuracy on epoch 171: 0.944\n",
      "test loss on epoch 171: 0.307\n",
      "test accuracy on epoch 171: 0.769\n",
      "train loss on epoch 172 : 0.338\n",
      "train accuracy on epoch 172: 0.889\n",
      "test loss on epoch 172: 0.297\n",
      "test accuracy on epoch 172: 0.769\n",
      "train loss on epoch 173 : 0.200\n",
      "train accuracy on epoch 173: 0.889\n",
      "test loss on epoch 173: 0.302\n",
      "test accuracy on epoch 173: 0.846\n",
      "train loss on epoch 174 : 0.315\n",
      "train accuracy on epoch 174: 0.833\n",
      "test loss on epoch 174: 0.307\n",
      "test accuracy on epoch 174: 0.769\n",
      "train loss on epoch 175 : 0.271\n",
      "train accuracy on epoch 175: 0.944\n",
      "test loss on epoch 175: 0.310\n",
      "test accuracy on epoch 175: 0.692\n",
      "train loss on epoch 176 : 0.306\n",
      "train accuracy on epoch 176: 0.889\n",
      "test loss on epoch 176: 0.306\n",
      "test accuracy on epoch 176: 0.769\n",
      "train loss on epoch 177 : 0.144\n",
      "train accuracy on epoch 177: 0.889\n",
      "test loss on epoch 177: 0.304\n",
      "test accuracy on epoch 177: 0.769\n",
      "train loss on epoch 178 : 0.216\n",
      "train accuracy on epoch 178: 0.889\n",
      "test loss on epoch 178: 0.300\n",
      "test accuracy on epoch 178: 0.769\n",
      "train loss on epoch 179 : 0.166\n",
      "train accuracy on epoch 179: 0.944\n",
      "test loss on epoch 179: 0.308\n",
      "test accuracy on epoch 179: 0.769\n",
      "train loss on epoch 180 : 0.121\n",
      "train accuracy on epoch 180: 0.944\n",
      "test loss on epoch 180: 0.305\n",
      "test accuracy on epoch 180: 0.769\n",
      "train loss on epoch 181 : 0.151\n",
      "train accuracy on epoch 181: 0.944\n",
      "test loss on epoch 181: 0.304\n",
      "test accuracy on epoch 181: 0.769\n",
      "train loss on epoch 182 : 0.125\n",
      "train accuracy on epoch 182: 0.944\n",
      "test loss on epoch 182: 0.302\n",
      "test accuracy on epoch 182: 0.769\n",
      "train loss on epoch 183 : 0.189\n",
      "train accuracy on epoch 183: 0.944\n",
      "test loss on epoch 183: 0.310\n",
      "test accuracy on epoch 183: 0.769\n",
      "train loss on epoch 184 : 0.068\n",
      "train accuracy on epoch 184: 1.000\n",
      "test loss on epoch 184: 0.302\n",
      "test accuracy on epoch 184: 0.769\n",
      "train loss on epoch 185 : 0.191\n",
      "train accuracy on epoch 185: 0.944\n",
      "test loss on epoch 185: 0.310\n",
      "test accuracy on epoch 185: 0.769\n",
      "train loss on epoch 186 : 0.258\n",
      "train accuracy on epoch 186: 0.889\n",
      "test loss on epoch 186: 0.312\n",
      "test accuracy on epoch 186: 0.769\n",
      "train loss on epoch 187 : 0.071\n",
      "train accuracy on epoch 187: 1.000\n",
      "test loss on epoch 187: 0.301\n",
      "test accuracy on epoch 187: 0.769\n",
      "train loss on epoch 188 : 0.183\n",
      "train accuracy on epoch 188: 0.889\n",
      "test loss on epoch 188: 0.302\n",
      "test accuracy on epoch 188: 0.769\n",
      "train loss on epoch 189 : 0.112\n",
      "train accuracy on epoch 189: 0.944\n",
      "test loss on epoch 189: 0.314\n",
      "test accuracy on epoch 189: 0.769\n",
      "train loss on epoch 190 : 0.155\n",
      "train accuracy on epoch 190: 0.944\n",
      "test loss on epoch 190: 0.305\n",
      "test accuracy on epoch 190: 0.769\n",
      "train loss on epoch 191 : 0.152\n",
      "train accuracy on epoch 191: 0.944\n",
      "test loss on epoch 191: 0.301\n",
      "test accuracy on epoch 191: 0.769\n",
      "train loss on epoch 192 : 0.061\n",
      "train accuracy on epoch 192: 1.000\n",
      "test loss on epoch 192: 0.310\n",
      "test accuracy on epoch 192: 0.769\n",
      "train loss on epoch 193 : 0.370\n",
      "train accuracy on epoch 193: 0.833\n",
      "test loss on epoch 193: 0.309\n",
      "test accuracy on epoch 193: 0.769\n",
      "train loss on epoch 194 : 0.131\n",
      "train accuracy on epoch 194: 0.944\n",
      "test loss on epoch 194: 0.302\n",
      "test accuracy on epoch 194: 0.769\n",
      "train loss on epoch 195 : 0.062\n",
      "train accuracy on epoch 195: 0.944\n",
      "test loss on epoch 195: 0.310\n",
      "test accuracy on epoch 195: 0.769\n",
      "train loss on epoch 196 : 0.083\n",
      "train accuracy on epoch 196: 1.000\n",
      "test loss on epoch 196: 0.302\n",
      "test accuracy on epoch 196: 0.846\n",
      "train loss on epoch 197 : 0.142\n",
      "train accuracy on epoch 197: 1.000\n",
      "test loss on epoch 197: 0.311\n",
      "test accuracy on epoch 197: 0.692\n",
      "train loss on epoch 198 : 0.204\n",
      "train accuracy on epoch 198: 0.889\n",
      "test loss on epoch 198: 0.295\n",
      "test accuracy on epoch 198: 0.769\n",
      "train loss on epoch 199 : 0.176\n",
      "train accuracy on epoch 199: 0.889\n",
      "test loss on epoch 199: 0.298\n",
      "test accuracy on epoch 199: 0.846\n",
      "train loss on epoch 200 : 0.347\n",
      "train accuracy on epoch 200: 0.889\n",
      "test loss on epoch 200: 0.300\n",
      "test accuracy on epoch 200: 0.769\n",
      "train loss on epoch 201 : 0.123\n",
      "train accuracy on epoch 201: 0.944\n",
      "test loss on epoch 201: 0.295\n",
      "test accuracy on epoch 201: 0.769\n",
      "train loss on epoch 202 : 0.170\n",
      "train accuracy on epoch 202: 0.889\n",
      "test loss on epoch 202: 0.311\n",
      "test accuracy on epoch 202: 0.769\n",
      "train loss on epoch 203 : 0.206\n",
      "train accuracy on epoch 203: 0.944\n",
      "test loss on epoch 203: 0.299\n",
      "test accuracy on epoch 203: 0.769\n",
      "train loss on epoch 204 : 0.086\n",
      "train accuracy on epoch 204: 1.000\n",
      "test loss on epoch 204: 0.312\n",
      "test accuracy on epoch 204: 0.769\n",
      "train loss on epoch 205 : 0.207\n",
      "train accuracy on epoch 205: 0.944\n",
      "test loss on epoch 205: 0.307\n",
      "test accuracy on epoch 205: 0.769\n",
      "train loss on epoch 206 : 0.073\n",
      "train accuracy on epoch 206: 0.944\n",
      "test loss on epoch 206: 0.298\n",
      "test accuracy on epoch 206: 0.769\n",
      "train loss on epoch 207 : 0.315\n",
      "train accuracy on epoch 207: 0.889\n",
      "test loss on epoch 207: 0.296\n",
      "test accuracy on epoch 207: 0.769\n",
      "train loss on epoch 208 : 0.147\n",
      "train accuracy on epoch 208: 0.944\n",
      "test loss on epoch 208: 0.310\n",
      "test accuracy on epoch 208: 0.769\n",
      "train loss on epoch 209 : 0.108\n",
      "train accuracy on epoch 209: 0.944\n",
      "test loss on epoch 209: 0.315\n",
      "test accuracy on epoch 209: 0.769\n",
      "train loss on epoch 210 : 0.226\n",
      "train accuracy on epoch 210: 0.889\n",
      "test loss on epoch 210: 0.311\n",
      "test accuracy on epoch 210: 0.769\n",
      "train loss on epoch 211 : 0.265\n",
      "train accuracy on epoch 211: 0.833\n",
      "test loss on epoch 211: 0.298\n",
      "test accuracy on epoch 211: 0.769\n",
      "train loss on epoch 212 : 0.263\n",
      "train accuracy on epoch 212: 0.889\n",
      "test loss on epoch 212: 0.319\n",
      "test accuracy on epoch 212: 0.769\n",
      "train loss on epoch 213 : 0.078\n",
      "train accuracy on epoch 213: 1.000\n",
      "test loss on epoch 213: 0.313\n",
      "test accuracy on epoch 213: 0.769\n",
      "train loss on epoch 214 : 0.274\n",
      "train accuracy on epoch 214: 0.944\n",
      "test loss on epoch 214: 0.302\n",
      "test accuracy on epoch 214: 0.769\n",
      "train loss on epoch 215 : 0.091\n",
      "train accuracy on epoch 215: 1.000\n",
      "test loss on epoch 215: 0.303\n",
      "test accuracy on epoch 215: 0.769\n",
      "train loss on epoch 216 : 0.173\n",
      "train accuracy on epoch 216: 0.944\n",
      "test loss on epoch 216: 0.302\n",
      "test accuracy on epoch 216: 0.846\n",
      "train loss on epoch 217 : 0.156\n",
      "train accuracy on epoch 217: 0.944\n",
      "test loss on epoch 217: 0.311\n",
      "test accuracy on epoch 217: 0.769\n",
      "train loss on epoch 218 : 0.105\n",
      "train accuracy on epoch 218: 0.944\n",
      "test loss on epoch 218: 0.310\n",
      "test accuracy on epoch 218: 0.769\n",
      "train loss on epoch 219 : 0.386\n",
      "train accuracy on epoch 219: 0.889\n",
      "test loss on epoch 219: 0.312\n",
      "test accuracy on epoch 219: 0.769\n",
      "train loss on epoch 220 : 0.359\n",
      "train accuracy on epoch 220: 0.889\n",
      "test loss on epoch 220: 0.313\n",
      "test accuracy on epoch 220: 0.769\n",
      "train loss on epoch 221 : 0.263\n",
      "train accuracy on epoch 221: 0.889\n",
      "test loss on epoch 221: 0.321\n",
      "test accuracy on epoch 221: 0.769\n",
      "train loss on epoch 222 : 0.186\n",
      "train accuracy on epoch 222: 0.889\n",
      "test loss on epoch 222: 0.320\n",
      "test accuracy on epoch 222: 0.769\n",
      "train loss on epoch 223 : 0.194\n",
      "train accuracy on epoch 223: 0.889\n",
      "test loss on epoch 223: 0.318\n",
      "test accuracy on epoch 223: 0.769\n",
      "train loss on epoch 224 : 0.263\n",
      "train accuracy on epoch 224: 0.889\n",
      "test loss on epoch 224: 0.312\n",
      "test accuracy on epoch 224: 0.692\n",
      "train loss on epoch 225 : 0.140\n",
      "train accuracy on epoch 225: 0.944\n",
      "test loss on epoch 225: 0.316\n",
      "test accuracy on epoch 225: 0.769\n",
      "train loss on epoch 226 : 0.199\n",
      "train accuracy on epoch 226: 0.889\n",
      "test loss on epoch 226: 0.311\n",
      "test accuracy on epoch 226: 0.769\n",
      "train loss on epoch 227 : 0.239\n",
      "train accuracy on epoch 227: 0.944\n",
      "test loss on epoch 227: 0.314\n",
      "test accuracy on epoch 227: 0.769\n",
      "train loss on epoch 228 : 0.210\n",
      "train accuracy on epoch 228: 0.889\n",
      "test loss on epoch 228: 0.315\n",
      "test accuracy on epoch 228: 0.769\n",
      "train loss on epoch 229 : 0.153\n",
      "train accuracy on epoch 229: 0.944\n",
      "test loss on epoch 229: 0.309\n",
      "test accuracy on epoch 229: 0.769\n",
      "train loss on epoch 230 : 0.440\n",
      "train accuracy on epoch 230: 0.889\n",
      "test loss on epoch 230: 0.306\n",
      "test accuracy on epoch 230: 0.769\n",
      "train loss on epoch 231 : 0.193\n",
      "train accuracy on epoch 231: 0.889\n",
      "test loss on epoch 231: 0.317\n",
      "test accuracy on epoch 231: 0.692\n",
      "train loss on epoch 232 : 0.122\n",
      "train accuracy on epoch 232: 0.944\n",
      "test loss on epoch 232: 0.323\n",
      "test accuracy on epoch 232: 0.692\n",
      "train loss on epoch 233 : 0.061\n",
      "train accuracy on epoch 233: 1.000\n",
      "test loss on epoch 233: 0.315\n",
      "test accuracy on epoch 233: 0.769\n",
      "train loss on epoch 234 : 0.167\n",
      "train accuracy on epoch 234: 0.889\n",
      "test loss on epoch 234: 0.315\n",
      "test accuracy on epoch 234: 0.769\n",
      "train loss on epoch 235 : 0.411\n",
      "train accuracy on epoch 235: 0.833\n",
      "test loss on epoch 235: 0.322\n",
      "test accuracy on epoch 235: 0.769\n",
      "train loss on epoch 236 : 0.208\n",
      "train accuracy on epoch 236: 0.889\n",
      "test loss on epoch 236: 0.313\n",
      "test accuracy on epoch 236: 0.769\n",
      "train loss on epoch 237 : 0.163\n",
      "train accuracy on epoch 237: 0.889\n",
      "test loss on epoch 237: 0.312\n",
      "test accuracy on epoch 237: 0.769\n",
      "train loss on epoch 238 : 0.270\n",
      "train accuracy on epoch 238: 0.944\n",
      "test loss on epoch 238: 0.325\n",
      "test accuracy on epoch 238: 0.769\n",
      "train loss on epoch 239 : 0.040\n",
      "train accuracy on epoch 239: 1.000\n",
      "test loss on epoch 239: 0.319\n",
      "test accuracy on epoch 239: 0.769\n",
      "train loss on epoch 240 : 0.186\n",
      "train accuracy on epoch 240: 0.889\n",
      "test loss on epoch 240: 0.323\n",
      "test accuracy on epoch 240: 0.769\n",
      "train loss on epoch 241 : 0.199\n",
      "train accuracy on epoch 241: 0.889\n",
      "test loss on epoch 241: 0.328\n",
      "test accuracy on epoch 241: 0.769\n",
      "train loss on epoch 242 : 0.276\n",
      "train accuracy on epoch 242: 0.889\n",
      "test loss on epoch 242: 0.318\n",
      "test accuracy on epoch 242: 0.769\n",
      "train loss on epoch 243 : 0.211\n",
      "train accuracy on epoch 243: 0.889\n",
      "test loss on epoch 243: 0.308\n",
      "test accuracy on epoch 243: 0.769\n",
      "train loss on epoch 244 : 0.124\n",
      "train accuracy on epoch 244: 0.944\n",
      "test loss on epoch 244: 0.318\n",
      "test accuracy on epoch 244: 0.769\n",
      "train loss on epoch 245 : 0.118\n",
      "train accuracy on epoch 245: 0.944\n",
      "test loss on epoch 245: 0.316\n",
      "test accuracy on epoch 245: 0.769\n",
      "train loss on epoch 246 : 0.204\n",
      "train accuracy on epoch 246: 0.889\n",
      "test loss on epoch 246: 0.305\n",
      "test accuracy on epoch 246: 0.769\n",
      "train loss on epoch 247 : 0.258\n",
      "train accuracy on epoch 247: 0.889\n",
      "test loss on epoch 247: 0.323\n",
      "test accuracy on epoch 247: 0.769\n",
      "train loss on epoch 248 : 0.237\n",
      "train accuracy on epoch 248: 0.944\n",
      "test loss on epoch 248: 0.323\n",
      "test accuracy on epoch 248: 0.769\n",
      "train loss on epoch 249 : 0.152\n",
      "train accuracy on epoch 249: 0.889\n",
      "test loss on epoch 249: 0.320\n",
      "test accuracy on epoch 249: 0.769\n",
      "train loss on epoch 250 : 0.154\n",
      "train accuracy on epoch 250: 0.944\n",
      "test loss on epoch 250: 0.309\n",
      "test accuracy on epoch 250: 0.769\n",
      "train loss on epoch 251 : 0.116\n",
      "train accuracy on epoch 251: 0.944\n",
      "test loss on epoch 251: 0.327\n",
      "test accuracy on epoch 251: 0.769\n",
      "train loss on epoch 252 : 0.202\n",
      "train accuracy on epoch 252: 0.889\n",
      "test loss on epoch 252: 0.306\n",
      "test accuracy on epoch 252: 0.769\n",
      "train loss on epoch 253 : 0.240\n",
      "train accuracy on epoch 253: 0.889\n",
      "test loss on epoch 253: 0.328\n",
      "test accuracy on epoch 253: 0.769\n",
      "train loss on epoch 254 : 0.183\n",
      "train accuracy on epoch 254: 0.944\n",
      "test loss on epoch 254: 0.316\n",
      "test accuracy on epoch 254: 0.769\n",
      "train loss on epoch 255 : 0.198\n",
      "train accuracy on epoch 255: 0.944\n",
      "test loss on epoch 255: 0.322\n",
      "test accuracy on epoch 255: 0.769\n",
      "train loss on epoch 256 : 0.151\n",
      "train accuracy on epoch 256: 0.889\n",
      "test loss on epoch 256: 0.303\n",
      "test accuracy on epoch 256: 0.769\n",
      "train loss on epoch 257 : 0.227\n",
      "train accuracy on epoch 257: 0.889\n",
      "test loss on epoch 257: 0.322\n",
      "test accuracy on epoch 257: 0.692\n",
      "train loss on epoch 258 : 0.056\n",
      "train accuracy on epoch 258: 1.000\n",
      "test loss on epoch 258: 0.318\n",
      "test accuracy on epoch 258: 0.692\n",
      "train loss on epoch 259 : 0.345\n",
      "train accuracy on epoch 259: 0.833\n",
      "test loss on epoch 259: 0.310\n",
      "test accuracy on epoch 259: 0.846\n",
      "train loss on epoch 260 : 0.225\n",
      "train accuracy on epoch 260: 0.944\n",
      "test loss on epoch 260: 0.319\n",
      "test accuracy on epoch 260: 0.769\n",
      "train loss on epoch 261 : 0.102\n",
      "train accuracy on epoch 261: 0.889\n",
      "test loss on epoch 261: 0.316\n",
      "test accuracy on epoch 261: 0.769\n",
      "train loss on epoch 262 : 0.207\n",
      "train accuracy on epoch 262: 0.889\n",
      "test loss on epoch 262: 0.316\n",
      "test accuracy on epoch 262: 0.769\n",
      "train loss on epoch 263 : 0.066\n",
      "train accuracy on epoch 263: 1.000\n",
      "test loss on epoch 263: 0.311\n",
      "test accuracy on epoch 263: 0.769\n",
      "train loss on epoch 264 : 0.206\n",
      "train accuracy on epoch 264: 0.889\n",
      "test loss on epoch 264: 0.320\n",
      "test accuracy on epoch 264: 0.769\n",
      "train loss on epoch 265 : 0.583\n",
      "train accuracy on epoch 265: 0.833\n",
      "test loss on epoch 265: 0.309\n",
      "test accuracy on epoch 265: 0.769\n",
      "train loss on epoch 266 : 0.271\n",
      "train accuracy on epoch 266: 0.889\n",
      "test loss on epoch 266: 0.319\n",
      "test accuracy on epoch 266: 0.769\n",
      "train loss on epoch 267 : 0.192\n",
      "train accuracy on epoch 267: 0.889\n",
      "test loss on epoch 267: 0.333\n",
      "test accuracy on epoch 267: 0.769\n",
      "train loss on epoch 268 : 0.046\n",
      "train accuracy on epoch 268: 1.000\n",
      "test loss on epoch 268: 0.317\n",
      "test accuracy on epoch 268: 0.769\n",
      "train loss on epoch 269 : 0.181\n",
      "train accuracy on epoch 269: 0.944\n",
      "test loss on epoch 269: 0.328\n",
      "test accuracy on epoch 269: 0.769\n",
      "train loss on epoch 270 : 0.165\n",
      "train accuracy on epoch 270: 0.944\n",
      "test loss on epoch 270: 0.313\n",
      "test accuracy on epoch 270: 0.769\n",
      "train loss on epoch 271 : 0.143\n",
      "train accuracy on epoch 271: 0.944\n",
      "test loss on epoch 271: 0.315\n",
      "test accuracy on epoch 271: 0.769\n",
      "train loss on epoch 272 : 0.211\n",
      "train accuracy on epoch 272: 0.944\n",
      "test loss on epoch 272: 0.324\n",
      "test accuracy on epoch 272: 0.769\n",
      "train loss on epoch 273 : 0.058\n",
      "train accuracy on epoch 273: 1.000\n",
      "test loss on epoch 273: 0.320\n",
      "test accuracy on epoch 273: 0.769\n",
      "train loss on epoch 274 : 0.143\n",
      "train accuracy on epoch 274: 0.944\n",
      "test loss on epoch 274: 0.310\n",
      "test accuracy on epoch 274: 0.769\n",
      "train loss on epoch 275 : 0.181\n",
      "train accuracy on epoch 275: 0.944\n",
      "test loss on epoch 275: 0.322\n",
      "test accuracy on epoch 275: 0.769\n",
      "train loss on epoch 276 : 0.074\n",
      "train accuracy on epoch 276: 1.000\n",
      "test loss on epoch 276: 0.305\n",
      "test accuracy on epoch 276: 0.769\n",
      "train loss on epoch 277 : 0.219\n",
      "train accuracy on epoch 277: 0.833\n",
      "test loss on epoch 277: 0.319\n",
      "test accuracy on epoch 277: 0.769\n",
      "train loss on epoch 278 : 0.207\n",
      "train accuracy on epoch 278: 0.889\n",
      "test loss on epoch 278: 0.310\n",
      "test accuracy on epoch 278: 0.769\n",
      "train loss on epoch 279 : 0.286\n",
      "train accuracy on epoch 279: 0.889\n",
      "test loss on epoch 279: 0.307\n",
      "test accuracy on epoch 279: 0.769\n",
      "train loss on epoch 280 : 0.245\n",
      "train accuracy on epoch 280: 0.944\n",
      "test loss on epoch 280: 0.307\n",
      "test accuracy on epoch 280: 0.769\n",
      "train loss on epoch 281 : 0.260\n",
      "train accuracy on epoch 281: 0.889\n",
      "test loss on epoch 281: 0.308\n",
      "test accuracy on epoch 281: 0.769\n",
      "train loss on epoch 282 : 0.142\n",
      "train accuracy on epoch 282: 0.944\n",
      "test loss on epoch 282: 0.314\n",
      "test accuracy on epoch 282: 0.769\n",
      "train loss on epoch 283 : 0.051\n",
      "train accuracy on epoch 283: 1.000\n",
      "test loss on epoch 283: 0.316\n",
      "test accuracy on epoch 283: 0.769\n",
      "train loss on epoch 284 : 0.182\n",
      "train accuracy on epoch 284: 0.889\n",
      "test loss on epoch 284: 0.321\n",
      "test accuracy on epoch 284: 0.769\n",
      "train loss on epoch 285 : 0.093\n",
      "train accuracy on epoch 285: 1.000\n",
      "test loss on epoch 285: 0.309\n",
      "test accuracy on epoch 285: 0.769\n",
      "train loss on epoch 286 : 0.078\n",
      "train accuracy on epoch 286: 0.944\n",
      "test loss on epoch 286: 0.306\n",
      "test accuracy on epoch 286: 0.769\n",
      "train loss on epoch 287 : 0.398\n",
      "train accuracy on epoch 287: 0.833\n",
      "test loss on epoch 287: 0.318\n",
      "test accuracy on epoch 287: 0.692\n",
      "train loss on epoch 288 : 0.202\n",
      "train accuracy on epoch 288: 0.944\n",
      "test loss on epoch 288: 0.320\n",
      "test accuracy on epoch 288: 0.769\n",
      "train loss on epoch 289 : 0.106\n",
      "train accuracy on epoch 289: 0.944\n",
      "test loss on epoch 289: 0.305\n",
      "test accuracy on epoch 289: 0.769\n",
      "train loss on epoch 290 : 0.272\n",
      "train accuracy on epoch 290: 0.889\n",
      "test loss on epoch 290: 0.301\n",
      "test accuracy on epoch 290: 0.769\n",
      "train loss on epoch 291 : 0.153\n",
      "train accuracy on epoch 291: 0.889\n",
      "test loss on epoch 291: 0.321\n",
      "test accuracy on epoch 291: 0.769\n",
      "train loss on epoch 292 : 0.124\n",
      "train accuracy on epoch 292: 0.944\n",
      "test loss on epoch 292: 0.327\n",
      "test accuracy on epoch 292: 0.769\n",
      "train loss on epoch 293 : 0.110\n",
      "train accuracy on epoch 293: 1.000\n",
      "test loss on epoch 293: 0.311\n",
      "test accuracy on epoch 293: 0.769\n",
      "train loss on epoch 294 : 0.169\n",
      "train accuracy on epoch 294: 0.889\n",
      "test loss on epoch 294: 0.326\n",
      "test accuracy on epoch 294: 0.769\n",
      "train loss on epoch 295 : 0.115\n",
      "train accuracy on epoch 295: 0.944\n",
      "test loss on epoch 295: 0.306\n",
      "test accuracy on epoch 295: 0.769\n",
      "train loss on epoch 296 : 0.156\n",
      "train accuracy on epoch 296: 0.944\n",
      "test loss on epoch 296: 0.304\n",
      "test accuracy on epoch 296: 0.769\n",
      "train loss on epoch 297 : 0.249\n",
      "train accuracy on epoch 297: 0.889\n",
      "test loss on epoch 297: 0.320\n",
      "test accuracy on epoch 297: 0.769\n",
      "train loss on epoch 298 : 0.159\n",
      "train accuracy on epoch 298: 0.944\n",
      "test loss on epoch 298: 0.311\n",
      "test accuracy on epoch 298: 0.769\n",
      "train loss on epoch 299 : 0.186\n",
      "train accuracy on epoch 299: 0.889\n",
      "test loss on epoch 299: 0.313\n",
      "test accuracy on epoch 299: 0.769\n",
      "train loss on epoch 300 : 0.044\n",
      "train accuracy on epoch 300: 1.000\n",
      "test loss on epoch 300: 0.323\n",
      "test accuracy on epoch 300: 0.769\n",
      "train loss on epoch 301 : 0.166\n",
      "train accuracy on epoch 301: 0.889\n",
      "test loss on epoch 301: 0.319\n",
      "test accuracy on epoch 301: 0.769\n",
      "train loss on epoch 302 : 0.121\n",
      "train accuracy on epoch 302: 1.000\n",
      "test loss on epoch 302: 0.320\n",
      "test accuracy on epoch 302: 0.769\n",
      "train loss on epoch 303 : 0.084\n",
      "train accuracy on epoch 303: 0.944\n",
      "test loss on epoch 303: 0.318\n",
      "test accuracy on epoch 303: 0.769\n",
      "train loss on epoch 304 : 0.106\n",
      "train accuracy on epoch 304: 1.000\n",
      "test loss on epoch 304: 0.314\n",
      "test accuracy on epoch 304: 0.769\n",
      "train loss on epoch 305 : 0.158\n",
      "train accuracy on epoch 305: 0.889\n",
      "test loss on epoch 305: 0.313\n",
      "test accuracy on epoch 305: 0.769\n",
      "train loss on epoch 306 : 0.317\n",
      "train accuracy on epoch 306: 0.889\n",
      "test loss on epoch 306: 0.319\n",
      "test accuracy on epoch 306: 0.769\n",
      "train loss on epoch 307 : 0.203\n",
      "train accuracy on epoch 307: 0.944\n",
      "test loss on epoch 307: 0.317\n",
      "test accuracy on epoch 307: 0.769\n",
      "train loss on epoch 308 : 0.210\n",
      "train accuracy on epoch 308: 0.889\n",
      "test loss on epoch 308: 0.318\n",
      "test accuracy on epoch 308: 0.769\n",
      "train loss on epoch 309 : 0.210\n",
      "train accuracy on epoch 309: 0.889\n",
      "test loss on epoch 309: 0.320\n",
      "test accuracy on epoch 309: 0.769\n",
      "train loss on epoch 310 : 0.104\n",
      "train accuracy on epoch 310: 1.000\n",
      "test loss on epoch 310: 0.316\n",
      "test accuracy on epoch 310: 0.769\n",
      "train loss on epoch 311 : 0.139\n",
      "train accuracy on epoch 311: 0.944\n",
      "test loss on epoch 311: 0.305\n",
      "test accuracy on epoch 311: 0.769\n",
      "train loss on epoch 312 : 0.304\n",
      "train accuracy on epoch 312: 0.778\n",
      "test loss on epoch 312: 0.301\n",
      "test accuracy on epoch 312: 0.769\n",
      "train loss on epoch 313 : 0.366\n",
      "train accuracy on epoch 313: 0.889\n",
      "test loss on epoch 313: 0.309\n",
      "test accuracy on epoch 313: 0.769\n",
      "train loss on epoch 314 : 0.247\n",
      "train accuracy on epoch 314: 0.889\n",
      "test loss on epoch 314: 0.317\n",
      "test accuracy on epoch 314: 0.692\n",
      "train loss on epoch 315 : 0.071\n",
      "train accuracy on epoch 315: 1.000\n",
      "test loss on epoch 315: 0.316\n",
      "test accuracy on epoch 315: 0.769\n",
      "train loss on epoch 316 : 0.057\n",
      "train accuracy on epoch 316: 1.000\n",
      "test loss on epoch 316: 0.315\n",
      "test accuracy on epoch 316: 0.769\n",
      "train loss on epoch 317 : 0.110\n",
      "train accuracy on epoch 317: 0.944\n",
      "test loss on epoch 317: 0.322\n",
      "test accuracy on epoch 317: 0.769\n",
      "train loss on epoch 318 : 0.184\n",
      "train accuracy on epoch 318: 0.889\n",
      "test loss on epoch 318: 0.318\n",
      "test accuracy on epoch 318: 0.769\n",
      "train loss on epoch 319 : 0.069\n",
      "train accuracy on epoch 319: 0.944\n",
      "test loss on epoch 319: 0.318\n",
      "test accuracy on epoch 319: 0.769\n",
      "train loss on epoch 320 : 0.093\n",
      "train accuracy on epoch 320: 0.944\n",
      "test loss on epoch 320: 0.316\n",
      "test accuracy on epoch 320: 0.769\n",
      "train loss on epoch 321 : 0.207\n",
      "train accuracy on epoch 321: 0.944\n",
      "test loss on epoch 321: 0.308\n",
      "test accuracy on epoch 321: 0.846\n",
      "train loss on epoch 322 : 0.167\n",
      "train accuracy on epoch 322: 0.944\n",
      "test loss on epoch 322: 0.319\n",
      "test accuracy on epoch 322: 0.769\n",
      "train loss on epoch 323 : 0.115\n",
      "train accuracy on epoch 323: 0.944\n",
      "test loss on epoch 323: 0.315\n",
      "test accuracy on epoch 323: 0.769\n",
      "train loss on epoch 324 : 0.177\n",
      "train accuracy on epoch 324: 0.944\n",
      "test loss on epoch 324: 0.309\n",
      "test accuracy on epoch 324: 0.769\n",
      "train loss on epoch 325 : 0.261\n",
      "train accuracy on epoch 325: 0.944\n",
      "test loss on epoch 325: 0.311\n",
      "test accuracy on epoch 325: 0.769\n",
      "train loss on epoch 326 : 0.160\n",
      "train accuracy on epoch 326: 0.889\n",
      "test loss on epoch 326: 0.307\n",
      "test accuracy on epoch 326: 0.769\n",
      "train loss on epoch 327 : 0.094\n",
      "train accuracy on epoch 327: 1.000\n",
      "test loss on epoch 327: 0.316\n",
      "test accuracy on epoch 327: 0.692\n",
      "train loss on epoch 328 : 0.042\n",
      "train accuracy on epoch 328: 1.000\n",
      "test loss on epoch 328: 0.314\n",
      "test accuracy on epoch 328: 0.769\n",
      "train loss on epoch 329 : 0.120\n",
      "train accuracy on epoch 329: 0.944\n",
      "test loss on epoch 329: 0.310\n",
      "test accuracy on epoch 329: 0.769\n",
      "train loss on epoch 330 : 0.195\n",
      "train accuracy on epoch 330: 0.944\n",
      "test loss on epoch 330: 0.316\n",
      "test accuracy on epoch 330: 0.769\n",
      "train loss on epoch 331 : 0.243\n",
      "train accuracy on epoch 331: 0.944\n",
      "test loss on epoch 331: 0.309\n",
      "test accuracy on epoch 331: 0.769\n",
      "train loss on epoch 332 : 0.242\n",
      "train accuracy on epoch 332: 0.889\n",
      "test loss on epoch 332: 0.313\n",
      "test accuracy on epoch 332: 0.769\n",
      "train loss on epoch 333 : 0.319\n",
      "train accuracy on epoch 333: 0.944\n",
      "test loss on epoch 333: 0.319\n",
      "test accuracy on epoch 333: 0.769\n",
      "train loss on epoch 334 : 0.193\n",
      "train accuracy on epoch 334: 0.944\n",
      "test loss on epoch 334: 0.322\n",
      "test accuracy on epoch 334: 0.769\n",
      "train loss on epoch 335 : 0.074\n",
      "train accuracy on epoch 335: 1.000\n",
      "test loss on epoch 335: 0.321\n",
      "test accuracy on epoch 335: 0.769\n",
      "train loss on epoch 336 : 0.233\n",
      "train accuracy on epoch 336: 0.944\n",
      "test loss on epoch 336: 0.319\n",
      "test accuracy on epoch 336: 0.769\n",
      "train loss on epoch 337 : 0.102\n",
      "train accuracy on epoch 337: 0.944\n",
      "test loss on epoch 337: 0.318\n",
      "test accuracy on epoch 337: 0.769\n",
      "train loss on epoch 338 : 0.074\n",
      "train accuracy on epoch 338: 1.000\n",
      "test loss on epoch 338: 0.318\n",
      "test accuracy on epoch 338: 0.769\n",
      "train loss on epoch 339 : 0.151\n",
      "train accuracy on epoch 339: 0.889\n",
      "test loss on epoch 339: 0.319\n",
      "test accuracy on epoch 339: 0.769\n",
      "train loss on epoch 340 : 0.137\n",
      "train accuracy on epoch 340: 0.944\n",
      "test loss on epoch 340: 0.311\n",
      "test accuracy on epoch 340: 0.769\n",
      "train loss on epoch 341 : 0.250\n",
      "train accuracy on epoch 341: 0.944\n",
      "test loss on epoch 341: 0.322\n",
      "test accuracy on epoch 341: 0.769\n",
      "train loss on epoch 342 : 0.148\n",
      "train accuracy on epoch 342: 0.944\n",
      "test loss on epoch 342: 0.328\n",
      "test accuracy on epoch 342: 0.769\n",
      "train loss on epoch 343 : 0.056\n",
      "train accuracy on epoch 343: 1.000\n",
      "test loss on epoch 343: 0.326\n",
      "test accuracy on epoch 343: 0.769\n",
      "train loss on epoch 344 : 0.043\n",
      "train accuracy on epoch 344: 1.000\n",
      "test loss on epoch 344: 0.323\n",
      "test accuracy on epoch 344: 0.769\n",
      "train loss on epoch 345 : 0.227\n",
      "train accuracy on epoch 345: 0.889\n",
      "test loss on epoch 345: 0.323\n",
      "test accuracy on epoch 345: 0.769\n",
      "train loss on epoch 346 : 0.085\n",
      "train accuracy on epoch 346: 1.000\n",
      "test loss on epoch 346: 0.306\n",
      "test accuracy on epoch 346: 0.769\n",
      "train loss on epoch 347 : 0.123\n",
      "train accuracy on epoch 347: 0.944\n",
      "test loss on epoch 347: 0.323\n",
      "test accuracy on epoch 347: 0.769\n",
      "train loss on epoch 348 : 0.147\n",
      "train accuracy on epoch 348: 0.944\n",
      "test loss on epoch 348: 0.319\n",
      "test accuracy on epoch 348: 0.769\n",
      "train loss on epoch 349 : 0.063\n",
      "train accuracy on epoch 349: 1.000\n",
      "test loss on epoch 349: 0.321\n",
      "test accuracy on epoch 349: 0.769\n",
      "train loss on epoch 350 : 0.199\n",
      "train accuracy on epoch 350: 0.944\n",
      "test loss on epoch 350: 0.312\n",
      "test accuracy on epoch 350: 0.769\n",
      "train loss on epoch 351 : 0.062\n",
      "train accuracy on epoch 351: 1.000\n",
      "test loss on epoch 351: 0.322\n",
      "test accuracy on epoch 351: 0.769\n",
      "train loss on epoch 352 : 0.135\n",
      "train accuracy on epoch 352: 0.944\n",
      "test loss on epoch 352: 0.308\n",
      "test accuracy on epoch 352: 0.769\n",
      "train loss on epoch 353 : 0.095\n",
      "train accuracy on epoch 353: 0.944\n",
      "test loss on epoch 353: 0.308\n",
      "test accuracy on epoch 353: 0.769\n",
      "train loss on epoch 354 : 0.055\n",
      "train accuracy on epoch 354: 1.000\n",
      "test loss on epoch 354: 0.306\n",
      "test accuracy on epoch 354: 0.769\n",
      "train loss on epoch 355 : 0.219\n",
      "train accuracy on epoch 355: 0.944\n",
      "test loss on epoch 355: 0.311\n",
      "test accuracy on epoch 355: 0.846\n",
      "train loss on epoch 356 : 0.316\n",
      "train accuracy on epoch 356: 0.889\n",
      "test loss on epoch 356: 0.313\n",
      "test accuracy on epoch 356: 0.846\n",
      "train loss on epoch 357 : 0.204\n",
      "train accuracy on epoch 357: 0.833\n",
      "test loss on epoch 357: 0.313\n",
      "test accuracy on epoch 357: 0.769\n",
      "train loss on epoch 358 : 0.252\n",
      "train accuracy on epoch 358: 0.944\n",
      "test loss on epoch 358: 0.326\n",
      "test accuracy on epoch 358: 0.769\n",
      "train loss on epoch 359 : 0.385\n",
      "train accuracy on epoch 359: 0.833\n",
      "test loss on epoch 359: 0.318\n",
      "test accuracy on epoch 359: 0.769\n",
      "train loss on epoch 360 : 0.248\n",
      "train accuracy on epoch 360: 0.944\n",
      "test loss on epoch 360: 0.328\n",
      "test accuracy on epoch 360: 0.769\n",
      "train loss on epoch 361 : 0.291\n",
      "train accuracy on epoch 361: 0.833\n",
      "test loss on epoch 361: 0.326\n",
      "test accuracy on epoch 361: 0.769\n",
      "train loss on epoch 362 : 0.315\n",
      "train accuracy on epoch 362: 0.889\n",
      "test loss on epoch 362: 0.321\n",
      "test accuracy on epoch 362: 0.769\n",
      "train loss on epoch 363 : 0.083\n",
      "train accuracy on epoch 363: 1.000\n",
      "test loss on epoch 363: 0.327\n",
      "test accuracy on epoch 363: 0.769\n",
      "train loss on epoch 364 : 0.339\n",
      "train accuracy on epoch 364: 0.889\n",
      "test loss on epoch 364: 0.323\n",
      "test accuracy on epoch 364: 0.769\n",
      "train loss on epoch 365 : 0.182\n",
      "train accuracy on epoch 365: 0.944\n",
      "test loss on epoch 365: 0.320\n",
      "test accuracy on epoch 365: 0.769\n",
      "train loss on epoch 366 : 0.343\n",
      "train accuracy on epoch 366: 0.833\n",
      "test loss on epoch 366: 0.310\n",
      "test accuracy on epoch 366: 0.769\n",
      "train loss on epoch 367 : 0.135\n",
      "train accuracy on epoch 367: 0.944\n",
      "test loss on epoch 367: 0.314\n",
      "test accuracy on epoch 367: 0.769\n",
      "train loss on epoch 368 : 0.162\n",
      "train accuracy on epoch 368: 0.889\n",
      "test loss on epoch 368: 0.315\n",
      "test accuracy on epoch 368: 0.769\n",
      "train loss on epoch 369 : 0.197\n",
      "train accuracy on epoch 369: 0.944\n",
      "test loss on epoch 369: 0.313\n",
      "test accuracy on epoch 369: 0.769\n",
      "train loss on epoch 370 : 0.194\n",
      "train accuracy on epoch 370: 0.889\n",
      "test loss on epoch 370: 0.314\n",
      "test accuracy on epoch 370: 0.769\n",
      "train loss on epoch 371 : 0.131\n",
      "train accuracy on epoch 371: 0.944\n",
      "test loss on epoch 371: 0.319\n",
      "test accuracy on epoch 371: 0.769\n",
      "train loss on epoch 372 : 0.093\n",
      "train accuracy on epoch 372: 1.000\n",
      "test loss on epoch 372: 0.322\n",
      "test accuracy on epoch 372: 0.769\n",
      "train loss on epoch 373 : 0.213\n",
      "train accuracy on epoch 373: 0.889\n",
      "test loss on epoch 373: 0.322\n",
      "test accuracy on epoch 373: 0.769\n",
      "train loss on epoch 374 : 0.078\n",
      "train accuracy on epoch 374: 0.944\n",
      "test loss on epoch 374: 0.320\n",
      "test accuracy on epoch 374: 0.769\n",
      "train loss on epoch 375 : 0.278\n",
      "train accuracy on epoch 375: 0.889\n",
      "test loss on epoch 375: 0.322\n",
      "test accuracy on epoch 375: 0.769\n",
      "train loss on epoch 376 : 0.125\n",
      "train accuracy on epoch 376: 0.944\n",
      "test loss on epoch 376: 0.328\n",
      "test accuracy on epoch 376: 0.769\n",
      "train loss on epoch 377 : 0.038\n",
      "train accuracy on epoch 377: 1.000\n",
      "test loss on epoch 377: 0.328\n",
      "test accuracy on epoch 377: 0.769\n",
      "train loss on epoch 378 : 0.253\n",
      "train accuracy on epoch 378: 0.944\n",
      "test loss on epoch 378: 0.332\n",
      "test accuracy on epoch 378: 0.769\n",
      "train loss on epoch 379 : 0.083\n",
      "train accuracy on epoch 379: 1.000\n",
      "test loss on epoch 379: 0.332\n",
      "test accuracy on epoch 379: 0.769\n",
      "train loss on epoch 380 : 0.140\n",
      "train accuracy on epoch 380: 0.889\n",
      "test loss on epoch 380: 0.333\n",
      "test accuracy on epoch 380: 0.769\n",
      "train loss on epoch 381 : 0.128\n",
      "train accuracy on epoch 381: 1.000\n",
      "test loss on epoch 381: 0.333\n",
      "test accuracy on epoch 381: 0.769\n",
      "train loss on epoch 382 : 0.166\n",
      "train accuracy on epoch 382: 0.889\n",
      "test loss on epoch 382: 0.342\n",
      "test accuracy on epoch 382: 0.769\n",
      "train loss on epoch 383 : 0.169\n",
      "train accuracy on epoch 383: 0.944\n",
      "test loss on epoch 383: 0.345\n",
      "test accuracy on epoch 383: 0.769\n",
      "train loss on epoch 384 : 0.079\n",
      "train accuracy on epoch 384: 1.000\n",
      "test loss on epoch 384: 0.344\n",
      "test accuracy on epoch 384: 0.769\n",
      "train loss on epoch 385 : 0.159\n",
      "train accuracy on epoch 385: 0.944\n",
      "test loss on epoch 385: 0.342\n",
      "test accuracy on epoch 385: 0.769\n",
      "train loss on epoch 386 : 0.328\n",
      "train accuracy on epoch 386: 0.778\n",
      "test loss on epoch 386: 0.330\n",
      "test accuracy on epoch 386: 0.769\n",
      "train loss on epoch 387 : 0.057\n",
      "train accuracy on epoch 387: 1.000\n",
      "test loss on epoch 387: 0.324\n",
      "test accuracy on epoch 387: 0.769\n",
      "train loss on epoch 388 : 0.056\n",
      "train accuracy on epoch 388: 1.000\n",
      "test loss on epoch 388: 0.316\n",
      "test accuracy on epoch 388: 0.769\n",
      "train loss on epoch 389 : 0.282\n",
      "train accuracy on epoch 389: 0.889\n",
      "test loss on epoch 389: 0.310\n",
      "test accuracy on epoch 389: 0.769\n",
      "train loss on epoch 390 : 0.155\n",
      "train accuracy on epoch 390: 0.944\n",
      "test loss on epoch 390: 0.319\n",
      "test accuracy on epoch 390: 0.769\n",
      "train loss on epoch 391 : 0.392\n",
      "train accuracy on epoch 391: 0.889\n",
      "test loss on epoch 391: 0.304\n",
      "test accuracy on epoch 391: 0.769\n",
      "train loss on epoch 392 : 0.218\n",
      "train accuracy on epoch 392: 0.889\n",
      "test loss on epoch 392: 0.307\n",
      "test accuracy on epoch 392: 0.769\n",
      "train loss on epoch 393 : 0.083\n",
      "train accuracy on epoch 393: 1.000\n",
      "test loss on epoch 393: 0.314\n",
      "test accuracy on epoch 393: 0.692\n",
      "train loss on epoch 394 : 0.164\n",
      "train accuracy on epoch 394: 0.889\n",
      "test loss on epoch 394: 0.314\n",
      "test accuracy on epoch 394: 0.769\n",
      "train loss on epoch 395 : 0.045\n",
      "train accuracy on epoch 395: 1.000\n",
      "test loss on epoch 395: 0.304\n",
      "test accuracy on epoch 395: 0.769\n",
      "train loss on epoch 396 : 0.127\n",
      "train accuracy on epoch 396: 0.944\n",
      "test loss on epoch 396: 0.304\n",
      "test accuracy on epoch 396: 0.769\n",
      "train loss on epoch 397 : 0.210\n",
      "train accuracy on epoch 397: 0.889\n",
      "test loss on epoch 397: 0.302\n",
      "test accuracy on epoch 397: 0.846\n",
      "train loss on epoch 398 : 0.048\n",
      "train accuracy on epoch 398: 1.000\n",
      "test loss on epoch 398: 0.317\n",
      "test accuracy on epoch 398: 0.692\n",
      "train loss on epoch 399 : 0.199\n",
      "train accuracy on epoch 399: 0.889\n",
      "test loss on epoch 399: 0.304\n",
      "test accuracy on epoch 399: 0.846\n",
      "train loss on epoch 400 : 0.256\n",
      "train accuracy on epoch 400: 0.889\n",
      "test loss on epoch 400: 0.304\n",
      "test accuracy on epoch 400: 0.846\n",
      "train loss on epoch 401 : 0.086\n",
      "train accuracy on epoch 401: 0.944\n",
      "test loss on epoch 401: 0.317\n",
      "test accuracy on epoch 401: 0.769\n",
      "train loss on epoch 402 : 0.189\n",
      "train accuracy on epoch 402: 0.889\n",
      "test loss on epoch 402: 0.301\n",
      "test accuracy on epoch 402: 0.769\n",
      "train loss on epoch 403 : 0.350\n",
      "train accuracy on epoch 403: 0.833\n",
      "test loss on epoch 403: 0.301\n",
      "test accuracy on epoch 403: 0.769\n",
      "train loss on epoch 404 : 0.152\n",
      "train accuracy on epoch 404: 0.944\n",
      "test loss on epoch 404: 0.300\n",
      "test accuracy on epoch 404: 0.769\n",
      "train loss on epoch 405 : 0.170\n",
      "train accuracy on epoch 405: 0.944\n",
      "test loss on epoch 405: 0.319\n",
      "test accuracy on epoch 405: 0.769\n",
      "train loss on epoch 406 : 0.062\n",
      "train accuracy on epoch 406: 1.000\n",
      "test loss on epoch 406: 0.300\n",
      "test accuracy on epoch 406: 0.769\n",
      "train loss on epoch 407 : 0.212\n",
      "train accuracy on epoch 407: 0.944\n",
      "test loss on epoch 407: 0.308\n",
      "test accuracy on epoch 407: 0.769\n",
      "train loss on epoch 408 : 0.155\n",
      "train accuracy on epoch 408: 0.889\n",
      "test loss on epoch 408: 0.318\n",
      "test accuracy on epoch 408: 0.769\n",
      "train loss on epoch 409 : 0.140\n",
      "train accuracy on epoch 409: 0.944\n",
      "test loss on epoch 409: 0.313\n",
      "test accuracy on epoch 409: 0.769\n",
      "train loss on epoch 410 : 0.189\n",
      "train accuracy on epoch 410: 0.889\n",
      "test loss on epoch 410: 0.302\n",
      "test accuracy on epoch 410: 0.769\n",
      "train loss on epoch 411 : 0.237\n",
      "train accuracy on epoch 411: 0.944\n",
      "test loss on epoch 411: 0.322\n",
      "test accuracy on epoch 411: 0.769\n",
      "train loss on epoch 412 : 0.278\n",
      "train accuracy on epoch 412: 0.944\n",
      "test loss on epoch 412: 0.316\n",
      "test accuracy on epoch 412: 0.769\n",
      "train loss on epoch 413 : 0.344\n",
      "train accuracy on epoch 413: 0.889\n",
      "test loss on epoch 413: 0.300\n",
      "test accuracy on epoch 413: 0.769\n",
      "train loss on epoch 414 : 0.086\n",
      "train accuracy on epoch 414: 1.000\n",
      "test loss on epoch 414: 0.301\n",
      "test accuracy on epoch 414: 0.846\n",
      "train loss on epoch 415 : 0.536\n",
      "train accuracy on epoch 415: 0.778\n",
      "test loss on epoch 415: 0.310\n",
      "test accuracy on epoch 415: 0.769\n",
      "train loss on epoch 416 : 0.051\n",
      "train accuracy on epoch 416: 1.000\n",
      "test loss on epoch 416: 0.300\n",
      "test accuracy on epoch 416: 0.769\n",
      "train loss on epoch 417 : 0.096\n",
      "train accuracy on epoch 417: 1.000\n",
      "test loss on epoch 417: 0.317\n",
      "test accuracy on epoch 417: 0.769\n",
      "train loss on epoch 418 : 0.243\n",
      "train accuracy on epoch 418: 0.889\n",
      "test loss on epoch 418: 0.319\n",
      "test accuracy on epoch 418: 0.769\n",
      "train loss on epoch 419 : 0.071\n",
      "train accuracy on epoch 419: 1.000\n",
      "test loss on epoch 419: 0.322\n",
      "test accuracy on epoch 419: 0.769\n",
      "train loss on epoch 420 : 0.086\n",
      "train accuracy on epoch 420: 0.944\n",
      "test loss on epoch 420: 0.320\n",
      "test accuracy on epoch 420: 0.769\n",
      "train loss on epoch 421 : 0.400\n",
      "train accuracy on epoch 421: 0.889\n",
      "test loss on epoch 421: 0.304\n",
      "test accuracy on epoch 421: 0.769\n",
      "train loss on epoch 422 : 0.210\n",
      "train accuracy on epoch 422: 0.889\n",
      "test loss on epoch 422: 0.323\n",
      "test accuracy on epoch 422: 0.692\n",
      "train loss on epoch 423 : 0.109\n",
      "train accuracy on epoch 423: 1.000\n",
      "test loss on epoch 423: 0.316\n",
      "test accuracy on epoch 423: 0.769\n",
      "train loss on epoch 424 : 0.183\n",
      "train accuracy on epoch 424: 0.889\n",
      "test loss on epoch 424: 0.316\n",
      "test accuracy on epoch 424: 0.769\n",
      "train loss on epoch 425 : 0.263\n",
      "train accuracy on epoch 425: 0.833\n",
      "test loss on epoch 425: 0.328\n",
      "test accuracy on epoch 425: 0.769\n",
      "train loss on epoch 426 : 0.162\n",
      "train accuracy on epoch 426: 0.889\n",
      "test loss on epoch 426: 0.328\n",
      "test accuracy on epoch 426: 0.769\n",
      "train loss on epoch 427 : 0.265\n",
      "train accuracy on epoch 427: 0.944\n",
      "test loss on epoch 427: 0.310\n",
      "test accuracy on epoch 427: 0.769\n",
      "train loss on epoch 428 : 0.079\n",
      "train accuracy on epoch 428: 1.000\n",
      "test loss on epoch 428: 0.321\n",
      "test accuracy on epoch 428: 0.692\n",
      "train loss on epoch 429 : 0.134\n",
      "train accuracy on epoch 429: 0.944\n",
      "test loss on epoch 429: 0.306\n",
      "test accuracy on epoch 429: 0.846\n",
      "train loss on epoch 430 : 0.075\n",
      "train accuracy on epoch 430: 0.944\n",
      "test loss on epoch 430: 0.313\n",
      "test accuracy on epoch 430: 0.769\n",
      "train loss on epoch 431 : 0.194\n",
      "train accuracy on epoch 431: 0.889\n",
      "test loss on epoch 431: 0.309\n",
      "test accuracy on epoch 431: 0.769\n",
      "train loss on epoch 432 : 0.118\n",
      "train accuracy on epoch 432: 0.944\n",
      "test loss on epoch 432: 0.311\n",
      "test accuracy on epoch 432: 0.769\n",
      "train loss on epoch 433 : 0.085\n",
      "train accuracy on epoch 433: 1.000\n",
      "test loss on epoch 433: 0.298\n",
      "test accuracy on epoch 433: 0.769\n",
      "train loss on epoch 434 : 0.111\n",
      "train accuracy on epoch 434: 0.944\n",
      "test loss on epoch 434: 0.306\n",
      "test accuracy on epoch 434: 0.769\n",
      "train loss on epoch 435 : 0.122\n",
      "train accuracy on epoch 435: 0.944\n",
      "test loss on epoch 435: 0.301\n",
      "test accuracy on epoch 435: 0.769\n",
      "train loss on epoch 436 : 0.180\n",
      "train accuracy on epoch 436: 0.889\n",
      "test loss on epoch 436: 0.314\n",
      "test accuracy on epoch 436: 0.769\n",
      "train loss on epoch 437 : 0.249\n",
      "train accuracy on epoch 437: 0.889\n",
      "test loss on epoch 437: 0.308\n",
      "test accuracy on epoch 437: 0.769\n",
      "train loss on epoch 438 : 0.193\n",
      "train accuracy on epoch 438: 0.889\n",
      "test loss on epoch 438: 0.303\n",
      "test accuracy on epoch 438: 0.846\n",
      "train loss on epoch 439 : 0.360\n",
      "train accuracy on epoch 439: 0.889\n",
      "test loss on epoch 439: 0.309\n",
      "test accuracy on epoch 439: 0.769\n",
      "train loss on epoch 440 : 0.167\n",
      "train accuracy on epoch 440: 0.889\n",
      "test loss on epoch 440: 0.302\n",
      "test accuracy on epoch 440: 0.769\n",
      "train loss on epoch 441 : 0.036\n",
      "train accuracy on epoch 441: 1.000\n",
      "test loss on epoch 441: 0.312\n",
      "test accuracy on epoch 441: 0.769\n",
      "train loss on epoch 442 : 0.068\n",
      "train accuracy on epoch 442: 1.000\n",
      "test loss on epoch 442: 0.314\n",
      "test accuracy on epoch 442: 0.769\n",
      "train loss on epoch 443 : 0.176\n",
      "train accuracy on epoch 443: 0.889\n",
      "test loss on epoch 443: 0.311\n",
      "test accuracy on epoch 443: 0.769\n",
      "train loss on epoch 444 : 0.215\n",
      "train accuracy on epoch 444: 0.889\n",
      "test loss on epoch 444: 0.314\n",
      "test accuracy on epoch 444: 0.692\n",
      "train loss on epoch 445 : 0.073\n",
      "train accuracy on epoch 445: 1.000\n",
      "test loss on epoch 445: 0.299\n",
      "test accuracy on epoch 445: 0.769\n",
      "train loss on epoch 446 : 0.195\n",
      "train accuracy on epoch 446: 0.944\n",
      "test loss on epoch 446: 0.318\n",
      "test accuracy on epoch 446: 0.692\n",
      "train loss on epoch 447 : 0.048\n",
      "train accuracy on epoch 447: 1.000\n",
      "test loss on epoch 447: 0.304\n",
      "test accuracy on epoch 447: 0.769\n",
      "train loss on epoch 448 : 0.229\n",
      "train accuracy on epoch 448: 0.833\n",
      "test loss on epoch 448: 0.307\n",
      "test accuracy on epoch 448: 0.769\n",
      "train loss on epoch 449 : 0.095\n",
      "train accuracy on epoch 449: 1.000\n",
      "test loss on epoch 449: 0.317\n",
      "test accuracy on epoch 449: 0.769\n",
      "train loss on epoch 450 : 0.210\n",
      "train accuracy on epoch 450: 0.944\n",
      "test loss on epoch 450: 0.303\n",
      "test accuracy on epoch 450: 0.846\n",
      "train loss on epoch 451 : 0.208\n",
      "train accuracy on epoch 451: 0.889\n",
      "test loss on epoch 451: 0.310\n",
      "test accuracy on epoch 451: 0.769\n",
      "train loss on epoch 452 : 0.213\n",
      "train accuracy on epoch 452: 0.833\n",
      "test loss on epoch 452: 0.318\n",
      "test accuracy on epoch 452: 0.769\n",
      "train loss on epoch 453 : 0.177\n",
      "train accuracy on epoch 453: 0.944\n",
      "test loss on epoch 453: 0.306\n",
      "test accuracy on epoch 453: 0.769\n",
      "train loss on epoch 454 : 0.692\n",
      "train accuracy on epoch 454: 0.667\n",
      "test loss on epoch 454: 0.313\n",
      "test accuracy on epoch 454: 0.769\n",
      "train loss on epoch 455 : 0.061\n",
      "train accuracy on epoch 455: 1.000\n",
      "test loss on epoch 455: 0.309\n",
      "test accuracy on epoch 455: 0.769\n",
      "train loss on epoch 456 : 0.050\n",
      "train accuracy on epoch 456: 1.000\n",
      "test loss on epoch 456: 0.324\n",
      "test accuracy on epoch 456: 0.769\n",
      "train loss on epoch 457 : 0.392\n",
      "train accuracy on epoch 457: 0.833\n",
      "test loss on epoch 457: 0.311\n",
      "test accuracy on epoch 457: 0.769\n",
      "train loss on epoch 458 : 0.298\n",
      "train accuracy on epoch 458: 0.889\n",
      "test loss on epoch 458: 0.328\n",
      "test accuracy on epoch 458: 0.769\n",
      "train loss on epoch 459 : 0.041\n",
      "train accuracy on epoch 459: 1.000\n",
      "test loss on epoch 459: 0.329\n",
      "test accuracy on epoch 459: 0.769\n",
      "train loss on epoch 460 : 0.187\n",
      "train accuracy on epoch 460: 0.944\n",
      "test loss on epoch 460: 0.333\n",
      "test accuracy on epoch 460: 0.769\n",
      "train loss on epoch 461 : 0.375\n",
      "train accuracy on epoch 461: 0.889\n",
      "test loss on epoch 461: 0.316\n",
      "test accuracy on epoch 461: 0.769\n",
      "train loss on epoch 462 : 0.419\n",
      "train accuracy on epoch 462: 0.833\n",
      "test loss on epoch 462: 0.313\n",
      "test accuracy on epoch 462: 0.769\n",
      "train loss on epoch 463 : 0.179\n",
      "train accuracy on epoch 463: 0.944\n",
      "test loss on epoch 463: 0.304\n",
      "test accuracy on epoch 463: 0.769\n",
      "train loss on epoch 464 : 0.211\n",
      "train accuracy on epoch 464: 0.889\n",
      "test loss on epoch 464: 0.318\n",
      "test accuracy on epoch 464: 0.769\n",
      "train loss on epoch 465 : 0.173\n",
      "train accuracy on epoch 465: 0.944\n",
      "test loss on epoch 465: 0.320\n",
      "test accuracy on epoch 465: 0.769\n",
      "train loss on epoch 466 : 0.040\n",
      "train accuracy on epoch 466: 1.000\n",
      "test loss on epoch 466: 0.326\n",
      "test accuracy on epoch 466: 0.769\n",
      "train loss on epoch 467 : 0.286\n",
      "train accuracy on epoch 467: 0.944\n",
      "test loss on epoch 467: 0.326\n",
      "test accuracy on epoch 467: 0.769\n",
      "train loss on epoch 468 : 0.187\n",
      "train accuracy on epoch 468: 0.889\n",
      "test loss on epoch 468: 0.321\n",
      "test accuracy on epoch 468: 0.769\n",
      "train loss on epoch 469 : 0.329\n",
      "train accuracy on epoch 469: 0.889\n",
      "test loss on epoch 469: 0.318\n",
      "test accuracy on epoch 469: 0.769\n",
      "train loss on epoch 470 : 0.365\n",
      "train accuracy on epoch 470: 0.889\n",
      "test loss on epoch 470: 0.302\n",
      "test accuracy on epoch 470: 0.769\n",
      "train loss on epoch 471 : 0.292\n",
      "train accuracy on epoch 471: 0.889\n",
      "test loss on epoch 471: 0.326\n",
      "test accuracy on epoch 471: 0.769\n",
      "train loss on epoch 472 : 0.047\n",
      "train accuracy on epoch 472: 1.000\n",
      "test loss on epoch 472: 0.304\n",
      "test accuracy on epoch 472: 0.769\n",
      "train loss on epoch 473 : 0.110\n",
      "train accuracy on epoch 473: 0.944\n",
      "test loss on epoch 473: 0.318\n",
      "test accuracy on epoch 473: 0.769\n",
      "train loss on epoch 474 : 0.139\n",
      "train accuracy on epoch 474: 0.944\n",
      "test loss on epoch 474: 0.306\n",
      "test accuracy on epoch 474: 0.769\n",
      "train loss on epoch 475 : 0.159\n",
      "train accuracy on epoch 475: 0.944\n",
      "test loss on epoch 475: 0.307\n",
      "test accuracy on epoch 475: 0.769\n",
      "train loss on epoch 476 : 0.131\n",
      "train accuracy on epoch 476: 0.944\n",
      "test loss on epoch 476: 0.320\n",
      "test accuracy on epoch 476: 0.769\n",
      "train loss on epoch 477 : 0.218\n",
      "train accuracy on epoch 477: 0.889\n",
      "test loss on epoch 477: 0.305\n",
      "test accuracy on epoch 477: 0.769\n",
      "train loss on epoch 478 : 0.074\n",
      "train accuracy on epoch 478: 0.944\n",
      "test loss on epoch 478: 0.323\n",
      "test accuracy on epoch 478: 0.769\n",
      "train loss on epoch 479 : 0.163\n",
      "train accuracy on epoch 479: 0.944\n",
      "test loss on epoch 479: 0.307\n",
      "test accuracy on epoch 479: 0.769\n",
      "train loss on epoch 480 : 0.424\n",
      "train accuracy on epoch 480: 0.778\n",
      "test loss on epoch 480: 0.312\n",
      "test accuracy on epoch 480: 0.769\n",
      "train loss on epoch 481 : 0.131\n",
      "train accuracy on epoch 481: 0.944\n",
      "test loss on epoch 481: 0.319\n",
      "test accuracy on epoch 481: 0.769\n",
      "train loss on epoch 482 : 0.083\n",
      "train accuracy on epoch 482: 0.944\n",
      "test loss on epoch 482: 0.322\n",
      "test accuracy on epoch 482: 0.769\n",
      "train loss on epoch 483 : 0.073\n",
      "train accuracy on epoch 483: 1.000\n",
      "test loss on epoch 483: 0.329\n",
      "test accuracy on epoch 483: 0.769\n",
      "train loss on epoch 484 : 0.159\n",
      "train accuracy on epoch 484: 0.944\n",
      "test loss on epoch 484: 0.307\n",
      "test accuracy on epoch 484: 0.769\n",
      "train loss on epoch 485 : 0.267\n",
      "train accuracy on epoch 485: 0.833\n",
      "test loss on epoch 485: 0.328\n",
      "test accuracy on epoch 485: 0.769\n",
      "train loss on epoch 486 : 0.110\n",
      "train accuracy on epoch 486: 0.944\n",
      "test loss on epoch 486: 0.327\n",
      "test accuracy on epoch 486: 0.769\n",
      "train loss on epoch 487 : 0.214\n",
      "train accuracy on epoch 487: 0.944\n",
      "test loss on epoch 487: 0.316\n",
      "test accuracy on epoch 487: 0.769\n",
      "train loss on epoch 488 : 0.245\n",
      "train accuracy on epoch 488: 0.889\n",
      "test loss on epoch 488: 0.316\n",
      "test accuracy on epoch 488: 0.769\n",
      "train loss on epoch 489 : 0.180\n",
      "train accuracy on epoch 489: 0.889\n",
      "test loss on epoch 489: 0.316\n",
      "test accuracy on epoch 489: 0.769\n",
      "train loss on epoch 490 : 0.090\n",
      "train accuracy on epoch 490: 0.944\n",
      "test loss on epoch 490: 0.308\n",
      "test accuracy on epoch 490: 0.769\n",
      "train loss on epoch 491 : 0.294\n",
      "train accuracy on epoch 491: 0.889\n",
      "test loss on epoch 491: 0.304\n",
      "test accuracy on epoch 491: 0.769\n",
      "train loss on epoch 492 : 0.063\n",
      "train accuracy on epoch 492: 1.000\n",
      "test loss on epoch 492: 0.317\n",
      "test accuracy on epoch 492: 0.769\n",
      "train loss on epoch 493 : 0.209\n",
      "train accuracy on epoch 493: 0.889\n",
      "test loss on epoch 493: 0.310\n",
      "test accuracy on epoch 493: 0.769\n",
      "train loss on epoch 494 : 0.199\n",
      "train accuracy on epoch 494: 0.944\n",
      "test loss on epoch 494: 0.308\n",
      "test accuracy on epoch 494: 0.769\n",
      "train loss on epoch 495 : 0.122\n",
      "train accuracy on epoch 495: 0.944\n",
      "test loss on epoch 495: 0.319\n",
      "test accuracy on epoch 495: 0.769\n",
      "train loss on epoch 496 : 0.234\n",
      "train accuracy on epoch 496: 0.944\n",
      "test loss on epoch 496: 0.308\n",
      "test accuracy on epoch 496: 0.769\n",
      "train loss on epoch 497 : 0.199\n",
      "train accuracy on epoch 497: 0.889\n",
      "test loss on epoch 497: 0.316\n",
      "test accuracy on epoch 497: 0.769\n",
      "train loss on epoch 498 : 0.113\n",
      "train accuracy on epoch 498: 0.944\n",
      "test loss on epoch 498: 0.318\n",
      "test accuracy on epoch 498: 0.769\n",
      "train loss on epoch 499 : 0.308\n",
      "train accuracy on epoch 499: 0.944\n",
      "test loss on epoch 499: 0.308\n",
      "test accuracy on epoch 499: 0.769\n",
      "train loss on epoch 500 : 0.321\n",
      "train accuracy on epoch 500: 0.889\n",
      "test loss on epoch 500: 0.308\n",
      "test accuracy on epoch 500: 0.769\n",
      "train loss on epoch 501 : 0.144\n",
      "train accuracy on epoch 501: 0.944\n",
      "test loss on epoch 501: 0.319\n",
      "test accuracy on epoch 501: 0.769\n",
      "train loss on epoch 502 : 0.147\n",
      "train accuracy on epoch 502: 0.944\n",
      "test loss on epoch 502: 0.304\n",
      "test accuracy on epoch 502: 0.769\n",
      "train loss on epoch 503 : 0.152\n",
      "train accuracy on epoch 503: 0.944\n",
      "test loss on epoch 503: 0.321\n",
      "test accuracy on epoch 503: 0.769\n",
      "train loss on epoch 504 : 0.039\n",
      "train accuracy on epoch 504: 1.000\n",
      "test loss on epoch 504: 0.321\n",
      "test accuracy on epoch 504: 0.769\n",
      "train loss on epoch 505 : 0.124\n",
      "train accuracy on epoch 505: 0.944\n",
      "test loss on epoch 505: 0.320\n",
      "test accuracy on epoch 505: 0.769\n",
      "train loss on epoch 506 : 0.139\n",
      "train accuracy on epoch 506: 0.944\n",
      "test loss on epoch 506: 0.326\n",
      "test accuracy on epoch 506: 0.769\n",
      "train loss on epoch 507 : 0.134\n",
      "train accuracy on epoch 507: 1.000\n",
      "test loss on epoch 507: 0.321\n",
      "test accuracy on epoch 507: 0.769\n",
      "train loss on epoch 508 : 0.144\n",
      "train accuracy on epoch 508: 0.944\n",
      "test loss on epoch 508: 0.309\n",
      "test accuracy on epoch 508: 0.769\n",
      "train loss on epoch 509 : 0.029\n",
      "train accuracy on epoch 509: 1.000\n",
      "test loss on epoch 509: 0.313\n",
      "test accuracy on epoch 509: 0.769\n",
      "train loss on epoch 510 : 0.497\n",
      "train accuracy on epoch 510: 0.889\n",
      "test loss on epoch 510: 0.314\n",
      "test accuracy on epoch 510: 0.769\n",
      "train loss on epoch 511 : 0.157\n",
      "train accuracy on epoch 511: 0.944\n",
      "test loss on epoch 511: 0.319\n",
      "test accuracy on epoch 511: 0.769\n",
      "train loss on epoch 512 : 0.253\n",
      "train accuracy on epoch 512: 0.944\n",
      "test loss on epoch 512: 0.310\n",
      "test accuracy on epoch 512: 0.769\n",
      "train loss on epoch 513 : 0.209\n",
      "train accuracy on epoch 513: 0.944\n",
      "test loss on epoch 513: 0.321\n",
      "test accuracy on epoch 513: 0.769\n",
      "train loss on epoch 514 : 0.288\n",
      "train accuracy on epoch 514: 0.889\n",
      "test loss on epoch 514: 0.319\n",
      "test accuracy on epoch 514: 0.769\n",
      "train loss on epoch 515 : 0.081\n",
      "train accuracy on epoch 515: 1.000\n",
      "test loss on epoch 515: 0.316\n",
      "test accuracy on epoch 515: 0.769\n",
      "train loss on epoch 516 : 0.149\n",
      "train accuracy on epoch 516: 0.944\n",
      "test loss on epoch 516: 0.322\n",
      "test accuracy on epoch 516: 0.692\n",
      "train loss on epoch 517 : 0.106\n",
      "train accuracy on epoch 517: 0.944\n",
      "test loss on epoch 517: 0.309\n",
      "test accuracy on epoch 517: 0.769\n",
      "train loss on epoch 518 : 0.136\n",
      "train accuracy on epoch 518: 0.944\n",
      "test loss on epoch 518: 0.306\n",
      "test accuracy on epoch 518: 0.769\n",
      "train loss on epoch 519 : 0.272\n",
      "train accuracy on epoch 519: 0.833\n",
      "test loss on epoch 519: 0.316\n",
      "test accuracy on epoch 519: 0.769\n",
      "train loss on epoch 520 : 0.290\n",
      "train accuracy on epoch 520: 0.889\n",
      "test loss on epoch 520: 0.319\n",
      "test accuracy on epoch 520: 0.769\n",
      "train loss on epoch 521 : 0.065\n",
      "train accuracy on epoch 521: 1.000\n",
      "test loss on epoch 521: 0.310\n",
      "test accuracy on epoch 521: 0.769\n",
      "train loss on epoch 522 : 0.130\n",
      "train accuracy on epoch 522: 0.944\n",
      "test loss on epoch 522: 0.317\n",
      "test accuracy on epoch 522: 0.769\n",
      "train loss on epoch 523 : 0.294\n",
      "train accuracy on epoch 523: 0.944\n",
      "test loss on epoch 523: 0.304\n",
      "test accuracy on epoch 523: 0.769\n",
      "train loss on epoch 524 : 0.286\n",
      "train accuracy on epoch 524: 0.889\n",
      "test loss on epoch 524: 0.308\n",
      "test accuracy on epoch 524: 0.769\n",
      "train loss on epoch 525 : 0.391\n",
      "train accuracy on epoch 525: 0.944\n",
      "test loss on epoch 525: 0.324\n",
      "test accuracy on epoch 525: 0.769\n",
      "train loss on epoch 526 : 0.115\n",
      "train accuracy on epoch 526: 0.944\n",
      "test loss on epoch 526: 0.316\n",
      "test accuracy on epoch 526: 0.769\n",
      "train loss on epoch 527 : 0.143\n",
      "train accuracy on epoch 527: 0.889\n",
      "test loss on epoch 527: 0.342\n",
      "test accuracy on epoch 527: 0.769\n",
      "train loss on epoch 528 : 0.078\n",
      "train accuracy on epoch 528: 1.000\n",
      "test loss on epoch 528: 0.321\n",
      "test accuracy on epoch 528: 0.769\n",
      "train loss on epoch 529 : 0.213\n",
      "train accuracy on epoch 529: 0.944\n",
      "test loss on epoch 529: 0.317\n",
      "test accuracy on epoch 529: 0.769\n",
      "train loss on epoch 530 : 0.032\n",
      "train accuracy on epoch 530: 1.000\n",
      "test loss on epoch 530: 0.320\n",
      "test accuracy on epoch 530: 0.769\n",
      "train loss on epoch 531 : 0.059\n",
      "train accuracy on epoch 531: 1.000\n",
      "test loss on epoch 531: 0.316\n",
      "test accuracy on epoch 531: 0.769\n",
      "train loss on epoch 532 : 0.158\n",
      "train accuracy on epoch 532: 0.944\n",
      "test loss on epoch 532: 0.334\n",
      "test accuracy on epoch 532: 0.769\n",
      "train loss on epoch 533 : 0.127\n",
      "train accuracy on epoch 533: 0.944\n",
      "test loss on epoch 533: 0.332\n",
      "test accuracy on epoch 533: 0.769\n",
      "train loss on epoch 534 : 0.057\n",
      "train accuracy on epoch 534: 1.000\n",
      "test loss on epoch 534: 0.332\n",
      "test accuracy on epoch 534: 0.769\n",
      "train loss on epoch 535 : 0.261\n",
      "train accuracy on epoch 535: 0.944\n",
      "test loss on epoch 535: 0.327\n",
      "test accuracy on epoch 535: 0.769\n",
      "train loss on epoch 536 : 0.088\n",
      "train accuracy on epoch 536: 1.000\n",
      "test loss on epoch 536: 0.310\n",
      "test accuracy on epoch 536: 0.769\n",
      "train loss on epoch 537 : 0.246\n",
      "train accuracy on epoch 537: 0.889\n",
      "test loss on epoch 537: 0.315\n",
      "test accuracy on epoch 537: 0.769\n",
      "train loss on epoch 538 : 0.148\n",
      "train accuracy on epoch 538: 0.944\n",
      "test loss on epoch 538: 0.311\n",
      "test accuracy on epoch 538: 0.769\n",
      "train loss on epoch 539 : 0.368\n",
      "train accuracy on epoch 539: 0.833\n",
      "test loss on epoch 539: 0.316\n",
      "test accuracy on epoch 539: 0.846\n",
      "train loss on epoch 540 : 0.378\n",
      "train accuracy on epoch 540: 0.722\n",
      "test loss on epoch 540: 0.321\n",
      "test accuracy on epoch 540: 0.769\n",
      "train loss on epoch 541 : 0.215\n",
      "train accuracy on epoch 541: 0.944\n",
      "test loss on epoch 541: 0.326\n",
      "test accuracy on epoch 541: 0.692\n",
      "train loss on epoch 542 : 0.133\n",
      "train accuracy on epoch 542: 0.944\n",
      "test loss on epoch 542: 0.317\n",
      "test accuracy on epoch 542: 0.769\n",
      "train loss on epoch 543 : 0.226\n",
      "train accuracy on epoch 543: 0.944\n",
      "test loss on epoch 543: 0.316\n",
      "test accuracy on epoch 543: 0.769\n",
      "train loss on epoch 544 : 0.170\n",
      "train accuracy on epoch 544: 0.944\n",
      "test loss on epoch 544: 0.310\n",
      "test accuracy on epoch 544: 0.769\n",
      "train loss on epoch 545 : 0.219\n",
      "train accuracy on epoch 545: 0.944\n",
      "test loss on epoch 545: 0.318\n",
      "test accuracy on epoch 545: 0.769\n",
      "train loss on epoch 546 : 0.148\n",
      "train accuracy on epoch 546: 0.944\n",
      "test loss on epoch 546: 0.312\n",
      "test accuracy on epoch 546: 0.769\n",
      "train loss on epoch 547 : 0.199\n",
      "train accuracy on epoch 547: 0.778\n",
      "test loss on epoch 547: 0.312\n",
      "test accuracy on epoch 547: 0.769\n",
      "train loss on epoch 548 : 0.132\n",
      "train accuracy on epoch 548: 0.944\n",
      "test loss on epoch 548: 0.308\n",
      "test accuracy on epoch 548: 0.769\n",
      "train loss on epoch 549 : 0.149\n",
      "train accuracy on epoch 549: 0.944\n",
      "test loss on epoch 549: 0.320\n",
      "test accuracy on epoch 549: 0.769\n",
      "train loss on epoch 550 : 0.168\n",
      "train accuracy on epoch 550: 0.944\n",
      "test loss on epoch 550: 0.314\n",
      "test accuracy on epoch 550: 0.769\n",
      "train loss on epoch 551 : 0.084\n",
      "train accuracy on epoch 551: 0.944\n",
      "test loss on epoch 551: 0.316\n",
      "test accuracy on epoch 551: 0.769\n",
      "train loss on epoch 552 : 0.379\n",
      "train accuracy on epoch 552: 0.889\n",
      "test loss on epoch 552: 0.308\n",
      "test accuracy on epoch 552: 0.769\n",
      "train loss on epoch 553 : 0.117\n",
      "train accuracy on epoch 553: 0.944\n",
      "test loss on epoch 553: 0.319\n",
      "test accuracy on epoch 553: 0.769\n",
      "train loss on epoch 554 : 0.186\n",
      "train accuracy on epoch 554: 0.889\n",
      "test loss on epoch 554: 0.319\n",
      "test accuracy on epoch 554: 0.769\n",
      "train loss on epoch 555 : 0.146\n",
      "train accuracy on epoch 555: 0.944\n",
      "test loss on epoch 555: 0.314\n",
      "test accuracy on epoch 555: 0.769\n",
      "train loss on epoch 556 : 0.417\n",
      "train accuracy on epoch 556: 0.889\n",
      "test loss on epoch 556: 0.304\n",
      "test accuracy on epoch 556: 0.769\n",
      "train loss on epoch 557 : 0.121\n",
      "train accuracy on epoch 557: 1.000\n",
      "test loss on epoch 557: 0.308\n",
      "test accuracy on epoch 557: 0.769\n",
      "train loss on epoch 558 : 0.173\n",
      "train accuracy on epoch 558: 0.944\n",
      "test loss on epoch 558: 0.310\n",
      "test accuracy on epoch 558: 0.769\n",
      "train loss on epoch 559 : 0.163\n",
      "train accuracy on epoch 559: 0.944\n",
      "test loss on epoch 559: 0.311\n",
      "test accuracy on epoch 559: 0.769\n",
      "train loss on epoch 560 : 0.046\n",
      "train accuracy on epoch 560: 1.000\n",
      "test loss on epoch 560: 0.309\n",
      "test accuracy on epoch 560: 0.769\n",
      "train loss on epoch 561 : 0.043\n",
      "train accuracy on epoch 561: 1.000\n",
      "test loss on epoch 561: 0.315\n",
      "test accuracy on epoch 561: 0.769\n",
      "train loss on epoch 562 : 0.166\n",
      "train accuracy on epoch 562: 0.944\n",
      "test loss on epoch 562: 0.309\n",
      "test accuracy on epoch 562: 0.769\n",
      "train loss on epoch 563 : 0.074\n",
      "train accuracy on epoch 563: 1.000\n",
      "test loss on epoch 563: 0.309\n",
      "test accuracy on epoch 563: 0.769\n",
      "train loss on epoch 564 : 0.190\n",
      "train accuracy on epoch 564: 0.889\n",
      "test loss on epoch 564: 0.306\n",
      "test accuracy on epoch 564: 0.769\n",
      "train loss on epoch 565 : 0.055\n",
      "train accuracy on epoch 565: 1.000\n",
      "test loss on epoch 565: 0.309\n",
      "test accuracy on epoch 565: 0.769\n",
      "train loss on epoch 566 : 0.330\n",
      "train accuracy on epoch 566: 0.889\n",
      "test loss on epoch 566: 0.310\n",
      "test accuracy on epoch 566: 0.769\n",
      "train loss on epoch 567 : 0.129\n",
      "train accuracy on epoch 567: 0.944\n",
      "test loss on epoch 567: 0.308\n",
      "test accuracy on epoch 567: 0.846\n",
      "train loss on epoch 568 : 0.060\n",
      "train accuracy on epoch 568: 1.000\n",
      "test loss on epoch 568: 0.315\n",
      "test accuracy on epoch 568: 0.692\n",
      "train loss on epoch 569 : 0.231\n",
      "train accuracy on epoch 569: 0.944\n",
      "test loss on epoch 569: 0.314\n",
      "test accuracy on epoch 569: 0.769\n",
      "train loss on epoch 570 : 0.088\n",
      "train accuracy on epoch 570: 0.944\n",
      "test loss on epoch 570: 0.316\n",
      "test accuracy on epoch 570: 0.769\n",
      "train loss on epoch 571 : 0.053\n",
      "train accuracy on epoch 571: 1.000\n",
      "test loss on epoch 571: 0.315\n",
      "test accuracy on epoch 571: 0.769\n",
      "train loss on epoch 572 : 0.064\n",
      "train accuracy on epoch 572: 1.000\n",
      "test loss on epoch 572: 0.325\n",
      "test accuracy on epoch 572: 0.769\n",
      "train loss on epoch 573 : 0.271\n",
      "train accuracy on epoch 573: 0.889\n",
      "test loss on epoch 573: 0.333\n",
      "test accuracy on epoch 573: 0.769\n",
      "train loss on epoch 574 : 0.217\n",
      "train accuracy on epoch 574: 0.944\n",
      "test loss on epoch 574: 0.338\n",
      "test accuracy on epoch 574: 0.769\n",
      "train loss on epoch 575 : 0.062\n",
      "train accuracy on epoch 575: 1.000\n",
      "test loss on epoch 575: 0.338\n",
      "test accuracy on epoch 575: 0.769\n",
      "train loss on epoch 576 : 0.269\n",
      "train accuracy on epoch 576: 0.889\n",
      "test loss on epoch 576: 0.340\n",
      "test accuracy on epoch 576: 0.769\n",
      "train loss on epoch 577 : 0.298\n",
      "train accuracy on epoch 577: 0.778\n",
      "test loss on epoch 577: 0.341\n",
      "test accuracy on epoch 577: 0.769\n",
      "train loss on epoch 578 : 0.133\n",
      "train accuracy on epoch 578: 0.944\n",
      "test loss on epoch 578: 0.335\n",
      "test accuracy on epoch 578: 0.769\n",
      "train loss on epoch 579 : 0.112\n",
      "train accuracy on epoch 579: 0.944\n",
      "test loss on epoch 579: 0.332\n",
      "test accuracy on epoch 579: 0.769\n",
      "train loss on epoch 580 : 0.249\n",
      "train accuracy on epoch 580: 0.833\n",
      "test loss on epoch 580: 0.325\n",
      "test accuracy on epoch 580: 0.769\n",
      "train loss on epoch 581 : 0.244\n",
      "train accuracy on epoch 581: 0.944\n",
      "test loss on epoch 581: 0.325\n",
      "test accuracy on epoch 581: 0.692\n",
      "train loss on epoch 582 : 0.282\n",
      "train accuracy on epoch 582: 0.944\n",
      "test loss on epoch 582: 0.316\n",
      "test accuracy on epoch 582: 0.769\n",
      "train loss on epoch 583 : 0.212\n",
      "train accuracy on epoch 583: 0.889\n",
      "test loss on epoch 583: 0.321\n",
      "test accuracy on epoch 583: 0.692\n",
      "train loss on epoch 584 : 0.253\n",
      "train accuracy on epoch 584: 0.833\n",
      "test loss on epoch 584: 0.313\n",
      "test accuracy on epoch 584: 0.769\n",
      "train loss on epoch 585 : 0.095\n",
      "train accuracy on epoch 585: 0.944\n",
      "test loss on epoch 585: 0.319\n",
      "test accuracy on epoch 585: 0.769\n",
      "train loss on epoch 586 : 0.107\n",
      "train accuracy on epoch 586: 0.944\n",
      "test loss on epoch 586: 0.314\n",
      "test accuracy on epoch 586: 0.769\n",
      "train loss on epoch 587 : 0.242\n",
      "train accuracy on epoch 587: 0.944\n",
      "test loss on epoch 587: 0.326\n",
      "test accuracy on epoch 587: 0.769\n",
      "train loss on epoch 588 : 0.132\n",
      "train accuracy on epoch 588: 0.944\n",
      "test loss on epoch 588: 0.322\n",
      "test accuracy on epoch 588: 0.769\n",
      "train loss on epoch 589 : 0.132\n",
      "train accuracy on epoch 589: 0.944\n",
      "test loss on epoch 589: 0.319\n",
      "test accuracy on epoch 589: 0.769\n",
      "train loss on epoch 590 : 0.190\n",
      "train accuracy on epoch 590: 0.944\n",
      "test loss on epoch 590: 0.324\n",
      "test accuracy on epoch 590: 0.769\n",
      "train loss on epoch 591 : 0.063\n",
      "train accuracy on epoch 591: 1.000\n",
      "test loss on epoch 591: 0.321\n",
      "test accuracy on epoch 591: 0.769\n",
      "train loss on epoch 592 : 0.054\n",
      "train accuracy on epoch 592: 1.000\n",
      "test loss on epoch 592: 0.322\n",
      "test accuracy on epoch 592: 0.769\n",
      "train loss on epoch 593 : 0.174\n",
      "train accuracy on epoch 593: 0.944\n",
      "test loss on epoch 593: 0.315\n",
      "test accuracy on epoch 593: 0.769\n",
      "train loss on epoch 594 : 0.140\n",
      "train accuracy on epoch 594: 0.944\n",
      "test loss on epoch 594: 0.326\n",
      "test accuracy on epoch 594: 0.769\n",
      "train loss on epoch 595 : 0.043\n",
      "train accuracy on epoch 595: 1.000\n",
      "test loss on epoch 595: 0.322\n",
      "test accuracy on epoch 595: 0.769\n",
      "train loss on epoch 596 : 0.111\n",
      "train accuracy on epoch 596: 0.944\n",
      "test loss on epoch 596: 0.321\n",
      "test accuracy on epoch 596: 0.769\n",
      "train loss on epoch 597 : 0.109\n",
      "train accuracy on epoch 597: 0.944\n",
      "test loss on epoch 597: 0.314\n",
      "test accuracy on epoch 597: 0.769\n",
      "train loss on epoch 598 : 0.086\n",
      "train accuracy on epoch 598: 0.944\n",
      "test loss on epoch 598: 0.313\n",
      "test accuracy on epoch 598: 0.769\n",
      "train loss on epoch 599 : 0.071\n",
      "train accuracy on epoch 599: 0.944\n",
      "test loss on epoch 599: 0.317\n",
      "test accuracy on epoch 599: 0.769\n",
      "train loss on epoch 600 : 0.250\n",
      "train accuracy on epoch 600: 0.889\n",
      "test loss on epoch 600: 0.323\n",
      "test accuracy on epoch 600: 0.769\n",
      "train loss on epoch 601 : 0.243\n",
      "train accuracy on epoch 601: 0.944\n",
      "test loss on epoch 601: 0.331\n",
      "test accuracy on epoch 601: 0.769\n",
      "train loss on epoch 602 : 0.129\n",
      "train accuracy on epoch 602: 0.944\n",
      "test loss on epoch 602: 0.332\n",
      "test accuracy on epoch 602: 0.769\n",
      "train loss on epoch 603 : 0.216\n",
      "train accuracy on epoch 603: 0.944\n",
      "test loss on epoch 603: 0.320\n",
      "test accuracy on epoch 603: 0.769\n",
      "train loss on epoch 604 : 0.221\n",
      "train accuracy on epoch 604: 0.889\n",
      "test loss on epoch 604: 0.325\n",
      "test accuracy on epoch 604: 0.769\n",
      "train loss on epoch 605 : 0.259\n",
      "train accuracy on epoch 605: 0.833\n",
      "test loss on epoch 605: 0.329\n",
      "test accuracy on epoch 605: 0.769\n",
      "train loss on epoch 606 : 0.419\n",
      "train accuracy on epoch 606: 0.889\n",
      "test loss on epoch 606: 0.333\n",
      "test accuracy on epoch 606: 0.769\n",
      "train loss on epoch 607 : 0.274\n",
      "train accuracy on epoch 607: 0.944\n",
      "test loss on epoch 607: 0.321\n",
      "test accuracy on epoch 607: 0.769\n",
      "train loss on epoch 608 : 0.093\n",
      "train accuracy on epoch 608: 0.944\n",
      "test loss on epoch 608: 0.318\n",
      "test accuracy on epoch 608: 0.769\n",
      "train loss on epoch 609 : 0.060\n",
      "train accuracy on epoch 609: 1.000\n",
      "test loss on epoch 609: 0.326\n",
      "test accuracy on epoch 609: 0.692\n",
      "train loss on epoch 610 : 0.054\n",
      "train accuracy on epoch 610: 1.000\n",
      "test loss on epoch 610: 0.317\n",
      "test accuracy on epoch 610: 0.769\n",
      "train loss on epoch 611 : 0.200\n",
      "train accuracy on epoch 611: 0.944\n",
      "test loss on epoch 611: 0.318\n",
      "test accuracy on epoch 611: 0.769\n",
      "train loss on epoch 612 : 0.167\n",
      "train accuracy on epoch 612: 0.944\n",
      "test loss on epoch 612: 0.321\n",
      "test accuracy on epoch 612: 0.769\n",
      "train loss on epoch 613 : 0.162\n",
      "train accuracy on epoch 613: 0.944\n",
      "test loss on epoch 613: 0.323\n",
      "test accuracy on epoch 613: 0.769\n",
      "train loss on epoch 614 : 0.485\n",
      "train accuracy on epoch 614: 0.889\n",
      "test loss on epoch 614: 0.317\n",
      "test accuracy on epoch 614: 0.769\n",
      "train loss on epoch 615 : 0.119\n",
      "train accuracy on epoch 615: 0.944\n",
      "test loss on epoch 615: 0.322\n",
      "test accuracy on epoch 615: 0.769\n",
      "train loss on epoch 616 : 0.195\n",
      "train accuracy on epoch 616: 0.944\n",
      "test loss on epoch 616: 0.313\n",
      "test accuracy on epoch 616: 0.846\n",
      "train loss on epoch 617 : 0.055\n",
      "train accuracy on epoch 617: 0.944\n",
      "test loss on epoch 617: 0.317\n",
      "test accuracy on epoch 617: 0.769\n",
      "train loss on epoch 618 : 0.343\n",
      "train accuracy on epoch 618: 0.833\n",
      "test loss on epoch 618: 0.308\n",
      "test accuracy on epoch 618: 0.846\n",
      "train loss on epoch 619 : 0.224\n",
      "train accuracy on epoch 619: 0.889\n",
      "test loss on epoch 619: 0.319\n",
      "test accuracy on epoch 619: 0.769\n",
      "train loss on epoch 620 : 0.127\n",
      "train accuracy on epoch 620: 0.944\n",
      "test loss on epoch 620: 0.316\n",
      "test accuracy on epoch 620: 0.769\n",
      "train loss on epoch 621 : 0.166\n",
      "train accuracy on epoch 621: 0.944\n",
      "test loss on epoch 621: 0.303\n",
      "test accuracy on epoch 621: 0.769\n",
      "train loss on epoch 622 : 0.146\n",
      "train accuracy on epoch 622: 0.889\n",
      "test loss on epoch 622: 0.320\n",
      "test accuracy on epoch 622: 0.769\n",
      "train loss on epoch 623 : 0.032\n",
      "train accuracy on epoch 623: 1.000\n",
      "test loss on epoch 623: 0.320\n",
      "test accuracy on epoch 623: 0.769\n",
      "train loss on epoch 624 : 0.254\n",
      "train accuracy on epoch 624: 0.944\n",
      "test loss on epoch 624: 0.317\n",
      "test accuracy on epoch 624: 0.769\n",
      "train loss on epoch 625 : 0.149\n",
      "train accuracy on epoch 625: 0.944\n",
      "test loss on epoch 625: 0.322\n",
      "test accuracy on epoch 625: 0.769\n",
      "train loss on epoch 626 : 0.047\n",
      "train accuracy on epoch 626: 1.000\n",
      "test loss on epoch 626: 0.313\n",
      "test accuracy on epoch 626: 0.769\n",
      "train loss on epoch 627 : 0.041\n",
      "train accuracy on epoch 627: 1.000\n",
      "test loss on epoch 627: 0.309\n",
      "test accuracy on epoch 627: 0.769\n",
      "train loss on epoch 628 : 0.052\n",
      "train accuracy on epoch 628: 1.000\n",
      "test loss on epoch 628: 0.304\n",
      "test accuracy on epoch 628: 0.769\n",
      "train loss on epoch 629 : 0.036\n",
      "train accuracy on epoch 629: 1.000\n",
      "test loss on epoch 629: 0.307\n",
      "test accuracy on epoch 629: 0.769\n",
      "train loss on epoch 630 : 0.152\n",
      "train accuracy on epoch 630: 0.944\n",
      "test loss on epoch 630: 0.325\n",
      "test accuracy on epoch 630: 0.769\n",
      "train loss on epoch 631 : 0.058\n",
      "train accuracy on epoch 631: 1.000\n",
      "test loss on epoch 631: 0.326\n",
      "test accuracy on epoch 631: 0.769\n",
      "train loss on epoch 632 : 0.047\n",
      "train accuracy on epoch 632: 1.000\n",
      "test loss on epoch 632: 0.324\n",
      "test accuracy on epoch 632: 0.769\n",
      "train loss on epoch 633 : 0.447\n",
      "train accuracy on epoch 633: 0.944\n",
      "test loss on epoch 633: 0.326\n",
      "test accuracy on epoch 633: 0.769\n",
      "train loss on epoch 634 : 0.093\n",
      "train accuracy on epoch 634: 1.000\n",
      "test loss on epoch 634: 0.328\n",
      "test accuracy on epoch 634: 0.769\n",
      "train loss on epoch 635 : 0.230\n",
      "train accuracy on epoch 635: 0.889\n",
      "test loss on epoch 635: 0.310\n",
      "test accuracy on epoch 635: 0.769\n",
      "train loss on epoch 636 : 0.379\n",
      "train accuracy on epoch 636: 0.833\n",
      "test loss on epoch 636: 0.328\n",
      "test accuracy on epoch 636: 0.769\n",
      "train loss on epoch 637 : 0.205\n",
      "train accuracy on epoch 637: 0.889\n",
      "test loss on epoch 637: 0.326\n",
      "test accuracy on epoch 637: 0.769\n",
      "train loss on epoch 638 : 0.086\n",
      "train accuracy on epoch 638: 1.000\n",
      "test loss on epoch 638: 0.320\n",
      "test accuracy on epoch 638: 0.769\n",
      "train loss on epoch 639 : 0.157\n",
      "train accuracy on epoch 639: 0.889\n",
      "test loss on epoch 639: 0.320\n",
      "test accuracy on epoch 639: 0.769\n",
      "train loss on epoch 640 : 0.179\n",
      "train accuracy on epoch 640: 0.889\n",
      "test loss on epoch 640: 0.313\n",
      "test accuracy on epoch 640: 0.769\n",
      "train loss on epoch 641 : 0.190\n",
      "train accuracy on epoch 641: 0.889\n",
      "test loss on epoch 641: 0.328\n",
      "test accuracy on epoch 641: 0.769\n",
      "train loss on epoch 642 : 0.234\n",
      "train accuracy on epoch 642: 0.944\n",
      "test loss on epoch 642: 0.334\n",
      "test accuracy on epoch 642: 0.769\n",
      "train loss on epoch 643 : 0.105\n",
      "train accuracy on epoch 643: 0.944\n",
      "test loss on epoch 643: 0.330\n",
      "test accuracy on epoch 643: 0.769\n",
      "train loss on epoch 644 : 0.052\n",
      "train accuracy on epoch 644: 1.000\n",
      "test loss on epoch 644: 0.328\n",
      "test accuracy on epoch 644: 0.769\n",
      "train loss on epoch 645 : 0.127\n",
      "train accuracy on epoch 645: 0.944\n",
      "test loss on epoch 645: 0.320\n",
      "test accuracy on epoch 645: 0.769\n",
      "train loss on epoch 646 : 0.245\n",
      "train accuracy on epoch 646: 0.889\n",
      "test loss on epoch 646: 0.328\n",
      "test accuracy on epoch 646: 0.769\n",
      "train loss on epoch 647 : 0.112\n",
      "train accuracy on epoch 647: 1.000\n",
      "test loss on epoch 647: 0.326\n",
      "test accuracy on epoch 647: 0.769\n",
      "train loss on epoch 648 : 0.042\n",
      "train accuracy on epoch 648: 1.000\n",
      "test loss on epoch 648: 0.325\n",
      "test accuracy on epoch 648: 0.769\n",
      "train loss on epoch 649 : 0.202\n",
      "train accuracy on epoch 649: 0.944\n",
      "test loss on epoch 649: 0.329\n",
      "test accuracy on epoch 649: 0.692\n",
      "train loss on epoch 650 : 0.077\n",
      "train accuracy on epoch 650: 0.944\n",
      "test loss on epoch 650: 0.324\n",
      "test accuracy on epoch 650: 0.769\n",
      "train loss on epoch 651 : 0.277\n",
      "train accuracy on epoch 651: 0.889\n",
      "test loss on epoch 651: 0.321\n",
      "test accuracy on epoch 651: 0.769\n",
      "train loss on epoch 652 : 0.271\n",
      "train accuracy on epoch 652: 0.833\n",
      "test loss on epoch 652: 0.321\n",
      "test accuracy on epoch 652: 0.769\n",
      "train loss on epoch 653 : 0.184\n",
      "train accuracy on epoch 653: 0.944\n",
      "test loss on epoch 653: 0.318\n",
      "test accuracy on epoch 653: 0.769\n",
      "train loss on epoch 654 : 0.075\n",
      "train accuracy on epoch 654: 1.000\n",
      "test loss on epoch 654: 0.332\n",
      "test accuracy on epoch 654: 0.769\n",
      "train loss on epoch 655 : 0.398\n",
      "train accuracy on epoch 655: 0.833\n",
      "test loss on epoch 655: 0.331\n",
      "test accuracy on epoch 655: 0.769\n",
      "train loss on epoch 656 : 0.202\n",
      "train accuracy on epoch 656: 0.889\n",
      "test loss on epoch 656: 0.322\n",
      "test accuracy on epoch 656: 0.769\n",
      "train loss on epoch 657 : 0.185\n",
      "train accuracy on epoch 657: 0.944\n",
      "test loss on epoch 657: 0.317\n",
      "test accuracy on epoch 657: 0.769\n",
      "train loss on epoch 658 : 0.530\n",
      "train accuracy on epoch 658: 0.889\n",
      "test loss on epoch 658: 0.334\n",
      "test accuracy on epoch 658: 0.769\n",
      "train loss on epoch 659 : 0.037\n",
      "train accuracy on epoch 659: 1.000\n",
      "test loss on epoch 659: 0.323\n",
      "test accuracy on epoch 659: 0.769\n",
      "train loss on epoch 660 : 0.112\n",
      "train accuracy on epoch 660: 0.944\n",
      "test loss on epoch 660: 0.321\n",
      "test accuracy on epoch 660: 0.769\n",
      "train loss on epoch 661 : 0.117\n",
      "train accuracy on epoch 661: 0.944\n",
      "test loss on epoch 661: 0.327\n",
      "test accuracy on epoch 661: 0.769\n",
      "train loss on epoch 662 : 0.098\n",
      "train accuracy on epoch 662: 0.944\n",
      "test loss on epoch 662: 0.328\n",
      "test accuracy on epoch 662: 0.769\n",
      "train loss on epoch 663 : 0.147\n",
      "train accuracy on epoch 663: 0.944\n",
      "test loss on epoch 663: 0.336\n",
      "test accuracy on epoch 663: 0.692\n",
      "train loss on epoch 664 : 0.081\n",
      "train accuracy on epoch 664: 0.944\n",
      "test loss on epoch 664: 0.339\n",
      "test accuracy on epoch 664: 0.769\n",
      "train loss on epoch 665 : 0.151\n",
      "train accuracy on epoch 665: 0.944\n",
      "test loss on epoch 665: 0.340\n",
      "test accuracy on epoch 665: 0.769\n",
      "train loss on epoch 666 : 0.127\n",
      "train accuracy on epoch 666: 1.000\n",
      "test loss on epoch 666: 0.339\n",
      "test accuracy on epoch 666: 0.769\n",
      "train loss on epoch 667 : 0.479\n",
      "train accuracy on epoch 667: 0.833\n",
      "test loss on epoch 667: 0.339\n",
      "test accuracy on epoch 667: 0.769\n",
      "train loss on epoch 668 : 0.209\n",
      "train accuracy on epoch 668: 0.889\n",
      "test loss on epoch 668: 0.332\n",
      "test accuracy on epoch 668: 0.769\n",
      "train loss on epoch 669 : 0.065\n",
      "train accuracy on epoch 669: 0.944\n",
      "test loss on epoch 669: 0.329\n",
      "test accuracy on epoch 669: 0.769\n",
      "train loss on epoch 670 : 0.168\n",
      "train accuracy on epoch 670: 0.889\n",
      "test loss on epoch 670: 0.327\n",
      "test accuracy on epoch 670: 0.692\n",
      "train loss on epoch 671 : 0.133\n",
      "train accuracy on epoch 671: 0.944\n",
      "test loss on epoch 671: 0.320\n",
      "test accuracy on epoch 671: 0.769\n",
      "train loss on epoch 672 : 0.261\n",
      "train accuracy on epoch 672: 0.889\n",
      "test loss on epoch 672: 0.318\n",
      "test accuracy on epoch 672: 0.769\n",
      "train loss on epoch 673 : 0.069\n",
      "train accuracy on epoch 673: 1.000\n",
      "test loss on epoch 673: 0.320\n",
      "test accuracy on epoch 673: 0.769\n",
      "train loss on epoch 674 : 0.153\n",
      "train accuracy on epoch 674: 0.889\n",
      "test loss on epoch 674: 0.317\n",
      "test accuracy on epoch 674: 0.769\n",
      "train loss on epoch 675 : 0.418\n",
      "train accuracy on epoch 675: 0.889\n",
      "test loss on epoch 675: 0.318\n",
      "test accuracy on epoch 675: 0.769\n",
      "train loss on epoch 676 : 0.042\n",
      "train accuracy on epoch 676: 1.000\n",
      "test loss on epoch 676: 0.317\n",
      "test accuracy on epoch 676: 0.769\n",
      "train loss on epoch 677 : 0.147\n",
      "train accuracy on epoch 677: 0.944\n",
      "test loss on epoch 677: 0.319\n",
      "test accuracy on epoch 677: 0.769\n",
      "train loss on epoch 678 : 0.043\n",
      "train accuracy on epoch 678: 1.000\n",
      "test loss on epoch 678: 0.323\n",
      "test accuracy on epoch 678: 0.692\n",
      "train loss on epoch 679 : 0.296\n",
      "train accuracy on epoch 679: 0.889\n",
      "test loss on epoch 679: 0.324\n",
      "test accuracy on epoch 679: 0.769\n",
      "train loss on epoch 680 : 0.177\n",
      "train accuracy on epoch 680: 0.944\n",
      "test loss on epoch 680: 0.316\n",
      "test accuracy on epoch 680: 0.769\n",
      "train loss on epoch 681 : 0.174\n",
      "train accuracy on epoch 681: 0.944\n",
      "test loss on epoch 681: 0.315\n",
      "test accuracy on epoch 681: 0.769\n",
      "train loss on epoch 682 : 0.157\n",
      "train accuracy on epoch 682: 0.889\n",
      "test loss on epoch 682: 0.317\n",
      "test accuracy on epoch 682: 0.769\n",
      "train loss on epoch 683 : 0.428\n",
      "train accuracy on epoch 683: 0.889\n",
      "test loss on epoch 683: 0.318\n",
      "test accuracy on epoch 683: 0.769\n",
      "train loss on epoch 684 : 0.076\n",
      "train accuracy on epoch 684: 1.000\n",
      "test loss on epoch 684: 0.320\n",
      "test accuracy on epoch 684: 0.769\n",
      "train loss on epoch 685 : 0.177\n",
      "train accuracy on epoch 685: 0.944\n",
      "test loss on epoch 685: 0.319\n",
      "test accuracy on epoch 685: 0.769\n",
      "train loss on epoch 686 : 0.170\n",
      "train accuracy on epoch 686: 0.889\n",
      "test loss on epoch 686: 0.325\n",
      "test accuracy on epoch 686: 0.769\n",
      "train loss on epoch 687 : 0.073\n",
      "train accuracy on epoch 687: 1.000\n",
      "test loss on epoch 687: 0.327\n",
      "test accuracy on epoch 687: 0.769\n",
      "train loss on epoch 688 : 0.130\n",
      "train accuracy on epoch 688: 1.000\n",
      "test loss on epoch 688: 0.325\n",
      "test accuracy on epoch 688: 0.769\n",
      "train loss on epoch 689 : 0.155\n",
      "train accuracy on epoch 689: 0.944\n",
      "test loss on epoch 689: 0.324\n",
      "test accuracy on epoch 689: 0.769\n",
      "train loss on epoch 690 : 0.279\n",
      "train accuracy on epoch 690: 0.944\n",
      "test loss on epoch 690: 0.321\n",
      "test accuracy on epoch 690: 0.769\n",
      "train loss on epoch 691 : 0.363\n",
      "train accuracy on epoch 691: 0.833\n",
      "test loss on epoch 691: 0.332\n",
      "test accuracy on epoch 691: 0.769\n",
      "train loss on epoch 692 : 0.046\n",
      "train accuracy on epoch 692: 1.000\n",
      "test loss on epoch 692: 0.334\n",
      "test accuracy on epoch 692: 0.769\n",
      "train loss on epoch 693 : 0.071\n",
      "train accuracy on epoch 693: 1.000\n",
      "test loss on epoch 693: 0.333\n",
      "test accuracy on epoch 693: 0.769\n",
      "train loss on epoch 694 : 0.173\n",
      "train accuracy on epoch 694: 0.944\n",
      "test loss on epoch 694: 0.335\n",
      "test accuracy on epoch 694: 0.692\n",
      "train loss on epoch 695 : 0.024\n",
      "train accuracy on epoch 695: 1.000\n",
      "test loss on epoch 695: 0.336\n",
      "test accuracy on epoch 695: 0.692\n",
      "train loss on epoch 696 : 0.172\n",
      "train accuracy on epoch 696: 0.944\n",
      "test loss on epoch 696: 0.331\n",
      "test accuracy on epoch 696: 0.769\n",
      "train loss on epoch 697 : 0.132\n",
      "train accuracy on epoch 697: 0.889\n",
      "test loss on epoch 697: 0.327\n",
      "test accuracy on epoch 697: 0.769\n",
      "train loss on epoch 698 : 0.207\n",
      "train accuracy on epoch 698: 0.944\n",
      "test loss on epoch 698: 0.337\n",
      "test accuracy on epoch 698: 0.769\n",
      "train loss on epoch 699 : 0.220\n",
      "train accuracy on epoch 699: 0.889\n",
      "test loss on epoch 699: 0.328\n",
      "test accuracy on epoch 699: 0.769\n",
      "train loss on epoch 700 : 0.137\n",
      "train accuracy on epoch 700: 0.944\n",
      "test loss on epoch 700: 0.334\n",
      "test accuracy on epoch 700: 0.769\n",
      "train loss on epoch 701 : 0.208\n",
      "train accuracy on epoch 701: 0.889\n",
      "test loss on epoch 701: 0.339\n",
      "test accuracy on epoch 701: 0.769\n",
      "train loss on epoch 702 : 0.122\n",
      "train accuracy on epoch 702: 0.944\n",
      "test loss on epoch 702: 0.334\n",
      "test accuracy on epoch 702: 0.769\n",
      "train loss on epoch 703 : 0.104\n",
      "train accuracy on epoch 703: 0.944\n",
      "test loss on epoch 703: 0.340\n",
      "test accuracy on epoch 703: 0.769\n",
      "train loss on epoch 704 : 0.081\n",
      "train accuracy on epoch 704: 1.000\n",
      "test loss on epoch 704: 0.325\n",
      "test accuracy on epoch 704: 0.769\n",
      "train loss on epoch 705 : 0.266\n",
      "train accuracy on epoch 705: 0.889\n",
      "test loss on epoch 705: 0.333\n",
      "test accuracy on epoch 705: 0.769\n",
      "train loss on epoch 706 : 0.129\n",
      "train accuracy on epoch 706: 0.944\n",
      "test loss on epoch 706: 0.332\n",
      "test accuracy on epoch 706: 0.769\n",
      "train loss on epoch 707 : 0.167\n",
      "train accuracy on epoch 707: 0.944\n",
      "test loss on epoch 707: 0.323\n",
      "test accuracy on epoch 707: 0.769\n",
      "train loss on epoch 708 : 0.320\n",
      "train accuracy on epoch 708: 0.833\n",
      "test loss on epoch 708: 0.330\n",
      "test accuracy on epoch 708: 0.769\n",
      "train loss on epoch 709 : 0.349\n",
      "train accuracy on epoch 709: 0.889\n",
      "test loss on epoch 709: 0.322\n",
      "test accuracy on epoch 709: 0.769\n",
      "train loss on epoch 710 : 0.304\n",
      "train accuracy on epoch 710: 0.889\n",
      "test loss on epoch 710: 0.329\n",
      "test accuracy on epoch 710: 0.769\n",
      "train loss on epoch 711 : 0.112\n",
      "train accuracy on epoch 711: 0.944\n",
      "test loss on epoch 711: 0.336\n",
      "test accuracy on epoch 711: 0.769\n",
      "train loss on epoch 712 : 0.115\n",
      "train accuracy on epoch 712: 0.944\n",
      "test loss on epoch 712: 0.337\n",
      "test accuracy on epoch 712: 0.769\n",
      "train loss on epoch 713 : 0.266\n",
      "train accuracy on epoch 713: 0.944\n",
      "test loss on epoch 713: 0.333\n",
      "test accuracy on epoch 713: 0.692\n",
      "train loss on epoch 714 : 0.100\n",
      "train accuracy on epoch 714: 0.944\n",
      "test loss on epoch 714: 0.324\n",
      "test accuracy on epoch 714: 0.769\n",
      "train loss on epoch 715 : 0.027\n",
      "train accuracy on epoch 715: 1.000\n",
      "test loss on epoch 715: 0.331\n",
      "test accuracy on epoch 715: 0.769\n",
      "train loss on epoch 716 : 0.055\n",
      "train accuracy on epoch 716: 1.000\n",
      "test loss on epoch 716: 0.322\n",
      "test accuracy on epoch 716: 0.769\n",
      "train loss on epoch 717 : 0.146\n",
      "train accuracy on epoch 717: 0.944\n",
      "test loss on epoch 717: 0.327\n",
      "test accuracy on epoch 717: 0.769\n",
      "train loss on epoch 718 : 0.399\n",
      "train accuracy on epoch 718: 0.889\n",
      "test loss on epoch 718: 0.324\n",
      "test accuracy on epoch 718: 0.769\n",
      "train loss on epoch 719 : 0.327\n",
      "train accuracy on epoch 719: 0.833\n",
      "test loss on epoch 719: 0.325\n",
      "test accuracy on epoch 719: 0.769\n",
      "train loss on epoch 720 : 0.682\n",
      "train accuracy on epoch 720: 0.889\n",
      "test loss on epoch 720: 0.330\n",
      "test accuracy on epoch 720: 0.769\n",
      "train loss on epoch 721 : 0.093\n",
      "train accuracy on epoch 721: 0.944\n",
      "test loss on epoch 721: 0.333\n",
      "test accuracy on epoch 721: 0.769\n",
      "train loss on epoch 722 : 0.192\n",
      "train accuracy on epoch 722: 0.944\n",
      "test loss on epoch 722: 0.322\n",
      "test accuracy on epoch 722: 0.769\n",
      "train loss on epoch 723 : 0.218\n",
      "train accuracy on epoch 723: 0.889\n",
      "test loss on epoch 723: 0.318\n",
      "test accuracy on epoch 723: 0.769\n",
      "train loss on epoch 724 : 0.217\n",
      "train accuracy on epoch 724: 0.944\n",
      "test loss on epoch 724: 0.327\n",
      "test accuracy on epoch 724: 0.769\n",
      "train loss on epoch 725 : 0.255\n",
      "train accuracy on epoch 725: 0.889\n",
      "test loss on epoch 725: 0.325\n",
      "test accuracy on epoch 725: 0.769\n",
      "train loss on epoch 726 : 0.077\n",
      "train accuracy on epoch 726: 1.000\n",
      "test loss on epoch 726: 0.320\n",
      "test accuracy on epoch 726: 0.769\n",
      "train loss on epoch 727 : 0.238\n",
      "train accuracy on epoch 727: 0.944\n",
      "test loss on epoch 727: 0.323\n",
      "test accuracy on epoch 727: 0.769\n",
      "train loss on epoch 728 : 0.193\n",
      "train accuracy on epoch 728: 0.944\n",
      "test loss on epoch 728: 0.322\n",
      "test accuracy on epoch 728: 0.769\n",
      "train loss on epoch 729 : 0.049\n",
      "train accuracy on epoch 729: 1.000\n",
      "test loss on epoch 729: 0.323\n",
      "test accuracy on epoch 729: 0.769\n",
      "train loss on epoch 730 : 0.071\n",
      "train accuracy on epoch 730: 1.000\n",
      "test loss on epoch 730: 0.324\n",
      "test accuracy on epoch 730: 0.769\n",
      "train loss on epoch 731 : 0.108\n",
      "train accuracy on epoch 731: 0.944\n",
      "test loss on epoch 731: 0.311\n",
      "test accuracy on epoch 731: 0.769\n",
      "train loss on epoch 732 : 0.171\n",
      "train accuracy on epoch 732: 0.944\n",
      "test loss on epoch 732: 0.327\n",
      "test accuracy on epoch 732: 0.769\n",
      "train loss on epoch 733 : 0.364\n",
      "train accuracy on epoch 733: 0.944\n",
      "test loss on epoch 733: 0.320\n",
      "test accuracy on epoch 733: 0.769\n",
      "train loss on epoch 734 : 0.083\n",
      "train accuracy on epoch 734: 0.944\n",
      "test loss on epoch 734: 0.321\n",
      "test accuracy on epoch 734: 0.769\n",
      "train loss on epoch 735 : 0.155\n",
      "train accuracy on epoch 735: 0.944\n",
      "test loss on epoch 735: 0.324\n",
      "test accuracy on epoch 735: 0.769\n",
      "train loss on epoch 736 : 0.131\n",
      "train accuracy on epoch 736: 0.944\n",
      "test loss on epoch 736: 0.316\n",
      "test accuracy on epoch 736: 0.769\n",
      "train loss on epoch 737 : 0.021\n",
      "train accuracy on epoch 737: 1.000\n",
      "test loss on epoch 737: 0.323\n",
      "test accuracy on epoch 737: 0.769\n",
      "train loss on epoch 738 : 0.146\n",
      "train accuracy on epoch 738: 0.889\n",
      "test loss on epoch 738: 0.331\n",
      "test accuracy on epoch 738: 0.769\n",
      "train loss on epoch 739 : 0.293\n",
      "train accuracy on epoch 739: 0.889\n",
      "test loss on epoch 739: 0.333\n",
      "test accuracy on epoch 739: 0.769\n",
      "train loss on epoch 740 : 0.175\n",
      "train accuracy on epoch 740: 0.889\n",
      "test loss on epoch 740: 0.336\n",
      "test accuracy on epoch 740: 0.769\n",
      "train loss on epoch 741 : 0.181\n",
      "train accuracy on epoch 741: 0.889\n",
      "test loss on epoch 741: 0.329\n",
      "test accuracy on epoch 741: 0.769\n",
      "train loss on epoch 742 : 0.091\n",
      "train accuracy on epoch 742: 0.944\n",
      "test loss on epoch 742: 0.328\n",
      "test accuracy on epoch 742: 0.769\n",
      "train loss on epoch 743 : 0.185\n",
      "train accuracy on epoch 743: 0.889\n",
      "test loss on epoch 743: 0.339\n",
      "test accuracy on epoch 743: 0.769\n",
      "train loss on epoch 744 : 0.127\n",
      "train accuracy on epoch 744: 0.944\n",
      "test loss on epoch 744: 0.329\n",
      "test accuracy on epoch 744: 0.769\n",
      "train loss on epoch 745 : 0.059\n",
      "train accuracy on epoch 745: 1.000\n",
      "test loss on epoch 745: 0.327\n",
      "test accuracy on epoch 745: 0.769\n",
      "train loss on epoch 746 : 0.142\n",
      "train accuracy on epoch 746: 0.889\n",
      "test loss on epoch 746: 0.334\n",
      "test accuracy on epoch 746: 0.769\n",
      "train loss on epoch 747 : 0.145\n",
      "train accuracy on epoch 747: 0.944\n",
      "test loss on epoch 747: 0.326\n",
      "test accuracy on epoch 747: 0.769\n",
      "train loss on epoch 748 : 0.118\n",
      "train accuracy on epoch 748: 0.944\n",
      "test loss on epoch 748: 0.325\n",
      "test accuracy on epoch 748: 0.769\n",
      "train loss on epoch 749 : 0.185\n",
      "train accuracy on epoch 749: 0.889\n",
      "test loss on epoch 749: 0.330\n",
      "test accuracy on epoch 749: 0.769\n",
      "train loss on epoch 750 : 0.061\n",
      "train accuracy on epoch 750: 0.944\n",
      "test loss on epoch 750: 0.326\n",
      "test accuracy on epoch 750: 0.769\n",
      "train loss on epoch 751 : 0.368\n",
      "train accuracy on epoch 751: 0.833\n",
      "test loss on epoch 751: 0.330\n",
      "test accuracy on epoch 751: 0.769\n",
      "train loss on epoch 752 : 0.062\n",
      "train accuracy on epoch 752: 1.000\n",
      "test loss on epoch 752: 0.330\n",
      "test accuracy on epoch 752: 0.769\n",
      "train loss on epoch 753 : 0.197\n",
      "train accuracy on epoch 753: 0.889\n",
      "test loss on epoch 753: 0.327\n",
      "test accuracy on epoch 753: 0.769\n",
      "train loss on epoch 754 : 0.125\n",
      "train accuracy on epoch 754: 1.000\n",
      "test loss on epoch 754: 0.325\n",
      "test accuracy on epoch 754: 0.769\n",
      "train loss on epoch 755 : 0.190\n",
      "train accuracy on epoch 755: 0.944\n",
      "test loss on epoch 755: 0.323\n",
      "test accuracy on epoch 755: 0.769\n",
      "train loss on epoch 756 : 0.362\n",
      "train accuracy on epoch 756: 0.889\n",
      "test loss on epoch 756: 0.319\n",
      "test accuracy on epoch 756: 0.769\n",
      "train loss on epoch 757 : 0.070\n",
      "train accuracy on epoch 757: 0.944\n",
      "test loss on epoch 757: 0.326\n",
      "test accuracy on epoch 757: 0.769\n",
      "train loss on epoch 758 : 0.105\n",
      "train accuracy on epoch 758: 0.944\n",
      "test loss on epoch 758: 0.323\n",
      "test accuracy on epoch 758: 0.769\n",
      "train loss on epoch 759 : 0.028\n",
      "train accuracy on epoch 759: 1.000\n",
      "test loss on epoch 759: 0.326\n",
      "test accuracy on epoch 759: 0.769\n",
      "train loss on epoch 760 : 0.068\n",
      "train accuracy on epoch 760: 0.944\n",
      "test loss on epoch 760: 0.321\n",
      "test accuracy on epoch 760: 0.769\n",
      "train loss on epoch 761 : 0.079\n",
      "train accuracy on epoch 761: 0.944\n",
      "test loss on epoch 761: 0.323\n",
      "test accuracy on epoch 761: 0.769\n",
      "train loss on epoch 762 : 0.215\n",
      "train accuracy on epoch 762: 0.889\n",
      "test loss on epoch 762: 0.317\n",
      "test accuracy on epoch 762: 0.846\n",
      "train loss on epoch 763 : 0.277\n",
      "train accuracy on epoch 763: 0.833\n",
      "test loss on epoch 763: 0.317\n",
      "test accuracy on epoch 763: 0.769\n",
      "train loss on epoch 764 : 0.252\n",
      "train accuracy on epoch 764: 0.889\n",
      "test loss on epoch 764: 0.325\n",
      "test accuracy on epoch 764: 0.769\n",
      "train loss on epoch 765 : 0.236\n",
      "train accuracy on epoch 765: 0.889\n",
      "test loss on epoch 765: 0.315\n",
      "test accuracy on epoch 765: 0.769\n",
      "train loss on epoch 766 : 0.392\n",
      "train accuracy on epoch 766: 0.889\n",
      "test loss on epoch 766: 0.317\n",
      "test accuracy on epoch 766: 0.769\n",
      "train loss on epoch 767 : 0.169\n",
      "train accuracy on epoch 767: 0.944\n",
      "test loss on epoch 767: 0.315\n",
      "test accuracy on epoch 767: 0.769\n",
      "train loss on epoch 768 : 0.227\n",
      "train accuracy on epoch 768: 0.889\n",
      "test loss on epoch 768: 0.319\n",
      "test accuracy on epoch 768: 0.692\n",
      "train loss on epoch 769 : 0.082\n",
      "train accuracy on epoch 769: 1.000\n",
      "test loss on epoch 769: 0.318\n",
      "test accuracy on epoch 769: 0.769\n",
      "train loss on epoch 770 : 0.188\n",
      "train accuracy on epoch 770: 0.944\n",
      "test loss on epoch 770: 0.313\n",
      "test accuracy on epoch 770: 0.769\n",
      "train loss on epoch 771 : 0.115\n",
      "train accuracy on epoch 771: 0.944\n",
      "test loss on epoch 771: 0.322\n",
      "test accuracy on epoch 771: 0.692\n",
      "train loss on epoch 772 : 0.082\n",
      "train accuracy on epoch 772: 0.944\n",
      "test loss on epoch 772: 0.316\n",
      "test accuracy on epoch 772: 0.769\n",
      "train loss on epoch 773 : 0.036\n",
      "train accuracy on epoch 773: 1.000\n",
      "test loss on epoch 773: 0.325\n",
      "test accuracy on epoch 773: 0.769\n",
      "train loss on epoch 774 : 0.161\n",
      "train accuracy on epoch 774: 0.944\n",
      "test loss on epoch 774: 0.321\n",
      "test accuracy on epoch 774: 0.769\n",
      "train loss on epoch 775 : 0.286\n",
      "train accuracy on epoch 775: 0.833\n",
      "test loss on epoch 775: 0.323\n",
      "test accuracy on epoch 775: 0.769\n",
      "train loss on epoch 776 : 0.077\n",
      "train accuracy on epoch 776: 1.000\n",
      "test loss on epoch 776: 0.318\n",
      "test accuracy on epoch 776: 0.769\n",
      "train loss on epoch 777 : 0.163\n",
      "train accuracy on epoch 777: 0.889\n",
      "test loss on epoch 777: 0.321\n",
      "test accuracy on epoch 777: 0.769\n",
      "train loss on epoch 778 : 0.261\n",
      "train accuracy on epoch 778: 0.889\n",
      "test loss on epoch 778: 0.315\n",
      "test accuracy on epoch 778: 0.769\n",
      "train loss on epoch 779 : 0.231\n",
      "train accuracy on epoch 779: 0.833\n",
      "test loss on epoch 779: 0.313\n",
      "test accuracy on epoch 779: 0.769\n",
      "train loss on epoch 780 : 0.252\n",
      "train accuracy on epoch 780: 0.889\n",
      "test loss on epoch 780: 0.312\n",
      "test accuracy on epoch 780: 0.769\n",
      "train loss on epoch 781 : 0.195\n",
      "train accuracy on epoch 781: 0.944\n",
      "test loss on epoch 781: 0.318\n",
      "test accuracy on epoch 781: 0.769\n",
      "train loss on epoch 782 : 0.278\n",
      "train accuracy on epoch 782: 0.889\n",
      "test loss on epoch 782: 0.307\n",
      "test accuracy on epoch 782: 0.769\n",
      "train loss on epoch 783 : 0.206\n",
      "train accuracy on epoch 783: 0.889\n",
      "test loss on epoch 783: 0.319\n",
      "test accuracy on epoch 783: 0.769\n",
      "train loss on epoch 784 : 0.172\n",
      "train accuracy on epoch 784: 0.889\n",
      "test loss on epoch 784: 0.321\n",
      "test accuracy on epoch 784: 0.769\n",
      "train loss on epoch 785 : 0.162\n",
      "train accuracy on epoch 785: 0.944\n",
      "test loss on epoch 785: 0.308\n",
      "test accuracy on epoch 785: 0.769\n",
      "train loss on epoch 786 : 0.183\n",
      "train accuracy on epoch 786: 0.944\n",
      "test loss on epoch 786: 0.317\n",
      "test accuracy on epoch 786: 0.769\n",
      "train loss on epoch 787 : 0.146\n",
      "train accuracy on epoch 787: 0.944\n",
      "test loss on epoch 787: 0.316\n",
      "test accuracy on epoch 787: 0.769\n",
      "train loss on epoch 788 : 0.206\n",
      "train accuracy on epoch 788: 0.889\n",
      "test loss on epoch 788: 0.317\n",
      "test accuracy on epoch 788: 0.769\n",
      "train loss on epoch 789 : 0.113\n",
      "train accuracy on epoch 789: 0.944\n",
      "test loss on epoch 789: 0.317\n",
      "test accuracy on epoch 789: 0.769\n",
      "train loss on epoch 790 : 0.213\n",
      "train accuracy on epoch 790: 0.889\n",
      "test loss on epoch 790: 0.319\n",
      "test accuracy on epoch 790: 0.769\n",
      "train loss on epoch 791 : 0.120\n",
      "train accuracy on epoch 791: 0.944\n",
      "test loss on epoch 791: 0.320\n",
      "test accuracy on epoch 791: 0.769\n",
      "train loss on epoch 792 : 0.422\n",
      "train accuracy on epoch 792: 0.833\n",
      "test loss on epoch 792: 0.311\n",
      "test accuracy on epoch 792: 0.769\n",
      "train loss on epoch 793 : 0.212\n",
      "train accuracy on epoch 793: 0.944\n",
      "test loss on epoch 793: 0.312\n",
      "test accuracy on epoch 793: 0.769\n",
      "train loss on epoch 794 : 0.407\n",
      "train accuracy on epoch 794: 0.889\n",
      "test loss on epoch 794: 0.316\n",
      "test accuracy on epoch 794: 0.769\n",
      "train loss on epoch 795 : 0.255\n",
      "train accuracy on epoch 795: 0.944\n",
      "test loss on epoch 795: 0.320\n",
      "test accuracy on epoch 795: 0.769\n",
      "train loss on epoch 796 : 0.340\n",
      "train accuracy on epoch 796: 0.889\n",
      "test loss on epoch 796: 0.316\n",
      "test accuracy on epoch 796: 0.769\n",
      "train loss on epoch 797 : 0.238\n",
      "train accuracy on epoch 797: 0.889\n",
      "test loss on epoch 797: 0.322\n",
      "test accuracy on epoch 797: 0.769\n",
      "train loss on epoch 798 : 0.294\n",
      "train accuracy on epoch 798: 0.833\n",
      "test loss on epoch 798: 0.322\n",
      "test accuracy on epoch 798: 0.769\n",
      "train loss on epoch 799 : 0.150\n",
      "train accuracy on epoch 799: 0.889\n",
      "test loss on epoch 799: 0.312\n",
      "test accuracy on epoch 799: 0.769\n",
      "train loss on epoch 800 : 0.157\n",
      "train accuracy on epoch 800: 0.944\n",
      "test loss on epoch 800: 0.311\n",
      "test accuracy on epoch 800: 0.769\n",
      "train loss on epoch 801 : 0.029\n",
      "train accuracy on epoch 801: 1.000\n",
      "test loss on epoch 801: 0.311\n",
      "test accuracy on epoch 801: 0.769\n",
      "train loss on epoch 802 : 0.207\n",
      "train accuracy on epoch 802: 0.833\n",
      "test loss on epoch 802: 0.309\n",
      "test accuracy on epoch 802: 0.769\n",
      "train loss on epoch 803 : 0.288\n",
      "train accuracy on epoch 803: 0.944\n",
      "test loss on epoch 803: 0.322\n",
      "test accuracy on epoch 803: 0.769\n",
      "train loss on epoch 804 : 0.092\n",
      "train accuracy on epoch 804: 1.000\n",
      "test loss on epoch 804: 0.327\n",
      "test accuracy on epoch 804: 0.769\n",
      "train loss on epoch 805 : 0.055\n",
      "train accuracy on epoch 805: 1.000\n",
      "test loss on epoch 805: 0.329\n",
      "test accuracy on epoch 805: 0.769\n",
      "train loss on epoch 806 : 0.058\n",
      "train accuracy on epoch 806: 1.000\n",
      "test loss on epoch 806: 0.336\n",
      "test accuracy on epoch 806: 0.769\n",
      "train loss on epoch 807 : 0.189\n",
      "train accuracy on epoch 807: 1.000\n",
      "test loss on epoch 807: 0.335\n",
      "test accuracy on epoch 807: 0.769\n",
      "train loss on epoch 808 : 0.223\n",
      "train accuracy on epoch 808: 0.944\n",
      "test loss on epoch 808: 0.322\n",
      "test accuracy on epoch 808: 0.769\n",
      "train loss on epoch 809 : 0.199\n",
      "train accuracy on epoch 809: 0.889\n",
      "test loss on epoch 809: 0.314\n",
      "test accuracy on epoch 809: 0.769\n",
      "train loss on epoch 810 : 0.013\n",
      "train accuracy on epoch 810: 1.000\n",
      "test loss on epoch 810: 0.316\n",
      "test accuracy on epoch 810: 0.769\n",
      "train loss on epoch 811 : 0.083\n",
      "train accuracy on epoch 811: 1.000\n",
      "test loss on epoch 811: 0.315\n",
      "test accuracy on epoch 811: 0.769\n",
      "train loss on epoch 812 : 0.235\n",
      "train accuracy on epoch 812: 0.944\n",
      "test loss on epoch 812: 0.315\n",
      "test accuracy on epoch 812: 0.769\n",
      "train loss on epoch 813 : 0.320\n",
      "train accuracy on epoch 813: 0.833\n",
      "test loss on epoch 813: 0.314\n",
      "test accuracy on epoch 813: 0.769\n",
      "train loss on epoch 814 : 0.349\n",
      "train accuracy on epoch 814: 0.889\n",
      "test loss on epoch 814: 0.311\n",
      "test accuracy on epoch 814: 0.769\n",
      "train loss on epoch 815 : 0.041\n",
      "train accuracy on epoch 815: 1.000\n",
      "test loss on epoch 815: 0.306\n",
      "test accuracy on epoch 815: 0.769\n",
      "train loss on epoch 816 : 0.186\n",
      "train accuracy on epoch 816: 0.944\n",
      "test loss on epoch 816: 0.312\n",
      "test accuracy on epoch 816: 0.769\n",
      "train loss on epoch 817 : 0.205\n",
      "train accuracy on epoch 817: 0.889\n",
      "test loss on epoch 817: 0.311\n",
      "test accuracy on epoch 817: 0.769\n",
      "train loss on epoch 818 : 0.103\n",
      "train accuracy on epoch 818: 0.944\n",
      "test loss on epoch 818: 0.309\n",
      "test accuracy on epoch 818: 0.769\n",
      "train loss on epoch 819 : 0.389\n",
      "train accuracy on epoch 819: 0.889\n",
      "test loss on epoch 819: 0.310\n",
      "test accuracy on epoch 819: 0.769\n",
      "train loss on epoch 820 : 0.097\n",
      "train accuracy on epoch 820: 1.000\n",
      "test loss on epoch 820: 0.311\n",
      "test accuracy on epoch 820: 0.769\n",
      "train loss on epoch 821 : 0.351\n",
      "train accuracy on epoch 821: 0.778\n",
      "test loss on epoch 821: 0.307\n",
      "test accuracy on epoch 821: 0.769\n",
      "train loss on epoch 822 : 0.138\n",
      "train accuracy on epoch 822: 0.944\n",
      "test loss on epoch 822: 0.309\n",
      "test accuracy on epoch 822: 0.769\n",
      "train loss on epoch 823 : 0.197\n",
      "train accuracy on epoch 823: 0.944\n",
      "test loss on epoch 823: 0.307\n",
      "test accuracy on epoch 823: 0.769\n",
      "train loss on epoch 824 : 0.101\n",
      "train accuracy on epoch 824: 0.944\n",
      "test loss on epoch 824: 0.309\n",
      "test accuracy on epoch 824: 0.769\n",
      "train loss on epoch 825 : 0.274\n",
      "train accuracy on epoch 825: 0.833\n",
      "test loss on epoch 825: 0.310\n",
      "test accuracy on epoch 825: 0.769\n",
      "train loss on epoch 826 : 0.044\n",
      "train accuracy on epoch 826: 1.000\n",
      "test loss on epoch 826: 0.311\n",
      "test accuracy on epoch 826: 0.769\n",
      "train loss on epoch 827 : 0.265\n",
      "train accuracy on epoch 827: 0.889\n",
      "test loss on epoch 827: 0.315\n",
      "test accuracy on epoch 827: 0.769\n",
      "train loss on epoch 828 : 0.261\n",
      "train accuracy on epoch 828: 0.944\n",
      "test loss on epoch 828: 0.314\n",
      "test accuracy on epoch 828: 0.769\n",
      "train loss on epoch 829 : 0.088\n",
      "train accuracy on epoch 829: 0.944\n",
      "test loss on epoch 829: 0.310\n",
      "test accuracy on epoch 829: 0.769\n",
      "train loss on epoch 830 : 0.073\n",
      "train accuracy on epoch 830: 1.000\n",
      "test loss on epoch 830: 0.302\n",
      "test accuracy on epoch 830: 0.769\n",
      "train loss on epoch 831 : 0.224\n",
      "train accuracy on epoch 831: 0.889\n",
      "test loss on epoch 831: 0.306\n",
      "test accuracy on epoch 831: 0.769\n",
      "train loss on epoch 832 : 0.237\n",
      "train accuracy on epoch 832: 0.889\n",
      "test loss on epoch 832: 0.299\n",
      "test accuracy on epoch 832: 0.769\n",
      "train loss on epoch 833 : 0.052\n",
      "train accuracy on epoch 833: 1.000\n",
      "test loss on epoch 833: 0.302\n",
      "test accuracy on epoch 833: 0.769\n",
      "train loss on epoch 834 : 0.133\n",
      "train accuracy on epoch 834: 0.944\n",
      "test loss on epoch 834: 0.310\n",
      "test accuracy on epoch 834: 0.769\n",
      "train loss on epoch 835 : 0.281\n",
      "train accuracy on epoch 835: 0.889\n",
      "test loss on epoch 835: 0.312\n",
      "test accuracy on epoch 835: 0.769\n",
      "train loss on epoch 836 : 0.249\n",
      "train accuracy on epoch 836: 0.944\n",
      "test loss on epoch 836: 0.304\n",
      "test accuracy on epoch 836: 0.769\n",
      "train loss on epoch 837 : 0.160\n",
      "train accuracy on epoch 837: 0.889\n",
      "test loss on epoch 837: 0.316\n",
      "test accuracy on epoch 837: 0.769\n",
      "train loss on epoch 838 : 0.166\n",
      "train accuracy on epoch 838: 0.889\n",
      "test loss on epoch 838: 0.311\n",
      "test accuracy on epoch 838: 0.769\n",
      "train loss on epoch 839 : 0.253\n",
      "train accuracy on epoch 839: 0.889\n",
      "test loss on epoch 839: 0.304\n",
      "test accuracy on epoch 839: 0.769\n",
      "train loss on epoch 840 : 0.062\n",
      "train accuracy on epoch 840: 0.944\n",
      "test loss on epoch 840: 0.310\n",
      "test accuracy on epoch 840: 0.769\n",
      "train loss on epoch 841 : 0.066\n",
      "train accuracy on epoch 841: 1.000\n",
      "test loss on epoch 841: 0.310\n",
      "test accuracy on epoch 841: 0.769\n",
      "train loss on epoch 842 : 0.175\n",
      "train accuracy on epoch 842: 0.944\n",
      "test loss on epoch 842: 0.311\n",
      "test accuracy on epoch 842: 0.769\n",
      "train loss on epoch 843 : 0.241\n",
      "train accuracy on epoch 843: 0.889\n",
      "test loss on epoch 843: 0.306\n",
      "test accuracy on epoch 843: 0.769\n",
      "train loss on epoch 844 : 0.225\n",
      "train accuracy on epoch 844: 0.944\n",
      "test loss on epoch 844: 0.319\n",
      "test accuracy on epoch 844: 0.769\n",
      "train loss on epoch 845 : 0.244\n",
      "train accuracy on epoch 845: 0.833\n",
      "test loss on epoch 845: 0.314\n",
      "test accuracy on epoch 845: 0.769\n",
      "train loss on epoch 846 : 0.129\n",
      "train accuracy on epoch 846: 0.944\n",
      "test loss on epoch 846: 0.316\n",
      "test accuracy on epoch 846: 0.769\n",
      "train loss on epoch 847 : 0.241\n",
      "train accuracy on epoch 847: 0.944\n",
      "test loss on epoch 847: 0.318\n",
      "test accuracy on epoch 847: 0.769\n",
      "train loss on epoch 848 : 0.135\n",
      "train accuracy on epoch 848: 0.944\n",
      "test loss on epoch 848: 0.317\n",
      "test accuracy on epoch 848: 0.769\n",
      "train loss on epoch 849 : 0.373\n",
      "train accuracy on epoch 849: 0.889\n",
      "test loss on epoch 849: 0.314\n",
      "test accuracy on epoch 849: 0.769\n",
      "train loss on epoch 850 : 0.143\n",
      "train accuracy on epoch 850: 0.944\n",
      "test loss on epoch 850: 0.303\n",
      "test accuracy on epoch 850: 0.769\n",
      "train loss on epoch 851 : 0.312\n",
      "train accuracy on epoch 851: 0.778\n",
      "test loss on epoch 851: 0.318\n",
      "test accuracy on epoch 851: 0.769\n",
      "train loss on epoch 852 : 0.278\n",
      "train accuracy on epoch 852: 0.833\n",
      "test loss on epoch 852: 0.307\n",
      "test accuracy on epoch 852: 0.769\n",
      "train loss on epoch 853 : 0.272\n",
      "train accuracy on epoch 853: 0.889\n",
      "test loss on epoch 853: 0.313\n",
      "test accuracy on epoch 853: 0.769\n",
      "train loss on epoch 854 : 0.242\n",
      "train accuracy on epoch 854: 0.944\n",
      "test loss on epoch 854: 0.313\n",
      "test accuracy on epoch 854: 0.769\n",
      "train loss on epoch 855 : 0.079\n",
      "train accuracy on epoch 855: 1.000\n",
      "test loss on epoch 855: 0.319\n",
      "test accuracy on epoch 855: 0.769\n",
      "train loss on epoch 856 : 0.178\n",
      "train accuracy on epoch 856: 0.944\n",
      "test loss on epoch 856: 0.314\n",
      "test accuracy on epoch 856: 0.769\n",
      "train loss on epoch 857 : 0.036\n",
      "train accuracy on epoch 857: 1.000\n",
      "test loss on epoch 857: 0.314\n",
      "test accuracy on epoch 857: 0.769\n",
      "train loss on epoch 858 : 0.082\n",
      "train accuracy on epoch 858: 0.944\n",
      "test loss on epoch 858: 0.314\n",
      "test accuracy on epoch 858: 0.769\n",
      "train loss on epoch 859 : 0.239\n",
      "train accuracy on epoch 859: 0.889\n",
      "test loss on epoch 859: 0.306\n",
      "test accuracy on epoch 859: 0.769\n",
      "train loss on epoch 860 : 0.376\n",
      "train accuracy on epoch 860: 0.889\n",
      "test loss on epoch 860: 0.317\n",
      "test accuracy on epoch 860: 0.769\n",
      "train loss on epoch 861 : 0.190\n",
      "train accuracy on epoch 861: 0.944\n",
      "test loss on epoch 861: 0.316\n",
      "test accuracy on epoch 861: 0.769\n",
      "train loss on epoch 862 : 0.108\n",
      "train accuracy on epoch 862: 0.889\n",
      "test loss on epoch 862: 0.316\n",
      "test accuracy on epoch 862: 0.769\n",
      "train loss on epoch 863 : 0.056\n",
      "train accuracy on epoch 863: 1.000\n",
      "test loss on epoch 863: 0.304\n",
      "test accuracy on epoch 863: 0.769\n",
      "train loss on epoch 864 : 0.409\n",
      "train accuracy on epoch 864: 0.889\n",
      "test loss on epoch 864: 0.321\n",
      "test accuracy on epoch 864: 0.769\n",
      "train loss on epoch 865 : 0.371\n",
      "train accuracy on epoch 865: 0.833\n",
      "test loss on epoch 865: 0.308\n",
      "test accuracy on epoch 865: 0.769\n",
      "train loss on epoch 866 : 0.166\n",
      "train accuracy on epoch 866: 0.889\n",
      "test loss on epoch 866: 0.303\n",
      "test accuracy on epoch 866: 0.769\n",
      "train loss on epoch 867 : 0.149\n",
      "train accuracy on epoch 867: 0.944\n",
      "test loss on epoch 867: 0.309\n",
      "test accuracy on epoch 867: 0.769\n",
      "train loss on epoch 868 : 0.446\n",
      "train accuracy on epoch 868: 0.833\n",
      "test loss on epoch 868: 0.323\n",
      "test accuracy on epoch 868: 0.769\n",
      "train loss on epoch 869 : 0.226\n",
      "train accuracy on epoch 869: 0.889\n",
      "test loss on epoch 869: 0.324\n",
      "test accuracy on epoch 869: 0.769\n",
      "train loss on epoch 870 : 0.273\n",
      "train accuracy on epoch 870: 0.889\n",
      "test loss on epoch 870: 0.323\n",
      "test accuracy on epoch 870: 0.769\n",
      "train loss on epoch 871 : 0.168\n",
      "train accuracy on epoch 871: 0.944\n",
      "test loss on epoch 871: 0.324\n",
      "test accuracy on epoch 871: 0.769\n",
      "train loss on epoch 872 : 0.055\n",
      "train accuracy on epoch 872: 1.000\n",
      "test loss on epoch 872: 0.320\n",
      "test accuracy on epoch 872: 0.769\n",
      "train loss on epoch 873 : 0.217\n",
      "train accuracy on epoch 873: 0.944\n",
      "test loss on epoch 873: 0.315\n",
      "test accuracy on epoch 873: 0.769\n",
      "train loss on epoch 874 : 0.090\n",
      "train accuracy on epoch 874: 0.944\n",
      "test loss on epoch 874: 0.322\n",
      "test accuracy on epoch 874: 0.769\n",
      "train loss on epoch 875 : 0.122\n",
      "train accuracy on epoch 875: 0.944\n",
      "test loss on epoch 875: 0.318\n",
      "test accuracy on epoch 875: 0.769\n",
      "train loss on epoch 876 : 0.081\n",
      "train accuracy on epoch 876: 1.000\n",
      "test loss on epoch 876: 0.303\n",
      "test accuracy on epoch 876: 0.769\n",
      "train loss on epoch 877 : 0.262\n",
      "train accuracy on epoch 877: 0.944\n",
      "test loss on epoch 877: 0.317\n",
      "test accuracy on epoch 877: 0.769\n",
      "train loss on epoch 878 : 0.245\n",
      "train accuracy on epoch 878: 0.944\n",
      "test loss on epoch 878: 0.320\n",
      "test accuracy on epoch 878: 0.769\n",
      "train loss on epoch 879 : 0.085\n",
      "train accuracy on epoch 879: 0.944\n",
      "test loss on epoch 879: 0.317\n",
      "test accuracy on epoch 879: 0.769\n",
      "train loss on epoch 880 : 0.171\n",
      "train accuracy on epoch 880: 0.889\n",
      "test loss on epoch 880: 0.328\n",
      "test accuracy on epoch 880: 0.769\n",
      "train loss on epoch 881 : 0.269\n",
      "train accuracy on epoch 881: 0.944\n",
      "test loss on epoch 881: 0.326\n",
      "test accuracy on epoch 881: 0.692\n",
      "train loss on epoch 882 : 0.157\n",
      "train accuracy on epoch 882: 0.944\n",
      "test loss on epoch 882: 0.326\n",
      "test accuracy on epoch 882: 0.692\n",
      "train loss on epoch 883 : 0.301\n",
      "train accuracy on epoch 883: 0.833\n",
      "test loss on epoch 883: 0.321\n",
      "test accuracy on epoch 883: 0.769\n",
      "train loss on epoch 884 : 0.235\n",
      "train accuracy on epoch 884: 0.833\n",
      "test loss on epoch 884: 0.317\n",
      "test accuracy on epoch 884: 0.769\n",
      "train loss on epoch 885 : 0.072\n",
      "train accuracy on epoch 885: 1.000\n",
      "test loss on epoch 885: 0.322\n",
      "test accuracy on epoch 885: 0.769\n",
      "train loss on epoch 886 : 0.209\n",
      "train accuracy on epoch 886: 0.889\n",
      "test loss on epoch 886: 0.308\n",
      "test accuracy on epoch 886: 0.769\n",
      "train loss on epoch 887 : 0.142\n",
      "train accuracy on epoch 887: 0.944\n",
      "test loss on epoch 887: 0.325\n",
      "test accuracy on epoch 887: 0.769\n",
      "train loss on epoch 888 : 0.234\n",
      "train accuracy on epoch 888: 0.944\n",
      "test loss on epoch 888: 0.324\n",
      "test accuracy on epoch 888: 0.769\n",
      "train loss on epoch 889 : 0.163\n",
      "train accuracy on epoch 889: 0.944\n",
      "test loss on epoch 889: 0.320\n",
      "test accuracy on epoch 889: 0.769\n",
      "train loss on epoch 890 : 0.165\n",
      "train accuracy on epoch 890: 0.944\n",
      "test loss on epoch 890: 0.328\n",
      "test accuracy on epoch 890: 0.769\n",
      "train loss on epoch 891 : 0.098\n",
      "train accuracy on epoch 891: 0.944\n",
      "test loss on epoch 891: 0.334\n",
      "test accuracy on epoch 891: 0.769\n",
      "train loss on epoch 892 : 0.244\n",
      "train accuracy on epoch 892: 0.944\n",
      "test loss on epoch 892: 0.325\n",
      "test accuracy on epoch 892: 0.769\n",
      "train loss on epoch 893 : 0.352\n",
      "train accuracy on epoch 893: 0.944\n",
      "test loss on epoch 893: 0.307\n",
      "test accuracy on epoch 893: 0.769\n",
      "train loss on epoch 894 : 0.229\n",
      "train accuracy on epoch 894: 0.833\n",
      "test loss on epoch 894: 0.304\n",
      "test accuracy on epoch 894: 0.769\n",
      "train loss on epoch 895 : 0.234\n",
      "train accuracy on epoch 895: 0.833\n",
      "test loss on epoch 895: 0.320\n",
      "test accuracy on epoch 895: 0.692\n",
      "train loss on epoch 896 : 0.212\n",
      "train accuracy on epoch 896: 0.889\n",
      "test loss on epoch 896: 0.319\n",
      "test accuracy on epoch 896: 0.769\n",
      "train loss on epoch 897 : 0.049\n",
      "train accuracy on epoch 897: 1.000\n",
      "test loss on epoch 897: 0.325\n",
      "test accuracy on epoch 897: 0.769\n",
      "train loss on epoch 898 : 0.316\n",
      "train accuracy on epoch 898: 0.889\n",
      "test loss on epoch 898: 0.328\n",
      "test accuracy on epoch 898: 0.769\n",
      "train loss on epoch 899 : 0.266\n",
      "train accuracy on epoch 899: 0.944\n",
      "test loss on epoch 899: 0.329\n",
      "test accuracy on epoch 899: 0.769\n",
      "train loss on epoch 900 : 0.100\n",
      "train accuracy on epoch 900: 1.000\n",
      "test loss on epoch 900: 0.327\n",
      "test accuracy on epoch 900: 0.769\n",
      "train loss on epoch 901 : 0.032\n",
      "train accuracy on epoch 901: 1.000\n",
      "test loss on epoch 901: 0.321\n",
      "test accuracy on epoch 901: 0.769\n",
      "train loss on epoch 902 : 0.202\n",
      "train accuracy on epoch 902: 0.889\n",
      "test loss on epoch 902: 0.321\n",
      "test accuracy on epoch 902: 0.769\n",
      "train loss on epoch 903 : 0.134\n",
      "train accuracy on epoch 903: 0.944\n",
      "test loss on epoch 903: 0.325\n",
      "test accuracy on epoch 903: 0.769\n",
      "train loss on epoch 904 : 0.123\n",
      "train accuracy on epoch 904: 0.889\n",
      "test loss on epoch 904: 0.326\n",
      "test accuracy on epoch 904: 0.769\n",
      "train loss on epoch 905 : 0.142\n",
      "train accuracy on epoch 905: 0.944\n",
      "test loss on epoch 905: 0.323\n",
      "test accuracy on epoch 905: 0.769\n",
      "train loss on epoch 906 : 0.084\n",
      "train accuracy on epoch 906: 0.944\n",
      "test loss on epoch 906: 0.318\n",
      "test accuracy on epoch 906: 0.769\n",
      "train loss on epoch 907 : 0.103\n",
      "train accuracy on epoch 907: 0.944\n",
      "test loss on epoch 907: 0.321\n",
      "test accuracy on epoch 907: 0.769\n",
      "train loss on epoch 908 : 0.150\n",
      "train accuracy on epoch 908: 0.889\n",
      "test loss on epoch 908: 0.315\n",
      "test accuracy on epoch 908: 0.769\n",
      "train loss on epoch 909 : 0.082\n",
      "train accuracy on epoch 909: 1.000\n",
      "test loss on epoch 909: 0.311\n",
      "test accuracy on epoch 909: 0.769\n",
      "train loss on epoch 910 : 0.201\n",
      "train accuracy on epoch 910: 0.944\n",
      "test loss on epoch 910: 0.315\n",
      "test accuracy on epoch 910: 0.769\n",
      "train loss on epoch 911 : 0.125\n",
      "train accuracy on epoch 911: 1.000\n",
      "test loss on epoch 911: 0.317\n",
      "test accuracy on epoch 911: 0.769\n",
      "train loss on epoch 912 : 0.325\n",
      "train accuracy on epoch 912: 0.889\n",
      "test loss on epoch 912: 0.323\n",
      "test accuracy on epoch 912: 0.769\n",
      "train loss on epoch 913 : 0.261\n",
      "train accuracy on epoch 913: 0.944\n",
      "test loss on epoch 913: 0.318\n",
      "test accuracy on epoch 913: 0.769\n",
      "train loss on epoch 914 : 0.157\n",
      "train accuracy on epoch 914: 0.889\n",
      "test loss on epoch 914: 0.313\n",
      "test accuracy on epoch 914: 0.769\n",
      "train loss on epoch 915 : 0.078\n",
      "train accuracy on epoch 915: 0.944\n",
      "test loss on epoch 915: 0.319\n",
      "test accuracy on epoch 915: 0.692\n",
      "train loss on epoch 916 : 0.103\n",
      "train accuracy on epoch 916: 0.944\n",
      "test loss on epoch 916: 0.315\n",
      "test accuracy on epoch 916: 0.769\n",
      "train loss on epoch 917 : 0.444\n",
      "train accuracy on epoch 917: 0.833\n",
      "test loss on epoch 917: 0.308\n",
      "test accuracy on epoch 917: 0.846\n",
      "train loss on epoch 918 : 0.155\n",
      "train accuracy on epoch 918: 0.944\n",
      "test loss on epoch 918: 0.310\n",
      "test accuracy on epoch 918: 0.769\n",
      "train loss on epoch 919 : 0.328\n",
      "train accuracy on epoch 919: 0.944\n",
      "test loss on epoch 919: 0.321\n",
      "test accuracy on epoch 919: 0.769\n",
      "train loss on epoch 920 : 0.313\n",
      "train accuracy on epoch 920: 0.889\n",
      "test loss on epoch 920: 0.320\n",
      "test accuracy on epoch 920: 0.769\n",
      "train loss on epoch 921 : 0.078\n",
      "train accuracy on epoch 921: 1.000\n",
      "test loss on epoch 921: 0.321\n",
      "test accuracy on epoch 921: 0.769\n",
      "train loss on epoch 922 : 0.090\n",
      "train accuracy on epoch 922: 0.944\n",
      "test loss on epoch 922: 0.316\n",
      "test accuracy on epoch 922: 0.769\n",
      "train loss on epoch 923 : 0.195\n",
      "train accuracy on epoch 923: 0.944\n",
      "test loss on epoch 923: 0.321\n",
      "test accuracy on epoch 923: 0.769\n",
      "train loss on epoch 924 : 0.219\n",
      "train accuracy on epoch 924: 0.889\n",
      "test loss on epoch 924: 0.314\n",
      "test accuracy on epoch 924: 0.769\n",
      "train loss on epoch 925 : 0.184\n",
      "train accuracy on epoch 925: 0.944\n",
      "test loss on epoch 925: 0.314\n",
      "test accuracy on epoch 925: 0.769\n",
      "train loss on epoch 926 : 0.069\n",
      "train accuracy on epoch 926: 1.000\n",
      "test loss on epoch 926: 0.316\n",
      "test accuracy on epoch 926: 0.769\n",
      "train loss on epoch 927 : 0.192\n",
      "train accuracy on epoch 927: 0.889\n",
      "test loss on epoch 927: 0.320\n",
      "test accuracy on epoch 927: 0.769\n",
      "train loss on epoch 928 : 0.149\n",
      "train accuracy on epoch 928: 0.944\n",
      "test loss on epoch 928: 0.317\n",
      "test accuracy on epoch 928: 0.769\n",
      "train loss on epoch 929 : 0.191\n",
      "train accuracy on epoch 929: 0.944\n",
      "test loss on epoch 929: 0.322\n",
      "test accuracy on epoch 929: 0.769\n",
      "train loss on epoch 930 : 0.129\n",
      "train accuracy on epoch 930: 0.944\n",
      "test loss on epoch 930: 0.327\n",
      "test accuracy on epoch 930: 0.769\n",
      "train loss on epoch 931 : 0.249\n",
      "train accuracy on epoch 931: 0.889\n",
      "test loss on epoch 931: 0.327\n",
      "test accuracy on epoch 931: 0.769\n",
      "train loss on epoch 932 : 0.484\n",
      "train accuracy on epoch 932: 0.889\n",
      "test loss on epoch 932: 0.330\n",
      "test accuracy on epoch 932: 0.769\n",
      "train loss on epoch 933 : 0.216\n",
      "train accuracy on epoch 933: 0.944\n",
      "test loss on epoch 933: 0.326\n",
      "test accuracy on epoch 933: 0.769\n",
      "train loss on epoch 934 : 0.209\n",
      "train accuracy on epoch 934: 0.889\n",
      "test loss on epoch 934: 0.326\n",
      "test accuracy on epoch 934: 0.769\n",
      "train loss on epoch 935 : 0.156\n",
      "train accuracy on epoch 935: 0.944\n",
      "test loss on epoch 935: 0.330\n",
      "test accuracy on epoch 935: 0.769\n",
      "train loss on epoch 936 : 0.121\n",
      "train accuracy on epoch 936: 0.944\n",
      "test loss on epoch 936: 0.332\n",
      "test accuracy on epoch 936: 0.769\n",
      "train loss on epoch 937 : 0.200\n",
      "train accuracy on epoch 937: 0.889\n",
      "test loss on epoch 937: 0.336\n",
      "test accuracy on epoch 937: 0.769\n",
      "train loss on epoch 938 : 0.089\n",
      "train accuracy on epoch 938: 0.944\n",
      "test loss on epoch 938: 0.330\n",
      "test accuracy on epoch 938: 0.769\n",
      "train loss on epoch 939 : 0.119\n",
      "train accuracy on epoch 939: 0.944\n",
      "test loss on epoch 939: 0.331\n",
      "test accuracy on epoch 939: 0.769\n",
      "train loss on epoch 940 : 0.094\n",
      "train accuracy on epoch 940: 1.000\n",
      "test loss on epoch 940: 0.329\n",
      "test accuracy on epoch 940: 0.769\n",
      "train loss on epoch 941 : 0.063\n",
      "train accuracy on epoch 941: 1.000\n",
      "test loss on epoch 941: 0.332\n",
      "test accuracy on epoch 941: 0.769\n",
      "train loss on epoch 942 : 0.076\n",
      "train accuracy on epoch 942: 1.000\n",
      "test loss on epoch 942: 0.332\n",
      "test accuracy on epoch 942: 0.769\n",
      "train loss on epoch 943 : 0.222\n",
      "train accuracy on epoch 943: 0.833\n",
      "test loss on epoch 943: 0.331\n",
      "test accuracy on epoch 943: 0.769\n",
      "train loss on epoch 944 : 0.272\n",
      "train accuracy on epoch 944: 0.833\n",
      "test loss on epoch 944: 0.325\n",
      "test accuracy on epoch 944: 0.769\n",
      "train loss on epoch 945 : 0.194\n",
      "train accuracy on epoch 945: 0.889\n",
      "test loss on epoch 945: 0.321\n",
      "test accuracy on epoch 945: 0.769\n",
      "train loss on epoch 946 : 0.366\n",
      "train accuracy on epoch 946: 0.833\n",
      "test loss on epoch 946: 0.324\n",
      "test accuracy on epoch 946: 0.769\n",
      "train loss on epoch 947 : 0.381\n",
      "train accuracy on epoch 947: 0.833\n",
      "test loss on epoch 947: 0.324\n",
      "test accuracy on epoch 947: 0.769\n",
      "train loss on epoch 948 : 0.075\n",
      "train accuracy on epoch 948: 1.000\n",
      "test loss on epoch 948: 0.324\n",
      "test accuracy on epoch 948: 0.769\n",
      "train loss on epoch 949 : 0.349\n",
      "train accuracy on epoch 949: 0.833\n",
      "test loss on epoch 949: 0.329\n",
      "test accuracy on epoch 949: 0.769\n",
      "train loss on epoch 950 : 0.299\n",
      "train accuracy on epoch 950: 0.833\n",
      "test loss on epoch 950: 0.330\n",
      "test accuracy on epoch 950: 0.769\n",
      "train loss on epoch 951 : 0.036\n",
      "train accuracy on epoch 951: 1.000\n",
      "test loss on epoch 951: 0.333\n",
      "test accuracy on epoch 951: 0.769\n",
      "train loss on epoch 952 : 0.222\n",
      "train accuracy on epoch 952: 0.889\n",
      "test loss on epoch 952: 0.336\n",
      "test accuracy on epoch 952: 0.769\n",
      "train loss on epoch 953 : 0.186\n",
      "train accuracy on epoch 953: 0.889\n",
      "test loss on epoch 953: 0.338\n",
      "test accuracy on epoch 953: 0.769\n",
      "train loss on epoch 954 : 0.230\n",
      "train accuracy on epoch 954: 0.889\n",
      "test loss on epoch 954: 0.339\n",
      "test accuracy on epoch 954: 0.769\n",
      "train loss on epoch 955 : 0.295\n",
      "train accuracy on epoch 955: 0.889\n",
      "test loss on epoch 955: 0.343\n",
      "test accuracy on epoch 955: 0.769\n",
      "train loss on epoch 956 : 0.060\n",
      "train accuracy on epoch 956: 1.000\n",
      "test loss on epoch 956: 0.337\n",
      "test accuracy on epoch 956: 0.769\n",
      "train loss on epoch 957 : 0.112\n",
      "train accuracy on epoch 957: 1.000\n",
      "test loss on epoch 957: 0.337\n",
      "test accuracy on epoch 957: 0.769\n",
      "train loss on epoch 958 : 0.148\n",
      "train accuracy on epoch 958: 0.944\n",
      "test loss on epoch 958: 0.338\n",
      "test accuracy on epoch 958: 0.769\n",
      "train loss on epoch 959 : 0.274\n",
      "train accuracy on epoch 959: 0.944\n",
      "test loss on epoch 959: 0.341\n",
      "test accuracy on epoch 959: 0.769\n",
      "train loss on epoch 960 : 0.400\n",
      "train accuracy on epoch 960: 0.889\n",
      "test loss on epoch 960: 0.341\n",
      "test accuracy on epoch 960: 0.769\n",
      "train loss on epoch 961 : 0.281\n",
      "train accuracy on epoch 961: 0.944\n",
      "test loss on epoch 961: 0.338\n",
      "test accuracy on epoch 961: 0.769\n",
      "train loss on epoch 962 : 0.223\n",
      "train accuracy on epoch 962: 0.944\n",
      "test loss on epoch 962: 0.343\n",
      "test accuracy on epoch 962: 0.769\n",
      "train loss on epoch 963 : 0.136\n",
      "train accuracy on epoch 963: 0.944\n",
      "test loss on epoch 963: 0.350\n",
      "test accuracy on epoch 963: 0.769\n",
      "train loss on epoch 964 : 0.093\n",
      "train accuracy on epoch 964: 0.944\n",
      "test loss on epoch 964: 0.349\n",
      "test accuracy on epoch 964: 0.769\n",
      "train loss on epoch 965 : 0.188\n",
      "train accuracy on epoch 965: 0.944\n",
      "test loss on epoch 965: 0.358\n",
      "test accuracy on epoch 965: 0.769\n",
      "train loss on epoch 966 : 0.164\n",
      "train accuracy on epoch 966: 0.889\n",
      "test loss on epoch 966: 0.372\n",
      "test accuracy on epoch 966: 0.769\n",
      "train loss on epoch 967 : 0.107\n",
      "train accuracy on epoch 967: 1.000\n",
      "test loss on epoch 967: 0.376\n",
      "test accuracy on epoch 967: 0.769\n",
      "train loss on epoch 968 : 0.126\n",
      "train accuracy on epoch 968: 0.889\n",
      "test loss on epoch 968: 0.381\n",
      "test accuracy on epoch 968: 0.769\n",
      "train loss on epoch 969 : 0.288\n",
      "train accuracy on epoch 969: 0.889\n",
      "test loss on epoch 969: 0.374\n",
      "test accuracy on epoch 969: 0.769\n",
      "train loss on epoch 970 : 0.305\n",
      "train accuracy on epoch 970: 0.889\n",
      "test loss on epoch 970: 0.354\n",
      "test accuracy on epoch 970: 0.769\n",
      "train loss on epoch 971 : 0.241\n",
      "train accuracy on epoch 971: 0.944\n",
      "test loss on epoch 971: 0.340\n",
      "test accuracy on epoch 971: 0.769\n",
      "train loss on epoch 972 : 0.104\n",
      "train accuracy on epoch 972: 0.944\n",
      "test loss on epoch 972: 0.340\n",
      "test accuracy on epoch 972: 0.769\n",
      "train loss on epoch 973 : 0.119\n",
      "train accuracy on epoch 973: 0.944\n",
      "test loss on epoch 973: 0.338\n",
      "test accuracy on epoch 973: 0.769\n",
      "train loss on epoch 974 : 0.075\n",
      "train accuracy on epoch 974: 1.000\n",
      "test loss on epoch 974: 0.339\n",
      "test accuracy on epoch 974: 0.769\n",
      "train loss on epoch 975 : 0.052\n",
      "train accuracy on epoch 975: 1.000\n",
      "test loss on epoch 975: 0.335\n",
      "test accuracy on epoch 975: 0.769\n",
      "train loss on epoch 976 : 0.061\n",
      "train accuracy on epoch 976: 1.000\n",
      "test loss on epoch 976: 0.332\n",
      "test accuracy on epoch 976: 0.769\n",
      "train loss on epoch 977 : 0.037\n",
      "train accuracy on epoch 977: 1.000\n",
      "test loss on epoch 977: 0.328\n",
      "test accuracy on epoch 977: 0.769\n",
      "train loss on epoch 978 : 0.048\n",
      "train accuracy on epoch 978: 1.000\n",
      "test loss on epoch 978: 0.330\n",
      "test accuracy on epoch 978: 0.769\n",
      "train loss on epoch 979 : 0.037\n",
      "train accuracy on epoch 979: 1.000\n",
      "test loss on epoch 979: 0.331\n",
      "test accuracy on epoch 979: 0.769\n",
      "train loss on epoch 980 : 0.243\n",
      "train accuracy on epoch 980: 0.944\n",
      "test loss on epoch 980: 0.334\n",
      "test accuracy on epoch 980: 0.769\n",
      "train loss on epoch 981 : 0.134\n",
      "train accuracy on epoch 981: 0.944\n",
      "test loss on epoch 981: 0.339\n",
      "test accuracy on epoch 981: 0.769\n",
      "train loss on epoch 982 : 0.171\n",
      "train accuracy on epoch 982: 0.889\n",
      "test loss on epoch 982: 0.339\n",
      "test accuracy on epoch 982: 0.769\n",
      "train loss on epoch 983 : 0.254\n",
      "train accuracy on epoch 983: 0.889\n",
      "test loss on epoch 983: 0.342\n",
      "test accuracy on epoch 983: 0.769\n",
      "train loss on epoch 984 : 0.288\n",
      "train accuracy on epoch 984: 0.889\n",
      "test loss on epoch 984: 0.346\n",
      "test accuracy on epoch 984: 0.769\n",
      "train loss on epoch 985 : 0.248\n",
      "train accuracy on epoch 985: 0.944\n",
      "test loss on epoch 985: 0.350\n",
      "test accuracy on epoch 985: 0.769\n",
      "train loss on epoch 986 : 0.106\n",
      "train accuracy on epoch 986: 0.944\n",
      "test loss on epoch 986: 0.352\n",
      "test accuracy on epoch 986: 0.769\n",
      "train loss on epoch 987 : 0.078\n",
      "train accuracy on epoch 987: 0.944\n",
      "test loss on epoch 987: 0.350\n",
      "test accuracy on epoch 987: 0.769\n",
      "train loss on epoch 988 : 0.194\n",
      "train accuracy on epoch 988: 0.889\n",
      "test loss on epoch 988: 0.348\n",
      "test accuracy on epoch 988: 0.769\n",
      "train loss on epoch 989 : 0.158\n",
      "train accuracy on epoch 989: 0.944\n",
      "test loss on epoch 989: 0.344\n",
      "test accuracy on epoch 989: 0.769\n",
      "train loss on epoch 990 : 0.319\n",
      "train accuracy on epoch 990: 0.944\n",
      "test loss on epoch 990: 0.336\n",
      "test accuracy on epoch 990: 0.769\n",
      "train loss on epoch 991 : 0.092\n",
      "train accuracy on epoch 991: 1.000\n",
      "test loss on epoch 991: 0.334\n",
      "test accuracy on epoch 991: 0.769\n",
      "train loss on epoch 992 : 0.483\n",
      "train accuracy on epoch 992: 0.833\n",
      "test loss on epoch 992: 0.334\n",
      "test accuracy on epoch 992: 0.769\n",
      "train loss on epoch 993 : 0.147\n",
      "train accuracy on epoch 993: 0.889\n",
      "test loss on epoch 993: 0.330\n",
      "test accuracy on epoch 993: 0.769\n",
      "train loss on epoch 994 : 0.179\n",
      "train accuracy on epoch 994: 0.944\n",
      "test loss on epoch 994: 0.329\n",
      "test accuracy on epoch 994: 0.769\n",
      "train loss on epoch 995 : 0.161\n",
      "train accuracy on epoch 995: 0.944\n",
      "test loss on epoch 995: 0.331\n",
      "test accuracy on epoch 995: 0.692\n",
      "train loss on epoch 996 : 0.101\n",
      "train accuracy on epoch 996: 1.000\n",
      "test loss on epoch 996: 0.322\n",
      "test accuracy on epoch 996: 0.769\n",
      "train loss on epoch 997 : 0.134\n",
      "train accuracy on epoch 997: 0.889\n",
      "test loss on epoch 997: 0.327\n",
      "test accuracy on epoch 997: 0.769\n",
      "train loss on epoch 998 : 0.086\n",
      "train accuracy on epoch 998: 1.000\n",
      "test loss on epoch 998: 0.329\n",
      "test accuracy on epoch 998: 0.769\n",
      "train loss on epoch 999 : 0.370\n",
      "train accuracy on epoch 999: 0.889\n",
      "test loss on epoch 999: 0.328\n",
      "test accuracy on epoch 999: 0.769\n",
      "train loss on epoch 1000 : 0.130\n",
      "train accuracy on epoch 1000: 0.944\n",
      "test loss on epoch 1000: 0.321\n",
      "test accuracy on epoch 1000: 0.846\n",
      "train loss on epoch 1001 : 0.043\n",
      "train accuracy on epoch 1001: 1.000\n",
      "test loss on epoch 1001: 0.322\n",
      "test accuracy on epoch 1001: 0.846\n",
      "train loss on epoch 1002 : 0.145\n",
      "train accuracy on epoch 1002: 0.944\n",
      "test loss on epoch 1002: 0.325\n",
      "test accuracy on epoch 1002: 0.769\n",
      "train loss on epoch 1003 : 0.056\n",
      "train accuracy on epoch 1003: 1.000\n",
      "test loss on epoch 1003: 0.330\n",
      "test accuracy on epoch 1003: 0.769\n",
      "train loss on epoch 1004 : 0.172\n",
      "train accuracy on epoch 1004: 0.944\n",
      "test loss on epoch 1004: 0.330\n",
      "test accuracy on epoch 1004: 0.769\n",
      "train loss on epoch 1005 : 0.201\n",
      "train accuracy on epoch 1005: 0.944\n",
      "test loss on epoch 1005: 0.322\n",
      "test accuracy on epoch 1005: 0.846\n",
      "train loss on epoch 1006 : 0.030\n",
      "train accuracy on epoch 1006: 1.000\n",
      "test loss on epoch 1006: 0.323\n",
      "test accuracy on epoch 1006: 0.769\n",
      "train loss on epoch 1007 : 0.077\n",
      "train accuracy on epoch 1007: 1.000\n",
      "test loss on epoch 1007: 0.324\n",
      "test accuracy on epoch 1007: 0.769\n",
      "train loss on epoch 1008 : 0.249\n",
      "train accuracy on epoch 1008: 0.889\n",
      "test loss on epoch 1008: 0.335\n",
      "test accuracy on epoch 1008: 0.769\n",
      "train loss on epoch 1009 : 0.356\n",
      "train accuracy on epoch 1009: 0.889\n",
      "test loss on epoch 1009: 0.324\n",
      "test accuracy on epoch 1009: 0.846\n",
      "train loss on epoch 1010 : 0.103\n",
      "train accuracy on epoch 1010: 0.944\n",
      "test loss on epoch 1010: 0.331\n",
      "test accuracy on epoch 1010: 0.769\n",
      "train loss on epoch 1011 : 0.195\n",
      "train accuracy on epoch 1011: 0.944\n",
      "test loss on epoch 1011: 0.340\n",
      "test accuracy on epoch 1011: 0.769\n",
      "train loss on epoch 1012 : 0.117\n",
      "train accuracy on epoch 1012: 0.889\n",
      "test loss on epoch 1012: 0.337\n",
      "test accuracy on epoch 1012: 0.769\n",
      "train loss on epoch 1013 : 0.108\n",
      "train accuracy on epoch 1013: 0.944\n",
      "test loss on epoch 1013: 0.338\n",
      "test accuracy on epoch 1013: 0.769\n",
      "train loss on epoch 1014 : 0.243\n",
      "train accuracy on epoch 1014: 0.944\n",
      "test loss on epoch 1014: 0.336\n",
      "test accuracy on epoch 1014: 0.769\n",
      "train loss on epoch 1015 : 0.102\n",
      "train accuracy on epoch 1015: 0.944\n",
      "test loss on epoch 1015: 0.334\n",
      "test accuracy on epoch 1015: 0.769\n",
      "train loss on epoch 1016 : 0.105\n",
      "train accuracy on epoch 1016: 0.944\n",
      "test loss on epoch 1016: 0.340\n",
      "test accuracy on epoch 1016: 0.769\n",
      "train loss on epoch 1017 : 0.317\n",
      "train accuracy on epoch 1017: 0.889\n",
      "test loss on epoch 1017: 0.324\n",
      "test accuracy on epoch 1017: 0.769\n",
      "train loss on epoch 1018 : 0.525\n",
      "train accuracy on epoch 1018: 0.833\n",
      "test loss on epoch 1018: 0.331\n",
      "test accuracy on epoch 1018: 0.769\n",
      "train loss on epoch 1019 : 0.154\n",
      "train accuracy on epoch 1019: 0.944\n",
      "test loss on epoch 1019: 0.329\n",
      "test accuracy on epoch 1019: 0.769\n",
      "train loss on epoch 1020 : 0.127\n",
      "train accuracy on epoch 1020: 0.944\n",
      "test loss on epoch 1020: 0.339\n",
      "test accuracy on epoch 1020: 0.769\n",
      "train loss on epoch 1021 : 0.244\n",
      "train accuracy on epoch 1021: 0.889\n",
      "test loss on epoch 1021: 0.330\n",
      "test accuracy on epoch 1021: 0.769\n",
      "train loss on epoch 1022 : 0.151\n",
      "train accuracy on epoch 1022: 0.944\n",
      "test loss on epoch 1022: 0.329\n",
      "test accuracy on epoch 1022: 0.769\n",
      "train loss on epoch 1023 : 0.097\n",
      "train accuracy on epoch 1023: 1.000\n",
      "test loss on epoch 1023: 0.335\n",
      "test accuracy on epoch 1023: 0.769\n",
      "train loss on epoch 1024 : 0.192\n",
      "train accuracy on epoch 1024: 0.889\n",
      "test loss on epoch 1024: 0.341\n",
      "test accuracy on epoch 1024: 0.769\n",
      "train loss on epoch 1025 : 0.153\n",
      "train accuracy on epoch 1025: 0.944\n",
      "test loss on epoch 1025: 0.341\n",
      "test accuracy on epoch 1025: 0.769\n",
      "train loss on epoch 1026 : 0.124\n",
      "train accuracy on epoch 1026: 0.944\n",
      "test loss on epoch 1026: 0.341\n",
      "test accuracy on epoch 1026: 0.769\n",
      "train loss on epoch 1027 : 0.181\n",
      "train accuracy on epoch 1027: 0.944\n",
      "test loss on epoch 1027: 0.339\n",
      "test accuracy on epoch 1027: 0.769\n",
      "train loss on epoch 1028 : 0.078\n",
      "train accuracy on epoch 1028: 1.000\n",
      "test loss on epoch 1028: 0.338\n",
      "test accuracy on epoch 1028: 0.769\n",
      "train loss on epoch 1029 : 0.218\n",
      "train accuracy on epoch 1029: 0.944\n",
      "test loss on epoch 1029: 0.335\n",
      "test accuracy on epoch 1029: 0.769\n",
      "train loss on epoch 1030 : 0.066\n",
      "train accuracy on epoch 1030: 1.000\n",
      "test loss on epoch 1030: 0.332\n",
      "test accuracy on epoch 1030: 0.769\n",
      "train loss on epoch 1031 : 0.256\n",
      "train accuracy on epoch 1031: 0.944\n",
      "test loss on epoch 1031: 0.335\n",
      "test accuracy on epoch 1031: 0.769\n",
      "train loss on epoch 1032 : 0.160\n",
      "train accuracy on epoch 1032: 0.944\n",
      "test loss on epoch 1032: 0.338\n",
      "test accuracy on epoch 1032: 0.769\n",
      "train loss on epoch 1033 : 0.152\n",
      "train accuracy on epoch 1033: 0.944\n",
      "test loss on epoch 1033: 0.337\n",
      "test accuracy on epoch 1033: 0.769\n",
      "train loss on epoch 1034 : 0.233\n",
      "train accuracy on epoch 1034: 0.889\n",
      "test loss on epoch 1034: 0.338\n",
      "test accuracy on epoch 1034: 0.769\n",
      "train loss on epoch 1035 : 0.065\n",
      "train accuracy on epoch 1035: 0.944\n",
      "test loss on epoch 1035: 0.339\n",
      "test accuracy on epoch 1035: 0.769\n",
      "train loss on epoch 1036 : 0.208\n",
      "train accuracy on epoch 1036: 0.889\n",
      "test loss on epoch 1036: 0.333\n",
      "test accuracy on epoch 1036: 0.769\n",
      "train loss on epoch 1037 : 0.044\n",
      "train accuracy on epoch 1037: 1.000\n",
      "test loss on epoch 1037: 0.337\n",
      "test accuracy on epoch 1037: 0.769\n",
      "train loss on epoch 1038 : 0.349\n",
      "train accuracy on epoch 1038: 0.833\n",
      "test loss on epoch 1038: 0.338\n",
      "test accuracy on epoch 1038: 0.769\n",
      "train loss on epoch 1039 : 0.161\n",
      "train accuracy on epoch 1039: 0.944\n",
      "test loss on epoch 1039: 0.340\n",
      "test accuracy on epoch 1039: 0.769\n",
      "train loss on epoch 1040 : 0.136\n",
      "train accuracy on epoch 1040: 0.944\n",
      "test loss on epoch 1040: 0.342\n",
      "test accuracy on epoch 1040: 0.769\n",
      "train loss on epoch 1041 : 0.342\n",
      "train accuracy on epoch 1041: 0.889\n",
      "test loss on epoch 1041: 0.338\n",
      "test accuracy on epoch 1041: 0.769\n",
      "train loss on epoch 1042 : 0.048\n",
      "train accuracy on epoch 1042: 1.000\n",
      "test loss on epoch 1042: 0.342\n",
      "test accuracy on epoch 1042: 0.769\n",
      "train loss on epoch 1043 : 0.191\n",
      "train accuracy on epoch 1043: 0.944\n",
      "test loss on epoch 1043: 0.342\n",
      "test accuracy on epoch 1043: 0.769\n",
      "train loss on epoch 1044 : 0.155\n",
      "train accuracy on epoch 1044: 0.889\n",
      "test loss on epoch 1044: 0.346\n",
      "test accuracy on epoch 1044: 0.769\n",
      "train loss on epoch 1045 : 0.055\n",
      "train accuracy on epoch 1045: 1.000\n",
      "test loss on epoch 1045: 0.350\n",
      "test accuracy on epoch 1045: 0.769\n",
      "train loss on epoch 1046 : 0.167\n",
      "train accuracy on epoch 1046: 0.889\n",
      "test loss on epoch 1046: 0.352\n",
      "test accuracy on epoch 1046: 0.769\n",
      "train loss on epoch 1047 : 0.078\n",
      "train accuracy on epoch 1047: 1.000\n",
      "test loss on epoch 1047: 0.359\n",
      "test accuracy on epoch 1047: 0.769\n",
      "train loss on epoch 1048 : 0.182\n",
      "train accuracy on epoch 1048: 0.944\n",
      "test loss on epoch 1048: 0.357\n",
      "test accuracy on epoch 1048: 0.769\n",
      "train loss on epoch 1049 : 0.089\n",
      "train accuracy on epoch 1049: 1.000\n",
      "test loss on epoch 1049: 0.340\n",
      "test accuracy on epoch 1049: 0.769\n",
      "train loss on epoch 1050 : 0.209\n",
      "train accuracy on epoch 1050: 0.889\n",
      "test loss on epoch 1050: 0.340\n",
      "test accuracy on epoch 1050: 0.769\n",
      "train loss on epoch 1051 : 0.299\n",
      "train accuracy on epoch 1051: 0.944\n",
      "test loss on epoch 1051: 0.336\n",
      "test accuracy on epoch 1051: 0.769\n",
      "train loss on epoch 1052 : 0.160\n",
      "train accuracy on epoch 1052: 0.944\n",
      "test loss on epoch 1052: 0.333\n",
      "test accuracy on epoch 1052: 0.769\n",
      "train loss on epoch 1053 : 0.154\n",
      "train accuracy on epoch 1053: 0.944\n",
      "test loss on epoch 1053: 0.330\n",
      "test accuracy on epoch 1053: 0.769\n",
      "train loss on epoch 1054 : 0.101\n",
      "train accuracy on epoch 1054: 1.000\n",
      "test loss on epoch 1054: 0.333\n",
      "test accuracy on epoch 1054: 0.769\n",
      "train loss on epoch 1055 : 0.095\n",
      "train accuracy on epoch 1055: 1.000\n",
      "test loss on epoch 1055: 0.335\n",
      "test accuracy on epoch 1055: 0.769\n",
      "train loss on epoch 1056 : 0.309\n",
      "train accuracy on epoch 1056: 0.889\n",
      "test loss on epoch 1056: 0.335\n",
      "test accuracy on epoch 1056: 0.769\n",
      "train loss on epoch 1057 : 0.198\n",
      "train accuracy on epoch 1057: 0.944\n",
      "test loss on epoch 1057: 0.339\n",
      "test accuracy on epoch 1057: 0.769\n",
      "train loss on epoch 1058 : 0.171\n",
      "train accuracy on epoch 1058: 0.889\n",
      "test loss on epoch 1058: 0.340\n",
      "test accuracy on epoch 1058: 0.769\n",
      "train loss on epoch 1059 : 0.073\n",
      "train accuracy on epoch 1059: 1.000\n",
      "test loss on epoch 1059: 0.340\n",
      "test accuracy on epoch 1059: 0.769\n",
      "train loss on epoch 1060 : 0.039\n",
      "train accuracy on epoch 1060: 1.000\n",
      "test loss on epoch 1060: 0.335\n",
      "test accuracy on epoch 1060: 0.769\n",
      "train loss on epoch 1061 : 0.201\n",
      "train accuracy on epoch 1061: 0.889\n",
      "test loss on epoch 1061: 0.333\n",
      "test accuracy on epoch 1061: 0.769\n",
      "train loss on epoch 1062 : 0.070\n",
      "train accuracy on epoch 1062: 1.000\n",
      "test loss on epoch 1062: 0.334\n",
      "test accuracy on epoch 1062: 0.769\n",
      "train loss on epoch 1063 : 0.291\n",
      "train accuracy on epoch 1063: 0.833\n",
      "test loss on epoch 1063: 0.333\n",
      "test accuracy on epoch 1063: 0.769\n",
      "train loss on epoch 1064 : 0.074\n",
      "train accuracy on epoch 1064: 0.944\n",
      "test loss on epoch 1064: 0.333\n",
      "test accuracy on epoch 1064: 0.769\n",
      "train loss on epoch 1065 : 0.463\n",
      "train accuracy on epoch 1065: 0.889\n",
      "test loss on epoch 1065: 0.333\n",
      "test accuracy on epoch 1065: 0.769\n",
      "train loss on epoch 1066 : 0.166\n",
      "train accuracy on epoch 1066: 0.944\n",
      "test loss on epoch 1066: 0.335\n",
      "test accuracy on epoch 1066: 0.769\n",
      "train loss on epoch 1067 : 0.124\n",
      "train accuracy on epoch 1067: 0.944\n",
      "test loss on epoch 1067: 0.332\n",
      "test accuracy on epoch 1067: 0.769\n",
      "train loss on epoch 1068 : 0.099\n",
      "train accuracy on epoch 1068: 0.944\n",
      "test loss on epoch 1068: 0.332\n",
      "test accuracy on epoch 1068: 0.769\n",
      "train loss on epoch 1069 : 0.322\n",
      "train accuracy on epoch 1069: 0.944\n",
      "test loss on epoch 1069: 0.334\n",
      "test accuracy on epoch 1069: 0.769\n",
      "train loss on epoch 1070 : 0.436\n",
      "train accuracy on epoch 1070: 0.833\n",
      "test loss on epoch 1070: 0.335\n",
      "test accuracy on epoch 1070: 0.769\n",
      "train loss on epoch 1071 : 0.060\n",
      "train accuracy on epoch 1071: 1.000\n",
      "test loss on epoch 1071: 0.338\n",
      "test accuracy on epoch 1071: 0.769\n",
      "train loss on epoch 1072 : 0.031\n",
      "train accuracy on epoch 1072: 1.000\n",
      "test loss on epoch 1072: 0.340\n",
      "test accuracy on epoch 1072: 0.769\n",
      "train loss on epoch 1073 : 0.104\n",
      "train accuracy on epoch 1073: 0.944\n",
      "test loss on epoch 1073: 0.341\n",
      "test accuracy on epoch 1073: 0.769\n",
      "train loss on epoch 1074 : 0.179\n",
      "train accuracy on epoch 1074: 0.889\n",
      "test loss on epoch 1074: 0.343\n",
      "test accuracy on epoch 1074: 0.769\n",
      "train loss on epoch 1075 : 0.092\n",
      "train accuracy on epoch 1075: 1.000\n",
      "test loss on epoch 1075: 0.342\n",
      "test accuracy on epoch 1075: 0.769\n",
      "train loss on epoch 1076 : 0.147\n",
      "train accuracy on epoch 1076: 0.889\n",
      "test loss on epoch 1076: 0.338\n",
      "test accuracy on epoch 1076: 0.769\n",
      "train loss on epoch 1077 : 0.121\n",
      "train accuracy on epoch 1077: 0.944\n",
      "test loss on epoch 1077: 0.336\n",
      "test accuracy on epoch 1077: 0.769\n",
      "train loss on epoch 1078 : 0.108\n",
      "train accuracy on epoch 1078: 0.944\n",
      "test loss on epoch 1078: 0.338\n",
      "test accuracy on epoch 1078: 0.769\n",
      "train loss on epoch 1079 : 0.074\n",
      "train accuracy on epoch 1079: 1.000\n",
      "test loss on epoch 1079: 0.337\n",
      "test accuracy on epoch 1079: 0.769\n",
      "train loss on epoch 1080 : 0.153\n",
      "train accuracy on epoch 1080: 0.889\n",
      "test loss on epoch 1080: 0.331\n",
      "test accuracy on epoch 1080: 0.769\n",
      "train loss on epoch 1081 : 0.070\n",
      "train accuracy on epoch 1081: 1.000\n",
      "test loss on epoch 1081: 0.330\n",
      "test accuracy on epoch 1081: 0.769\n",
      "train loss on epoch 1082 : 0.200\n",
      "train accuracy on epoch 1082: 0.889\n",
      "test loss on epoch 1082: 0.333\n",
      "test accuracy on epoch 1082: 0.692\n",
      "train loss on epoch 1083 : 0.046\n",
      "train accuracy on epoch 1083: 1.000\n",
      "test loss on epoch 1083: 0.329\n",
      "test accuracy on epoch 1083: 0.769\n",
      "train loss on epoch 1084 : 0.197\n",
      "train accuracy on epoch 1084: 0.889\n",
      "test loss on epoch 1084: 0.330\n",
      "test accuracy on epoch 1084: 0.769\n",
      "train loss on epoch 1085 : 0.179\n",
      "train accuracy on epoch 1085: 0.889\n",
      "test loss on epoch 1085: 0.328\n",
      "test accuracy on epoch 1085: 0.769\n",
      "train loss on epoch 1086 : 0.102\n",
      "train accuracy on epoch 1086: 0.944\n",
      "test loss on epoch 1086: 0.331\n",
      "test accuracy on epoch 1086: 0.769\n",
      "train loss on epoch 1087 : 0.126\n",
      "train accuracy on epoch 1087: 0.944\n",
      "test loss on epoch 1087: 0.340\n",
      "test accuracy on epoch 1087: 0.769\n",
      "train loss on epoch 1088 : 0.199\n",
      "train accuracy on epoch 1088: 0.889\n",
      "test loss on epoch 1088: 0.336\n",
      "test accuracy on epoch 1088: 0.769\n",
      "train loss on epoch 1089 : 0.254\n",
      "train accuracy on epoch 1089: 0.889\n",
      "test loss on epoch 1089: 0.338\n",
      "test accuracy on epoch 1089: 0.769\n",
      "train loss on epoch 1090 : 0.239\n",
      "train accuracy on epoch 1090: 0.889\n",
      "test loss on epoch 1090: 0.336\n",
      "test accuracy on epoch 1090: 0.769\n",
      "train loss on epoch 1091 : 0.186\n",
      "train accuracy on epoch 1091: 0.889\n",
      "test loss on epoch 1091: 0.344\n",
      "test accuracy on epoch 1091: 0.769\n",
      "train loss on epoch 1092 : 0.382\n",
      "train accuracy on epoch 1092: 0.889\n",
      "test loss on epoch 1092: 0.343\n",
      "test accuracy on epoch 1092: 0.769\n",
      "train loss on epoch 1093 : 0.167\n",
      "train accuracy on epoch 1093: 0.889\n",
      "test loss on epoch 1093: 0.341\n",
      "test accuracy on epoch 1093: 0.769\n",
      "train loss on epoch 1094 : 0.225\n",
      "train accuracy on epoch 1094: 0.889\n",
      "test loss on epoch 1094: 0.349\n",
      "test accuracy on epoch 1094: 0.769\n",
      "train loss on epoch 1095 : 0.109\n",
      "train accuracy on epoch 1095: 0.944\n",
      "test loss on epoch 1095: 0.345\n",
      "test accuracy on epoch 1095: 0.769\n",
      "train loss on epoch 1096 : 0.322\n",
      "train accuracy on epoch 1096: 0.944\n",
      "test loss on epoch 1096: 0.340\n",
      "test accuracy on epoch 1096: 0.769\n",
      "train loss on epoch 1097 : 0.221\n",
      "train accuracy on epoch 1097: 0.944\n",
      "test loss on epoch 1097: 0.342\n",
      "test accuracy on epoch 1097: 0.769\n",
      "train loss on epoch 1098 : 0.060\n",
      "train accuracy on epoch 1098: 1.000\n",
      "test loss on epoch 1098: 0.344\n",
      "test accuracy on epoch 1098: 0.769\n",
      "train loss on epoch 1099 : 0.148\n",
      "train accuracy on epoch 1099: 0.944\n",
      "test loss on epoch 1099: 0.342\n",
      "test accuracy on epoch 1099: 0.769\n",
      "train loss on epoch 1100 : 0.222\n",
      "train accuracy on epoch 1100: 0.889\n",
      "test loss on epoch 1100: 0.345\n",
      "test accuracy on epoch 1100: 0.769\n",
      "train loss on epoch 1101 : 0.176\n",
      "train accuracy on epoch 1101: 0.944\n",
      "test loss on epoch 1101: 0.341\n",
      "test accuracy on epoch 1101: 0.769\n",
      "train loss on epoch 1102 : 0.064\n",
      "train accuracy on epoch 1102: 1.000\n",
      "test loss on epoch 1102: 0.341\n",
      "test accuracy on epoch 1102: 0.769\n",
      "train loss on epoch 1103 : 0.090\n",
      "train accuracy on epoch 1103: 1.000\n",
      "test loss on epoch 1103: 0.337\n",
      "test accuracy on epoch 1103: 0.769\n",
      "train loss on epoch 1104 : 0.085\n",
      "train accuracy on epoch 1104: 1.000\n",
      "test loss on epoch 1104: 0.341\n",
      "test accuracy on epoch 1104: 0.769\n",
      "train loss on epoch 1105 : 0.104\n",
      "train accuracy on epoch 1105: 0.944\n",
      "test loss on epoch 1105: 0.334\n",
      "test accuracy on epoch 1105: 0.769\n",
      "train loss on epoch 1106 : 0.102\n",
      "train accuracy on epoch 1106: 0.944\n",
      "test loss on epoch 1106: 0.331\n",
      "test accuracy on epoch 1106: 0.769\n",
      "train loss on epoch 1107 : 0.243\n",
      "train accuracy on epoch 1107: 0.833\n",
      "test loss on epoch 1107: 0.337\n",
      "test accuracy on epoch 1107: 0.769\n",
      "train loss on epoch 1108 : 0.118\n",
      "train accuracy on epoch 1108: 0.944\n",
      "test loss on epoch 1108: 0.338\n",
      "test accuracy on epoch 1108: 0.769\n",
      "train loss on epoch 1109 : 0.247\n",
      "train accuracy on epoch 1109: 0.944\n",
      "test loss on epoch 1109: 0.335\n",
      "test accuracy on epoch 1109: 0.769\n",
      "train loss on epoch 1110 : 0.503\n",
      "train accuracy on epoch 1110: 0.889\n",
      "test loss on epoch 1110: 0.336\n",
      "test accuracy on epoch 1110: 0.769\n",
      "train loss on epoch 1111 : 0.120\n",
      "train accuracy on epoch 1111: 1.000\n",
      "test loss on epoch 1111: 0.339\n",
      "test accuracy on epoch 1111: 0.769\n",
      "train loss on epoch 1112 : 0.278\n",
      "train accuracy on epoch 1112: 0.889\n",
      "test loss on epoch 1112: 0.341\n",
      "test accuracy on epoch 1112: 0.769\n",
      "train loss on epoch 1113 : 0.437\n",
      "train accuracy on epoch 1113: 0.833\n",
      "test loss on epoch 1113: 0.355\n",
      "test accuracy on epoch 1113: 0.769\n",
      "train loss on epoch 1114 : 0.139\n",
      "train accuracy on epoch 1114: 0.944\n",
      "test loss on epoch 1114: 0.361\n",
      "test accuracy on epoch 1114: 0.769\n",
      "train loss on epoch 1115 : 0.148\n",
      "train accuracy on epoch 1115: 0.889\n",
      "test loss on epoch 1115: 0.363\n",
      "test accuracy on epoch 1115: 0.769\n",
      "train loss on epoch 1116 : 0.247\n",
      "train accuracy on epoch 1116: 0.889\n",
      "test loss on epoch 1116: 0.360\n",
      "test accuracy on epoch 1116: 0.769\n",
      "train loss on epoch 1117 : 0.312\n",
      "train accuracy on epoch 1117: 0.889\n",
      "test loss on epoch 1117: 0.357\n",
      "test accuracy on epoch 1117: 0.769\n",
      "train loss on epoch 1118 : 0.116\n",
      "train accuracy on epoch 1118: 0.944\n",
      "test loss on epoch 1118: 0.348\n",
      "test accuracy on epoch 1118: 0.769\n",
      "train loss on epoch 1119 : 0.194\n",
      "train accuracy on epoch 1119: 0.889\n",
      "test loss on epoch 1119: 0.350\n",
      "test accuracy on epoch 1119: 0.769\n",
      "train loss on epoch 1120 : 0.023\n",
      "train accuracy on epoch 1120: 1.000\n",
      "test loss on epoch 1120: 0.347\n",
      "test accuracy on epoch 1120: 0.769\n",
      "train loss on epoch 1121 : 0.285\n",
      "train accuracy on epoch 1121: 0.944\n",
      "test loss on epoch 1121: 0.349\n",
      "test accuracy on epoch 1121: 0.769\n",
      "train loss on epoch 1122 : 0.156\n",
      "train accuracy on epoch 1122: 0.944\n",
      "test loss on epoch 1122: 0.347\n",
      "test accuracy on epoch 1122: 0.769\n",
      "train loss on epoch 1123 : 0.254\n",
      "train accuracy on epoch 1123: 0.889\n",
      "test loss on epoch 1123: 0.342\n",
      "test accuracy on epoch 1123: 0.769\n",
      "train loss on epoch 1124 : 0.094\n",
      "train accuracy on epoch 1124: 1.000\n",
      "test loss on epoch 1124: 0.344\n",
      "test accuracy on epoch 1124: 0.769\n",
      "train loss on epoch 1125 : 0.175\n",
      "train accuracy on epoch 1125: 0.889\n",
      "test loss on epoch 1125: 0.338\n",
      "test accuracy on epoch 1125: 0.769\n",
      "train loss on epoch 1126 : 0.345\n",
      "train accuracy on epoch 1126: 0.889\n",
      "test loss on epoch 1126: 0.339\n",
      "test accuracy on epoch 1126: 0.769\n",
      "train loss on epoch 1127 : 0.155\n",
      "train accuracy on epoch 1127: 0.944\n",
      "test loss on epoch 1127: 0.342\n",
      "test accuracy on epoch 1127: 0.769\n",
      "train loss on epoch 1128 : 0.137\n",
      "train accuracy on epoch 1128: 0.944\n",
      "test loss on epoch 1128: 0.344\n",
      "test accuracy on epoch 1128: 0.769\n",
      "train loss on epoch 1129 : 0.151\n",
      "train accuracy on epoch 1129: 0.944\n",
      "test loss on epoch 1129: 0.338\n",
      "test accuracy on epoch 1129: 0.769\n",
      "train loss on epoch 1130 : 0.130\n",
      "train accuracy on epoch 1130: 0.944\n",
      "test loss on epoch 1130: 0.338\n",
      "test accuracy on epoch 1130: 0.769\n",
      "train loss on epoch 1131 : 0.070\n",
      "train accuracy on epoch 1131: 0.944\n",
      "test loss on epoch 1131: 0.340\n",
      "test accuracy on epoch 1131: 0.769\n",
      "train loss on epoch 1132 : 0.072\n",
      "train accuracy on epoch 1132: 1.000\n",
      "test loss on epoch 1132: 0.337\n",
      "test accuracy on epoch 1132: 0.769\n",
      "train loss on epoch 1133 : 0.082\n",
      "train accuracy on epoch 1133: 1.000\n",
      "test loss on epoch 1133: 0.340\n",
      "test accuracy on epoch 1133: 0.769\n",
      "train loss on epoch 1134 : 0.245\n",
      "train accuracy on epoch 1134: 0.889\n",
      "test loss on epoch 1134: 0.343\n",
      "test accuracy on epoch 1134: 0.769\n",
      "train loss on epoch 1135 : 0.287\n",
      "train accuracy on epoch 1135: 0.889\n",
      "test loss on epoch 1135: 0.343\n",
      "test accuracy on epoch 1135: 0.769\n",
      "train loss on epoch 1136 : 0.175\n",
      "train accuracy on epoch 1136: 0.889\n",
      "test loss on epoch 1136: 0.344\n",
      "test accuracy on epoch 1136: 0.769\n",
      "train loss on epoch 1137 : 0.207\n",
      "train accuracy on epoch 1137: 0.944\n",
      "test loss on epoch 1137: 0.355\n",
      "test accuracy on epoch 1137: 0.769\n",
      "train loss on epoch 1138 : 0.105\n",
      "train accuracy on epoch 1138: 0.944\n",
      "test loss on epoch 1138: 0.357\n",
      "test accuracy on epoch 1138: 0.769\n",
      "train loss on epoch 1139 : 0.217\n",
      "train accuracy on epoch 1139: 0.944\n",
      "test loss on epoch 1139: 0.365\n",
      "test accuracy on epoch 1139: 0.769\n",
      "train loss on epoch 1140 : 0.132\n",
      "train accuracy on epoch 1140: 0.944\n",
      "test loss on epoch 1140: 0.367\n",
      "test accuracy on epoch 1140: 0.769\n",
      "train loss on epoch 1141 : 0.284\n",
      "train accuracy on epoch 1141: 0.944\n",
      "test loss on epoch 1141: 0.360\n",
      "test accuracy on epoch 1141: 0.769\n",
      "train loss on epoch 1142 : 0.065\n",
      "train accuracy on epoch 1142: 1.000\n",
      "test loss on epoch 1142: 0.356\n",
      "test accuracy on epoch 1142: 0.769\n",
      "train loss on epoch 1143 : 0.423\n",
      "train accuracy on epoch 1143: 0.833\n",
      "test loss on epoch 1143: 0.345\n",
      "test accuracy on epoch 1143: 0.769\n",
      "train loss on epoch 1144 : 0.101\n",
      "train accuracy on epoch 1144: 0.944\n",
      "test loss on epoch 1144: 0.344\n",
      "test accuracy on epoch 1144: 0.769\n",
      "train loss on epoch 1145 : 0.055\n",
      "train accuracy on epoch 1145: 1.000\n",
      "test loss on epoch 1145: 0.344\n",
      "test accuracy on epoch 1145: 0.769\n",
      "train loss on epoch 1146 : 0.061\n",
      "train accuracy on epoch 1146: 1.000\n",
      "test loss on epoch 1146: 0.340\n",
      "test accuracy on epoch 1146: 0.769\n",
      "train loss on epoch 1147 : 0.237\n",
      "train accuracy on epoch 1147: 0.944\n",
      "test loss on epoch 1147: 0.336\n",
      "test accuracy on epoch 1147: 0.769\n",
      "train loss on epoch 1148 : 0.063\n",
      "train accuracy on epoch 1148: 0.944\n",
      "test loss on epoch 1148: 0.341\n",
      "test accuracy on epoch 1148: 0.769\n",
      "train loss on epoch 1149 : 0.305\n",
      "train accuracy on epoch 1149: 0.944\n",
      "test loss on epoch 1149: 0.344\n",
      "test accuracy on epoch 1149: 0.769\n",
      "train loss on epoch 1150 : 0.500\n",
      "train accuracy on epoch 1150: 0.833\n",
      "test loss on epoch 1150: 0.340\n",
      "test accuracy on epoch 1150: 0.769\n",
      "train loss on epoch 1151 : 0.256\n",
      "train accuracy on epoch 1151: 0.833\n",
      "test loss on epoch 1151: 0.340\n",
      "test accuracy on epoch 1151: 0.769\n",
      "train loss on epoch 1152 : 0.110\n",
      "train accuracy on epoch 1152: 0.944\n",
      "test loss on epoch 1152: 0.341\n",
      "test accuracy on epoch 1152: 0.769\n",
      "train loss on epoch 1153 : 0.285\n",
      "train accuracy on epoch 1153: 0.889\n",
      "test loss on epoch 1153: 0.340\n",
      "test accuracy on epoch 1153: 0.769\n",
      "train loss on epoch 1154 : 0.284\n",
      "train accuracy on epoch 1154: 0.889\n",
      "test loss on epoch 1154: 0.347\n",
      "test accuracy on epoch 1154: 0.769\n",
      "train loss on epoch 1155 : 0.212\n",
      "train accuracy on epoch 1155: 0.944\n",
      "test loss on epoch 1155: 0.352\n",
      "test accuracy on epoch 1155: 0.769\n",
      "train loss on epoch 1156 : 0.069\n",
      "train accuracy on epoch 1156: 1.000\n",
      "test loss on epoch 1156: 0.358\n",
      "test accuracy on epoch 1156: 0.769\n",
      "train loss on epoch 1157 : 0.094\n",
      "train accuracy on epoch 1157: 0.944\n",
      "test loss on epoch 1157: 0.361\n",
      "test accuracy on epoch 1157: 0.769\n",
      "train loss on epoch 1158 : 0.064\n",
      "train accuracy on epoch 1158: 0.944\n",
      "test loss on epoch 1158: 0.364\n",
      "test accuracy on epoch 1158: 0.769\n",
      "train loss on epoch 1159 : 0.141\n",
      "train accuracy on epoch 1159: 0.944\n",
      "test loss on epoch 1159: 0.368\n",
      "test accuracy on epoch 1159: 0.769\n",
      "train loss on epoch 1160 : 0.043\n",
      "train accuracy on epoch 1160: 1.000\n",
      "test loss on epoch 1160: 0.368\n",
      "test accuracy on epoch 1160: 0.769\n",
      "train loss on epoch 1161 : 0.087\n",
      "train accuracy on epoch 1161: 0.944\n",
      "test loss on epoch 1161: 0.380\n",
      "test accuracy on epoch 1161: 0.769\n",
      "train loss on epoch 1162 : 0.163\n",
      "train accuracy on epoch 1162: 0.889\n",
      "test loss on epoch 1162: 0.380\n",
      "test accuracy on epoch 1162: 0.769\n",
      "train loss on epoch 1163 : 0.164\n",
      "train accuracy on epoch 1163: 0.944\n",
      "test loss on epoch 1163: 0.386\n",
      "test accuracy on epoch 1163: 0.769\n",
      "train loss on epoch 1164 : 0.158\n",
      "train accuracy on epoch 1164: 0.889\n",
      "test loss on epoch 1164: 0.376\n",
      "test accuracy on epoch 1164: 0.769\n",
      "train loss on epoch 1165 : 0.215\n",
      "train accuracy on epoch 1165: 0.944\n",
      "test loss on epoch 1165: 0.370\n",
      "test accuracy on epoch 1165: 0.769\n",
      "train loss on epoch 1166 : 0.074\n",
      "train accuracy on epoch 1166: 0.944\n",
      "test loss on epoch 1166: 0.356\n",
      "test accuracy on epoch 1166: 0.769\n",
      "train loss on epoch 1167 : 0.159\n",
      "train accuracy on epoch 1167: 0.889\n",
      "test loss on epoch 1167: 0.355\n",
      "test accuracy on epoch 1167: 0.769\n",
      "train loss on epoch 1168 : 0.328\n",
      "train accuracy on epoch 1168: 0.889\n",
      "test loss on epoch 1168: 0.357\n",
      "test accuracy on epoch 1168: 0.769\n",
      "train loss on epoch 1169 : 0.055\n",
      "train accuracy on epoch 1169: 1.000\n",
      "test loss on epoch 1169: 0.355\n",
      "test accuracy on epoch 1169: 0.769\n",
      "train loss on epoch 1170 : 0.098\n",
      "train accuracy on epoch 1170: 0.944\n",
      "test loss on epoch 1170: 0.350\n",
      "test accuracy on epoch 1170: 0.769\n",
      "train loss on epoch 1171 : 0.170\n",
      "train accuracy on epoch 1171: 0.944\n",
      "test loss on epoch 1171: 0.353\n",
      "test accuracy on epoch 1171: 0.769\n",
      "train loss on epoch 1172 : 0.299\n",
      "train accuracy on epoch 1172: 0.889\n",
      "test loss on epoch 1172: 0.345\n",
      "test accuracy on epoch 1172: 0.769\n",
      "train loss on epoch 1173 : 0.050\n",
      "train accuracy on epoch 1173: 1.000\n",
      "test loss on epoch 1173: 0.343\n",
      "test accuracy on epoch 1173: 0.769\n",
      "train loss on epoch 1174 : 0.117\n",
      "train accuracy on epoch 1174: 0.944\n",
      "test loss on epoch 1174: 0.343\n",
      "test accuracy on epoch 1174: 0.769\n",
      "train loss on epoch 1175 : 0.079\n",
      "train accuracy on epoch 1175: 1.000\n",
      "test loss on epoch 1175: 0.343\n",
      "test accuracy on epoch 1175: 0.769\n",
      "train loss on epoch 1176 : 0.083\n",
      "train accuracy on epoch 1176: 0.944\n",
      "test loss on epoch 1176: 0.338\n",
      "test accuracy on epoch 1176: 0.769\n",
      "train loss on epoch 1177 : 0.045\n",
      "train accuracy on epoch 1177: 1.000\n",
      "test loss on epoch 1177: 0.335\n",
      "test accuracy on epoch 1177: 0.769\n",
      "train loss on epoch 1178 : 0.161\n",
      "train accuracy on epoch 1178: 0.944\n",
      "test loss on epoch 1178: 0.336\n",
      "test accuracy on epoch 1178: 0.769\n",
      "train loss on epoch 1179 : 0.177\n",
      "train accuracy on epoch 1179: 0.889\n",
      "test loss on epoch 1179: 0.335\n",
      "test accuracy on epoch 1179: 0.769\n",
      "train loss on epoch 1180 : 0.231\n",
      "train accuracy on epoch 1180: 0.944\n",
      "test loss on epoch 1180: 0.339\n",
      "test accuracy on epoch 1180: 0.769\n",
      "train loss on epoch 1181 : 0.080\n",
      "train accuracy on epoch 1181: 0.944\n",
      "test loss on epoch 1181: 0.346\n",
      "test accuracy on epoch 1181: 0.769\n",
      "train loss on epoch 1182 : 0.170\n",
      "train accuracy on epoch 1182: 0.889\n",
      "test loss on epoch 1182: 0.345\n",
      "test accuracy on epoch 1182: 0.769\n",
      "train loss on epoch 1183 : 0.032\n",
      "train accuracy on epoch 1183: 1.000\n",
      "test loss on epoch 1183: 0.340\n",
      "test accuracy on epoch 1183: 0.769\n",
      "train loss on epoch 1184 : 0.088\n",
      "train accuracy on epoch 1184: 1.000\n",
      "test loss on epoch 1184: 0.334\n",
      "test accuracy on epoch 1184: 0.769\n",
      "train loss on epoch 1185 : 0.062\n",
      "train accuracy on epoch 1185: 0.944\n",
      "test loss on epoch 1185: 0.332\n",
      "test accuracy on epoch 1185: 0.769\n",
      "train loss on epoch 1186 : 0.015\n",
      "train accuracy on epoch 1186: 1.000\n",
      "test loss on epoch 1186: 0.334\n",
      "test accuracy on epoch 1186: 0.769\n",
      "train loss on epoch 1187 : 0.150\n",
      "train accuracy on epoch 1187: 0.944\n",
      "test loss on epoch 1187: 0.340\n",
      "test accuracy on epoch 1187: 0.769\n",
      "train loss on epoch 1188 : 0.062\n",
      "train accuracy on epoch 1188: 1.000\n",
      "test loss on epoch 1188: 0.349\n",
      "test accuracy on epoch 1188: 0.769\n",
      "train loss on epoch 1189 : 0.035\n",
      "train accuracy on epoch 1189: 1.000\n",
      "test loss on epoch 1189: 0.351\n",
      "test accuracy on epoch 1189: 0.769\n",
      "train loss on epoch 1190 : 0.224\n",
      "train accuracy on epoch 1190: 0.889\n",
      "test loss on epoch 1190: 0.345\n",
      "test accuracy on epoch 1190: 0.769\n",
      "train loss on epoch 1191 : 0.165\n",
      "train accuracy on epoch 1191: 0.944\n",
      "test loss on epoch 1191: 0.343\n",
      "test accuracy on epoch 1191: 0.769\n",
      "train loss on epoch 1192 : 0.193\n",
      "train accuracy on epoch 1192: 0.944\n",
      "test loss on epoch 1192: 0.347\n",
      "test accuracy on epoch 1192: 0.769\n",
      "train loss on epoch 1193 : 0.248\n",
      "train accuracy on epoch 1193: 0.889\n",
      "test loss on epoch 1193: 0.346\n",
      "test accuracy on epoch 1193: 0.769\n",
      "train loss on epoch 1194 : 0.146\n",
      "train accuracy on epoch 1194: 0.944\n",
      "test loss on epoch 1194: 0.348\n",
      "test accuracy on epoch 1194: 0.769\n",
      "train loss on epoch 1195 : 0.200\n",
      "train accuracy on epoch 1195: 0.944\n",
      "test loss on epoch 1195: 0.350\n",
      "test accuracy on epoch 1195: 0.769\n",
      "train loss on epoch 1196 : 0.147\n",
      "train accuracy on epoch 1196: 0.944\n",
      "test loss on epoch 1196: 0.355\n",
      "test accuracy on epoch 1196: 0.769\n",
      "train loss on epoch 1197 : 0.268\n",
      "train accuracy on epoch 1197: 0.944\n",
      "test loss on epoch 1197: 0.361\n",
      "test accuracy on epoch 1197: 0.769\n",
      "train loss on epoch 1198 : 0.071\n",
      "train accuracy on epoch 1198: 0.944\n",
      "test loss on epoch 1198: 0.360\n",
      "test accuracy on epoch 1198: 0.769\n",
      "train loss on epoch 1199 : 0.187\n",
      "train accuracy on epoch 1199: 0.889\n",
      "test loss on epoch 1199: 0.356\n",
      "test accuracy on epoch 1199: 0.769\n",
      "train loss on epoch 1200 : 0.334\n",
      "train accuracy on epoch 1200: 0.889\n",
      "test loss on epoch 1200: 0.364\n",
      "test accuracy on epoch 1200: 0.769\n",
      "train loss on epoch 1201 : 0.094\n",
      "train accuracy on epoch 1201: 1.000\n",
      "test loss on epoch 1201: 0.370\n",
      "test accuracy on epoch 1201: 0.769\n",
      "train loss on epoch 1202 : 0.169\n",
      "train accuracy on epoch 1202: 0.889\n",
      "test loss on epoch 1202: 0.360\n",
      "test accuracy on epoch 1202: 0.769\n",
      "train loss on epoch 1203 : 0.159\n",
      "train accuracy on epoch 1203: 0.944\n",
      "test loss on epoch 1203: 0.352\n",
      "test accuracy on epoch 1203: 0.769\n",
      "train loss on epoch 1204 : 0.037\n",
      "train accuracy on epoch 1204: 1.000\n",
      "test loss on epoch 1204: 0.346\n",
      "test accuracy on epoch 1204: 0.769\n",
      "train loss on epoch 1205 : 0.152\n",
      "train accuracy on epoch 1205: 0.889\n",
      "test loss on epoch 1205: 0.343\n",
      "test accuracy on epoch 1205: 0.769\n",
      "train loss on epoch 1206 : 0.220\n",
      "train accuracy on epoch 1206: 0.944\n",
      "test loss on epoch 1206: 0.345\n",
      "test accuracy on epoch 1206: 0.769\n",
      "train loss on epoch 1207 : 0.126\n",
      "train accuracy on epoch 1207: 0.944\n",
      "test loss on epoch 1207: 0.336\n",
      "test accuracy on epoch 1207: 0.769\n",
      "train loss on epoch 1208 : 0.217\n",
      "train accuracy on epoch 1208: 0.944\n",
      "test loss on epoch 1208: 0.340\n",
      "test accuracy on epoch 1208: 0.769\n",
      "train loss on epoch 1209 : 0.149\n",
      "train accuracy on epoch 1209: 0.944\n",
      "test loss on epoch 1209: 0.342\n",
      "test accuracy on epoch 1209: 0.769\n",
      "train loss on epoch 1210 : 0.141\n",
      "train accuracy on epoch 1210: 0.944\n",
      "test loss on epoch 1210: 0.350\n",
      "test accuracy on epoch 1210: 0.769\n",
      "train loss on epoch 1211 : 0.150\n",
      "train accuracy on epoch 1211: 0.944\n",
      "test loss on epoch 1211: 0.358\n",
      "test accuracy on epoch 1211: 0.769\n",
      "train loss on epoch 1212 : 0.061\n",
      "train accuracy on epoch 1212: 0.944\n",
      "test loss on epoch 1212: 0.366\n",
      "test accuracy on epoch 1212: 0.769\n",
      "train loss on epoch 1213 : 0.068\n",
      "train accuracy on epoch 1213: 1.000\n",
      "test loss on epoch 1213: 0.366\n",
      "test accuracy on epoch 1213: 0.769\n",
      "train loss on epoch 1214 : 0.153\n",
      "train accuracy on epoch 1214: 0.889\n",
      "test loss on epoch 1214: 0.370\n",
      "test accuracy on epoch 1214: 0.769\n",
      "train loss on epoch 1215 : 0.150\n",
      "train accuracy on epoch 1215: 0.944\n",
      "test loss on epoch 1215: 0.375\n",
      "test accuracy on epoch 1215: 0.769\n",
      "train loss on epoch 1216 : 0.162\n",
      "train accuracy on epoch 1216: 0.944\n",
      "test loss on epoch 1216: 0.372\n",
      "test accuracy on epoch 1216: 0.769\n",
      "train loss on epoch 1217 : 0.278\n",
      "train accuracy on epoch 1217: 0.889\n",
      "test loss on epoch 1217: 0.369\n",
      "test accuracy on epoch 1217: 0.769\n",
      "train loss on epoch 1218 : 0.213\n",
      "train accuracy on epoch 1218: 0.889\n",
      "test loss on epoch 1218: 0.368\n",
      "test accuracy on epoch 1218: 0.769\n",
      "train loss on epoch 1219 : 0.145\n",
      "train accuracy on epoch 1219: 0.889\n",
      "test loss on epoch 1219: 0.370\n",
      "test accuracy on epoch 1219: 0.769\n",
      "train loss on epoch 1220 : 0.119\n",
      "train accuracy on epoch 1220: 0.944\n",
      "test loss on epoch 1220: 0.373\n",
      "test accuracy on epoch 1220: 0.769\n",
      "train loss on epoch 1221 : 0.163\n",
      "train accuracy on epoch 1221: 0.944\n",
      "test loss on epoch 1221: 0.377\n",
      "test accuracy on epoch 1221: 0.769\n",
      "train loss on epoch 1222 : 0.040\n",
      "train accuracy on epoch 1222: 1.000\n",
      "test loss on epoch 1222: 0.377\n",
      "test accuracy on epoch 1222: 0.769\n",
      "train loss on epoch 1223 : 0.091\n",
      "train accuracy on epoch 1223: 0.944\n",
      "test loss on epoch 1223: 0.374\n",
      "test accuracy on epoch 1223: 0.769\n",
      "train loss on epoch 1224 : 0.251\n",
      "train accuracy on epoch 1224: 0.944\n",
      "test loss on epoch 1224: 0.368\n",
      "test accuracy on epoch 1224: 0.769\n",
      "train loss on epoch 1225 : 0.420\n",
      "train accuracy on epoch 1225: 0.944\n",
      "test loss on epoch 1225: 0.360\n",
      "test accuracy on epoch 1225: 0.769\n",
      "train loss on epoch 1226 : 0.064\n",
      "train accuracy on epoch 1226: 1.000\n",
      "test loss on epoch 1226: 0.355\n",
      "test accuracy on epoch 1226: 0.769\n",
      "train loss on epoch 1227 : 0.147\n",
      "train accuracy on epoch 1227: 0.833\n",
      "test loss on epoch 1227: 0.350\n",
      "test accuracy on epoch 1227: 0.769\n",
      "train loss on epoch 1228 : 0.170\n",
      "train accuracy on epoch 1228: 0.833\n",
      "test loss on epoch 1228: 0.345\n",
      "test accuracy on epoch 1228: 0.769\n",
      "train loss on epoch 1229 : 0.208\n",
      "train accuracy on epoch 1229: 0.944\n",
      "test loss on epoch 1229: 0.345\n",
      "test accuracy on epoch 1229: 0.769\n",
      "train loss on epoch 1230 : 0.131\n",
      "train accuracy on epoch 1230: 0.944\n",
      "test loss on epoch 1230: 0.346\n",
      "test accuracy on epoch 1230: 0.769\n",
      "train loss on epoch 1231 : 0.158\n",
      "train accuracy on epoch 1231: 0.944\n",
      "test loss on epoch 1231: 0.357\n",
      "test accuracy on epoch 1231: 0.769\n",
      "train loss on epoch 1232 : 0.155\n",
      "train accuracy on epoch 1232: 0.889\n",
      "test loss on epoch 1232: 0.368\n",
      "test accuracy on epoch 1232: 0.769\n",
      "train loss on epoch 1233 : 0.178\n",
      "train accuracy on epoch 1233: 0.944\n",
      "test loss on epoch 1233: 0.393\n",
      "test accuracy on epoch 1233: 0.769\n",
      "train loss on epoch 1234 : 0.220\n",
      "train accuracy on epoch 1234: 0.944\n",
      "test loss on epoch 1234: 0.414\n",
      "test accuracy on epoch 1234: 0.769\n",
      "train loss on epoch 1235 : 0.210\n",
      "train accuracy on epoch 1235: 0.889\n",
      "test loss on epoch 1235: 0.421\n",
      "test accuracy on epoch 1235: 0.769\n",
      "train loss on epoch 1236 : 0.432\n",
      "train accuracy on epoch 1236: 0.833\n",
      "test loss on epoch 1236: 0.422\n",
      "test accuracy on epoch 1236: 0.769\n",
      "train loss on epoch 1237 : 0.123\n",
      "train accuracy on epoch 1237: 0.944\n",
      "test loss on epoch 1237: 0.417\n",
      "test accuracy on epoch 1237: 0.769\n",
      "train loss on epoch 1238 : 0.096\n",
      "train accuracy on epoch 1238: 0.944\n",
      "test loss on epoch 1238: 0.401\n",
      "test accuracy on epoch 1238: 0.769\n",
      "train loss on epoch 1239 : 0.249\n",
      "train accuracy on epoch 1239: 0.944\n",
      "test loss on epoch 1239: 0.382\n",
      "test accuracy on epoch 1239: 0.769\n",
      "train loss on epoch 1240 : 0.087\n",
      "train accuracy on epoch 1240: 0.944\n",
      "test loss on epoch 1240: 0.372\n",
      "test accuracy on epoch 1240: 0.769\n",
      "train loss on epoch 1241 : 0.038\n",
      "train accuracy on epoch 1241: 1.000\n",
      "test loss on epoch 1241: 0.370\n",
      "test accuracy on epoch 1241: 0.769\n",
      "train loss on epoch 1242 : 0.234\n",
      "train accuracy on epoch 1242: 0.889\n",
      "test loss on epoch 1242: 0.370\n",
      "test accuracy on epoch 1242: 0.769\n",
      "train loss on epoch 1243 : 0.300\n",
      "train accuracy on epoch 1243: 0.944\n",
      "test loss on epoch 1243: 0.364\n",
      "test accuracy on epoch 1243: 0.769\n",
      "train loss on epoch 1244 : 0.181\n",
      "train accuracy on epoch 1244: 0.889\n",
      "test loss on epoch 1244: 0.356\n",
      "test accuracy on epoch 1244: 0.769\n",
      "train loss on epoch 1245 : 0.201\n",
      "train accuracy on epoch 1245: 0.944\n",
      "test loss on epoch 1245: 0.360\n",
      "test accuracy on epoch 1245: 0.769\n",
      "train loss on epoch 1246 : 0.065\n",
      "train accuracy on epoch 1246: 0.944\n",
      "test loss on epoch 1246: 0.356\n",
      "test accuracy on epoch 1246: 0.769\n",
      "train loss on epoch 1247 : 0.260\n",
      "train accuracy on epoch 1247: 0.944\n",
      "test loss on epoch 1247: 0.361\n",
      "test accuracy on epoch 1247: 0.769\n",
      "train loss on epoch 1248 : 0.130\n",
      "train accuracy on epoch 1248: 0.944\n",
      "test loss on epoch 1248: 0.359\n",
      "test accuracy on epoch 1248: 0.769\n",
      "train loss on epoch 1249 : 0.111\n",
      "train accuracy on epoch 1249: 0.944\n",
      "test loss on epoch 1249: 0.354\n",
      "test accuracy on epoch 1249: 0.769\n",
      "train loss on epoch 1250 : 0.099\n",
      "train accuracy on epoch 1250: 0.944\n",
      "test loss on epoch 1250: 0.346\n",
      "test accuracy on epoch 1250: 0.769\n",
      "train loss on epoch 1251 : 0.297\n",
      "train accuracy on epoch 1251: 0.889\n",
      "test loss on epoch 1251: 0.339\n",
      "test accuracy on epoch 1251: 0.769\n",
      "train loss on epoch 1252 : 0.238\n",
      "train accuracy on epoch 1252: 0.889\n",
      "test loss on epoch 1252: 0.332\n",
      "test accuracy on epoch 1252: 0.769\n",
      "train loss on epoch 1253 : 0.099\n",
      "train accuracy on epoch 1253: 1.000\n",
      "test loss on epoch 1253: 0.335\n",
      "test accuracy on epoch 1253: 0.769\n",
      "train loss on epoch 1254 : 0.207\n",
      "train accuracy on epoch 1254: 0.889\n",
      "test loss on epoch 1254: 0.338\n",
      "test accuracy on epoch 1254: 0.769\n",
      "train loss on epoch 1255 : 0.260\n",
      "train accuracy on epoch 1255: 0.944\n",
      "test loss on epoch 1255: 0.336\n",
      "test accuracy on epoch 1255: 0.769\n",
      "train loss on epoch 1256 : 0.103\n",
      "train accuracy on epoch 1256: 0.944\n",
      "test loss on epoch 1256: 0.334\n",
      "test accuracy on epoch 1256: 0.769\n",
      "train loss on epoch 1257 : 0.163\n",
      "train accuracy on epoch 1257: 0.944\n",
      "test loss on epoch 1257: 0.332\n",
      "test accuracy on epoch 1257: 0.769\n",
      "train loss on epoch 1258 : 0.130\n",
      "train accuracy on epoch 1258: 0.944\n",
      "test loss on epoch 1258: 0.335\n",
      "test accuracy on epoch 1258: 0.769\n",
      "train loss on epoch 1259 : 0.302\n",
      "train accuracy on epoch 1259: 0.944\n",
      "test loss on epoch 1259: 0.338\n",
      "test accuracy on epoch 1259: 0.769\n",
      "train loss on epoch 1260 : 0.117\n",
      "train accuracy on epoch 1260: 0.944\n",
      "test loss on epoch 1260: 0.339\n",
      "test accuracy on epoch 1260: 0.769\n",
      "train loss on epoch 1261 : 0.107\n",
      "train accuracy on epoch 1261: 0.944\n",
      "test loss on epoch 1261: 0.341\n",
      "test accuracy on epoch 1261: 0.769\n",
      "train loss on epoch 1262 : 0.243\n",
      "train accuracy on epoch 1262: 0.944\n",
      "test loss on epoch 1262: 0.345\n",
      "test accuracy on epoch 1262: 0.769\n",
      "train loss on epoch 1263 : 0.242\n",
      "train accuracy on epoch 1263: 0.889\n",
      "test loss on epoch 1263: 0.348\n",
      "test accuracy on epoch 1263: 0.769\n",
      "train loss on epoch 1264 : 0.358\n",
      "train accuracy on epoch 1264: 0.889\n",
      "test loss on epoch 1264: 0.351\n",
      "test accuracy on epoch 1264: 0.769\n",
      "train loss on epoch 1265 : 0.149\n",
      "train accuracy on epoch 1265: 0.944\n",
      "test loss on epoch 1265: 0.350\n",
      "test accuracy on epoch 1265: 0.769\n",
      "train loss on epoch 1266 : 0.206\n",
      "train accuracy on epoch 1266: 0.889\n",
      "test loss on epoch 1266: 0.348\n",
      "test accuracy on epoch 1266: 0.769\n",
      "train loss on epoch 1267 : 0.183\n",
      "train accuracy on epoch 1267: 0.889\n",
      "test loss on epoch 1267: 0.352\n",
      "test accuracy on epoch 1267: 0.769\n",
      "train loss on epoch 1268 : 0.164\n",
      "train accuracy on epoch 1268: 0.944\n",
      "test loss on epoch 1268: 0.354\n",
      "test accuracy on epoch 1268: 0.769\n",
      "train loss on epoch 1269 : 0.048\n",
      "train accuracy on epoch 1269: 1.000\n",
      "test loss on epoch 1269: 0.354\n",
      "test accuracy on epoch 1269: 0.769\n",
      "train loss on epoch 1270 : 0.131\n",
      "train accuracy on epoch 1270: 0.944\n",
      "test loss on epoch 1270: 0.356\n",
      "test accuracy on epoch 1270: 0.769\n",
      "train loss on epoch 1271 : 0.385\n",
      "train accuracy on epoch 1271: 0.833\n",
      "test loss on epoch 1271: 0.360\n",
      "test accuracy on epoch 1271: 0.769\n",
      "train loss on epoch 1272 : 0.146\n",
      "train accuracy on epoch 1272: 0.889\n",
      "test loss on epoch 1272: 0.370\n",
      "test accuracy on epoch 1272: 0.769\n",
      "train loss on epoch 1273 : 0.123\n",
      "train accuracy on epoch 1273: 0.889\n",
      "test loss on epoch 1273: 0.373\n",
      "test accuracy on epoch 1273: 0.769\n",
      "train loss on epoch 1274 : 0.301\n",
      "train accuracy on epoch 1274: 0.722\n",
      "test loss on epoch 1274: 0.362\n",
      "test accuracy on epoch 1274: 0.769\n",
      "train loss on epoch 1275 : 0.077\n",
      "train accuracy on epoch 1275: 1.000\n",
      "test loss on epoch 1275: 0.353\n",
      "test accuracy on epoch 1275: 0.769\n",
      "train loss on epoch 1276 : 0.095\n",
      "train accuracy on epoch 1276: 0.944\n",
      "test loss on epoch 1276: 0.344\n",
      "test accuracy on epoch 1276: 0.769\n",
      "train loss on epoch 1277 : 0.203\n",
      "train accuracy on epoch 1277: 0.889\n",
      "test loss on epoch 1277: 0.347\n",
      "test accuracy on epoch 1277: 0.769\n",
      "train loss on epoch 1278 : 0.164\n",
      "train accuracy on epoch 1278: 0.889\n",
      "test loss on epoch 1278: 0.345\n",
      "test accuracy on epoch 1278: 0.769\n",
      "train loss on epoch 1279 : 0.085\n",
      "train accuracy on epoch 1279: 0.944\n",
      "test loss on epoch 1279: 0.343\n",
      "test accuracy on epoch 1279: 0.769\n",
      "train loss on epoch 1280 : 0.258\n",
      "train accuracy on epoch 1280: 0.889\n",
      "test loss on epoch 1280: 0.349\n",
      "test accuracy on epoch 1280: 0.769\n",
      "train loss on epoch 1281 : 0.185\n",
      "train accuracy on epoch 1281: 0.889\n",
      "test loss on epoch 1281: 0.354\n",
      "test accuracy on epoch 1281: 0.769\n",
      "train loss on epoch 1282 : 0.294\n",
      "train accuracy on epoch 1282: 0.889\n",
      "test loss on epoch 1282: 0.356\n",
      "test accuracy on epoch 1282: 0.769\n",
      "train loss on epoch 1283 : 0.257\n",
      "train accuracy on epoch 1283: 0.889\n",
      "test loss on epoch 1283: 0.356\n",
      "test accuracy on epoch 1283: 0.769\n",
      "train loss on epoch 1284 : 0.245\n",
      "train accuracy on epoch 1284: 0.889\n",
      "test loss on epoch 1284: 0.363\n",
      "test accuracy on epoch 1284: 0.769\n",
      "train loss on epoch 1285 : 0.214\n",
      "train accuracy on epoch 1285: 0.944\n",
      "test loss on epoch 1285: 0.383\n",
      "test accuracy on epoch 1285: 0.769\n",
      "train loss on epoch 1286 : 0.197\n",
      "train accuracy on epoch 1286: 0.944\n",
      "test loss on epoch 1286: 0.380\n",
      "test accuracy on epoch 1286: 0.769\n",
      "train loss on epoch 1287 : 0.235\n",
      "train accuracy on epoch 1287: 0.944\n",
      "test loss on epoch 1287: 0.375\n",
      "test accuracy on epoch 1287: 0.769\n",
      "train loss on epoch 1288 : 0.215\n",
      "train accuracy on epoch 1288: 0.889\n",
      "test loss on epoch 1288: 0.372\n",
      "test accuracy on epoch 1288: 0.769\n",
      "train loss on epoch 1289 : 0.315\n",
      "train accuracy on epoch 1289: 0.889\n",
      "test loss on epoch 1289: 0.365\n",
      "test accuracy on epoch 1289: 0.769\n",
      "train loss on epoch 1290 : 0.180\n",
      "train accuracy on epoch 1290: 0.889\n",
      "test loss on epoch 1290: 0.361\n",
      "test accuracy on epoch 1290: 0.769\n",
      "train loss on epoch 1291 : 0.188\n",
      "train accuracy on epoch 1291: 0.944\n",
      "test loss on epoch 1291: 0.366\n",
      "test accuracy on epoch 1291: 0.769\n",
      "train loss on epoch 1292 : 0.050\n",
      "train accuracy on epoch 1292: 1.000\n",
      "test loss on epoch 1292: 0.367\n",
      "test accuracy on epoch 1292: 0.769\n",
      "train loss on epoch 1293 : 0.099\n",
      "train accuracy on epoch 1293: 1.000\n",
      "test loss on epoch 1293: 0.364\n",
      "test accuracy on epoch 1293: 0.769\n",
      "train loss on epoch 1294 : 0.103\n",
      "train accuracy on epoch 1294: 0.944\n",
      "test loss on epoch 1294: 0.360\n",
      "test accuracy on epoch 1294: 0.769\n",
      "train loss on epoch 1295 : 0.154\n",
      "train accuracy on epoch 1295: 0.944\n",
      "test loss on epoch 1295: 0.361\n",
      "test accuracy on epoch 1295: 0.769\n",
      "train loss on epoch 1296 : 0.045\n",
      "train accuracy on epoch 1296: 1.000\n",
      "test loss on epoch 1296: 0.362\n",
      "test accuracy on epoch 1296: 0.769\n",
      "train loss on epoch 1297 : 0.162\n",
      "train accuracy on epoch 1297: 0.944\n",
      "test loss on epoch 1297: 0.364\n",
      "test accuracy on epoch 1297: 0.769\n",
      "train loss on epoch 1298 : 0.337\n",
      "train accuracy on epoch 1298: 0.833\n",
      "test loss on epoch 1298: 0.353\n",
      "test accuracy on epoch 1298: 0.769\n",
      "train loss on epoch 1299 : 0.228\n",
      "train accuracy on epoch 1299: 0.889\n",
      "test loss on epoch 1299: 0.351\n",
      "test accuracy on epoch 1299: 0.769\n",
      "train loss on epoch 1300 : 0.261\n",
      "train accuracy on epoch 1300: 0.833\n",
      "test loss on epoch 1300: 0.344\n",
      "test accuracy on epoch 1300: 0.769\n",
      "train loss on epoch 1301 : 0.035\n",
      "train accuracy on epoch 1301: 1.000\n",
      "test loss on epoch 1301: 0.339\n",
      "test accuracy on epoch 1301: 0.769\n",
      "train loss on epoch 1302 : 0.181\n",
      "train accuracy on epoch 1302: 0.944\n",
      "test loss on epoch 1302: 0.338\n",
      "test accuracy on epoch 1302: 0.769\n",
      "train loss on epoch 1303 : 0.431\n",
      "train accuracy on epoch 1303: 0.889\n",
      "test loss on epoch 1303: 0.340\n",
      "test accuracy on epoch 1303: 0.769\n",
      "train loss on epoch 1304 : 0.290\n",
      "train accuracy on epoch 1304: 0.889\n",
      "test loss on epoch 1304: 0.347\n",
      "test accuracy on epoch 1304: 0.769\n",
      "train loss on epoch 1305 : 0.039\n",
      "train accuracy on epoch 1305: 1.000\n",
      "test loss on epoch 1305: 0.345\n",
      "test accuracy on epoch 1305: 0.769\n",
      "train loss on epoch 1306 : 0.230\n",
      "train accuracy on epoch 1306: 0.833\n",
      "test loss on epoch 1306: 0.349\n",
      "test accuracy on epoch 1306: 0.769\n",
      "train loss on epoch 1307 : 0.226\n",
      "train accuracy on epoch 1307: 0.944\n",
      "test loss on epoch 1307: 0.348\n",
      "test accuracy on epoch 1307: 0.769\n",
      "train loss on epoch 1308 : 0.125\n",
      "train accuracy on epoch 1308: 0.944\n",
      "test loss on epoch 1308: 0.345\n",
      "test accuracy on epoch 1308: 0.769\n",
      "train loss on epoch 1309 : 0.094\n",
      "train accuracy on epoch 1309: 1.000\n",
      "test loss on epoch 1309: 0.349\n",
      "test accuracy on epoch 1309: 0.769\n",
      "train loss on epoch 1310 : 0.136\n",
      "train accuracy on epoch 1310: 0.889\n",
      "test loss on epoch 1310: 0.356\n",
      "test accuracy on epoch 1310: 0.769\n",
      "train loss on epoch 1311 : 0.247\n",
      "train accuracy on epoch 1311: 0.833\n",
      "test loss on epoch 1311: 0.351\n",
      "test accuracy on epoch 1311: 0.769\n",
      "train loss on epoch 1312 : 0.085\n",
      "train accuracy on epoch 1312: 0.944\n",
      "test loss on epoch 1312: 0.358\n",
      "test accuracy on epoch 1312: 0.769\n",
      "train loss on epoch 1313 : 0.058\n",
      "train accuracy on epoch 1313: 1.000\n",
      "test loss on epoch 1313: 0.359\n",
      "test accuracy on epoch 1313: 0.769\n",
      "train loss on epoch 1314 : 0.401\n",
      "train accuracy on epoch 1314: 0.889\n",
      "test loss on epoch 1314: 0.363\n",
      "test accuracy on epoch 1314: 0.769\n",
      "train loss on epoch 1315 : 0.253\n",
      "train accuracy on epoch 1315: 0.944\n",
      "test loss on epoch 1315: 0.370\n",
      "test accuracy on epoch 1315: 0.769\n",
      "train loss on epoch 1316 : 0.126\n",
      "train accuracy on epoch 1316: 0.889\n",
      "test loss on epoch 1316: 0.375\n",
      "test accuracy on epoch 1316: 0.769\n",
      "train loss on epoch 1317 : 0.111\n",
      "train accuracy on epoch 1317: 0.889\n",
      "test loss on epoch 1317: 0.376\n",
      "test accuracy on epoch 1317: 0.769\n",
      "train loss on epoch 1318 : 0.212\n",
      "train accuracy on epoch 1318: 0.889\n",
      "test loss on epoch 1318: 0.376\n",
      "test accuracy on epoch 1318: 0.769\n",
      "train loss on epoch 1319 : 0.297\n",
      "train accuracy on epoch 1319: 0.944\n",
      "test loss on epoch 1319: 0.377\n",
      "test accuracy on epoch 1319: 0.769\n",
      "train loss on epoch 1320 : 0.122\n",
      "train accuracy on epoch 1320: 0.944\n",
      "test loss on epoch 1320: 0.374\n",
      "test accuracy on epoch 1320: 0.769\n",
      "train loss on epoch 1321 : 0.102\n",
      "train accuracy on epoch 1321: 0.944\n",
      "test loss on epoch 1321: 0.374\n",
      "test accuracy on epoch 1321: 0.769\n",
      "train loss on epoch 1322 : 0.194\n",
      "train accuracy on epoch 1322: 0.944\n",
      "test loss on epoch 1322: 0.370\n",
      "test accuracy on epoch 1322: 0.769\n",
      "train loss on epoch 1323 : 0.080\n",
      "train accuracy on epoch 1323: 1.000\n",
      "test loss on epoch 1323: 0.365\n",
      "test accuracy on epoch 1323: 0.769\n",
      "train loss on epoch 1324 : 0.153\n",
      "train accuracy on epoch 1324: 0.944\n",
      "test loss on epoch 1324: 0.361\n",
      "test accuracy on epoch 1324: 0.769\n",
      "train loss on epoch 1325 : 0.183\n",
      "train accuracy on epoch 1325: 0.889\n",
      "test loss on epoch 1325: 0.358\n",
      "test accuracy on epoch 1325: 0.769\n",
      "train loss on epoch 1326 : 0.079\n",
      "train accuracy on epoch 1326: 1.000\n",
      "test loss on epoch 1326: 0.362\n",
      "test accuracy on epoch 1326: 0.769\n",
      "train loss on epoch 1327 : 0.076\n",
      "train accuracy on epoch 1327: 1.000\n",
      "test loss on epoch 1327: 0.359\n",
      "test accuracy on epoch 1327: 0.769\n",
      "train loss on epoch 1328 : 0.112\n",
      "train accuracy on epoch 1328: 0.944\n",
      "test loss on epoch 1328: 0.361\n",
      "test accuracy on epoch 1328: 0.769\n",
      "train loss on epoch 1329 : 0.094\n",
      "train accuracy on epoch 1329: 1.000\n",
      "test loss on epoch 1329: 0.363\n",
      "test accuracy on epoch 1329: 0.769\n",
      "train loss on epoch 1330 : 0.102\n",
      "train accuracy on epoch 1330: 1.000\n",
      "test loss on epoch 1330: 0.362\n",
      "test accuracy on epoch 1330: 0.769\n",
      "train loss on epoch 1331 : 0.376\n",
      "train accuracy on epoch 1331: 0.944\n",
      "test loss on epoch 1331: 0.371\n",
      "test accuracy on epoch 1331: 0.769\n",
      "train loss on epoch 1332 : 0.122\n",
      "train accuracy on epoch 1332: 0.889\n",
      "test loss on epoch 1332: 0.370\n",
      "test accuracy on epoch 1332: 0.769\n",
      "train loss on epoch 1333 : 0.176\n",
      "train accuracy on epoch 1333: 0.889\n",
      "test loss on epoch 1333: 0.370\n",
      "test accuracy on epoch 1333: 0.769\n",
      "train loss on epoch 1334 : 0.192\n",
      "train accuracy on epoch 1334: 0.889\n",
      "test loss on epoch 1334: 0.371\n",
      "test accuracy on epoch 1334: 0.769\n",
      "train loss on epoch 1335 : 0.375\n",
      "train accuracy on epoch 1335: 0.889\n",
      "test loss on epoch 1335: 0.365\n",
      "test accuracy on epoch 1335: 0.769\n",
      "train loss on epoch 1336 : 0.202\n",
      "train accuracy on epoch 1336: 0.889\n",
      "test loss on epoch 1336: 0.364\n",
      "test accuracy on epoch 1336: 0.769\n",
      "train loss on epoch 1337 : 0.277\n",
      "train accuracy on epoch 1337: 0.889\n",
      "test loss on epoch 1337: 0.355\n",
      "test accuracy on epoch 1337: 0.769\n",
      "train loss on epoch 1338 : 0.265\n",
      "train accuracy on epoch 1338: 0.889\n",
      "test loss on epoch 1338: 0.346\n",
      "test accuracy on epoch 1338: 0.769\n",
      "train loss on epoch 1339 : 0.476\n",
      "train accuracy on epoch 1339: 0.889\n",
      "test loss on epoch 1339: 0.347\n",
      "test accuracy on epoch 1339: 0.769\n",
      "train loss on epoch 1340 : 0.277\n",
      "train accuracy on epoch 1340: 0.889\n",
      "test loss on epoch 1340: 0.342\n",
      "test accuracy on epoch 1340: 0.769\n",
      "train loss on epoch 1341 : 0.183\n",
      "train accuracy on epoch 1341: 0.889\n",
      "test loss on epoch 1341: 0.349\n",
      "test accuracy on epoch 1341: 0.769\n",
      "train loss on epoch 1342 : 0.247\n",
      "train accuracy on epoch 1342: 0.778\n",
      "test loss on epoch 1342: 0.349\n",
      "test accuracy on epoch 1342: 0.769\n",
      "train loss on epoch 1343 : 0.126\n",
      "train accuracy on epoch 1343: 0.944\n",
      "test loss on epoch 1343: 0.358\n",
      "test accuracy on epoch 1343: 0.769\n",
      "train loss on epoch 1344 : 0.026\n",
      "train accuracy on epoch 1344: 1.000\n",
      "test loss on epoch 1344: 0.361\n",
      "test accuracy on epoch 1344: 0.769\n",
      "train loss on epoch 1345 : 0.415\n",
      "train accuracy on epoch 1345: 0.833\n",
      "test loss on epoch 1345: 0.368\n",
      "test accuracy on epoch 1345: 0.769\n",
      "train loss on epoch 1346 : 0.140\n",
      "train accuracy on epoch 1346: 0.889\n",
      "test loss on epoch 1346: 0.361\n",
      "test accuracy on epoch 1346: 0.769\n",
      "train loss on epoch 1347 : 0.118\n",
      "train accuracy on epoch 1347: 0.944\n",
      "test loss on epoch 1347: 0.361\n",
      "test accuracy on epoch 1347: 0.769\n",
      "train loss on epoch 1348 : 0.053\n",
      "train accuracy on epoch 1348: 1.000\n",
      "test loss on epoch 1348: 0.369\n",
      "test accuracy on epoch 1348: 0.769\n",
      "train loss on epoch 1349 : 0.108\n",
      "train accuracy on epoch 1349: 0.944\n",
      "test loss on epoch 1349: 0.372\n",
      "test accuracy on epoch 1349: 0.769\n",
      "train loss on epoch 1350 : 0.035\n",
      "train accuracy on epoch 1350: 1.000\n",
      "test loss on epoch 1350: 0.368\n",
      "test accuracy on epoch 1350: 0.769\n",
      "train loss on epoch 1351 : 0.162\n",
      "train accuracy on epoch 1351: 0.944\n",
      "test loss on epoch 1351: 0.375\n",
      "test accuracy on epoch 1351: 0.769\n",
      "train loss on epoch 1352 : 0.294\n",
      "train accuracy on epoch 1352: 0.889\n",
      "test loss on epoch 1352: 0.363\n",
      "test accuracy on epoch 1352: 0.769\n",
      "train loss on epoch 1353 : 0.103\n",
      "train accuracy on epoch 1353: 0.944\n",
      "test loss on epoch 1353: 0.367\n",
      "test accuracy on epoch 1353: 0.769\n",
      "train loss on epoch 1354 : 0.206\n",
      "train accuracy on epoch 1354: 0.944\n",
      "test loss on epoch 1354: 0.363\n",
      "test accuracy on epoch 1354: 0.769\n",
      "train loss on epoch 1355 : 0.164\n",
      "train accuracy on epoch 1355: 0.889\n",
      "test loss on epoch 1355: 0.356\n",
      "test accuracy on epoch 1355: 0.769\n",
      "train loss on epoch 1356 : 0.128\n",
      "train accuracy on epoch 1356: 0.944\n",
      "test loss on epoch 1356: 0.359\n",
      "test accuracy on epoch 1356: 0.769\n",
      "train loss on epoch 1357 : 0.227\n",
      "train accuracy on epoch 1357: 0.889\n",
      "test loss on epoch 1357: 0.358\n",
      "test accuracy on epoch 1357: 0.769\n",
      "train loss on epoch 1358 : 0.164\n",
      "train accuracy on epoch 1358: 0.944\n",
      "test loss on epoch 1358: 0.357\n",
      "test accuracy on epoch 1358: 0.769\n",
      "train loss on epoch 1359 : 0.187\n",
      "train accuracy on epoch 1359: 0.944\n",
      "test loss on epoch 1359: 0.361\n",
      "test accuracy on epoch 1359: 0.769\n",
      "train loss on epoch 1360 : 0.141\n",
      "train accuracy on epoch 1360: 0.944\n",
      "test loss on epoch 1360: 0.362\n",
      "test accuracy on epoch 1360: 0.769\n",
      "train loss on epoch 1361 : 0.159\n",
      "train accuracy on epoch 1361: 0.889\n",
      "test loss on epoch 1361: 0.365\n",
      "test accuracy on epoch 1361: 0.769\n",
      "train loss on epoch 1362 : 0.209\n",
      "train accuracy on epoch 1362: 0.889\n",
      "test loss on epoch 1362: 0.364\n",
      "test accuracy on epoch 1362: 0.769\n",
      "train loss on epoch 1363 : 0.157\n",
      "train accuracy on epoch 1363: 0.889\n",
      "test loss on epoch 1363: 0.350\n",
      "test accuracy on epoch 1363: 0.769\n",
      "train loss on epoch 1364 : 0.129\n",
      "train accuracy on epoch 1364: 0.944\n",
      "test loss on epoch 1364: 0.351\n",
      "test accuracy on epoch 1364: 0.769\n",
      "train loss on epoch 1365 : 0.140\n",
      "train accuracy on epoch 1365: 0.944\n",
      "test loss on epoch 1365: 0.343\n",
      "test accuracy on epoch 1365: 0.769\n",
      "train loss on epoch 1366 : 0.143\n",
      "train accuracy on epoch 1366: 0.889\n",
      "test loss on epoch 1366: 0.343\n",
      "test accuracy on epoch 1366: 0.769\n",
      "train loss on epoch 1367 : 0.133\n",
      "train accuracy on epoch 1367: 0.944\n",
      "test loss on epoch 1367: 0.337\n",
      "test accuracy on epoch 1367: 0.769\n",
      "train loss on epoch 1368 : 0.261\n",
      "train accuracy on epoch 1368: 0.889\n",
      "test loss on epoch 1368: 0.333\n",
      "test accuracy on epoch 1368: 0.769\n",
      "train loss on epoch 1369 : 0.137\n",
      "train accuracy on epoch 1369: 0.944\n",
      "test loss on epoch 1369: 0.336\n",
      "test accuracy on epoch 1369: 0.769\n",
      "train loss on epoch 1370 : 0.071\n",
      "train accuracy on epoch 1370: 0.944\n",
      "test loss on epoch 1370: 0.331\n",
      "test accuracy on epoch 1370: 0.769\n",
      "train loss on epoch 1371 : 0.354\n",
      "train accuracy on epoch 1371: 0.889\n",
      "test loss on epoch 1371: 0.338\n",
      "test accuracy on epoch 1371: 0.769\n",
      "train loss on epoch 1372 : 0.045\n",
      "train accuracy on epoch 1372: 1.000\n",
      "test loss on epoch 1372: 0.344\n",
      "test accuracy on epoch 1372: 0.769\n",
      "train loss on epoch 1373 : 0.210\n",
      "train accuracy on epoch 1373: 0.944\n",
      "test loss on epoch 1373: 0.350\n",
      "test accuracy on epoch 1373: 0.769\n",
      "train loss on epoch 1374 : 0.070\n",
      "train accuracy on epoch 1374: 1.000\n",
      "test loss on epoch 1374: 0.345\n",
      "test accuracy on epoch 1374: 0.769\n",
      "train loss on epoch 1375 : 0.349\n",
      "train accuracy on epoch 1375: 0.889\n",
      "test loss on epoch 1375: 0.344\n",
      "test accuracy on epoch 1375: 0.769\n",
      "train loss on epoch 1376 : 0.141\n",
      "train accuracy on epoch 1376: 0.944\n",
      "test loss on epoch 1376: 0.347\n",
      "test accuracy on epoch 1376: 0.769\n",
      "train loss on epoch 1377 : 0.174\n",
      "train accuracy on epoch 1377: 0.889\n",
      "test loss on epoch 1377: 0.352\n",
      "test accuracy on epoch 1377: 0.769\n",
      "train loss on epoch 1378 : 0.042\n",
      "train accuracy on epoch 1378: 1.000\n",
      "test loss on epoch 1378: 0.353\n",
      "test accuracy on epoch 1378: 0.769\n",
      "train loss on epoch 1379 : 0.209\n",
      "train accuracy on epoch 1379: 0.889\n",
      "test loss on epoch 1379: 0.359\n",
      "test accuracy on epoch 1379: 0.769\n",
      "train loss on epoch 1380 : 0.147\n",
      "train accuracy on epoch 1380: 0.944\n",
      "test loss on epoch 1380: 0.360\n",
      "test accuracy on epoch 1380: 0.769\n",
      "train loss on epoch 1381 : 0.150\n",
      "train accuracy on epoch 1381: 0.889\n",
      "test loss on epoch 1381: 0.358\n",
      "test accuracy on epoch 1381: 0.769\n",
      "train loss on epoch 1382 : 0.063\n",
      "train accuracy on epoch 1382: 1.000\n",
      "test loss on epoch 1382: 0.355\n",
      "test accuracy on epoch 1382: 0.769\n",
      "train loss on epoch 1383 : 0.583\n",
      "train accuracy on epoch 1383: 0.889\n",
      "test loss on epoch 1383: 0.348\n",
      "test accuracy on epoch 1383: 0.769\n",
      "train loss on epoch 1384 : 0.100\n",
      "train accuracy on epoch 1384: 0.944\n",
      "test loss on epoch 1384: 0.351\n",
      "test accuracy on epoch 1384: 0.769\n",
      "train loss on epoch 1385 : 0.245\n",
      "train accuracy on epoch 1385: 0.944\n",
      "test loss on epoch 1385: 0.346\n",
      "test accuracy on epoch 1385: 0.769\n",
      "train loss on epoch 1386 : 0.025\n",
      "train accuracy on epoch 1386: 1.000\n",
      "test loss on epoch 1386: 0.343\n",
      "test accuracy on epoch 1386: 0.769\n",
      "train loss on epoch 1387 : 0.070\n",
      "train accuracy on epoch 1387: 1.000\n",
      "test loss on epoch 1387: 0.339\n",
      "test accuracy on epoch 1387: 0.769\n",
      "train loss on epoch 1388 : 0.138\n",
      "train accuracy on epoch 1388: 0.944\n",
      "test loss on epoch 1388: 0.344\n",
      "test accuracy on epoch 1388: 0.769\n",
      "train loss on epoch 1389 : 0.098\n",
      "train accuracy on epoch 1389: 0.944\n",
      "test loss on epoch 1389: 0.340\n",
      "test accuracy on epoch 1389: 0.769\n",
      "train loss on epoch 1390 : 0.241\n",
      "train accuracy on epoch 1390: 0.889\n",
      "test loss on epoch 1390: 0.340\n",
      "test accuracy on epoch 1390: 0.769\n",
      "train loss on epoch 1391 : 0.052\n",
      "train accuracy on epoch 1391: 1.000\n",
      "test loss on epoch 1391: 0.341\n",
      "test accuracy on epoch 1391: 0.769\n",
      "train loss on epoch 1392 : 0.099\n",
      "train accuracy on epoch 1392: 0.944\n",
      "test loss on epoch 1392: 0.343\n",
      "test accuracy on epoch 1392: 0.769\n",
      "train loss on epoch 1393 : 0.349\n",
      "train accuracy on epoch 1393: 0.889\n",
      "test loss on epoch 1393: 0.339\n",
      "test accuracy on epoch 1393: 0.769\n",
      "train loss on epoch 1394 : 0.147\n",
      "train accuracy on epoch 1394: 0.944\n",
      "test loss on epoch 1394: 0.342\n",
      "test accuracy on epoch 1394: 0.769\n",
      "train loss on epoch 1395 : 0.218\n",
      "train accuracy on epoch 1395: 0.889\n",
      "test loss on epoch 1395: 0.340\n",
      "test accuracy on epoch 1395: 0.769\n",
      "train loss on epoch 1396 : 0.125\n",
      "train accuracy on epoch 1396: 0.944\n",
      "test loss on epoch 1396: 0.342\n",
      "test accuracy on epoch 1396: 0.769\n",
      "train loss on epoch 1397 : 0.132\n",
      "train accuracy on epoch 1397: 0.944\n",
      "test loss on epoch 1397: 0.347\n",
      "test accuracy on epoch 1397: 0.769\n",
      "train loss on epoch 1398 : 0.193\n",
      "train accuracy on epoch 1398: 0.944\n",
      "test loss on epoch 1398: 0.354\n",
      "test accuracy on epoch 1398: 0.769\n",
      "train loss on epoch 1399 : 0.218\n",
      "train accuracy on epoch 1399: 0.889\n",
      "test loss on epoch 1399: 0.360\n",
      "test accuracy on epoch 1399: 0.769\n",
      "train loss on epoch 1400 : 0.164\n",
      "train accuracy on epoch 1400: 0.944\n",
      "test loss on epoch 1400: 0.366\n",
      "test accuracy on epoch 1400: 0.769\n",
      "train loss on epoch 1401 : 0.192\n",
      "train accuracy on epoch 1401: 0.833\n",
      "test loss on epoch 1401: 0.365\n",
      "test accuracy on epoch 1401: 0.769\n",
      "train loss on epoch 1402 : 0.039\n",
      "train accuracy on epoch 1402: 1.000\n",
      "test loss on epoch 1402: 0.363\n",
      "test accuracy on epoch 1402: 0.769\n",
      "train loss on epoch 1403 : 0.213\n",
      "train accuracy on epoch 1403: 0.944\n",
      "test loss on epoch 1403: 0.357\n",
      "test accuracy on epoch 1403: 0.769\n",
      "train loss on epoch 1404 : 0.116\n",
      "train accuracy on epoch 1404: 0.944\n",
      "test loss on epoch 1404: 0.357\n",
      "test accuracy on epoch 1404: 0.769\n",
      "train loss on epoch 1405 : 0.037\n",
      "train accuracy on epoch 1405: 1.000\n",
      "test loss on epoch 1405: 0.363\n",
      "test accuracy on epoch 1405: 0.769\n",
      "train loss on epoch 1406 : 0.364\n",
      "train accuracy on epoch 1406: 0.889\n",
      "test loss on epoch 1406: 0.361\n",
      "test accuracy on epoch 1406: 0.769\n",
      "train loss on epoch 1407 : 0.235\n",
      "train accuracy on epoch 1407: 0.889\n",
      "test loss on epoch 1407: 0.358\n",
      "test accuracy on epoch 1407: 0.769\n",
      "train loss on epoch 1408 : 0.467\n",
      "train accuracy on epoch 1408: 0.778\n",
      "test loss on epoch 1408: 0.363\n",
      "test accuracy on epoch 1408: 0.769\n",
      "train loss on epoch 1409 : 0.232\n",
      "train accuracy on epoch 1409: 0.889\n",
      "test loss on epoch 1409: 0.357\n",
      "test accuracy on epoch 1409: 0.769\n",
      "train loss on epoch 1410 : 0.119\n",
      "train accuracy on epoch 1410: 0.944\n",
      "test loss on epoch 1410: 0.355\n",
      "test accuracy on epoch 1410: 0.769\n",
      "train loss on epoch 1411 : 0.094\n",
      "train accuracy on epoch 1411: 1.000\n",
      "test loss on epoch 1411: 0.355\n",
      "test accuracy on epoch 1411: 0.769\n",
      "train loss on epoch 1412 : 0.186\n",
      "train accuracy on epoch 1412: 0.944\n",
      "test loss on epoch 1412: 0.353\n",
      "test accuracy on epoch 1412: 0.769\n",
      "train loss on epoch 1413 : 0.356\n",
      "train accuracy on epoch 1413: 0.778\n",
      "test loss on epoch 1413: 0.348\n",
      "test accuracy on epoch 1413: 0.769\n",
      "train loss on epoch 1414 : 0.110\n",
      "train accuracy on epoch 1414: 0.944\n",
      "test loss on epoch 1414: 0.355\n",
      "test accuracy on epoch 1414: 0.769\n",
      "train loss on epoch 1415 : 0.097\n",
      "train accuracy on epoch 1415: 0.944\n",
      "test loss on epoch 1415: 0.365\n",
      "test accuracy on epoch 1415: 0.769\n",
      "train loss on epoch 1416 : 0.127\n",
      "train accuracy on epoch 1416: 1.000\n",
      "test loss on epoch 1416: 0.370\n",
      "test accuracy on epoch 1416: 0.769\n",
      "train loss on epoch 1417 : 0.255\n",
      "train accuracy on epoch 1417: 0.889\n",
      "test loss on epoch 1417: 0.363\n",
      "test accuracy on epoch 1417: 0.769\n",
      "train loss on epoch 1418 : 0.434\n",
      "train accuracy on epoch 1418: 0.833\n",
      "test loss on epoch 1418: 0.360\n",
      "test accuracy on epoch 1418: 0.769\n",
      "train loss on epoch 1419 : 0.055\n",
      "train accuracy on epoch 1419: 1.000\n",
      "test loss on epoch 1419: 0.349\n",
      "test accuracy on epoch 1419: 0.769\n",
      "train loss on epoch 1420 : 0.137\n",
      "train accuracy on epoch 1420: 0.889\n",
      "test loss on epoch 1420: 0.340\n",
      "test accuracy on epoch 1420: 0.769\n",
      "train loss on epoch 1421 : 0.205\n",
      "train accuracy on epoch 1421: 0.944\n",
      "test loss on epoch 1421: 0.339\n",
      "test accuracy on epoch 1421: 0.769\n",
      "train loss on epoch 1422 : 0.552\n",
      "train accuracy on epoch 1422: 0.778\n",
      "test loss on epoch 1422: 0.335\n",
      "test accuracy on epoch 1422: 0.769\n",
      "train loss on epoch 1423 : 0.253\n",
      "train accuracy on epoch 1423: 0.833\n",
      "test loss on epoch 1423: 0.326\n",
      "test accuracy on epoch 1423: 0.769\n",
      "train loss on epoch 1424 : 0.284\n",
      "train accuracy on epoch 1424: 0.889\n",
      "test loss on epoch 1424: 0.327\n",
      "test accuracy on epoch 1424: 0.769\n",
      "train loss on epoch 1425 : 0.219\n",
      "train accuracy on epoch 1425: 0.889\n",
      "test loss on epoch 1425: 0.325\n",
      "test accuracy on epoch 1425: 0.769\n",
      "train loss on epoch 1426 : 0.135\n",
      "train accuracy on epoch 1426: 1.000\n",
      "test loss on epoch 1426: 0.329\n",
      "test accuracy on epoch 1426: 0.769\n",
      "train loss on epoch 1427 : 0.396\n",
      "train accuracy on epoch 1427: 0.833\n",
      "test loss on epoch 1427: 0.333\n",
      "test accuracy on epoch 1427: 0.769\n",
      "train loss on epoch 1428 : 0.199\n",
      "train accuracy on epoch 1428: 0.944\n",
      "test loss on epoch 1428: 0.332\n",
      "test accuracy on epoch 1428: 0.692\n",
      "train loss on epoch 1429 : 0.075\n",
      "train accuracy on epoch 1429: 1.000\n",
      "test loss on epoch 1429: 0.326\n",
      "test accuracy on epoch 1429: 0.769\n",
      "train loss on epoch 1430 : 0.184\n",
      "train accuracy on epoch 1430: 0.944\n",
      "test loss on epoch 1430: 0.331\n",
      "test accuracy on epoch 1430: 0.692\n",
      "train loss on epoch 1431 : 0.368\n",
      "train accuracy on epoch 1431: 0.889\n",
      "test loss on epoch 1431: 0.333\n",
      "test accuracy on epoch 1431: 0.692\n",
      "train loss on epoch 1432 : 0.088\n",
      "train accuracy on epoch 1432: 1.000\n",
      "test loss on epoch 1432: 0.330\n",
      "test accuracy on epoch 1432: 0.769\n",
      "train loss on epoch 1433 : 0.174\n",
      "train accuracy on epoch 1433: 0.944\n",
      "test loss on epoch 1433: 0.331\n",
      "test accuracy on epoch 1433: 0.769\n",
      "train loss on epoch 1434 : 0.132\n",
      "train accuracy on epoch 1434: 0.944\n",
      "test loss on epoch 1434: 0.333\n",
      "test accuracy on epoch 1434: 0.769\n",
      "train loss on epoch 1435 : 0.320\n",
      "train accuracy on epoch 1435: 0.833\n",
      "test loss on epoch 1435: 0.320\n",
      "test accuracy on epoch 1435: 0.846\n",
      "train loss on epoch 1436 : 0.138\n",
      "train accuracy on epoch 1436: 0.889\n",
      "test loss on epoch 1436: 0.320\n",
      "test accuracy on epoch 1436: 0.769\n",
      "train loss on epoch 1437 : 0.540\n",
      "train accuracy on epoch 1437: 0.778\n",
      "test loss on epoch 1437: 0.319\n",
      "test accuracy on epoch 1437: 0.846\n",
      "train loss on epoch 1438 : 0.168\n",
      "train accuracy on epoch 1438: 0.889\n",
      "test loss on epoch 1438: 0.328\n",
      "test accuracy on epoch 1438: 0.769\n",
      "train loss on epoch 1439 : 0.162\n",
      "train accuracy on epoch 1439: 0.944\n",
      "test loss on epoch 1439: 0.336\n",
      "test accuracy on epoch 1439: 0.769\n",
      "train loss on epoch 1440 : 0.333\n",
      "train accuracy on epoch 1440: 0.889\n",
      "test loss on epoch 1440: 0.346\n",
      "test accuracy on epoch 1440: 0.769\n",
      "train loss on epoch 1441 : 0.125\n",
      "train accuracy on epoch 1441: 0.944\n",
      "test loss on epoch 1441: 0.354\n",
      "test accuracy on epoch 1441: 0.769\n",
      "train loss on epoch 1442 : 0.068\n",
      "train accuracy on epoch 1442: 1.000\n",
      "test loss on epoch 1442: 0.357\n",
      "test accuracy on epoch 1442: 0.769\n",
      "train loss on epoch 1443 : 0.052\n",
      "train accuracy on epoch 1443: 1.000\n",
      "test loss on epoch 1443: 0.358\n",
      "test accuracy on epoch 1443: 0.769\n",
      "train loss on epoch 1444 : 0.115\n",
      "train accuracy on epoch 1444: 0.944\n",
      "test loss on epoch 1444: 0.365\n",
      "test accuracy on epoch 1444: 0.769\n",
      "train loss on epoch 1445 : 0.170\n",
      "train accuracy on epoch 1445: 0.944\n",
      "test loss on epoch 1445: 0.365\n",
      "test accuracy on epoch 1445: 0.769\n",
      "train loss on epoch 1446 : 0.146\n",
      "train accuracy on epoch 1446: 0.889\n",
      "test loss on epoch 1446: 0.367\n",
      "test accuracy on epoch 1446: 0.769\n",
      "train loss on epoch 1447 : 0.149\n",
      "train accuracy on epoch 1447: 0.944\n",
      "test loss on epoch 1447: 0.368\n",
      "test accuracy on epoch 1447: 0.769\n",
      "train loss on epoch 1448 : 0.298\n",
      "train accuracy on epoch 1448: 0.833\n",
      "test loss on epoch 1448: 0.373\n",
      "test accuracy on epoch 1448: 0.769\n",
      "train loss on epoch 1449 : 0.215\n",
      "train accuracy on epoch 1449: 0.833\n",
      "test loss on epoch 1449: 0.379\n",
      "test accuracy on epoch 1449: 0.769\n",
      "train loss on epoch 1450 : 0.225\n",
      "train accuracy on epoch 1450: 0.889\n",
      "test loss on epoch 1450: 0.372\n",
      "test accuracy on epoch 1450: 0.769\n",
      "train loss on epoch 1451 : 0.032\n",
      "train accuracy on epoch 1451: 1.000\n",
      "test loss on epoch 1451: 0.366\n",
      "test accuracy on epoch 1451: 0.769\n",
      "train loss on epoch 1452 : 0.354\n",
      "train accuracy on epoch 1452: 0.889\n",
      "test loss on epoch 1452: 0.365\n",
      "test accuracy on epoch 1452: 0.769\n",
      "train loss on epoch 1453 : 0.099\n",
      "train accuracy on epoch 1453: 0.944\n",
      "test loss on epoch 1453: 0.365\n",
      "test accuracy on epoch 1453: 0.769\n",
      "train loss on epoch 1454 : 0.315\n",
      "train accuracy on epoch 1454: 0.889\n",
      "test loss on epoch 1454: 0.373\n",
      "test accuracy on epoch 1454: 0.769\n",
      "train loss on epoch 1455 : 0.491\n",
      "train accuracy on epoch 1455: 0.889\n",
      "test loss on epoch 1455: 0.371\n",
      "test accuracy on epoch 1455: 0.769\n",
      "train loss on epoch 1456 : 0.373\n",
      "train accuracy on epoch 1456: 0.833\n",
      "test loss on epoch 1456: 0.359\n",
      "test accuracy on epoch 1456: 0.769\n",
      "train loss on epoch 1457 : 0.263\n",
      "train accuracy on epoch 1457: 0.944\n",
      "test loss on epoch 1457: 0.353\n",
      "test accuracy on epoch 1457: 0.769\n",
      "train loss on epoch 1458 : 0.334\n",
      "train accuracy on epoch 1458: 0.889\n",
      "test loss on epoch 1458: 0.339\n",
      "test accuracy on epoch 1458: 0.769\n",
      "train loss on epoch 1459 : 0.176\n",
      "train accuracy on epoch 1459: 0.944\n",
      "test loss on epoch 1459: 0.332\n",
      "test accuracy on epoch 1459: 0.769\n",
      "train loss on epoch 1460 : 0.198\n",
      "train accuracy on epoch 1460: 0.944\n",
      "test loss on epoch 1460: 0.331\n",
      "test accuracy on epoch 1460: 0.769\n",
      "train loss on epoch 1461 : 0.333\n",
      "train accuracy on epoch 1461: 0.889\n",
      "test loss on epoch 1461: 0.329\n",
      "test accuracy on epoch 1461: 0.769\n",
      "train loss on epoch 1462 : 0.340\n",
      "train accuracy on epoch 1462: 0.889\n",
      "test loss on epoch 1462: 0.329\n",
      "test accuracy on epoch 1462: 0.769\n",
      "train loss on epoch 1463 : 0.257\n",
      "train accuracy on epoch 1463: 0.889\n",
      "test loss on epoch 1463: 0.329\n",
      "test accuracy on epoch 1463: 0.769\n",
      "train loss on epoch 1464 : 0.213\n",
      "train accuracy on epoch 1464: 0.944\n",
      "test loss on epoch 1464: 0.333\n",
      "test accuracy on epoch 1464: 0.769\n",
      "train loss on epoch 1465 : 0.222\n",
      "train accuracy on epoch 1465: 0.944\n",
      "test loss on epoch 1465: 0.338\n",
      "test accuracy on epoch 1465: 0.769\n",
      "train loss on epoch 1466 : 0.228\n",
      "train accuracy on epoch 1466: 0.889\n",
      "test loss on epoch 1466: 0.347\n",
      "test accuracy on epoch 1466: 0.769\n",
      "train loss on epoch 1467 : 0.160\n",
      "train accuracy on epoch 1467: 0.944\n",
      "test loss on epoch 1467: 0.349\n",
      "test accuracy on epoch 1467: 0.769\n",
      "train loss on epoch 1468 : 0.105\n",
      "train accuracy on epoch 1468: 0.944\n",
      "test loss on epoch 1468: 0.346\n",
      "test accuracy on epoch 1468: 0.769\n",
      "train loss on epoch 1469 : 0.117\n",
      "train accuracy on epoch 1469: 0.944\n",
      "test loss on epoch 1469: 0.344\n",
      "test accuracy on epoch 1469: 0.769\n",
      "train loss on epoch 1470 : 0.157\n",
      "train accuracy on epoch 1470: 0.944\n",
      "test loss on epoch 1470: 0.345\n",
      "test accuracy on epoch 1470: 0.769\n",
      "train loss on epoch 1471 : 0.404\n",
      "train accuracy on epoch 1471: 0.889\n",
      "test loss on epoch 1471: 0.338\n",
      "test accuracy on epoch 1471: 0.769\n",
      "train loss on epoch 1472 : 0.096\n",
      "train accuracy on epoch 1472: 0.944\n",
      "test loss on epoch 1472: 0.337\n",
      "test accuracy on epoch 1472: 0.769\n",
      "train loss on epoch 1473 : 0.177\n",
      "train accuracy on epoch 1473: 0.944\n",
      "test loss on epoch 1473: 0.338\n",
      "test accuracy on epoch 1473: 0.769\n",
      "train loss on epoch 1474 : 0.132\n",
      "train accuracy on epoch 1474: 0.944\n",
      "test loss on epoch 1474: 0.333\n",
      "test accuracy on epoch 1474: 0.769\n",
      "train loss on epoch 1475 : 0.153\n",
      "train accuracy on epoch 1475: 0.944\n",
      "test loss on epoch 1475: 0.338\n",
      "test accuracy on epoch 1475: 0.769\n",
      "train loss on epoch 1476 : 0.242\n",
      "train accuracy on epoch 1476: 0.889\n",
      "test loss on epoch 1476: 0.338\n",
      "test accuracy on epoch 1476: 0.769\n",
      "train loss on epoch 1477 : 0.163\n",
      "train accuracy on epoch 1477: 0.944\n",
      "test loss on epoch 1477: 0.334\n",
      "test accuracy on epoch 1477: 0.769\n",
      "train loss on epoch 1478 : 0.087\n",
      "train accuracy on epoch 1478: 1.000\n",
      "test loss on epoch 1478: 0.338\n",
      "test accuracy on epoch 1478: 0.769\n",
      "train loss on epoch 1479 : 0.306\n",
      "train accuracy on epoch 1479: 0.944\n",
      "test loss on epoch 1479: 0.340\n",
      "test accuracy on epoch 1479: 0.769\n",
      "train loss on epoch 1480 : 0.119\n",
      "train accuracy on epoch 1480: 1.000\n",
      "test loss on epoch 1480: 0.349\n",
      "test accuracy on epoch 1480: 0.769\n",
      "train loss on epoch 1481 : 0.149\n",
      "train accuracy on epoch 1481: 0.944\n",
      "test loss on epoch 1481: 0.349\n",
      "test accuracy on epoch 1481: 0.769\n",
      "train loss on epoch 1482 : 0.257\n",
      "train accuracy on epoch 1482: 0.944\n",
      "test loss on epoch 1482: 0.364\n",
      "test accuracy on epoch 1482: 0.769\n",
      "train loss on epoch 1483 : 0.314\n",
      "train accuracy on epoch 1483: 0.944\n",
      "test loss on epoch 1483: 0.365\n",
      "test accuracy on epoch 1483: 0.769\n",
      "train loss on epoch 1484 : 0.194\n",
      "train accuracy on epoch 1484: 0.889\n",
      "test loss on epoch 1484: 0.377\n",
      "test accuracy on epoch 1484: 0.769\n",
      "train loss on epoch 1485 : 0.071\n",
      "train accuracy on epoch 1485: 1.000\n",
      "test loss on epoch 1485: 0.376\n",
      "test accuracy on epoch 1485: 0.769\n",
      "train loss on epoch 1486 : 0.387\n",
      "train accuracy on epoch 1486: 0.889\n",
      "test loss on epoch 1486: 0.366\n",
      "test accuracy on epoch 1486: 0.769\n",
      "train loss on epoch 1487 : 0.284\n",
      "train accuracy on epoch 1487: 0.944\n",
      "test loss on epoch 1487: 0.343\n",
      "test accuracy on epoch 1487: 0.769\n",
      "train loss on epoch 1488 : 0.239\n",
      "train accuracy on epoch 1488: 0.833\n",
      "test loss on epoch 1488: 0.341\n",
      "test accuracy on epoch 1488: 0.769\n",
      "train loss on epoch 1489 : 0.118\n",
      "train accuracy on epoch 1489: 0.944\n",
      "test loss on epoch 1489: 0.331\n",
      "test accuracy on epoch 1489: 0.769\n",
      "train loss on epoch 1490 : 0.168\n",
      "train accuracy on epoch 1490: 0.944\n",
      "test loss on epoch 1490: 0.331\n",
      "test accuracy on epoch 1490: 0.769\n",
      "train loss on epoch 1491 : 0.114\n",
      "train accuracy on epoch 1491: 0.889\n",
      "test loss on epoch 1491: 0.328\n",
      "test accuracy on epoch 1491: 0.769\n",
      "train loss on epoch 1492 : 0.050\n",
      "train accuracy on epoch 1492: 1.000\n",
      "test loss on epoch 1492: 0.330\n",
      "test accuracy on epoch 1492: 0.769\n",
      "train loss on epoch 1493 : 0.155\n",
      "train accuracy on epoch 1493: 0.944\n",
      "test loss on epoch 1493: 0.333\n",
      "test accuracy on epoch 1493: 0.769\n",
      "train loss on epoch 1494 : 0.361\n",
      "train accuracy on epoch 1494: 0.889\n",
      "test loss on epoch 1494: 0.341\n",
      "test accuracy on epoch 1494: 0.769\n",
      "train loss on epoch 1495 : 0.222\n",
      "train accuracy on epoch 1495: 0.833\n",
      "test loss on epoch 1495: 0.339\n",
      "test accuracy on epoch 1495: 0.769\n",
      "train loss on epoch 1496 : 0.135\n",
      "train accuracy on epoch 1496: 0.944\n",
      "test loss on epoch 1496: 0.340\n",
      "test accuracy on epoch 1496: 0.769\n",
      "train loss on epoch 1497 : 0.119\n",
      "train accuracy on epoch 1497: 0.944\n",
      "test loss on epoch 1497: 0.334\n",
      "test accuracy on epoch 1497: 0.769\n",
      "train loss on epoch 1498 : 0.280\n",
      "train accuracy on epoch 1498: 0.889\n",
      "test loss on epoch 1498: 0.340\n",
      "test accuracy on epoch 1498: 0.769\n",
      "train loss on epoch 1499 : 0.053\n",
      "train accuracy on epoch 1499: 1.000\n",
      "test loss on epoch 1499: 0.340\n",
      "test accuracy on epoch 1499: 0.769\n",
      "train loss on epoch 1500 : 0.279\n",
      "train accuracy on epoch 1500: 0.889\n",
      "test loss on epoch 1500: 0.337\n",
      "test accuracy on epoch 1500: 0.769\n",
      "train loss on epoch 1501 : 0.221\n",
      "train accuracy on epoch 1501: 0.889\n",
      "test loss on epoch 1501: 0.337\n",
      "test accuracy on epoch 1501: 0.769\n",
      "train loss on epoch 1502 : 0.399\n",
      "train accuracy on epoch 1502: 0.833\n",
      "test loss on epoch 1502: 0.330\n",
      "test accuracy on epoch 1502: 0.769\n",
      "train loss on epoch 1503 : 0.288\n",
      "train accuracy on epoch 1503: 0.889\n",
      "test loss on epoch 1503: 0.330\n",
      "test accuracy on epoch 1503: 0.769\n",
      "train loss on epoch 1504 : 0.471\n",
      "train accuracy on epoch 1504: 0.833\n",
      "test loss on epoch 1504: 0.341\n",
      "test accuracy on epoch 1504: 0.769\n",
      "train loss on epoch 1505 : 0.287\n",
      "train accuracy on epoch 1505: 0.944\n",
      "test loss on epoch 1505: 0.336\n",
      "test accuracy on epoch 1505: 0.769\n",
      "train loss on epoch 1506 : 0.333\n",
      "train accuracy on epoch 1506: 0.889\n",
      "test loss on epoch 1506: 0.339\n",
      "test accuracy on epoch 1506: 0.769\n",
      "train loss on epoch 1507 : 0.421\n",
      "train accuracy on epoch 1507: 0.889\n",
      "test loss on epoch 1507: 0.337\n",
      "test accuracy on epoch 1507: 0.769\n",
      "train loss on epoch 1508 : 0.147\n",
      "train accuracy on epoch 1508: 0.944\n",
      "test loss on epoch 1508: 0.332\n",
      "test accuracy on epoch 1508: 0.769\n",
      "train loss on epoch 1509 : 0.154\n",
      "train accuracy on epoch 1509: 0.944\n",
      "test loss on epoch 1509: 0.328\n",
      "test accuracy on epoch 1509: 0.769\n",
      "train loss on epoch 1510 : 0.365\n",
      "train accuracy on epoch 1510: 0.833\n",
      "test loss on epoch 1510: 0.326\n",
      "test accuracy on epoch 1510: 0.769\n",
      "train loss on epoch 1511 : 0.095\n",
      "train accuracy on epoch 1511: 0.944\n",
      "test loss on epoch 1511: 0.328\n",
      "test accuracy on epoch 1511: 0.769\n",
      "train loss on epoch 1512 : 0.202\n",
      "train accuracy on epoch 1512: 0.889\n",
      "test loss on epoch 1512: 0.330\n",
      "test accuracy on epoch 1512: 0.769\n",
      "train loss on epoch 1513 : 0.220\n",
      "train accuracy on epoch 1513: 0.833\n",
      "test loss on epoch 1513: 0.333\n",
      "test accuracy on epoch 1513: 0.769\n",
      "train loss on epoch 1514 : 0.257\n",
      "train accuracy on epoch 1514: 0.889\n",
      "test loss on epoch 1514: 0.340\n",
      "test accuracy on epoch 1514: 0.769\n",
      "train loss on epoch 1515 : 0.174\n",
      "train accuracy on epoch 1515: 0.889\n",
      "test loss on epoch 1515: 0.349\n",
      "test accuracy on epoch 1515: 0.769\n",
      "train loss on epoch 1516 : 0.081\n",
      "train accuracy on epoch 1516: 1.000\n",
      "test loss on epoch 1516: 0.352\n",
      "test accuracy on epoch 1516: 0.769\n",
      "train loss on epoch 1517 : 0.257\n",
      "train accuracy on epoch 1517: 0.944\n",
      "test loss on epoch 1517: 0.347\n",
      "test accuracy on epoch 1517: 0.769\n",
      "train loss on epoch 1518 : 0.204\n",
      "train accuracy on epoch 1518: 0.944\n",
      "test loss on epoch 1518: 0.346\n",
      "test accuracy on epoch 1518: 0.769\n",
      "train loss on epoch 1519 : 0.229\n",
      "train accuracy on epoch 1519: 0.889\n",
      "test loss on epoch 1519: 0.337\n",
      "test accuracy on epoch 1519: 0.769\n",
      "train loss on epoch 1520 : 0.038\n",
      "train accuracy on epoch 1520: 1.000\n",
      "test loss on epoch 1520: 0.331\n",
      "test accuracy on epoch 1520: 0.769\n",
      "train loss on epoch 1521 : 0.253\n",
      "train accuracy on epoch 1521: 0.889\n",
      "test loss on epoch 1521: 0.324\n",
      "test accuracy on epoch 1521: 0.769\n",
      "train loss on epoch 1522 : 0.315\n",
      "train accuracy on epoch 1522: 0.944\n",
      "test loss on epoch 1522: 0.324\n",
      "test accuracy on epoch 1522: 0.769\n",
      "train loss on epoch 1523 : 0.049\n",
      "train accuracy on epoch 1523: 1.000\n",
      "test loss on epoch 1523: 0.316\n",
      "test accuracy on epoch 1523: 0.769\n",
      "train loss on epoch 1524 : 0.332\n",
      "train accuracy on epoch 1524: 0.944\n",
      "test loss on epoch 1524: 0.318\n",
      "test accuracy on epoch 1524: 0.769\n",
      "train loss on epoch 1525 : 0.187\n",
      "train accuracy on epoch 1525: 0.833\n",
      "test loss on epoch 1525: 0.324\n",
      "test accuracy on epoch 1525: 0.692\n",
      "train loss on epoch 1526 : 0.244\n",
      "train accuracy on epoch 1526: 0.944\n",
      "test loss on epoch 1526: 0.322\n",
      "test accuracy on epoch 1526: 0.769\n",
      "train loss on epoch 1527 : 0.085\n",
      "train accuracy on epoch 1527: 0.944\n",
      "test loss on epoch 1527: 0.314\n",
      "test accuracy on epoch 1527: 0.769\n",
      "train loss on epoch 1528 : 0.359\n",
      "train accuracy on epoch 1528: 0.833\n",
      "test loss on epoch 1528: 0.326\n",
      "test accuracy on epoch 1528: 0.769\n",
      "train loss on epoch 1529 : 0.138\n",
      "train accuracy on epoch 1529: 0.944\n",
      "test loss on epoch 1529: 0.319\n",
      "test accuracy on epoch 1529: 0.769\n",
      "train loss on epoch 1530 : 0.121\n",
      "train accuracy on epoch 1530: 0.944\n",
      "test loss on epoch 1530: 0.321\n",
      "test accuracy on epoch 1530: 0.769\n",
      "train loss on epoch 1531 : 0.284\n",
      "train accuracy on epoch 1531: 0.944\n",
      "test loss on epoch 1531: 0.325\n",
      "test accuracy on epoch 1531: 0.769\n",
      "train loss on epoch 1532 : 0.255\n",
      "train accuracy on epoch 1532: 0.944\n",
      "test loss on epoch 1532: 0.318\n",
      "test accuracy on epoch 1532: 0.769\n",
      "train loss on epoch 1533 : 0.129\n",
      "train accuracy on epoch 1533: 0.944\n",
      "test loss on epoch 1533: 0.320\n",
      "test accuracy on epoch 1533: 0.769\n",
      "train loss on epoch 1534 : 0.277\n",
      "train accuracy on epoch 1534: 0.889\n",
      "test loss on epoch 1534: 0.313\n",
      "test accuracy on epoch 1534: 0.769\n",
      "train loss on epoch 1535 : 0.128\n",
      "train accuracy on epoch 1535: 0.944\n",
      "test loss on epoch 1535: 0.311\n",
      "test accuracy on epoch 1535: 0.846\n",
      "train loss on epoch 1536 : 0.311\n",
      "train accuracy on epoch 1536: 0.889\n",
      "test loss on epoch 1536: 0.314\n",
      "test accuracy on epoch 1536: 0.769\n",
      "train loss on epoch 1537 : 0.240\n",
      "train accuracy on epoch 1537: 0.889\n",
      "test loss on epoch 1537: 0.315\n",
      "test accuracy on epoch 1537: 0.769\n",
      "train loss on epoch 1538 : 0.326\n",
      "train accuracy on epoch 1538: 0.944\n",
      "test loss on epoch 1538: 0.330\n",
      "test accuracy on epoch 1538: 0.769\n",
      "train loss on epoch 1539 : 0.185\n",
      "train accuracy on epoch 1539: 0.889\n",
      "test loss on epoch 1539: 0.326\n",
      "test accuracy on epoch 1539: 0.769\n",
      "train loss on epoch 1540 : 0.237\n",
      "train accuracy on epoch 1540: 0.944\n",
      "test loss on epoch 1540: 0.337\n",
      "test accuracy on epoch 1540: 0.769\n",
      "train loss on epoch 1541 : 0.096\n",
      "train accuracy on epoch 1541: 0.944\n",
      "test loss on epoch 1541: 0.337\n",
      "test accuracy on epoch 1541: 0.769\n",
      "train loss on epoch 1542 : 0.112\n",
      "train accuracy on epoch 1542: 0.944\n",
      "test loss on epoch 1542: 0.348\n",
      "test accuracy on epoch 1542: 0.769\n",
      "train loss on epoch 1543 : 0.071\n",
      "train accuracy on epoch 1543: 1.000\n",
      "test loss on epoch 1543: 0.349\n",
      "test accuracy on epoch 1543: 0.769\n",
      "train loss on epoch 1544 : 0.367\n",
      "train accuracy on epoch 1544: 0.889\n",
      "test loss on epoch 1544: 0.340\n",
      "test accuracy on epoch 1544: 0.769\n",
      "train loss on epoch 1545 : 0.129\n",
      "train accuracy on epoch 1545: 0.944\n",
      "test loss on epoch 1545: 0.331\n",
      "test accuracy on epoch 1545: 0.769\n",
      "train loss on epoch 1546 : 0.207\n",
      "train accuracy on epoch 1546: 0.944\n",
      "test loss on epoch 1546: 0.333\n",
      "test accuracy on epoch 1546: 0.769\n",
      "train loss on epoch 1547 : 0.068\n",
      "train accuracy on epoch 1547: 1.000\n",
      "test loss on epoch 1547: 0.323\n",
      "test accuracy on epoch 1547: 0.769\n",
      "train loss on epoch 1548 : 0.188\n",
      "train accuracy on epoch 1548: 0.889\n",
      "test loss on epoch 1548: 0.322\n",
      "test accuracy on epoch 1548: 0.769\n",
      "train loss on epoch 1549 : 0.225\n",
      "train accuracy on epoch 1549: 0.889\n",
      "test loss on epoch 1549: 0.317\n",
      "test accuracy on epoch 1549: 0.769\n",
      "train loss on epoch 1550 : 0.192\n",
      "train accuracy on epoch 1550: 1.000\n",
      "test loss on epoch 1550: 0.327\n",
      "test accuracy on epoch 1550: 0.692\n",
      "train loss on epoch 1551 : 0.173\n",
      "train accuracy on epoch 1551: 0.889\n",
      "test loss on epoch 1551: 0.320\n",
      "test accuracy on epoch 1551: 0.769\n",
      "train loss on epoch 1552 : 0.074\n",
      "train accuracy on epoch 1552: 1.000\n",
      "test loss on epoch 1552: 0.312\n",
      "test accuracy on epoch 1552: 0.769\n",
      "train loss on epoch 1553 : 0.197\n",
      "train accuracy on epoch 1553: 0.944\n",
      "test loss on epoch 1553: 0.309\n",
      "test accuracy on epoch 1553: 0.769\n",
      "train loss on epoch 1554 : 0.094\n",
      "train accuracy on epoch 1554: 1.000\n",
      "test loss on epoch 1554: 0.327\n",
      "test accuracy on epoch 1554: 0.769\n",
      "train loss on epoch 1555 : 0.122\n",
      "train accuracy on epoch 1555: 0.944\n",
      "test loss on epoch 1555: 0.324\n",
      "test accuracy on epoch 1555: 0.769\n",
      "train loss on epoch 1556 : 0.161\n",
      "train accuracy on epoch 1556: 0.889\n",
      "test loss on epoch 1556: 0.318\n",
      "test accuracy on epoch 1556: 0.769\n",
      "train loss on epoch 1557 : 0.137\n",
      "train accuracy on epoch 1557: 0.944\n",
      "test loss on epoch 1557: 0.316\n",
      "test accuracy on epoch 1557: 0.769\n",
      "train loss on epoch 1558 : 0.228\n",
      "train accuracy on epoch 1558: 0.944\n",
      "test loss on epoch 1558: 0.323\n",
      "test accuracy on epoch 1558: 0.769\n",
      "train loss on epoch 1559 : 0.054\n",
      "train accuracy on epoch 1559: 1.000\n",
      "test loss on epoch 1559: 0.311\n",
      "test accuracy on epoch 1559: 0.769\n",
      "train loss on epoch 1560 : 0.372\n",
      "train accuracy on epoch 1560: 0.944\n",
      "test loss on epoch 1560: 0.320\n",
      "test accuracy on epoch 1560: 0.769\n",
      "train loss on epoch 1561 : 0.121\n",
      "train accuracy on epoch 1561: 0.889\n",
      "test loss on epoch 1561: 0.323\n",
      "test accuracy on epoch 1561: 0.769\n",
      "train loss on epoch 1562 : 0.105\n",
      "train accuracy on epoch 1562: 0.944\n",
      "test loss on epoch 1562: 0.331\n",
      "test accuracy on epoch 1562: 0.769\n",
      "train loss on epoch 1563 : 0.253\n",
      "train accuracy on epoch 1563: 0.889\n",
      "test loss on epoch 1563: 0.325\n",
      "test accuracy on epoch 1563: 0.769\n",
      "train loss on epoch 1564 : 0.147\n",
      "train accuracy on epoch 1564: 0.944\n",
      "test loss on epoch 1564: 0.334\n",
      "test accuracy on epoch 1564: 0.769\n",
      "train loss on epoch 1565 : 0.127\n",
      "train accuracy on epoch 1565: 0.944\n",
      "test loss on epoch 1565: 0.333\n",
      "test accuracy on epoch 1565: 0.769\n",
      "train loss on epoch 1566 : 0.137\n",
      "train accuracy on epoch 1566: 0.944\n",
      "test loss on epoch 1566: 0.324\n",
      "test accuracy on epoch 1566: 0.769\n",
      "train loss on epoch 1567 : 0.236\n",
      "train accuracy on epoch 1567: 0.833\n",
      "test loss on epoch 1567: 0.317\n",
      "test accuracy on epoch 1567: 0.846\n",
      "train loss on epoch 1568 : 0.343\n",
      "train accuracy on epoch 1568: 0.778\n",
      "test loss on epoch 1568: 0.318\n",
      "test accuracy on epoch 1568: 0.846\n",
      "train loss on epoch 1569 : 0.352\n",
      "train accuracy on epoch 1569: 0.889\n",
      "test loss on epoch 1569: 0.326\n",
      "test accuracy on epoch 1569: 0.769\n",
      "train loss on epoch 1570 : 0.137\n",
      "train accuracy on epoch 1570: 0.889\n",
      "test loss on epoch 1570: 0.332\n",
      "test accuracy on epoch 1570: 0.769\n",
      "train loss on epoch 1571 : 0.302\n",
      "train accuracy on epoch 1571: 0.889\n",
      "test loss on epoch 1571: 0.344\n",
      "test accuracy on epoch 1571: 0.769\n",
      "train loss on epoch 1572 : 0.104\n",
      "train accuracy on epoch 1572: 0.944\n",
      "test loss on epoch 1572: 0.338\n",
      "test accuracy on epoch 1572: 0.769\n",
      "train loss on epoch 1573 : 0.073\n",
      "train accuracy on epoch 1573: 1.000\n",
      "test loss on epoch 1573: 0.337\n",
      "test accuracy on epoch 1573: 0.769\n",
      "train loss on epoch 1574 : 0.238\n",
      "train accuracy on epoch 1574: 0.944\n",
      "test loss on epoch 1574: 0.334\n",
      "test accuracy on epoch 1574: 0.769\n",
      "train loss on epoch 1575 : 0.084\n",
      "train accuracy on epoch 1575: 1.000\n",
      "test loss on epoch 1575: 0.348\n",
      "test accuracy on epoch 1575: 0.769\n",
      "train loss on epoch 1576 : 0.131\n",
      "train accuracy on epoch 1576: 0.944\n",
      "test loss on epoch 1576: 0.349\n",
      "test accuracy on epoch 1576: 0.769\n",
      "train loss on epoch 1577 : 0.197\n",
      "train accuracy on epoch 1577: 0.944\n",
      "test loss on epoch 1577: 0.328\n",
      "test accuracy on epoch 1577: 0.769\n",
      "train loss on epoch 1578 : 0.112\n",
      "train accuracy on epoch 1578: 0.944\n",
      "test loss on epoch 1578: 0.339\n",
      "test accuracy on epoch 1578: 0.769\n",
      "train loss on epoch 1579 : 0.191\n",
      "train accuracy on epoch 1579: 0.944\n",
      "test loss on epoch 1579: 0.331\n",
      "test accuracy on epoch 1579: 0.769\n",
      "train loss on epoch 1580 : 0.228\n",
      "train accuracy on epoch 1580: 0.944\n",
      "test loss on epoch 1580: 0.347\n",
      "test accuracy on epoch 1580: 0.769\n",
      "train loss on epoch 1581 : 0.218\n",
      "train accuracy on epoch 1581: 0.944\n",
      "test loss on epoch 1581: 0.348\n",
      "test accuracy on epoch 1581: 0.769\n",
      "train loss on epoch 1582 : 0.315\n",
      "train accuracy on epoch 1582: 0.889\n",
      "test loss on epoch 1582: 0.352\n",
      "test accuracy on epoch 1582: 0.769\n",
      "train loss on epoch 1583 : 0.114\n",
      "train accuracy on epoch 1583: 0.944\n",
      "test loss on epoch 1583: 0.354\n",
      "test accuracy on epoch 1583: 0.769\n",
      "train loss on epoch 1584 : 0.216\n",
      "train accuracy on epoch 1584: 0.944\n",
      "test loss on epoch 1584: 0.344\n",
      "test accuracy on epoch 1584: 0.769\n",
      "train loss on epoch 1585 : 0.080\n",
      "train accuracy on epoch 1585: 1.000\n",
      "test loss on epoch 1585: 0.349\n",
      "test accuracy on epoch 1585: 0.769\n",
      "train loss on epoch 1586 : 0.170\n",
      "train accuracy on epoch 1586: 0.944\n",
      "test loss on epoch 1586: 0.360\n",
      "test accuracy on epoch 1586: 0.769\n",
      "train loss on epoch 1587 : 0.079\n",
      "train accuracy on epoch 1587: 0.944\n",
      "test loss on epoch 1587: 0.356\n",
      "test accuracy on epoch 1587: 0.769\n",
      "train loss on epoch 1588 : 0.256\n",
      "train accuracy on epoch 1588: 0.944\n",
      "test loss on epoch 1588: 0.352\n",
      "test accuracy on epoch 1588: 0.769\n",
      "train loss on epoch 1589 : 0.221\n",
      "train accuracy on epoch 1589: 0.833\n",
      "test loss on epoch 1589: 0.359\n",
      "test accuracy on epoch 1589: 0.769\n",
      "train loss on epoch 1590 : 0.346\n",
      "train accuracy on epoch 1590: 0.889\n",
      "test loss on epoch 1590: 0.339\n",
      "test accuracy on epoch 1590: 0.769\n",
      "train loss on epoch 1591 : 0.144\n",
      "train accuracy on epoch 1591: 0.944\n",
      "test loss on epoch 1591: 0.346\n",
      "test accuracy on epoch 1591: 0.769\n",
      "train loss on epoch 1592 : 0.297\n",
      "train accuracy on epoch 1592: 0.889\n",
      "test loss on epoch 1592: 0.353\n",
      "test accuracy on epoch 1592: 0.769\n",
      "train loss on epoch 1593 : 0.214\n",
      "train accuracy on epoch 1593: 0.944\n",
      "test loss on epoch 1593: 0.351\n",
      "test accuracy on epoch 1593: 0.769\n",
      "train loss on epoch 1594 : 0.259\n",
      "train accuracy on epoch 1594: 0.889\n",
      "test loss on epoch 1594: 0.346\n",
      "test accuracy on epoch 1594: 0.769\n",
      "train loss on epoch 1595 : 0.102\n",
      "train accuracy on epoch 1595: 0.944\n",
      "test loss on epoch 1595: 0.350\n",
      "test accuracy on epoch 1595: 0.769\n",
      "train loss on epoch 1596 : 0.172\n",
      "train accuracy on epoch 1596: 0.944\n",
      "test loss on epoch 1596: 0.355\n",
      "test accuracy on epoch 1596: 0.769\n",
      "train loss on epoch 1597 : 0.212\n",
      "train accuracy on epoch 1597: 0.944\n",
      "test loss on epoch 1597: 0.357\n",
      "test accuracy on epoch 1597: 0.769\n",
      "train loss on epoch 1598 : 0.370\n",
      "train accuracy on epoch 1598: 0.833\n",
      "test loss on epoch 1598: 0.364\n",
      "test accuracy on epoch 1598: 0.769\n",
      "train loss on epoch 1599 : 0.066\n",
      "train accuracy on epoch 1599: 1.000\n",
      "test loss on epoch 1599: 0.372\n",
      "test accuracy on epoch 1599: 0.769\n",
      "train loss on epoch 1600 : 0.232\n",
      "train accuracy on epoch 1600: 0.889\n",
      "test loss on epoch 1600: 0.371\n",
      "test accuracy on epoch 1600: 0.769\n",
      "train loss on epoch 1601 : 0.097\n",
      "train accuracy on epoch 1601: 0.944\n",
      "test loss on epoch 1601: 0.374\n",
      "test accuracy on epoch 1601: 0.769\n",
      "train loss on epoch 1602 : 0.105\n",
      "train accuracy on epoch 1602: 0.944\n",
      "test loss on epoch 1602: 0.369\n",
      "test accuracy on epoch 1602: 0.769\n",
      "train loss on epoch 1603 : 0.160\n",
      "train accuracy on epoch 1603: 0.944\n",
      "test loss on epoch 1603: 0.358\n",
      "test accuracy on epoch 1603: 0.769\n",
      "train loss on epoch 1604 : 0.083\n",
      "train accuracy on epoch 1604: 1.000\n",
      "test loss on epoch 1604: 0.356\n",
      "test accuracy on epoch 1604: 0.769\n",
      "train loss on epoch 1605 : 0.078\n",
      "train accuracy on epoch 1605: 1.000\n",
      "test loss on epoch 1605: 0.357\n",
      "test accuracy on epoch 1605: 0.769\n",
      "train loss on epoch 1606 : 0.119\n",
      "train accuracy on epoch 1606: 0.944\n",
      "test loss on epoch 1606: 0.345\n",
      "test accuracy on epoch 1606: 0.769\n",
      "train loss on epoch 1607 : 0.061\n",
      "train accuracy on epoch 1607: 1.000\n",
      "test loss on epoch 1607: 0.353\n",
      "test accuracy on epoch 1607: 0.769\n",
      "train loss on epoch 1608 : 0.208\n",
      "train accuracy on epoch 1608: 0.889\n",
      "test loss on epoch 1608: 0.347\n",
      "test accuracy on epoch 1608: 0.769\n",
      "train loss on epoch 1609 : 0.195\n",
      "train accuracy on epoch 1609: 0.944\n",
      "test loss on epoch 1609: 0.346\n",
      "test accuracy on epoch 1609: 0.769\n",
      "train loss on epoch 1610 : 0.213\n",
      "train accuracy on epoch 1610: 0.889\n",
      "test loss on epoch 1610: 0.351\n",
      "test accuracy on epoch 1610: 0.769\n",
      "train loss on epoch 1611 : 0.181\n",
      "train accuracy on epoch 1611: 0.944\n",
      "test loss on epoch 1611: 0.353\n",
      "test accuracy on epoch 1611: 0.769\n",
      "train loss on epoch 1612 : 0.335\n",
      "train accuracy on epoch 1612: 0.833\n",
      "test loss on epoch 1612: 0.360\n",
      "test accuracy on epoch 1612: 0.769\n",
      "train loss on epoch 1613 : 0.387\n",
      "train accuracy on epoch 1613: 0.944\n",
      "test loss on epoch 1613: 0.351\n",
      "test accuracy on epoch 1613: 0.769\n",
      "train loss on epoch 1614 : 0.106\n",
      "train accuracy on epoch 1614: 0.944\n",
      "test loss on epoch 1614: 0.345\n",
      "test accuracy on epoch 1614: 0.769\n",
      "train loss on epoch 1615 : 0.117\n",
      "train accuracy on epoch 1615: 0.944\n",
      "test loss on epoch 1615: 0.349\n",
      "test accuracy on epoch 1615: 0.769\n",
      "train loss on epoch 1616 : 0.234\n",
      "train accuracy on epoch 1616: 0.944\n",
      "test loss on epoch 1616: 0.345\n",
      "test accuracy on epoch 1616: 0.769\n",
      "train loss on epoch 1617 : 0.353\n",
      "train accuracy on epoch 1617: 0.833\n",
      "test loss on epoch 1617: 0.345\n",
      "test accuracy on epoch 1617: 0.769\n",
      "train loss on epoch 1618 : 0.116\n",
      "train accuracy on epoch 1618: 1.000\n",
      "test loss on epoch 1618: 0.351\n",
      "test accuracy on epoch 1618: 0.769\n",
      "train loss on epoch 1619 : 0.160\n",
      "train accuracy on epoch 1619: 0.944\n",
      "test loss on epoch 1619: 0.341\n",
      "test accuracy on epoch 1619: 0.769\n",
      "train loss on epoch 1620 : 0.146\n",
      "train accuracy on epoch 1620: 0.889\n",
      "test loss on epoch 1620: 0.340\n",
      "test accuracy on epoch 1620: 0.769\n",
      "train loss on epoch 1621 : 0.330\n",
      "train accuracy on epoch 1621: 0.889\n",
      "test loss on epoch 1621: 0.338\n",
      "test accuracy on epoch 1621: 0.769\n",
      "train loss on epoch 1622 : 0.202\n",
      "train accuracy on epoch 1622: 0.944\n",
      "test loss on epoch 1622: 0.336\n",
      "test accuracy on epoch 1622: 0.769\n",
      "train loss on epoch 1623 : 0.216\n",
      "train accuracy on epoch 1623: 0.944\n",
      "test loss on epoch 1623: 0.354\n",
      "test accuracy on epoch 1623: 0.769\n",
      "train loss on epoch 1624 : 0.280\n",
      "train accuracy on epoch 1624: 0.889\n",
      "test loss on epoch 1624: 0.355\n",
      "test accuracy on epoch 1624: 0.769\n",
      "train loss on epoch 1625 : 0.104\n",
      "train accuracy on epoch 1625: 0.944\n",
      "test loss on epoch 1625: 0.337\n",
      "test accuracy on epoch 1625: 0.846\n",
      "train loss on epoch 1626 : 0.200\n",
      "train accuracy on epoch 1626: 0.889\n",
      "test loss on epoch 1626: 0.355\n",
      "test accuracy on epoch 1626: 0.692\n",
      "train loss on epoch 1627 : 0.127\n",
      "train accuracy on epoch 1627: 0.944\n",
      "test loss on epoch 1627: 0.354\n",
      "test accuracy on epoch 1627: 0.692\n",
      "train loss on epoch 1628 : 0.067\n",
      "train accuracy on epoch 1628: 1.000\n",
      "test loss on epoch 1628: 0.348\n",
      "test accuracy on epoch 1628: 0.769\n",
      "train loss on epoch 1629 : 0.303\n",
      "train accuracy on epoch 1629: 0.944\n",
      "test loss on epoch 1629: 0.349\n",
      "test accuracy on epoch 1629: 0.769\n",
      "train loss on epoch 1630 : 0.142\n",
      "train accuracy on epoch 1630: 0.944\n",
      "test loss on epoch 1630: 0.347\n",
      "test accuracy on epoch 1630: 0.769\n",
      "train loss on epoch 1631 : 0.218\n",
      "train accuracy on epoch 1631: 0.889\n",
      "test loss on epoch 1631: 0.353\n",
      "test accuracy on epoch 1631: 0.769\n",
      "train loss on epoch 1632 : 0.267\n",
      "train accuracy on epoch 1632: 0.944\n",
      "test loss on epoch 1632: 0.362\n",
      "test accuracy on epoch 1632: 0.769\n",
      "train loss on epoch 1633 : 0.121\n",
      "train accuracy on epoch 1633: 0.944\n",
      "test loss on epoch 1633: 0.364\n",
      "test accuracy on epoch 1633: 0.769\n",
      "train loss on epoch 1634 : 0.271\n",
      "train accuracy on epoch 1634: 0.889\n",
      "test loss on epoch 1634: 0.348\n",
      "test accuracy on epoch 1634: 0.769\n",
      "train loss on epoch 1635 : 0.314\n",
      "train accuracy on epoch 1635: 0.778\n",
      "test loss on epoch 1635: 0.349\n",
      "test accuracy on epoch 1635: 0.769\n",
      "train loss on epoch 1636 : 0.067\n",
      "train accuracy on epoch 1636: 1.000\n",
      "test loss on epoch 1636: 0.362\n",
      "test accuracy on epoch 1636: 0.769\n",
      "train loss on epoch 1637 : 0.218\n",
      "train accuracy on epoch 1637: 0.944\n",
      "test loss on epoch 1637: 0.362\n",
      "test accuracy on epoch 1637: 0.769\n",
      "train loss on epoch 1638 : 0.320\n",
      "train accuracy on epoch 1638: 0.833\n",
      "test loss on epoch 1638: 0.355\n",
      "test accuracy on epoch 1638: 0.769\n",
      "train loss on epoch 1639 : 0.168\n",
      "train accuracy on epoch 1639: 0.889\n",
      "test loss on epoch 1639: 0.365\n",
      "test accuracy on epoch 1639: 0.769\n",
      "train loss on epoch 1640 : 0.129\n",
      "train accuracy on epoch 1640: 0.944\n",
      "test loss on epoch 1640: 0.350\n",
      "test accuracy on epoch 1640: 0.769\n",
      "train loss on epoch 1641 : 0.132\n",
      "train accuracy on epoch 1641: 0.944\n",
      "test loss on epoch 1641: 0.350\n",
      "test accuracy on epoch 1641: 0.769\n",
      "train loss on epoch 1642 : 0.350\n",
      "train accuracy on epoch 1642: 0.833\n",
      "test loss on epoch 1642: 0.340\n",
      "test accuracy on epoch 1642: 0.769\n",
      "train loss on epoch 1643 : 0.058\n",
      "train accuracy on epoch 1643: 1.000\n",
      "test loss on epoch 1643: 0.357\n",
      "test accuracy on epoch 1643: 0.769\n",
      "train loss on epoch 1644 : 0.178\n",
      "train accuracy on epoch 1644: 0.889\n",
      "test loss on epoch 1644: 0.355\n",
      "test accuracy on epoch 1644: 0.769\n",
      "train loss on epoch 1645 : 0.202\n",
      "train accuracy on epoch 1645: 0.889\n",
      "test loss on epoch 1645: 0.368\n",
      "test accuracy on epoch 1645: 0.692\n",
      "train loss on epoch 1646 : 0.365\n",
      "train accuracy on epoch 1646: 0.778\n",
      "test loss on epoch 1646: 0.371\n",
      "test accuracy on epoch 1646: 0.769\n",
      "train loss on epoch 1647 : 0.103\n",
      "train accuracy on epoch 1647: 0.944\n",
      "test loss on epoch 1647: 0.370\n",
      "test accuracy on epoch 1647: 0.769\n",
      "train loss on epoch 1648 : 0.222\n",
      "train accuracy on epoch 1648: 0.944\n",
      "test loss on epoch 1648: 0.347\n",
      "test accuracy on epoch 1648: 0.769\n",
      "train loss on epoch 1649 : 0.144\n",
      "train accuracy on epoch 1649: 0.944\n",
      "test loss on epoch 1649: 0.365\n",
      "test accuracy on epoch 1649: 0.769\n",
      "train loss on epoch 1650 : 0.295\n",
      "train accuracy on epoch 1650: 0.944\n",
      "test loss on epoch 1650: 0.352\n",
      "test accuracy on epoch 1650: 0.769\n",
      "train loss on epoch 1651 : 0.217\n",
      "train accuracy on epoch 1651: 0.833\n",
      "test loss on epoch 1651: 0.352\n",
      "test accuracy on epoch 1651: 0.769\n",
      "train loss on epoch 1652 : 0.044\n",
      "train accuracy on epoch 1652: 1.000\n",
      "test loss on epoch 1652: 0.349\n",
      "test accuracy on epoch 1652: 0.769\n",
      "train loss on epoch 1653 : 0.083\n",
      "train accuracy on epoch 1653: 1.000\n",
      "test loss on epoch 1653: 0.339\n",
      "test accuracy on epoch 1653: 0.769\n",
      "train loss on epoch 1654 : 0.224\n",
      "train accuracy on epoch 1654: 0.944\n",
      "test loss on epoch 1654: 0.339\n",
      "test accuracy on epoch 1654: 0.846\n",
      "train loss on epoch 1655 : 0.088\n",
      "train accuracy on epoch 1655: 0.944\n",
      "test loss on epoch 1655: 0.364\n",
      "test accuracy on epoch 1655: 0.692\n",
      "train loss on epoch 1656 : 0.145\n",
      "train accuracy on epoch 1656: 0.944\n",
      "test loss on epoch 1656: 0.364\n",
      "test accuracy on epoch 1656: 0.692\n",
      "train loss on epoch 1657 : 0.216\n",
      "train accuracy on epoch 1657: 0.944\n",
      "test loss on epoch 1657: 0.349\n",
      "test accuracy on epoch 1657: 0.769\n",
      "train loss on epoch 1658 : 0.121\n",
      "train accuracy on epoch 1658: 0.944\n",
      "test loss on epoch 1658: 0.351\n",
      "test accuracy on epoch 1658: 0.769\n",
      "train loss on epoch 1659 : 0.209\n",
      "train accuracy on epoch 1659: 0.889\n",
      "test loss on epoch 1659: 0.356\n",
      "test accuracy on epoch 1659: 0.769\n",
      "train loss on epoch 1660 : 0.242\n",
      "train accuracy on epoch 1660: 0.944\n",
      "test loss on epoch 1660: 0.368\n",
      "test accuracy on epoch 1660: 0.692\n",
      "train loss on epoch 1661 : 0.086\n",
      "train accuracy on epoch 1661: 1.000\n",
      "test loss on epoch 1661: 0.370\n",
      "test accuracy on epoch 1661: 0.692\n",
      "train loss on epoch 1662 : 0.136\n",
      "train accuracy on epoch 1662: 0.889\n",
      "test loss on epoch 1662: 0.358\n",
      "test accuracy on epoch 1662: 0.769\n",
      "train loss on epoch 1663 : 0.122\n",
      "train accuracy on epoch 1663: 0.889\n",
      "test loss on epoch 1663: 0.371\n",
      "test accuracy on epoch 1663: 0.692\n",
      "train loss on epoch 1664 : 0.146\n",
      "train accuracy on epoch 1664: 0.944\n",
      "test loss on epoch 1664: 0.351\n",
      "test accuracy on epoch 1664: 0.769\n",
      "train loss on epoch 1665 : 0.159\n",
      "train accuracy on epoch 1665: 0.889\n",
      "test loss on epoch 1665: 0.353\n",
      "test accuracy on epoch 1665: 0.769\n",
      "train loss on epoch 1666 : 0.431\n",
      "train accuracy on epoch 1666: 0.833\n",
      "test loss on epoch 1666: 0.372\n",
      "test accuracy on epoch 1666: 0.769\n",
      "train loss on epoch 1667 : 0.176\n",
      "train accuracy on epoch 1667: 0.944\n",
      "test loss on epoch 1667: 0.365\n",
      "test accuracy on epoch 1667: 0.769\n",
      "train loss on epoch 1668 : 0.283\n",
      "train accuracy on epoch 1668: 0.833\n",
      "test loss on epoch 1668: 0.361\n",
      "test accuracy on epoch 1668: 0.769\n",
      "train loss on epoch 1669 : 0.196\n",
      "train accuracy on epoch 1669: 0.944\n",
      "test loss on epoch 1669: 0.377\n",
      "test accuracy on epoch 1669: 0.769\n",
      "train loss on epoch 1670 : 0.254\n",
      "train accuracy on epoch 1670: 0.833\n",
      "test loss on epoch 1670: 0.352\n",
      "test accuracy on epoch 1670: 0.769\n",
      "train loss on epoch 1671 : 0.070\n",
      "train accuracy on epoch 1671: 1.000\n",
      "test loss on epoch 1671: 0.373\n",
      "test accuracy on epoch 1671: 0.769\n",
      "train loss on epoch 1672 : 0.112\n",
      "train accuracy on epoch 1672: 1.000\n",
      "test loss on epoch 1672: 0.373\n",
      "test accuracy on epoch 1672: 0.692\n",
      "train loss on epoch 1673 : 0.168\n",
      "train accuracy on epoch 1673: 0.944\n",
      "test loss on epoch 1673: 0.361\n",
      "test accuracy on epoch 1673: 0.769\n",
      "train loss on epoch 1674 : 0.158\n",
      "train accuracy on epoch 1674: 0.944\n",
      "test loss on epoch 1674: 0.346\n",
      "test accuracy on epoch 1674: 0.846\n",
      "train loss on epoch 1675 : 0.346\n",
      "train accuracy on epoch 1675: 0.944\n",
      "test loss on epoch 1675: 0.367\n",
      "test accuracy on epoch 1675: 0.769\n",
      "train loss on epoch 1676 : 0.328\n",
      "train accuracy on epoch 1676: 0.833\n",
      "test loss on epoch 1676: 0.359\n",
      "test accuracy on epoch 1676: 0.769\n",
      "train loss on epoch 1677 : 0.300\n",
      "train accuracy on epoch 1677: 0.889\n",
      "test loss on epoch 1677: 0.372\n",
      "test accuracy on epoch 1677: 0.769\n",
      "train loss on epoch 1678 : 0.424\n",
      "train accuracy on epoch 1678: 0.889\n",
      "test loss on epoch 1678: 0.359\n",
      "test accuracy on epoch 1678: 0.769\n",
      "train loss on epoch 1679 : 0.069\n",
      "train accuracy on epoch 1679: 0.944\n",
      "test loss on epoch 1679: 0.342\n",
      "test accuracy on epoch 1679: 0.769\n",
      "train loss on epoch 1680 : 0.170\n",
      "train accuracy on epoch 1680: 0.889\n",
      "test loss on epoch 1680: 0.367\n",
      "test accuracy on epoch 1680: 0.769\n",
      "train loss on epoch 1681 : 0.130\n",
      "train accuracy on epoch 1681: 0.944\n",
      "test loss on epoch 1681: 0.350\n",
      "test accuracy on epoch 1681: 0.769\n",
      "train loss on epoch 1682 : 0.097\n",
      "train accuracy on epoch 1682: 0.944\n",
      "test loss on epoch 1682: 0.340\n",
      "test accuracy on epoch 1682: 0.769\n",
      "train loss on epoch 1683 : 0.119\n",
      "train accuracy on epoch 1683: 0.944\n",
      "test loss on epoch 1683: 0.365\n",
      "test accuracy on epoch 1683: 0.769\n",
      "train loss on epoch 1684 : 0.156\n",
      "train accuracy on epoch 1684: 0.944\n",
      "test loss on epoch 1684: 0.350\n",
      "test accuracy on epoch 1684: 0.769\n",
      "train loss on epoch 1685 : 0.357\n",
      "train accuracy on epoch 1685: 0.889\n",
      "test loss on epoch 1685: 0.365\n",
      "test accuracy on epoch 1685: 0.769\n",
      "train loss on epoch 1686 : 0.106\n",
      "train accuracy on epoch 1686: 0.889\n",
      "test loss on epoch 1686: 0.340\n",
      "test accuracy on epoch 1686: 0.769\n",
      "train loss on epoch 1687 : 0.143\n",
      "train accuracy on epoch 1687: 0.944\n",
      "test loss on epoch 1687: 0.351\n",
      "test accuracy on epoch 1687: 0.769\n",
      "train loss on epoch 1688 : 0.242\n",
      "train accuracy on epoch 1688: 0.944\n",
      "test loss on epoch 1688: 0.340\n",
      "test accuracy on epoch 1688: 0.846\n",
      "train loss on epoch 1689 : 0.065\n",
      "train accuracy on epoch 1689: 1.000\n",
      "test loss on epoch 1689: 0.342\n",
      "test accuracy on epoch 1689: 0.846\n",
      "train loss on epoch 1690 : 0.075\n",
      "train accuracy on epoch 1690: 1.000\n",
      "test loss on epoch 1690: 0.341\n",
      "test accuracy on epoch 1690: 0.846\n",
      "train loss on epoch 1691 : 0.164\n",
      "train accuracy on epoch 1691: 0.889\n",
      "test loss on epoch 1691: 0.343\n",
      "test accuracy on epoch 1691: 0.846\n",
      "train loss on epoch 1692 : 0.181\n",
      "train accuracy on epoch 1692: 0.889\n",
      "test loss on epoch 1692: 0.353\n",
      "test accuracy on epoch 1692: 0.769\n",
      "train loss on epoch 1693 : 0.147\n",
      "train accuracy on epoch 1693: 0.889\n",
      "test loss on epoch 1693: 0.341\n",
      "test accuracy on epoch 1693: 0.846\n",
      "train loss on epoch 1694 : 0.107\n",
      "train accuracy on epoch 1694: 1.000\n",
      "test loss on epoch 1694: 0.339\n",
      "test accuracy on epoch 1694: 0.769\n",
      "train loss on epoch 1695 : 0.219\n",
      "train accuracy on epoch 1695: 0.889\n",
      "test loss on epoch 1695: 0.366\n",
      "test accuracy on epoch 1695: 0.769\n",
      "train loss on epoch 1696 : 0.121\n",
      "train accuracy on epoch 1696: 0.944\n",
      "test loss on epoch 1696: 0.350\n",
      "test accuracy on epoch 1696: 0.769\n",
      "train loss on epoch 1697 : 0.047\n",
      "train accuracy on epoch 1697: 1.000\n",
      "test loss on epoch 1697: 0.340\n",
      "test accuracy on epoch 1697: 0.769\n",
      "train loss on epoch 1698 : 0.092\n",
      "train accuracy on epoch 1698: 0.944\n",
      "test loss on epoch 1698: 0.365\n",
      "test accuracy on epoch 1698: 0.769\n",
      "train loss on epoch 1699 : 0.364\n",
      "train accuracy on epoch 1699: 0.944\n",
      "test loss on epoch 1699: 0.364\n",
      "test accuracy on epoch 1699: 0.692\n",
      "train loss on epoch 1700 : 0.090\n",
      "train accuracy on epoch 1700: 0.944\n",
      "test loss on epoch 1700: 0.355\n",
      "test accuracy on epoch 1700: 0.769\n",
      "train loss on epoch 1701 : 0.091\n",
      "train accuracy on epoch 1701: 0.944\n",
      "test loss on epoch 1701: 0.367\n",
      "test accuracy on epoch 1701: 0.769\n",
      "train loss on epoch 1702 : 0.066\n",
      "train accuracy on epoch 1702: 1.000\n",
      "test loss on epoch 1702: 0.360\n",
      "test accuracy on epoch 1702: 0.769\n",
      "train loss on epoch 1703 : 0.263\n",
      "train accuracy on epoch 1703: 0.944\n",
      "test loss on epoch 1703: 0.349\n",
      "test accuracy on epoch 1703: 0.769\n",
      "train loss on epoch 1704 : 0.376\n",
      "train accuracy on epoch 1704: 0.833\n",
      "test loss on epoch 1704: 0.346\n",
      "test accuracy on epoch 1704: 0.846\n",
      "train loss on epoch 1705 : 0.150\n",
      "train accuracy on epoch 1705: 0.944\n",
      "test loss on epoch 1705: 0.355\n",
      "test accuracy on epoch 1705: 0.769\n",
      "train loss on epoch 1706 : 0.197\n",
      "train accuracy on epoch 1706: 0.889\n",
      "test loss on epoch 1706: 0.340\n",
      "test accuracy on epoch 1706: 0.769\n",
      "train loss on epoch 1707 : 0.046\n",
      "train accuracy on epoch 1707: 1.000\n",
      "test loss on epoch 1707: 0.368\n",
      "test accuracy on epoch 1707: 0.769\n",
      "train loss on epoch 1708 : 0.083\n",
      "train accuracy on epoch 1708: 1.000\n",
      "test loss on epoch 1708: 0.343\n",
      "test accuracy on epoch 1708: 0.769\n",
      "train loss on epoch 1709 : 0.089\n",
      "train accuracy on epoch 1709: 0.944\n",
      "test loss on epoch 1709: 0.352\n",
      "test accuracy on epoch 1709: 0.769\n",
      "train loss on epoch 1710 : 0.293\n",
      "train accuracy on epoch 1710: 0.944\n",
      "test loss on epoch 1710: 0.357\n",
      "test accuracy on epoch 1710: 0.769\n",
      "train loss on epoch 1711 : 0.197\n",
      "train accuracy on epoch 1711: 0.944\n",
      "test loss on epoch 1711: 0.356\n",
      "test accuracy on epoch 1711: 0.769\n",
      "train loss on epoch 1712 : 0.089\n",
      "train accuracy on epoch 1712: 0.944\n",
      "test loss on epoch 1712: 0.366\n",
      "test accuracy on epoch 1712: 0.769\n",
      "train loss on epoch 1713 : 0.084\n",
      "train accuracy on epoch 1713: 0.944\n",
      "test loss on epoch 1713: 0.368\n",
      "test accuracy on epoch 1713: 0.769\n",
      "train loss on epoch 1714 : 0.051\n",
      "train accuracy on epoch 1714: 1.000\n",
      "test loss on epoch 1714: 0.369\n",
      "test accuracy on epoch 1714: 0.769\n",
      "train loss on epoch 1715 : 0.148\n",
      "train accuracy on epoch 1715: 0.944\n",
      "test loss on epoch 1715: 0.343\n",
      "test accuracy on epoch 1715: 0.769\n",
      "train loss on epoch 1716 : 0.107\n",
      "train accuracy on epoch 1716: 0.889\n",
      "test loss on epoch 1716: 0.340\n",
      "test accuracy on epoch 1716: 0.769\n",
      "train loss on epoch 1717 : 0.162\n",
      "train accuracy on epoch 1717: 0.889\n",
      "test loss on epoch 1717: 0.355\n",
      "test accuracy on epoch 1717: 0.769\n",
      "train loss on epoch 1718 : 0.018\n",
      "train accuracy on epoch 1718: 1.000\n",
      "test loss on epoch 1718: 0.354\n",
      "test accuracy on epoch 1718: 0.769\n",
      "train loss on epoch 1719 : 0.151\n",
      "train accuracy on epoch 1719: 0.944\n",
      "test loss on epoch 1719: 0.350\n",
      "test accuracy on epoch 1719: 0.769\n",
      "train loss on epoch 1720 : 0.164\n",
      "train accuracy on epoch 1720: 0.944\n",
      "test loss on epoch 1720: 0.337\n",
      "test accuracy on epoch 1720: 0.846\n",
      "train loss on epoch 1721 : 0.213\n",
      "train accuracy on epoch 1721: 0.944\n",
      "test loss on epoch 1721: 0.347\n",
      "test accuracy on epoch 1721: 0.769\n",
      "train loss on epoch 1722 : 0.179\n",
      "train accuracy on epoch 1722: 0.944\n",
      "test loss on epoch 1722: 0.336\n",
      "test accuracy on epoch 1722: 0.769\n",
      "train loss on epoch 1723 : 0.131\n",
      "train accuracy on epoch 1723: 0.889\n",
      "test loss on epoch 1723: 0.363\n",
      "test accuracy on epoch 1723: 0.769\n",
      "train loss on epoch 1724 : 0.133\n",
      "train accuracy on epoch 1724: 0.889\n",
      "test loss on epoch 1724: 0.336\n",
      "test accuracy on epoch 1724: 0.769\n",
      "train loss on epoch 1725 : 0.284\n",
      "train accuracy on epoch 1725: 0.889\n",
      "test loss on epoch 1725: 0.336\n",
      "test accuracy on epoch 1725: 0.769\n",
      "train loss on epoch 1726 : 0.099\n",
      "train accuracy on epoch 1726: 0.944\n",
      "test loss on epoch 1726: 0.360\n",
      "test accuracy on epoch 1726: 0.769\n",
      "train loss on epoch 1727 : 0.126\n",
      "train accuracy on epoch 1727: 0.944\n",
      "test loss on epoch 1727: 0.336\n",
      "test accuracy on epoch 1727: 0.769\n",
      "train loss on epoch 1728 : 0.084\n",
      "train accuracy on epoch 1728: 0.944\n",
      "test loss on epoch 1728: 0.339\n",
      "test accuracy on epoch 1728: 0.846\n",
      "train loss on epoch 1729 : 0.273\n",
      "train accuracy on epoch 1729: 0.889\n",
      "test loss on epoch 1729: 0.339\n",
      "test accuracy on epoch 1729: 0.846\n",
      "train loss on epoch 1730 : 0.137\n",
      "train accuracy on epoch 1730: 0.944\n",
      "test loss on epoch 1730: 0.353\n",
      "test accuracy on epoch 1730: 0.769\n",
      "train loss on epoch 1731 : 0.110\n",
      "train accuracy on epoch 1731: 0.889\n",
      "test loss on epoch 1731: 0.345\n",
      "test accuracy on epoch 1731: 0.769\n",
      "train loss on epoch 1732 : 0.301\n",
      "train accuracy on epoch 1732: 0.778\n",
      "test loss on epoch 1732: 0.360\n",
      "test accuracy on epoch 1732: 0.769\n",
      "train loss on epoch 1733 : 0.091\n",
      "train accuracy on epoch 1733: 1.000\n",
      "test loss on epoch 1733: 0.356\n",
      "test accuracy on epoch 1733: 0.769\n",
      "train loss on epoch 1734 : 0.096\n",
      "train accuracy on epoch 1734: 0.944\n",
      "test loss on epoch 1734: 0.352\n",
      "test accuracy on epoch 1734: 0.769\n",
      "train loss on epoch 1735 : 0.328\n",
      "train accuracy on epoch 1735: 0.889\n",
      "test loss on epoch 1735: 0.356\n",
      "test accuracy on epoch 1735: 0.769\n",
      "train loss on epoch 1736 : 0.274\n",
      "train accuracy on epoch 1736: 0.889\n",
      "test loss on epoch 1736: 0.342\n",
      "test accuracy on epoch 1736: 0.769\n",
      "train loss on epoch 1737 : 0.347\n",
      "train accuracy on epoch 1737: 0.889\n",
      "test loss on epoch 1737: 0.340\n",
      "test accuracy on epoch 1737: 0.769\n",
      "train loss on epoch 1738 : 0.326\n",
      "train accuracy on epoch 1738: 0.889\n",
      "test loss on epoch 1738: 0.339\n",
      "test accuracy on epoch 1738: 0.769\n",
      "train loss on epoch 1739 : 0.055\n",
      "train accuracy on epoch 1739: 1.000\n",
      "test loss on epoch 1739: 0.343\n",
      "test accuracy on epoch 1739: 0.769\n",
      "train loss on epoch 1740 : 0.262\n",
      "train accuracy on epoch 1740: 0.833\n",
      "test loss on epoch 1740: 0.346\n",
      "test accuracy on epoch 1740: 0.769\n",
      "train loss on epoch 1741 : 0.072\n",
      "train accuracy on epoch 1741: 1.000\n",
      "test loss on epoch 1741: 0.366\n",
      "test accuracy on epoch 1741: 0.769\n",
      "train loss on epoch 1742 : 0.220\n",
      "train accuracy on epoch 1742: 0.944\n",
      "test loss on epoch 1742: 0.346\n",
      "test accuracy on epoch 1742: 0.769\n",
      "train loss on epoch 1743 : 0.286\n",
      "train accuracy on epoch 1743: 0.889\n",
      "test loss on epoch 1743: 0.369\n",
      "test accuracy on epoch 1743: 0.769\n",
      "train loss on epoch 1744 : 0.385\n",
      "train accuracy on epoch 1744: 0.833\n",
      "test loss on epoch 1744: 0.367\n",
      "test accuracy on epoch 1744: 0.769\n",
      "train loss on epoch 1745 : 0.224\n",
      "train accuracy on epoch 1745: 0.889\n",
      "test loss on epoch 1745: 0.366\n",
      "test accuracy on epoch 1745: 0.692\n",
      "train loss on epoch 1746 : 0.149\n",
      "train accuracy on epoch 1746: 0.889\n",
      "test loss on epoch 1746: 0.344\n",
      "test accuracy on epoch 1746: 0.846\n",
      "train loss on epoch 1747 : 0.171\n",
      "train accuracy on epoch 1747: 0.944\n",
      "test loss on epoch 1747: 0.346\n",
      "test accuracy on epoch 1747: 0.769\n",
      "train loss on epoch 1748 : 0.042\n",
      "train accuracy on epoch 1748: 1.000\n",
      "test loss on epoch 1748: 0.348\n",
      "test accuracy on epoch 1748: 0.769\n",
      "train loss on epoch 1749 : 0.102\n",
      "train accuracy on epoch 1749: 1.000\n",
      "test loss on epoch 1749: 0.364\n",
      "test accuracy on epoch 1749: 0.769\n",
      "train loss on epoch 1750 : 0.137\n",
      "train accuracy on epoch 1750: 0.944\n",
      "test loss on epoch 1750: 0.367\n",
      "test accuracy on epoch 1750: 0.769\n",
      "train loss on epoch 1751 : 0.205\n",
      "train accuracy on epoch 1751: 0.889\n",
      "test loss on epoch 1751: 0.366\n",
      "test accuracy on epoch 1751: 0.769\n",
      "train loss on epoch 1752 : 0.180\n",
      "train accuracy on epoch 1752: 0.889\n",
      "test loss on epoch 1752: 0.350\n",
      "test accuracy on epoch 1752: 0.769\n",
      "train loss on epoch 1753 : 0.137\n",
      "train accuracy on epoch 1753: 0.944\n",
      "test loss on epoch 1753: 0.367\n",
      "test accuracy on epoch 1753: 0.769\n",
      "train loss on epoch 1754 : 0.429\n",
      "train accuracy on epoch 1754: 0.778\n",
      "test loss on epoch 1754: 0.348\n",
      "test accuracy on epoch 1754: 0.769\n",
      "train loss on epoch 1755 : 0.187\n",
      "train accuracy on epoch 1755: 0.889\n",
      "test loss on epoch 1755: 0.349\n",
      "test accuracy on epoch 1755: 0.769\n",
      "train loss on epoch 1756 : 0.094\n",
      "train accuracy on epoch 1756: 1.000\n",
      "test loss on epoch 1756: 0.355\n",
      "test accuracy on epoch 1756: 0.769\n",
      "train loss on epoch 1757 : 0.149\n",
      "train accuracy on epoch 1757: 0.889\n",
      "test loss on epoch 1757: 0.370\n",
      "test accuracy on epoch 1757: 0.769\n",
      "train loss on epoch 1758 : 0.128\n",
      "train accuracy on epoch 1758: 0.944\n",
      "test loss on epoch 1758: 0.367\n",
      "test accuracy on epoch 1758: 0.769\n",
      "train loss on epoch 1759 : 0.054\n",
      "train accuracy on epoch 1759: 1.000\n",
      "test loss on epoch 1759: 0.347\n",
      "test accuracy on epoch 1759: 0.769\n",
      "train loss on epoch 1760 : 0.020\n",
      "train accuracy on epoch 1760: 1.000\n",
      "test loss on epoch 1760: 0.358\n",
      "test accuracy on epoch 1760: 0.769\n",
      "train loss on epoch 1761 : 0.157\n",
      "train accuracy on epoch 1761: 0.944\n",
      "test loss on epoch 1761: 0.344\n",
      "test accuracy on epoch 1761: 0.769\n",
      "train loss on epoch 1762 : 0.088\n",
      "train accuracy on epoch 1762: 1.000\n",
      "test loss on epoch 1762: 0.345\n",
      "test accuracy on epoch 1762: 0.769\n",
      "train loss on epoch 1763 : 0.087\n",
      "train accuracy on epoch 1763: 0.944\n",
      "test loss on epoch 1763: 0.354\n",
      "test accuracy on epoch 1763: 0.769\n",
      "train loss on epoch 1764 : 0.333\n",
      "train accuracy on epoch 1764: 0.889\n",
      "test loss on epoch 1764: 0.349\n",
      "test accuracy on epoch 1764: 0.769\n",
      "train loss on epoch 1765 : 0.164\n",
      "train accuracy on epoch 1765: 0.944\n",
      "test loss on epoch 1765: 0.351\n",
      "test accuracy on epoch 1765: 0.769\n",
      "train loss on epoch 1766 : 0.092\n",
      "train accuracy on epoch 1766: 0.944\n",
      "test loss on epoch 1766: 0.357\n",
      "test accuracy on epoch 1766: 0.769\n",
      "train loss on epoch 1767 : 0.248\n",
      "train accuracy on epoch 1767: 0.944\n",
      "test loss on epoch 1767: 0.363\n",
      "test accuracy on epoch 1767: 0.769\n",
      "train loss on epoch 1768 : 0.164\n",
      "train accuracy on epoch 1768: 0.944\n",
      "test loss on epoch 1768: 0.346\n",
      "test accuracy on epoch 1768: 0.769\n",
      "train loss on epoch 1769 : 0.326\n",
      "train accuracy on epoch 1769: 0.889\n",
      "test loss on epoch 1769: 0.353\n",
      "test accuracy on epoch 1769: 0.769\n",
      "train loss on epoch 1770 : 0.127\n",
      "train accuracy on epoch 1770: 0.944\n",
      "test loss on epoch 1770: 0.346\n",
      "test accuracy on epoch 1770: 0.769\n",
      "train loss on epoch 1771 : 0.044\n",
      "train accuracy on epoch 1771: 1.000\n",
      "test loss on epoch 1771: 0.348\n",
      "test accuracy on epoch 1771: 0.769\n",
      "train loss on epoch 1772 : 0.053\n",
      "train accuracy on epoch 1772: 1.000\n",
      "test loss on epoch 1772: 0.357\n",
      "test accuracy on epoch 1772: 0.769\n",
      "train loss on epoch 1773 : 0.207\n",
      "train accuracy on epoch 1773: 0.944\n",
      "test loss on epoch 1773: 0.358\n",
      "test accuracy on epoch 1773: 0.769\n",
      "train loss on epoch 1774 : 0.153\n",
      "train accuracy on epoch 1774: 0.944\n",
      "test loss on epoch 1774: 0.360\n",
      "test accuracy on epoch 1774: 0.769\n",
      "train loss on epoch 1775 : 0.275\n",
      "train accuracy on epoch 1775: 0.889\n",
      "test loss on epoch 1775: 0.360\n",
      "test accuracy on epoch 1775: 0.769\n",
      "train loss on epoch 1776 : 0.205\n",
      "train accuracy on epoch 1776: 0.889\n",
      "test loss on epoch 1776: 0.355\n",
      "test accuracy on epoch 1776: 0.769\n",
      "train loss on epoch 1777 : 0.276\n",
      "train accuracy on epoch 1777: 0.944\n",
      "test loss on epoch 1777: 0.345\n",
      "test accuracy on epoch 1777: 0.769\n",
      "train loss on epoch 1778 : 0.150\n",
      "train accuracy on epoch 1778: 0.944\n",
      "test loss on epoch 1778: 0.347\n",
      "test accuracy on epoch 1778: 0.769\n",
      "train loss on epoch 1779 : 0.053\n",
      "train accuracy on epoch 1779: 1.000\n",
      "test loss on epoch 1779: 0.358\n",
      "test accuracy on epoch 1779: 0.769\n",
      "train loss on epoch 1780 : 0.419\n",
      "train accuracy on epoch 1780: 0.889\n",
      "test loss on epoch 1780: 0.343\n",
      "test accuracy on epoch 1780: 0.769\n",
      "train loss on epoch 1781 : 0.458\n",
      "train accuracy on epoch 1781: 0.833\n",
      "test loss on epoch 1781: 0.349\n",
      "test accuracy on epoch 1781: 0.769\n",
      "train loss on epoch 1782 : 0.361\n",
      "train accuracy on epoch 1782: 0.944\n",
      "test loss on epoch 1782: 0.340\n",
      "test accuracy on epoch 1782: 0.769\n",
      "train loss on epoch 1783 : 0.244\n",
      "train accuracy on epoch 1783: 0.889\n",
      "test loss on epoch 1783: 0.352\n",
      "test accuracy on epoch 1783: 0.769\n",
      "train loss on epoch 1784 : 0.226\n",
      "train accuracy on epoch 1784: 0.889\n",
      "test loss on epoch 1784: 0.347\n",
      "test accuracy on epoch 1784: 0.769\n",
      "train loss on epoch 1785 : 0.142\n",
      "train accuracy on epoch 1785: 0.944\n",
      "test loss on epoch 1785: 0.341\n",
      "test accuracy on epoch 1785: 0.846\n",
      "train loss on epoch 1786 : 0.155\n",
      "train accuracy on epoch 1786: 0.944\n",
      "test loss on epoch 1786: 0.349\n",
      "test accuracy on epoch 1786: 0.769\n",
      "train loss on epoch 1787 : 0.177\n",
      "train accuracy on epoch 1787: 0.889\n",
      "test loss on epoch 1787: 0.353\n",
      "test accuracy on epoch 1787: 0.692\n",
      "train loss on epoch 1788 : 0.132\n",
      "train accuracy on epoch 1788: 0.944\n",
      "test loss on epoch 1788: 0.350\n",
      "test accuracy on epoch 1788: 0.769\n",
      "train loss on epoch 1789 : 0.233\n",
      "train accuracy on epoch 1789: 0.944\n",
      "test loss on epoch 1789: 0.358\n",
      "test accuracy on epoch 1789: 0.692\n",
      "train loss on epoch 1790 : 0.335\n",
      "train accuracy on epoch 1790: 0.778\n",
      "test loss on epoch 1790: 0.360\n",
      "test accuracy on epoch 1790: 0.692\n",
      "train loss on epoch 1791 : 0.096\n",
      "train accuracy on epoch 1791: 0.944\n",
      "test loss on epoch 1791: 0.338\n",
      "test accuracy on epoch 1791: 0.846\n",
      "train loss on epoch 1792 : 0.481\n",
      "train accuracy on epoch 1792: 0.889\n",
      "test loss on epoch 1792: 0.355\n",
      "test accuracy on epoch 1792: 0.692\n",
      "train loss on epoch 1793 : 0.249\n",
      "train accuracy on epoch 1793: 0.889\n",
      "test loss on epoch 1793: 0.351\n",
      "test accuracy on epoch 1793: 0.769\n",
      "train loss on epoch 1794 : 0.274\n",
      "train accuracy on epoch 1794: 0.889\n",
      "test loss on epoch 1794: 0.344\n",
      "test accuracy on epoch 1794: 0.769\n",
      "train loss on epoch 1795 : 0.344\n",
      "train accuracy on epoch 1795: 0.889\n",
      "test loss on epoch 1795: 0.350\n",
      "test accuracy on epoch 1795: 0.769\n",
      "train loss on epoch 1796 : 0.269\n",
      "train accuracy on epoch 1796: 0.889\n",
      "test loss on epoch 1796: 0.351\n",
      "test accuracy on epoch 1796: 0.769\n",
      "train loss on epoch 1797 : 0.096\n",
      "train accuracy on epoch 1797: 0.944\n",
      "test loss on epoch 1797: 0.364\n",
      "test accuracy on epoch 1797: 0.769\n",
      "train loss on epoch 1798 : 0.039\n",
      "train accuracy on epoch 1798: 1.000\n",
      "test loss on epoch 1798: 0.355\n",
      "test accuracy on epoch 1798: 0.769\n",
      "train loss on epoch 1799 : 0.306\n",
      "train accuracy on epoch 1799: 0.889\n",
      "test loss on epoch 1799: 0.361\n",
      "test accuracy on epoch 1799: 0.769\n",
      "train loss on epoch 1800 : 0.468\n",
      "train accuracy on epoch 1800: 0.833\n",
      "test loss on epoch 1800: 0.348\n",
      "test accuracy on epoch 1800: 0.769\n",
      "train loss on epoch 1801 : 0.152\n",
      "train accuracy on epoch 1801: 0.889\n",
      "test loss on epoch 1801: 0.355\n",
      "test accuracy on epoch 1801: 0.692\n",
      "train loss on epoch 1802 : 0.239\n",
      "train accuracy on epoch 1802: 0.944\n",
      "test loss on epoch 1802: 0.338\n",
      "test accuracy on epoch 1802: 0.769\n",
      "train loss on epoch 1803 : 0.130\n",
      "train accuracy on epoch 1803: 0.944\n",
      "test loss on epoch 1803: 0.338\n",
      "test accuracy on epoch 1803: 0.769\n",
      "train loss on epoch 1804 : 0.134\n",
      "train accuracy on epoch 1804: 0.944\n",
      "test loss on epoch 1804: 0.352\n",
      "test accuracy on epoch 1804: 0.769\n",
      "train loss on epoch 1805 : 0.209\n",
      "train accuracy on epoch 1805: 0.944\n",
      "test loss on epoch 1805: 0.361\n",
      "test accuracy on epoch 1805: 0.769\n",
      "train loss on epoch 1806 : 0.138\n",
      "train accuracy on epoch 1806: 0.944\n",
      "test loss on epoch 1806: 0.354\n",
      "test accuracy on epoch 1806: 0.769\n",
      "train loss on epoch 1807 : 0.094\n",
      "train accuracy on epoch 1807: 1.000\n",
      "test loss on epoch 1807: 0.345\n",
      "test accuracy on epoch 1807: 0.769\n",
      "train loss on epoch 1808 : 0.163\n",
      "train accuracy on epoch 1808: 0.944\n",
      "test loss on epoch 1808: 0.346\n",
      "test accuracy on epoch 1808: 0.769\n",
      "train loss on epoch 1809 : 0.268\n",
      "train accuracy on epoch 1809: 0.833\n",
      "test loss on epoch 1809: 0.342\n",
      "test accuracy on epoch 1809: 0.769\n",
      "train loss on epoch 1810 : 0.125\n",
      "train accuracy on epoch 1810: 1.000\n",
      "test loss on epoch 1810: 0.359\n",
      "test accuracy on epoch 1810: 0.769\n",
      "train loss on epoch 1811 : 0.193\n",
      "train accuracy on epoch 1811: 0.889\n",
      "test loss on epoch 1811: 0.357\n",
      "test accuracy on epoch 1811: 0.769\n",
      "train loss on epoch 1812 : 0.208\n",
      "train accuracy on epoch 1812: 0.944\n",
      "test loss on epoch 1812: 0.358\n",
      "test accuracy on epoch 1812: 0.769\n",
      "train loss on epoch 1813 : 0.233\n",
      "train accuracy on epoch 1813: 0.889\n",
      "test loss on epoch 1813: 0.357\n",
      "test accuracy on epoch 1813: 0.769\n",
      "train loss on epoch 1814 : 0.385\n",
      "train accuracy on epoch 1814: 0.889\n",
      "test loss on epoch 1814: 0.354\n",
      "test accuracy on epoch 1814: 0.769\n",
      "train loss on epoch 1815 : 0.225\n",
      "train accuracy on epoch 1815: 0.889\n",
      "test loss on epoch 1815: 0.347\n",
      "test accuracy on epoch 1815: 0.769\n",
      "train loss on epoch 1816 : 0.137\n",
      "train accuracy on epoch 1816: 0.944\n",
      "test loss on epoch 1816: 0.352\n",
      "test accuracy on epoch 1816: 0.769\n",
      "train loss on epoch 1817 : 0.165\n",
      "train accuracy on epoch 1817: 0.944\n",
      "test loss on epoch 1817: 0.357\n",
      "test accuracy on epoch 1817: 0.692\n",
      "train loss on epoch 1818 : 0.150\n",
      "train accuracy on epoch 1818: 0.944\n",
      "test loss on epoch 1818: 0.356\n",
      "test accuracy on epoch 1818: 0.692\n",
      "train loss on epoch 1819 : 0.096\n",
      "train accuracy on epoch 1819: 0.944\n",
      "test loss on epoch 1819: 0.345\n",
      "test accuracy on epoch 1819: 0.769\n",
      "train loss on epoch 1820 : 0.255\n",
      "train accuracy on epoch 1820: 0.889\n",
      "test loss on epoch 1820: 0.352\n",
      "test accuracy on epoch 1820: 0.692\n",
      "train loss on epoch 1821 : 0.232\n",
      "train accuracy on epoch 1821: 0.833\n",
      "test loss on epoch 1821: 0.346\n",
      "test accuracy on epoch 1821: 0.769\n",
      "train loss on epoch 1822 : 0.122\n",
      "train accuracy on epoch 1822: 0.944\n",
      "test loss on epoch 1822: 0.342\n",
      "test accuracy on epoch 1822: 0.769\n",
      "train loss on epoch 1823 : 0.052\n",
      "train accuracy on epoch 1823: 1.000\n",
      "test loss on epoch 1823: 0.358\n",
      "test accuracy on epoch 1823: 0.769\n",
      "train loss on epoch 1824 : 0.170\n",
      "train accuracy on epoch 1824: 0.944\n",
      "test loss on epoch 1824: 0.360\n",
      "test accuracy on epoch 1824: 0.769\n",
      "train loss on epoch 1825 : 0.082\n",
      "train accuracy on epoch 1825: 1.000\n",
      "test loss on epoch 1825: 0.366\n",
      "test accuracy on epoch 1825: 0.769\n",
      "train loss on epoch 1826 : 0.090\n",
      "train accuracy on epoch 1826: 0.944\n",
      "test loss on epoch 1826: 0.365\n",
      "test accuracy on epoch 1826: 0.769\n",
      "train loss on epoch 1827 : 0.235\n",
      "train accuracy on epoch 1827: 0.944\n",
      "test loss on epoch 1827: 0.358\n",
      "test accuracy on epoch 1827: 0.769\n",
      "train loss on epoch 1828 : 0.068\n",
      "train accuracy on epoch 1828: 0.944\n",
      "test loss on epoch 1828: 0.370\n",
      "test accuracy on epoch 1828: 0.769\n",
      "train loss on epoch 1829 : 0.289\n",
      "train accuracy on epoch 1829: 0.889\n",
      "test loss on epoch 1829: 0.357\n",
      "test accuracy on epoch 1829: 0.769\n",
      "train loss on epoch 1830 : 0.183\n",
      "train accuracy on epoch 1830: 0.889\n",
      "test loss on epoch 1830: 0.367\n",
      "test accuracy on epoch 1830: 0.769\n",
      "train loss on epoch 1831 : 0.182\n",
      "train accuracy on epoch 1831: 0.889\n",
      "test loss on epoch 1831: 0.367\n",
      "test accuracy on epoch 1831: 0.769\n",
      "train loss on epoch 1832 : 0.045\n",
      "train accuracy on epoch 1832: 1.000\n",
      "test loss on epoch 1832: 0.350\n",
      "test accuracy on epoch 1832: 0.769\n",
      "train loss on epoch 1833 : 0.175\n",
      "train accuracy on epoch 1833: 0.944\n",
      "test loss on epoch 1833: 0.348\n",
      "test accuracy on epoch 1833: 0.769\n",
      "train loss on epoch 1834 : 0.178\n",
      "train accuracy on epoch 1834: 0.889\n",
      "test loss on epoch 1834: 0.344\n",
      "test accuracy on epoch 1834: 0.769\n",
      "train loss on epoch 1835 : 0.192\n",
      "train accuracy on epoch 1835: 0.944\n",
      "test loss on epoch 1835: 0.348\n",
      "test accuracy on epoch 1835: 0.769\n",
      "train loss on epoch 1836 : 0.084\n",
      "train accuracy on epoch 1836: 0.944\n",
      "test loss on epoch 1836: 0.339\n",
      "test accuracy on epoch 1836: 0.846\n",
      "train loss on epoch 1837 : 0.092\n",
      "train accuracy on epoch 1837: 0.944\n",
      "test loss on epoch 1837: 0.352\n",
      "test accuracy on epoch 1837: 0.692\n",
      "train loss on epoch 1838 : 0.197\n",
      "train accuracy on epoch 1838: 0.944\n",
      "test loss on epoch 1838: 0.344\n",
      "test accuracy on epoch 1838: 0.769\n",
      "train loss on epoch 1839 : 0.087\n",
      "train accuracy on epoch 1839: 0.944\n",
      "test loss on epoch 1839: 0.353\n",
      "test accuracy on epoch 1839: 0.769\n",
      "train loss on epoch 1840 : 0.088\n",
      "train accuracy on epoch 1840: 0.944\n",
      "test loss on epoch 1840: 0.357\n",
      "test accuracy on epoch 1840: 0.769\n",
      "train loss on epoch 1841 : 0.235\n",
      "train accuracy on epoch 1841: 0.889\n",
      "test loss on epoch 1841: 0.361\n",
      "test accuracy on epoch 1841: 0.769\n",
      "train loss on epoch 1842 : 0.184\n",
      "train accuracy on epoch 1842: 0.889\n",
      "test loss on epoch 1842: 0.346\n",
      "test accuracy on epoch 1842: 0.769\n",
      "train loss on epoch 1843 : 0.150\n",
      "train accuracy on epoch 1843: 0.889\n",
      "test loss on epoch 1843: 0.349\n",
      "test accuracy on epoch 1843: 0.769\n",
      "train loss on epoch 1844 : 0.066\n",
      "train accuracy on epoch 1844: 1.000\n",
      "test loss on epoch 1844: 0.361\n",
      "test accuracy on epoch 1844: 0.769\n",
      "train loss on epoch 1845 : 0.048\n",
      "train accuracy on epoch 1845: 1.000\n",
      "test loss on epoch 1845: 0.356\n",
      "test accuracy on epoch 1845: 0.769\n",
      "train loss on epoch 1846 : 0.194\n",
      "train accuracy on epoch 1846: 0.944\n",
      "test loss on epoch 1846: 0.359\n",
      "test accuracy on epoch 1846: 0.769\n",
      "train loss on epoch 1847 : 0.240\n",
      "train accuracy on epoch 1847: 0.833\n",
      "test loss on epoch 1847: 0.357\n",
      "test accuracy on epoch 1847: 0.769\n",
      "train loss on epoch 1848 : 0.385\n",
      "train accuracy on epoch 1848: 0.889\n",
      "test loss on epoch 1848: 0.359\n",
      "test accuracy on epoch 1848: 0.769\n",
      "train loss on epoch 1849 : 0.060\n",
      "train accuracy on epoch 1849: 1.000\n",
      "test loss on epoch 1849: 0.367\n",
      "test accuracy on epoch 1849: 0.769\n",
      "train loss on epoch 1850 : 0.109\n",
      "train accuracy on epoch 1850: 0.944\n",
      "test loss on epoch 1850: 0.353\n",
      "test accuracy on epoch 1850: 0.769\n",
      "train loss on epoch 1851 : 0.242\n",
      "train accuracy on epoch 1851: 0.944\n",
      "test loss on epoch 1851: 0.366\n",
      "test accuracy on epoch 1851: 0.769\n",
      "train loss on epoch 1852 : 0.120\n",
      "train accuracy on epoch 1852: 0.944\n",
      "test loss on epoch 1852: 0.357\n",
      "test accuracy on epoch 1852: 0.769\n",
      "train loss on epoch 1853 : 0.131\n",
      "train accuracy on epoch 1853: 0.944\n",
      "test loss on epoch 1853: 0.354\n",
      "test accuracy on epoch 1853: 0.769\n",
      "train loss on epoch 1854 : 0.197\n",
      "train accuracy on epoch 1854: 0.889\n",
      "test loss on epoch 1854: 0.351\n",
      "test accuracy on epoch 1854: 0.769\n",
      "train loss on epoch 1855 : 0.332\n",
      "train accuracy on epoch 1855: 0.889\n",
      "test loss on epoch 1855: 0.363\n",
      "test accuracy on epoch 1855: 0.692\n",
      "train loss on epoch 1856 : 0.152\n",
      "train accuracy on epoch 1856: 0.944\n",
      "test loss on epoch 1856: 0.364\n",
      "test accuracy on epoch 1856: 0.769\n",
      "train loss on epoch 1857 : 0.113\n",
      "train accuracy on epoch 1857: 0.944\n",
      "test loss on epoch 1857: 0.340\n",
      "test accuracy on epoch 1857: 0.846\n",
      "train loss on epoch 1858 : 0.299\n",
      "train accuracy on epoch 1858: 0.889\n",
      "test loss on epoch 1858: 0.352\n",
      "test accuracy on epoch 1858: 0.769\n",
      "train loss on epoch 1859 : 0.090\n",
      "train accuracy on epoch 1859: 1.000\n",
      "test loss on epoch 1859: 0.354\n",
      "test accuracy on epoch 1859: 0.769\n",
      "train loss on epoch 1860 : 0.105\n",
      "train accuracy on epoch 1860: 0.944\n",
      "test loss on epoch 1860: 0.340\n",
      "test accuracy on epoch 1860: 0.846\n",
      "train loss on epoch 1861 : 0.106\n",
      "train accuracy on epoch 1861: 0.944\n",
      "test loss on epoch 1861: 0.344\n",
      "test accuracy on epoch 1861: 0.846\n",
      "train loss on epoch 1862 : 0.125\n",
      "train accuracy on epoch 1862: 0.944\n",
      "test loss on epoch 1862: 0.363\n",
      "test accuracy on epoch 1862: 0.692\n",
      "train loss on epoch 1863 : 0.254\n",
      "train accuracy on epoch 1863: 0.944\n",
      "test loss on epoch 1863: 0.365\n",
      "test accuracy on epoch 1863: 0.692\n",
      "train loss on epoch 1864 : 0.275\n",
      "train accuracy on epoch 1864: 0.889\n",
      "test loss on epoch 1864: 0.351\n",
      "test accuracy on epoch 1864: 0.769\n",
      "train loss on epoch 1865 : 0.276\n",
      "train accuracy on epoch 1865: 0.944\n",
      "test loss on epoch 1865: 0.371\n",
      "test accuracy on epoch 1865: 0.769\n",
      "train loss on epoch 1866 : 0.222\n",
      "train accuracy on epoch 1866: 0.889\n",
      "test loss on epoch 1866: 0.360\n",
      "test accuracy on epoch 1866: 0.769\n",
      "train loss on epoch 1867 : 0.210\n",
      "train accuracy on epoch 1867: 0.944\n",
      "test loss on epoch 1867: 0.376\n",
      "test accuracy on epoch 1867: 0.769\n",
      "train loss on epoch 1868 : 0.065\n",
      "train accuracy on epoch 1868: 1.000\n",
      "test loss on epoch 1868: 0.365\n",
      "test accuracy on epoch 1868: 0.769\n",
      "train loss on epoch 1869 : 0.138\n",
      "train accuracy on epoch 1869: 0.889\n",
      "test loss on epoch 1869: 0.371\n",
      "test accuracy on epoch 1869: 0.769\n",
      "train loss on epoch 1870 : 0.057\n",
      "train accuracy on epoch 1870: 1.000\n",
      "test loss on epoch 1870: 0.381\n",
      "test accuracy on epoch 1870: 0.769\n",
      "train loss on epoch 1871 : 0.176\n",
      "train accuracy on epoch 1871: 0.944\n",
      "test loss on epoch 1871: 0.376\n",
      "test accuracy on epoch 1871: 0.769\n",
      "train loss on epoch 1872 : 0.181\n",
      "train accuracy on epoch 1872: 0.833\n",
      "test loss on epoch 1872: 0.369\n",
      "test accuracy on epoch 1872: 0.769\n",
      "train loss on epoch 1873 : 0.239\n",
      "train accuracy on epoch 1873: 0.833\n",
      "test loss on epoch 1873: 0.382\n",
      "test accuracy on epoch 1873: 0.769\n",
      "train loss on epoch 1874 : 0.249\n",
      "train accuracy on epoch 1874: 0.889\n",
      "test loss on epoch 1874: 0.363\n",
      "test accuracy on epoch 1874: 0.769\n",
      "train loss on epoch 1875 : 0.046\n",
      "train accuracy on epoch 1875: 1.000\n",
      "test loss on epoch 1875: 0.380\n",
      "test accuracy on epoch 1875: 0.769\n",
      "train loss on epoch 1876 : 0.174\n",
      "train accuracy on epoch 1876: 0.944\n",
      "test loss on epoch 1876: 0.377\n",
      "test accuracy on epoch 1876: 0.769\n",
      "train loss on epoch 1877 : 0.188\n",
      "train accuracy on epoch 1877: 0.944\n",
      "test loss on epoch 1877: 0.373\n",
      "test accuracy on epoch 1877: 0.769\n",
      "train loss on epoch 1878 : 0.120\n",
      "train accuracy on epoch 1878: 1.000\n",
      "test loss on epoch 1878: 0.360\n",
      "test accuracy on epoch 1878: 0.769\n",
      "train loss on epoch 1879 : 0.183\n",
      "train accuracy on epoch 1879: 0.889\n",
      "test loss on epoch 1879: 0.356\n",
      "test accuracy on epoch 1879: 0.769\n",
      "train loss on epoch 1880 : 0.115\n",
      "train accuracy on epoch 1880: 1.000\n",
      "test loss on epoch 1880: 0.367\n",
      "test accuracy on epoch 1880: 0.769\n",
      "train loss on epoch 1881 : 0.238\n",
      "train accuracy on epoch 1881: 0.944\n",
      "test loss on epoch 1881: 0.368\n",
      "test accuracy on epoch 1881: 0.769\n",
      "train loss on epoch 1882 : 0.126\n",
      "train accuracy on epoch 1882: 0.944\n",
      "test loss on epoch 1882: 0.372\n",
      "test accuracy on epoch 1882: 0.769\n",
      "train loss on epoch 1883 : 0.397\n",
      "train accuracy on epoch 1883: 0.833\n",
      "test loss on epoch 1883: 0.351\n",
      "test accuracy on epoch 1883: 0.769\n",
      "train loss on epoch 1884 : 0.145\n",
      "train accuracy on epoch 1884: 0.889\n",
      "test loss on epoch 1884: 0.341\n",
      "test accuracy on epoch 1884: 0.769\n",
      "train loss on epoch 1885 : 0.322\n",
      "train accuracy on epoch 1885: 0.889\n",
      "test loss on epoch 1885: 0.365\n",
      "test accuracy on epoch 1885: 0.692\n",
      "train loss on epoch 1886 : 0.122\n",
      "train accuracy on epoch 1886: 0.944\n",
      "test loss on epoch 1886: 0.342\n",
      "test accuracy on epoch 1886: 0.846\n",
      "train loss on epoch 1887 : 0.080\n",
      "train accuracy on epoch 1887: 1.000\n",
      "test loss on epoch 1887: 0.368\n",
      "test accuracy on epoch 1887: 0.692\n",
      "train loss on epoch 1888 : 0.262\n",
      "train accuracy on epoch 1888: 0.889\n",
      "test loss on epoch 1888: 0.355\n",
      "test accuracy on epoch 1888: 0.769\n",
      "train loss on epoch 1889 : 0.136\n",
      "train accuracy on epoch 1889: 0.944\n",
      "test loss on epoch 1889: 0.358\n",
      "test accuracy on epoch 1889: 0.769\n",
      "train loss on epoch 1890 : 0.199\n",
      "train accuracy on epoch 1890: 0.889\n",
      "test loss on epoch 1890: 0.358\n",
      "test accuracy on epoch 1890: 0.769\n",
      "train loss on epoch 1891 : 0.264\n",
      "train accuracy on epoch 1891: 0.889\n",
      "test loss on epoch 1891: 0.347\n",
      "test accuracy on epoch 1891: 0.769\n",
      "train loss on epoch 1892 : 0.083\n",
      "train accuracy on epoch 1892: 0.944\n",
      "test loss on epoch 1892: 0.360\n",
      "test accuracy on epoch 1892: 0.769\n",
      "train loss on epoch 1893 : 0.135\n",
      "train accuracy on epoch 1893: 0.944\n",
      "test loss on epoch 1893: 0.348\n",
      "test accuracy on epoch 1893: 0.769\n",
      "train loss on epoch 1894 : 0.246\n",
      "train accuracy on epoch 1894: 0.944\n",
      "test loss on epoch 1894: 0.365\n",
      "test accuracy on epoch 1894: 0.692\n",
      "train loss on epoch 1895 : 0.315\n",
      "train accuracy on epoch 1895: 0.944\n",
      "test loss on epoch 1895: 0.351\n",
      "test accuracy on epoch 1895: 0.769\n",
      "train loss on epoch 1896 : 0.266\n",
      "train accuracy on epoch 1896: 0.889\n",
      "test loss on epoch 1896: 0.342\n",
      "test accuracy on epoch 1896: 0.769\n",
      "train loss on epoch 1897 : 0.397\n",
      "train accuracy on epoch 1897: 0.833\n",
      "test loss on epoch 1897: 0.365\n",
      "test accuracy on epoch 1897: 0.769\n",
      "train loss on epoch 1898 : 0.200\n",
      "train accuracy on epoch 1898: 0.944\n",
      "test loss on epoch 1898: 0.345\n",
      "test accuracy on epoch 1898: 0.769\n",
      "train loss on epoch 1899 : 0.080\n",
      "train accuracy on epoch 1899: 1.000\n",
      "test loss on epoch 1899: 0.366\n",
      "test accuracy on epoch 1899: 0.769\n",
      "train loss on epoch 1900 : 0.222\n",
      "train accuracy on epoch 1900: 0.889\n",
      "test loss on epoch 1900: 0.367\n",
      "test accuracy on epoch 1900: 0.692\n",
      "train loss on epoch 1901 : 0.194\n",
      "train accuracy on epoch 1901: 0.889\n",
      "test loss on epoch 1901: 0.346\n",
      "test accuracy on epoch 1901: 0.769\n",
      "train loss on epoch 1902 : 0.194\n",
      "train accuracy on epoch 1902: 0.889\n",
      "test loss on epoch 1902: 0.370\n",
      "test accuracy on epoch 1902: 0.769\n",
      "train loss on epoch 1903 : 0.167\n",
      "train accuracy on epoch 1903: 0.944\n",
      "test loss on epoch 1903: 0.368\n",
      "test accuracy on epoch 1903: 0.692\n",
      "train loss on epoch 1904 : 0.566\n",
      "train accuracy on epoch 1904: 0.833\n",
      "test loss on epoch 1904: 0.351\n",
      "test accuracy on epoch 1904: 0.846\n",
      "train loss on epoch 1905 : 0.244\n",
      "train accuracy on epoch 1905: 0.889\n",
      "test loss on epoch 1905: 0.362\n",
      "test accuracy on epoch 1905: 0.769\n",
      "train loss on epoch 1906 : 0.153\n",
      "train accuracy on epoch 1906: 1.000\n",
      "test loss on epoch 1906: 0.355\n",
      "test accuracy on epoch 1906: 0.769\n",
      "train loss on epoch 1907 : 0.195\n",
      "train accuracy on epoch 1907: 0.889\n",
      "test loss on epoch 1907: 0.356\n",
      "test accuracy on epoch 1907: 0.769\n",
      "train loss on epoch 1908 : 0.227\n",
      "train accuracy on epoch 1908: 0.889\n",
      "test loss on epoch 1908: 0.375\n",
      "test accuracy on epoch 1908: 0.769\n",
      "train loss on epoch 1909 : 0.157\n",
      "train accuracy on epoch 1909: 0.944\n",
      "test loss on epoch 1909: 0.363\n",
      "test accuracy on epoch 1909: 0.769\n",
      "train loss on epoch 1910 : 0.212\n",
      "train accuracy on epoch 1910: 0.889\n",
      "test loss on epoch 1910: 0.367\n",
      "test accuracy on epoch 1910: 0.769\n",
      "train loss on epoch 1911 : 0.227\n",
      "train accuracy on epoch 1911: 0.889\n",
      "test loss on epoch 1911: 0.372\n",
      "test accuracy on epoch 1911: 0.769\n",
      "train loss on epoch 1912 : 0.226\n",
      "train accuracy on epoch 1912: 0.889\n",
      "test loss on epoch 1912: 0.368\n",
      "test accuracy on epoch 1912: 0.692\n",
      "train loss on epoch 1913 : 0.041\n",
      "train accuracy on epoch 1913: 1.000\n",
      "test loss on epoch 1913: 0.357\n",
      "test accuracy on epoch 1913: 0.769\n",
      "train loss on epoch 1914 : 0.200\n",
      "train accuracy on epoch 1914: 0.889\n",
      "test loss on epoch 1914: 0.341\n",
      "test accuracy on epoch 1914: 0.769\n",
      "train loss on epoch 1915 : 0.049\n",
      "train accuracy on epoch 1915: 1.000\n",
      "test loss on epoch 1915: 0.365\n",
      "test accuracy on epoch 1915: 0.769\n",
      "train loss on epoch 1916 : 0.105\n",
      "train accuracy on epoch 1916: 0.944\n",
      "test loss on epoch 1916: 0.344\n",
      "test accuracy on epoch 1916: 0.846\n",
      "train loss on epoch 1917 : 0.128\n",
      "train accuracy on epoch 1917: 0.944\n",
      "test loss on epoch 1917: 0.368\n",
      "test accuracy on epoch 1917: 0.692\n",
      "train loss on epoch 1918 : 0.091\n",
      "train accuracy on epoch 1918: 0.944\n",
      "test loss on epoch 1918: 0.360\n",
      "test accuracy on epoch 1918: 0.769\n",
      "train loss on epoch 1919 : 0.073\n",
      "train accuracy on epoch 1919: 0.944\n",
      "test loss on epoch 1919: 0.349\n",
      "test accuracy on epoch 1919: 0.846\n",
      "train loss on epoch 1920 : 0.180\n",
      "train accuracy on epoch 1920: 0.944\n",
      "test loss on epoch 1920: 0.373\n",
      "test accuracy on epoch 1920: 0.769\n",
      "train loss on epoch 1921 : 0.134\n",
      "train accuracy on epoch 1921: 0.944\n",
      "test loss on epoch 1921: 0.356\n",
      "test accuracy on epoch 1921: 0.769\n",
      "train loss on epoch 1922 : 0.092\n",
      "train accuracy on epoch 1922: 0.944\n",
      "test loss on epoch 1922: 0.367\n",
      "test accuracy on epoch 1922: 0.769\n",
      "train loss on epoch 1923 : 0.065\n",
      "train accuracy on epoch 1923: 0.944\n",
      "test loss on epoch 1923: 0.366\n",
      "test accuracy on epoch 1923: 0.769\n",
      "train loss on epoch 1924 : 0.080\n",
      "train accuracy on epoch 1924: 0.944\n",
      "test loss on epoch 1924: 0.367\n",
      "test accuracy on epoch 1924: 0.769\n",
      "train loss on epoch 1925 : 0.355\n",
      "train accuracy on epoch 1925: 0.889\n",
      "test loss on epoch 1925: 0.359\n",
      "test accuracy on epoch 1925: 0.769\n",
      "train loss on epoch 1926 : 0.145\n",
      "train accuracy on epoch 1926: 0.944\n",
      "test loss on epoch 1926: 0.364\n",
      "test accuracy on epoch 1926: 0.769\n",
      "train loss on epoch 1927 : 0.112\n",
      "train accuracy on epoch 1927: 0.944\n",
      "test loss on epoch 1927: 0.352\n",
      "test accuracy on epoch 1927: 0.846\n",
      "train loss on epoch 1928 : 0.093\n",
      "train accuracy on epoch 1928: 1.000\n",
      "test loss on epoch 1928: 0.359\n",
      "test accuracy on epoch 1928: 0.769\n",
      "train loss on epoch 1929 : 0.173\n",
      "train accuracy on epoch 1929: 0.889\n",
      "test loss on epoch 1929: 0.359\n",
      "test accuracy on epoch 1929: 0.769\n",
      "train loss on epoch 1930 : 0.082\n",
      "train accuracy on epoch 1930: 1.000\n",
      "test loss on epoch 1930: 0.356\n",
      "test accuracy on epoch 1930: 0.769\n",
      "train loss on epoch 1931 : 0.408\n",
      "train accuracy on epoch 1931: 0.889\n",
      "test loss on epoch 1931: 0.368\n",
      "test accuracy on epoch 1931: 0.692\n",
      "train loss on epoch 1932 : 0.073\n",
      "train accuracy on epoch 1932: 0.944\n",
      "test loss on epoch 1932: 0.369\n",
      "test accuracy on epoch 1932: 0.692\n",
      "train loss on epoch 1933 : 0.051\n",
      "train accuracy on epoch 1933: 1.000\n",
      "test loss on epoch 1933: 0.365\n",
      "test accuracy on epoch 1933: 0.692\n",
      "train loss on epoch 1934 : 0.113\n",
      "train accuracy on epoch 1934: 0.889\n",
      "test loss on epoch 1934: 0.351\n",
      "test accuracy on epoch 1934: 0.846\n",
      "train loss on epoch 1935 : 0.108\n",
      "train accuracy on epoch 1935: 0.944\n",
      "test loss on epoch 1935: 0.358\n",
      "test accuracy on epoch 1935: 0.769\n",
      "train loss on epoch 1936 : 0.110\n",
      "train accuracy on epoch 1936: 0.944\n",
      "test loss on epoch 1936: 0.349\n",
      "test accuracy on epoch 1936: 0.846\n",
      "train loss on epoch 1937 : 0.156\n",
      "train accuracy on epoch 1937: 0.944\n",
      "test loss on epoch 1937: 0.356\n",
      "test accuracy on epoch 1937: 0.769\n",
      "train loss on epoch 1938 : 0.462\n",
      "train accuracy on epoch 1938: 0.833\n",
      "test loss on epoch 1938: 0.366\n",
      "test accuracy on epoch 1938: 0.692\n",
      "train loss on epoch 1939 : 0.112\n",
      "train accuracy on epoch 1939: 0.944\n",
      "test loss on epoch 1939: 0.370\n",
      "test accuracy on epoch 1939: 0.692\n",
      "train loss on epoch 1940 : 0.080\n",
      "train accuracy on epoch 1940: 1.000\n",
      "test loss on epoch 1940: 0.347\n",
      "test accuracy on epoch 1940: 0.846\n",
      "train loss on epoch 1941 : 0.069\n",
      "train accuracy on epoch 1941: 0.944\n",
      "test loss on epoch 1941: 0.369\n",
      "test accuracy on epoch 1941: 0.692\n",
      "train loss on epoch 1942 : 0.147\n",
      "train accuracy on epoch 1942: 0.944\n",
      "test loss on epoch 1942: 0.348\n",
      "test accuracy on epoch 1942: 0.846\n",
      "train loss on epoch 1943 : 0.130\n",
      "train accuracy on epoch 1943: 0.944\n",
      "test loss on epoch 1943: 0.354\n",
      "test accuracy on epoch 1943: 0.769\n",
      "train loss on epoch 1944 : 0.220\n",
      "train accuracy on epoch 1944: 0.944\n",
      "test loss on epoch 1944: 0.373\n",
      "test accuracy on epoch 1944: 0.692\n",
      "train loss on epoch 1945 : 0.119\n",
      "train accuracy on epoch 1945: 0.944\n",
      "test loss on epoch 1945: 0.354\n",
      "test accuracy on epoch 1945: 0.846\n",
      "train loss on epoch 1946 : 0.149\n",
      "train accuracy on epoch 1946: 0.944\n",
      "test loss on epoch 1946: 0.374\n",
      "test accuracy on epoch 1946: 0.769\n",
      "train loss on epoch 1947 : 0.366\n",
      "train accuracy on epoch 1947: 0.889\n",
      "test loss on epoch 1947: 0.378\n",
      "test accuracy on epoch 1947: 0.769\n",
      "train loss on epoch 1948 : 0.288\n",
      "train accuracy on epoch 1948: 0.889\n",
      "test loss on epoch 1948: 0.369\n",
      "test accuracy on epoch 1948: 0.769\n",
      "train loss on epoch 1949 : 0.207\n",
      "train accuracy on epoch 1949: 0.944\n",
      "test loss on epoch 1949: 0.373\n",
      "test accuracy on epoch 1949: 0.769\n",
      "train loss on epoch 1950 : 0.382\n",
      "train accuracy on epoch 1950: 0.889\n",
      "test loss on epoch 1950: 0.368\n",
      "test accuracy on epoch 1950: 0.769\n",
      "train loss on epoch 1951 : 0.172\n",
      "train accuracy on epoch 1951: 0.944\n",
      "test loss on epoch 1951: 0.372\n",
      "test accuracy on epoch 1951: 0.769\n",
      "train loss on epoch 1952 : 0.056\n",
      "train accuracy on epoch 1952: 1.000\n",
      "test loss on epoch 1952: 0.371\n",
      "test accuracy on epoch 1952: 0.769\n",
      "train loss on epoch 1953 : 0.299\n",
      "train accuracy on epoch 1953: 0.944\n",
      "test loss on epoch 1953: 0.373\n",
      "test accuracy on epoch 1953: 0.769\n",
      "train loss on epoch 1954 : 0.109\n",
      "train accuracy on epoch 1954: 0.944\n",
      "test loss on epoch 1954: 0.361\n",
      "test accuracy on epoch 1954: 0.769\n",
      "train loss on epoch 1955 : 0.081\n",
      "train accuracy on epoch 1955: 1.000\n",
      "test loss on epoch 1955: 0.375\n",
      "test accuracy on epoch 1955: 0.692\n",
      "train loss on epoch 1956 : 0.182\n",
      "train accuracy on epoch 1956: 0.944\n",
      "test loss on epoch 1956: 0.373\n",
      "test accuracy on epoch 1956: 0.692\n",
      "train loss on epoch 1957 : 0.144\n",
      "train accuracy on epoch 1957: 0.944\n",
      "test loss on epoch 1957: 0.358\n",
      "test accuracy on epoch 1957: 0.846\n",
      "train loss on epoch 1958 : 0.098\n",
      "train accuracy on epoch 1958: 1.000\n",
      "test loss on epoch 1958: 0.375\n",
      "test accuracy on epoch 1958: 0.692\n",
      "train loss on epoch 1959 : 0.580\n",
      "train accuracy on epoch 1959: 0.833\n",
      "test loss on epoch 1959: 0.361\n",
      "test accuracy on epoch 1959: 0.769\n",
      "train loss on epoch 1960 : 0.153\n",
      "train accuracy on epoch 1960: 0.944\n",
      "test loss on epoch 1960: 0.368\n",
      "test accuracy on epoch 1960: 0.769\n",
      "train loss on epoch 1961 : 0.122\n",
      "train accuracy on epoch 1961: 0.944\n",
      "test loss on epoch 1961: 0.370\n",
      "test accuracy on epoch 1961: 0.769\n",
      "train loss on epoch 1962 : 0.313\n",
      "train accuracy on epoch 1962: 0.889\n",
      "test loss on epoch 1962: 0.371\n",
      "test accuracy on epoch 1962: 0.769\n",
      "train loss on epoch 1963 : 0.196\n",
      "train accuracy on epoch 1963: 0.944\n",
      "test loss on epoch 1963: 0.368\n",
      "test accuracy on epoch 1963: 0.769\n",
      "train loss on epoch 1964 : 0.048\n",
      "train accuracy on epoch 1964: 1.000\n",
      "test loss on epoch 1964: 0.360\n",
      "test accuracy on epoch 1964: 0.769\n",
      "train loss on epoch 1965 : 0.126\n",
      "train accuracy on epoch 1965: 0.944\n",
      "test loss on epoch 1965: 0.368\n",
      "test accuracy on epoch 1965: 0.769\n",
      "train loss on epoch 1966 : 0.256\n",
      "train accuracy on epoch 1966: 0.944\n",
      "test loss on epoch 1966: 0.381\n",
      "test accuracy on epoch 1966: 0.769\n",
      "train loss on epoch 1967 : 0.140\n",
      "train accuracy on epoch 1967: 0.944\n",
      "test loss on epoch 1967: 0.370\n",
      "test accuracy on epoch 1967: 0.769\n",
      "train loss on epoch 1968 : 0.172\n",
      "train accuracy on epoch 1968: 0.944\n",
      "test loss on epoch 1968: 0.377\n",
      "test accuracy on epoch 1968: 0.769\n",
      "train loss on epoch 1969 : 0.103\n",
      "train accuracy on epoch 1969: 0.944\n",
      "test loss on epoch 1969: 0.362\n",
      "test accuracy on epoch 1969: 0.846\n",
      "train loss on epoch 1970 : 0.105\n",
      "train accuracy on epoch 1970: 1.000\n",
      "test loss on epoch 1970: 0.378\n",
      "test accuracy on epoch 1970: 0.692\n",
      "train loss on epoch 1971 : 0.220\n",
      "train accuracy on epoch 1971: 0.889\n",
      "test loss on epoch 1971: 0.368\n",
      "test accuracy on epoch 1971: 0.769\n",
      "train loss on epoch 1972 : 0.108\n",
      "train accuracy on epoch 1972: 1.000\n",
      "test loss on epoch 1972: 0.362\n",
      "test accuracy on epoch 1972: 0.769\n",
      "train loss on epoch 1973 : 0.323\n",
      "train accuracy on epoch 1973: 0.889\n",
      "test loss on epoch 1973: 0.364\n",
      "test accuracy on epoch 1973: 0.769\n",
      "train loss on epoch 1974 : 0.155\n",
      "train accuracy on epoch 1974: 0.944\n",
      "test loss on epoch 1974: 0.366\n",
      "test accuracy on epoch 1974: 0.769\n",
      "train loss on epoch 1975 : 0.278\n",
      "train accuracy on epoch 1975: 0.889\n",
      "test loss on epoch 1975: 0.367\n",
      "test accuracy on epoch 1975: 0.769\n",
      "train loss on epoch 1976 : 0.154\n",
      "train accuracy on epoch 1976: 0.889\n",
      "test loss on epoch 1976: 0.374\n",
      "test accuracy on epoch 1976: 0.769\n",
      "train loss on epoch 1977 : 0.225\n",
      "train accuracy on epoch 1977: 0.944\n",
      "test loss on epoch 1977: 0.384\n",
      "test accuracy on epoch 1977: 0.769\n",
      "train loss on epoch 1978 : 0.087\n",
      "train accuracy on epoch 1978: 1.000\n",
      "test loss on epoch 1978: 0.379\n",
      "test accuracy on epoch 1978: 0.769\n",
      "train loss on epoch 1979 : 0.285\n",
      "train accuracy on epoch 1979: 0.889\n",
      "test loss on epoch 1979: 0.375\n",
      "test accuracy on epoch 1979: 0.769\n",
      "train loss on epoch 1980 : 0.187\n",
      "train accuracy on epoch 1980: 0.944\n",
      "test loss on epoch 1980: 0.388\n",
      "test accuracy on epoch 1980: 0.769\n",
      "train loss on epoch 1981 : 0.094\n",
      "train accuracy on epoch 1981: 1.000\n",
      "test loss on epoch 1981: 0.368\n",
      "test accuracy on epoch 1981: 0.769\n",
      "train loss on epoch 1982 : 0.116\n",
      "train accuracy on epoch 1982: 0.944\n",
      "test loss on epoch 1982: 0.371\n",
      "test accuracy on epoch 1982: 0.769\n",
      "train loss on epoch 1983 : 0.288\n",
      "train accuracy on epoch 1983: 0.889\n",
      "test loss on epoch 1983: 0.359\n",
      "test accuracy on epoch 1983: 0.769\n",
      "train loss on epoch 1984 : 0.095\n",
      "train accuracy on epoch 1984: 1.000\n",
      "test loss on epoch 1984: 0.364\n",
      "test accuracy on epoch 1984: 0.769\n",
      "train loss on epoch 1985 : 0.074\n",
      "train accuracy on epoch 1985: 0.944\n",
      "test loss on epoch 1985: 0.358\n",
      "test accuracy on epoch 1985: 0.769\n",
      "train loss on epoch 1986 : 0.091\n",
      "train accuracy on epoch 1986: 1.000\n",
      "test loss on epoch 1986: 0.363\n",
      "test accuracy on epoch 1986: 0.769\n",
      "train loss on epoch 1987 : 0.167\n",
      "train accuracy on epoch 1987: 0.944\n",
      "test loss on epoch 1987: 0.359\n",
      "test accuracy on epoch 1987: 0.769\n",
      "train loss on epoch 1988 : 0.266\n",
      "train accuracy on epoch 1988: 0.889\n",
      "test loss on epoch 1988: 0.359\n",
      "test accuracy on epoch 1988: 0.769\n",
      "train loss on epoch 1989 : 0.139\n",
      "train accuracy on epoch 1989: 0.944\n",
      "test loss on epoch 1989: 0.359\n",
      "test accuracy on epoch 1989: 0.769\n",
      "train loss on epoch 1990 : 0.248\n",
      "train accuracy on epoch 1990: 0.944\n",
      "test loss on epoch 1990: 0.367\n",
      "test accuracy on epoch 1990: 0.769\n",
      "train loss on epoch 1991 : 0.268\n",
      "train accuracy on epoch 1991: 0.889\n",
      "test loss on epoch 1991: 0.353\n",
      "test accuracy on epoch 1991: 0.846\n",
      "train loss on epoch 1992 : 0.128\n",
      "train accuracy on epoch 1992: 1.000\n",
      "test loss on epoch 1992: 0.376\n",
      "test accuracy on epoch 1992: 0.692\n",
      "train loss on epoch 1993 : 0.247\n",
      "train accuracy on epoch 1993: 0.944\n",
      "test loss on epoch 1993: 0.361\n",
      "test accuracy on epoch 1993: 0.769\n",
      "train loss on epoch 1994 : 0.123\n",
      "train accuracy on epoch 1994: 0.944\n",
      "test loss on epoch 1994: 0.370\n",
      "test accuracy on epoch 1994: 0.769\n",
      "train loss on epoch 1995 : 0.160\n",
      "train accuracy on epoch 1995: 0.889\n",
      "test loss on epoch 1995: 0.377\n",
      "test accuracy on epoch 1995: 0.769\n",
      "train loss on epoch 1996 : 0.181\n",
      "train accuracy on epoch 1996: 0.944\n",
      "test loss on epoch 1996: 0.385\n",
      "test accuracy on epoch 1996: 0.769\n",
      "train loss on epoch 1997 : 0.348\n",
      "train accuracy on epoch 1997: 0.889\n",
      "test loss on epoch 1997: 0.386\n",
      "test accuracy on epoch 1997: 0.769\n",
      "train loss on epoch 1998 : 0.142\n",
      "train accuracy on epoch 1998: 0.889\n",
      "test loss on epoch 1998: 0.370\n",
      "test accuracy on epoch 1998: 0.769\n",
      "train loss on epoch 1999 : 0.394\n",
      "train accuracy on epoch 1999: 0.889\n",
      "test loss on epoch 1999: 0.387\n",
      "test accuracy on epoch 1999: 0.769\n",
      "train loss on epoch 2000 : 0.093\n",
      "train accuracy on epoch 2000: 0.944\n",
      "test loss on epoch 2000: 0.384\n",
      "test accuracy on epoch 2000: 0.769\n",
      "train loss on epoch 2001 : 0.093\n",
      "train accuracy on epoch 2001: 1.000\n",
      "test loss on epoch 2001: 0.369\n",
      "test accuracy on epoch 2001: 0.769\n",
      "train loss on epoch 2002 : 0.142\n",
      "train accuracy on epoch 2002: 0.944\n",
      "test loss on epoch 2002: 0.375\n",
      "test accuracy on epoch 2002: 0.769\n",
      "train loss on epoch 2003 : 0.105\n",
      "train accuracy on epoch 2003: 0.944\n",
      "test loss on epoch 2003: 0.384\n",
      "test accuracy on epoch 2003: 0.769\n",
      "train loss on epoch 2004 : 0.308\n",
      "train accuracy on epoch 2004: 0.889\n",
      "test loss on epoch 2004: 0.385\n",
      "test accuracy on epoch 2004: 0.769\n",
      "train loss on epoch 2005 : 0.157\n",
      "train accuracy on epoch 2005: 0.944\n",
      "test loss on epoch 2005: 0.374\n",
      "test accuracy on epoch 2005: 0.769\n",
      "train loss on epoch 2006 : 0.217\n",
      "train accuracy on epoch 2006: 0.944\n",
      "test loss on epoch 2006: 0.370\n",
      "test accuracy on epoch 2006: 0.769\n",
      "train loss on epoch 2007 : 0.332\n",
      "train accuracy on epoch 2007: 0.944\n",
      "test loss on epoch 2007: 0.359\n",
      "test accuracy on epoch 2007: 0.769\n",
      "train loss on epoch 2008 : 0.172\n",
      "train accuracy on epoch 2008: 0.944\n",
      "test loss on epoch 2008: 0.376\n",
      "test accuracy on epoch 2008: 0.692\n",
      "train loss on epoch 2009 : 0.149\n",
      "train accuracy on epoch 2009: 0.944\n",
      "test loss on epoch 2009: 0.377\n",
      "test accuracy on epoch 2009: 0.769\n",
      "train loss on epoch 2010 : 0.167\n",
      "train accuracy on epoch 2010: 0.833\n",
      "test loss on epoch 2010: 0.352\n",
      "test accuracy on epoch 2010: 0.769\n",
      "train loss on epoch 2011 : 0.121\n",
      "train accuracy on epoch 2011: 0.944\n",
      "test loss on epoch 2011: 0.349\n",
      "test accuracy on epoch 2011: 0.769\n",
      "train loss on epoch 2012 : 0.141\n",
      "train accuracy on epoch 2012: 0.889\n",
      "test loss on epoch 2012: 0.352\n",
      "test accuracy on epoch 2012: 0.769\n",
      "train loss on epoch 2013 : 0.093\n",
      "train accuracy on epoch 2013: 0.944\n",
      "test loss on epoch 2013: 0.380\n",
      "test accuracy on epoch 2013: 0.769\n",
      "train loss on epoch 2014 : 0.137\n",
      "train accuracy on epoch 2014: 0.889\n",
      "test loss on epoch 2014: 0.367\n",
      "test accuracy on epoch 2014: 0.769\n",
      "train loss on epoch 2015 : 0.113\n",
      "train accuracy on epoch 2015: 1.000\n",
      "test loss on epoch 2015: 0.367\n",
      "test accuracy on epoch 2015: 0.769\n",
      "train loss on epoch 2016 : 0.097\n",
      "train accuracy on epoch 2016: 1.000\n",
      "test loss on epoch 2016: 0.358\n",
      "test accuracy on epoch 2016: 0.769\n",
      "train loss on epoch 2017 : 0.127\n",
      "train accuracy on epoch 2017: 0.889\n",
      "test loss on epoch 2017: 0.360\n",
      "test accuracy on epoch 2017: 0.769\n",
      "train loss on epoch 2018 : 0.065\n",
      "train accuracy on epoch 2018: 1.000\n",
      "test loss on epoch 2018: 0.361\n",
      "test accuracy on epoch 2018: 0.769\n",
      "train loss on epoch 2019 : 0.498\n",
      "train accuracy on epoch 2019: 0.833\n",
      "test loss on epoch 2019: 0.365\n",
      "test accuracy on epoch 2019: 0.769\n",
      "train loss on epoch 2020 : 0.124\n",
      "train accuracy on epoch 2020: 1.000\n",
      "test loss on epoch 2020: 0.348\n",
      "test accuracy on epoch 2020: 0.846\n",
      "train loss on epoch 2021 : 0.287\n",
      "train accuracy on epoch 2021: 0.889\n",
      "test loss on epoch 2021: 0.371\n",
      "test accuracy on epoch 2021: 0.692\n",
      "train loss on epoch 2022 : 0.067\n",
      "train accuracy on epoch 2022: 0.944\n",
      "test loss on epoch 2022: 0.350\n",
      "test accuracy on epoch 2022: 0.846\n",
      "train loss on epoch 2023 : 0.115\n",
      "train accuracy on epoch 2023: 0.944\n",
      "test loss on epoch 2023: 0.347\n",
      "test accuracy on epoch 2023: 0.769\n",
      "train loss on epoch 2024 : 0.146\n",
      "train accuracy on epoch 2024: 0.889\n",
      "test loss on epoch 2024: 0.370\n",
      "test accuracy on epoch 2024: 0.692\n",
      "train loss on epoch 2025 : 0.117\n",
      "train accuracy on epoch 2025: 0.944\n",
      "test loss on epoch 2025: 0.372\n",
      "test accuracy on epoch 2025: 0.769\n",
      "train loss on epoch 2026 : 0.150\n",
      "train accuracy on epoch 2026: 1.000\n",
      "test loss on epoch 2026: 0.362\n",
      "test accuracy on epoch 2026: 0.769\n",
      "train loss on epoch 2027 : 0.163\n",
      "train accuracy on epoch 2027: 0.889\n",
      "test loss on epoch 2027: 0.348\n",
      "test accuracy on epoch 2027: 0.769\n",
      "train loss on epoch 2028 : 0.466\n",
      "train accuracy on epoch 2028: 0.889\n",
      "test loss on epoch 2028: 0.362\n",
      "test accuracy on epoch 2028: 0.769\n",
      "train loss on epoch 2029 : 0.157\n",
      "train accuracy on epoch 2029: 0.944\n",
      "test loss on epoch 2029: 0.373\n",
      "test accuracy on epoch 2029: 0.769\n",
      "train loss on epoch 2030 : 0.161\n",
      "train accuracy on epoch 2030: 0.944\n",
      "test loss on epoch 2030: 0.349\n",
      "test accuracy on epoch 2030: 0.769\n",
      "train loss on epoch 2031 : 0.120\n",
      "train accuracy on epoch 2031: 0.889\n",
      "test loss on epoch 2031: 0.372\n",
      "test accuracy on epoch 2031: 0.692\n",
      "train loss on epoch 2032 : 0.296\n",
      "train accuracy on epoch 2032: 0.944\n",
      "test loss on epoch 2032: 0.375\n",
      "test accuracy on epoch 2032: 0.692\n",
      "train loss on epoch 2033 : 0.421\n",
      "train accuracy on epoch 2033: 0.889\n",
      "test loss on epoch 2033: 0.359\n",
      "test accuracy on epoch 2033: 0.769\n",
      "train loss on epoch 2034 : 0.214\n",
      "train accuracy on epoch 2034: 0.889\n",
      "test loss on epoch 2034: 0.364\n",
      "test accuracy on epoch 2034: 0.769\n",
      "train loss on epoch 2035 : 0.173\n",
      "train accuracy on epoch 2035: 0.944\n",
      "test loss on epoch 2035: 0.379\n",
      "test accuracy on epoch 2035: 0.769\n",
      "train loss on epoch 2036 : 0.258\n",
      "train accuracy on epoch 2036: 0.889\n",
      "test loss on epoch 2036: 0.392\n",
      "test accuracy on epoch 2036: 0.769\n",
      "train loss on epoch 2037 : 0.127\n",
      "train accuracy on epoch 2037: 0.944\n",
      "test loss on epoch 2037: 0.382\n",
      "test accuracy on epoch 2037: 0.769\n",
      "train loss on epoch 2038 : 0.187\n",
      "train accuracy on epoch 2038: 0.944\n",
      "test loss on epoch 2038: 0.387\n",
      "test accuracy on epoch 2038: 0.769\n",
      "train loss on epoch 2039 : 0.152\n",
      "train accuracy on epoch 2039: 0.889\n",
      "test loss on epoch 2039: 0.369\n",
      "test accuracy on epoch 2039: 0.769\n",
      "train loss on epoch 2040 : 0.084\n",
      "train accuracy on epoch 2040: 0.944\n",
      "test loss on epoch 2040: 0.383\n",
      "test accuracy on epoch 2040: 0.769\n",
      "train loss on epoch 2041 : 0.135\n",
      "train accuracy on epoch 2041: 0.944\n",
      "test loss on epoch 2041: 0.368\n",
      "test accuracy on epoch 2041: 0.769\n",
      "train loss on epoch 2042 : 0.021\n",
      "train accuracy on epoch 2042: 1.000\n",
      "test loss on epoch 2042: 0.377\n",
      "test accuracy on epoch 2042: 0.769\n",
      "train loss on epoch 2043 : 0.283\n",
      "train accuracy on epoch 2043: 0.889\n",
      "test loss on epoch 2043: 0.372\n",
      "test accuracy on epoch 2043: 0.692\n",
      "train loss on epoch 2044 : 0.202\n",
      "train accuracy on epoch 2044: 0.944\n",
      "test loss on epoch 2044: 0.345\n",
      "test accuracy on epoch 2044: 0.846\n",
      "train loss on epoch 2045 : 0.211\n",
      "train accuracy on epoch 2045: 0.944\n",
      "test loss on epoch 2045: 0.359\n",
      "test accuracy on epoch 2045: 0.769\n",
      "train loss on epoch 2046 : 0.114\n",
      "train accuracy on epoch 2046: 1.000\n",
      "test loss on epoch 2046: 0.361\n",
      "test accuracy on epoch 2046: 0.769\n",
      "train loss on epoch 2047 : 0.093\n",
      "train accuracy on epoch 2047: 1.000\n",
      "test loss on epoch 2047: 0.352\n",
      "test accuracy on epoch 2047: 0.769\n",
      "train loss on epoch 2048 : 0.232\n",
      "train accuracy on epoch 2048: 0.889\n",
      "test loss on epoch 2048: 0.363\n",
      "test accuracy on epoch 2048: 0.769\n",
      "train loss on epoch 2049 : 0.329\n",
      "train accuracy on epoch 2049: 0.889\n",
      "test loss on epoch 2049: 0.346\n",
      "test accuracy on epoch 2049: 0.769\n",
      "train loss on epoch 2050 : 0.171\n",
      "train accuracy on epoch 2050: 0.944\n",
      "test loss on epoch 2050: 0.352\n",
      "test accuracy on epoch 2050: 0.769\n",
      "train loss on epoch 2051 : 0.208\n",
      "train accuracy on epoch 2051: 0.889\n",
      "test loss on epoch 2051: 0.344\n",
      "test accuracy on epoch 2051: 0.769\n",
      "train loss on epoch 2052 : 0.044\n",
      "train accuracy on epoch 2052: 1.000\n",
      "test loss on epoch 2052: 0.352\n",
      "test accuracy on epoch 2052: 0.769\n",
      "train loss on epoch 2053 : 0.152\n",
      "train accuracy on epoch 2053: 0.944\n",
      "test loss on epoch 2053: 0.361\n",
      "test accuracy on epoch 2053: 0.769\n",
      "train loss on epoch 2054 : 0.280\n",
      "train accuracy on epoch 2054: 0.944\n",
      "test loss on epoch 2054: 0.373\n",
      "test accuracy on epoch 2054: 0.769\n",
      "train loss on epoch 2055 : 0.202\n",
      "train accuracy on epoch 2055: 0.889\n",
      "test loss on epoch 2055: 0.374\n",
      "test accuracy on epoch 2055: 0.769\n",
      "train loss on epoch 2056 : 0.157\n",
      "train accuracy on epoch 2056: 0.889\n",
      "test loss on epoch 2056: 0.344\n",
      "test accuracy on epoch 2056: 0.769\n",
      "train loss on epoch 2057 : 0.202\n",
      "train accuracy on epoch 2057: 0.833\n",
      "test loss on epoch 2057: 0.361\n",
      "test accuracy on epoch 2057: 0.769\n",
      "train loss on epoch 2058 : 0.106\n",
      "train accuracy on epoch 2058: 0.944\n",
      "test loss on epoch 2058: 0.369\n",
      "test accuracy on epoch 2058: 0.769\n",
      "train loss on epoch 2059 : 0.092\n",
      "train accuracy on epoch 2059: 1.000\n",
      "test loss on epoch 2059: 0.368\n",
      "test accuracy on epoch 2059: 0.769\n",
      "train loss on epoch 2060 : 0.138\n",
      "train accuracy on epoch 2060: 0.944\n",
      "test loss on epoch 2060: 0.345\n",
      "test accuracy on epoch 2060: 0.846\n",
      "train loss on epoch 2061 : 0.094\n",
      "train accuracy on epoch 2061: 0.944\n",
      "test loss on epoch 2061: 0.347\n",
      "test accuracy on epoch 2061: 0.846\n",
      "train loss on epoch 2062 : 0.180\n",
      "train accuracy on epoch 2062: 0.944\n",
      "test loss on epoch 2062: 0.371\n",
      "test accuracy on epoch 2062: 0.692\n",
      "train loss on epoch 2063 : 0.302\n",
      "train accuracy on epoch 2063: 0.889\n",
      "test loss on epoch 2063: 0.349\n",
      "test accuracy on epoch 2063: 0.846\n",
      "train loss on epoch 2064 : 0.178\n",
      "train accuracy on epoch 2064: 0.944\n",
      "test loss on epoch 2064: 0.372\n",
      "test accuracy on epoch 2064: 0.769\n",
      "train loss on epoch 2065 : 0.196\n",
      "train accuracy on epoch 2065: 0.889\n",
      "test loss on epoch 2065: 0.372\n",
      "test accuracy on epoch 2065: 0.769\n",
      "train loss on epoch 2066 : 0.197\n",
      "train accuracy on epoch 2066: 0.889\n",
      "test loss on epoch 2066: 0.362\n",
      "test accuracy on epoch 2066: 0.769\n",
      "train loss on epoch 2067 : 0.334\n",
      "train accuracy on epoch 2067: 0.889\n",
      "test loss on epoch 2067: 0.374\n",
      "test accuracy on epoch 2067: 0.769\n",
      "train loss on epoch 2068 : 0.173\n",
      "train accuracy on epoch 2068: 0.944\n",
      "test loss on epoch 2068: 0.377\n",
      "test accuracy on epoch 2068: 0.769\n",
      "train loss on epoch 2069 : 0.111\n",
      "train accuracy on epoch 2069: 0.944\n",
      "test loss on epoch 2069: 0.364\n",
      "test accuracy on epoch 2069: 0.769\n",
      "train loss on epoch 2070 : 0.310\n",
      "train accuracy on epoch 2070: 0.833\n",
      "test loss on epoch 2070: 0.382\n",
      "test accuracy on epoch 2070: 0.769\n",
      "train loss on epoch 2071 : 0.098\n",
      "train accuracy on epoch 2071: 0.944\n",
      "test loss on epoch 2071: 0.375\n",
      "test accuracy on epoch 2071: 0.769\n",
      "train loss on epoch 2072 : 0.086\n",
      "train accuracy on epoch 2072: 0.944\n",
      "test loss on epoch 2072: 0.374\n",
      "test accuracy on epoch 2072: 0.769\n",
      "train loss on epoch 2073 : 0.052\n",
      "train accuracy on epoch 2073: 1.000\n",
      "test loss on epoch 2073: 0.363\n",
      "test accuracy on epoch 2073: 0.769\n",
      "train loss on epoch 2074 : 0.074\n",
      "train accuracy on epoch 2074: 1.000\n",
      "test loss on epoch 2074: 0.367\n",
      "test accuracy on epoch 2074: 0.769\n",
      "train loss on epoch 2075 : 0.113\n",
      "train accuracy on epoch 2075: 0.944\n",
      "test loss on epoch 2075: 0.378\n",
      "test accuracy on epoch 2075: 0.769\n",
      "train loss on epoch 2076 : 0.204\n",
      "train accuracy on epoch 2076: 0.889\n",
      "test loss on epoch 2076: 0.370\n",
      "test accuracy on epoch 2076: 0.769\n",
      "train loss on epoch 2077 : 0.240\n",
      "train accuracy on epoch 2077: 0.944\n",
      "test loss on epoch 2077: 0.365\n",
      "test accuracy on epoch 2077: 0.769\n",
      "train loss on epoch 2078 : 0.155\n",
      "train accuracy on epoch 2078: 0.889\n",
      "test loss on epoch 2078: 0.368\n",
      "test accuracy on epoch 2078: 0.769\n",
      "train loss on epoch 2079 : 0.173\n",
      "train accuracy on epoch 2079: 0.889\n",
      "test loss on epoch 2079: 0.362\n",
      "test accuracy on epoch 2079: 0.769\n",
      "train loss on epoch 2080 : 0.126\n",
      "train accuracy on epoch 2080: 0.944\n",
      "test loss on epoch 2080: 0.368\n",
      "test accuracy on epoch 2080: 0.769\n",
      "train loss on epoch 2081 : 0.032\n",
      "train accuracy on epoch 2081: 1.000\n",
      "test loss on epoch 2081: 0.370\n",
      "test accuracy on epoch 2081: 0.769\n",
      "train loss on epoch 2082 : 0.277\n",
      "train accuracy on epoch 2082: 0.833\n",
      "test loss on epoch 2082: 0.368\n",
      "test accuracy on epoch 2082: 0.769\n",
      "train loss on epoch 2083 : 0.192\n",
      "train accuracy on epoch 2083: 0.944\n",
      "test loss on epoch 2083: 0.360\n",
      "test accuracy on epoch 2083: 0.769\n",
      "train loss on epoch 2084 : 0.308\n",
      "train accuracy on epoch 2084: 0.889\n",
      "test loss on epoch 2084: 0.362\n",
      "test accuracy on epoch 2084: 0.769\n",
      "train loss on epoch 2085 : 0.042\n",
      "train accuracy on epoch 2085: 1.000\n",
      "test loss on epoch 2085: 0.368\n",
      "test accuracy on epoch 2085: 0.769\n",
      "train loss on epoch 2086 : 0.130\n",
      "train accuracy on epoch 2086: 0.944\n",
      "test loss on epoch 2086: 0.368\n",
      "test accuracy on epoch 2086: 0.769\n",
      "train loss on epoch 2087 : 0.217\n",
      "train accuracy on epoch 2087: 0.944\n",
      "test loss on epoch 2087: 0.380\n",
      "test accuracy on epoch 2087: 0.769\n",
      "train loss on epoch 2088 : 0.381\n",
      "train accuracy on epoch 2088: 0.889\n",
      "test loss on epoch 2088: 0.384\n",
      "test accuracy on epoch 2088: 0.769\n",
      "train loss on epoch 2089 : 0.192\n",
      "train accuracy on epoch 2089: 0.944\n",
      "test loss on epoch 2089: 0.383\n",
      "test accuracy on epoch 2089: 0.769\n",
      "train loss on epoch 2090 : 0.087\n",
      "train accuracy on epoch 2090: 0.944\n",
      "test loss on epoch 2090: 0.385\n",
      "test accuracy on epoch 2090: 0.769\n",
      "train loss on epoch 2091 : 0.118\n",
      "train accuracy on epoch 2091: 0.944\n",
      "test loss on epoch 2091: 0.395\n",
      "test accuracy on epoch 2091: 0.769\n",
      "train loss on epoch 2092 : 0.208\n",
      "train accuracy on epoch 2092: 0.833\n",
      "test loss on epoch 2092: 0.395\n",
      "test accuracy on epoch 2092: 0.769\n",
      "train loss on epoch 2093 : 0.194\n",
      "train accuracy on epoch 2093: 0.944\n",
      "test loss on epoch 2093: 0.383\n",
      "test accuracy on epoch 2093: 0.769\n",
      "train loss on epoch 2094 : 0.250\n",
      "train accuracy on epoch 2094: 0.944\n",
      "test loss on epoch 2094: 0.374\n",
      "test accuracy on epoch 2094: 0.769\n",
      "train loss on epoch 2095 : 0.287\n",
      "train accuracy on epoch 2095: 0.889\n",
      "test loss on epoch 2095: 0.363\n",
      "test accuracy on epoch 2095: 0.769\n",
      "train loss on epoch 2096 : 0.076\n",
      "train accuracy on epoch 2096: 1.000\n",
      "test loss on epoch 2096: 0.363\n",
      "test accuracy on epoch 2096: 0.769\n",
      "train loss on epoch 2097 : 0.102\n",
      "train accuracy on epoch 2097: 0.889\n",
      "test loss on epoch 2097: 0.360\n",
      "test accuracy on epoch 2097: 0.769\n",
      "train loss on epoch 2098 : 0.125\n",
      "train accuracy on epoch 2098: 0.944\n",
      "test loss on epoch 2098: 0.352\n",
      "test accuracy on epoch 2098: 0.769\n",
      "train loss on epoch 2099 : 0.286\n",
      "train accuracy on epoch 2099: 0.889\n",
      "test loss on epoch 2099: 0.351\n",
      "test accuracy on epoch 2099: 0.769\n",
      "train loss on epoch 2100 : 0.242\n",
      "train accuracy on epoch 2100: 0.944\n",
      "test loss on epoch 2100: 0.351\n",
      "test accuracy on epoch 2100: 0.769\n",
      "train loss on epoch 2101 : 0.127\n",
      "train accuracy on epoch 2101: 1.000\n",
      "test loss on epoch 2101: 0.356\n",
      "test accuracy on epoch 2101: 0.769\n",
      "train loss on epoch 2102 : 0.212\n",
      "train accuracy on epoch 2102: 0.889\n",
      "test loss on epoch 2102: 0.369\n",
      "test accuracy on epoch 2102: 0.769\n",
      "train loss on epoch 2103 : 0.056\n",
      "train accuracy on epoch 2103: 1.000\n",
      "test loss on epoch 2103: 0.370\n",
      "test accuracy on epoch 2103: 0.769\n",
      "train loss on epoch 2104 : 0.240\n",
      "train accuracy on epoch 2104: 0.889\n",
      "test loss on epoch 2104: 0.379\n",
      "test accuracy on epoch 2104: 0.769\n",
      "train loss on epoch 2105 : 0.308\n",
      "train accuracy on epoch 2105: 0.889\n",
      "test loss on epoch 2105: 0.380\n",
      "test accuracy on epoch 2105: 0.769\n",
      "train loss on epoch 2106 : 0.056\n",
      "train accuracy on epoch 2106: 1.000\n",
      "test loss on epoch 2106: 0.378\n",
      "test accuracy on epoch 2106: 0.769\n",
      "train loss on epoch 2107 : 0.301\n",
      "train accuracy on epoch 2107: 0.889\n",
      "test loss on epoch 2107: 0.375\n",
      "test accuracy on epoch 2107: 0.769\n",
      "train loss on epoch 2108 : 0.079\n",
      "train accuracy on epoch 2108: 1.000\n",
      "test loss on epoch 2108: 0.374\n",
      "test accuracy on epoch 2108: 0.769\n",
      "train loss on epoch 2109 : 0.328\n",
      "train accuracy on epoch 2109: 0.889\n",
      "test loss on epoch 2109: 0.377\n",
      "test accuracy on epoch 2109: 0.769\n",
      "train loss on epoch 2110 : 0.049\n",
      "train accuracy on epoch 2110: 1.000\n",
      "test loss on epoch 2110: 0.372\n",
      "test accuracy on epoch 2110: 0.769\n",
      "train loss on epoch 2111 : 0.240\n",
      "train accuracy on epoch 2111: 0.944\n",
      "test loss on epoch 2111: 0.372\n",
      "test accuracy on epoch 2111: 0.769\n",
      "train loss on epoch 2112 : 0.250\n",
      "train accuracy on epoch 2112: 0.833\n",
      "test loss on epoch 2112: 0.359\n",
      "test accuracy on epoch 2112: 0.769\n",
      "train loss on epoch 2113 : 0.223\n",
      "train accuracy on epoch 2113: 0.889\n",
      "test loss on epoch 2113: 0.362\n",
      "test accuracy on epoch 2113: 0.769\n",
      "train loss on epoch 2114 : 0.174\n",
      "train accuracy on epoch 2114: 0.889\n",
      "test loss on epoch 2114: 0.349\n",
      "test accuracy on epoch 2114: 0.769\n",
      "train loss on epoch 2115 : 0.213\n",
      "train accuracy on epoch 2115: 0.889\n",
      "test loss on epoch 2115: 0.349\n",
      "test accuracy on epoch 2115: 0.769\n",
      "train loss on epoch 2116 : 0.167\n",
      "train accuracy on epoch 2116: 0.889\n",
      "test loss on epoch 2116: 0.361\n",
      "test accuracy on epoch 2116: 0.769\n",
      "train loss on epoch 2117 : 0.155\n",
      "train accuracy on epoch 2117: 0.944\n",
      "test loss on epoch 2117: 0.358\n",
      "test accuracy on epoch 2117: 0.769\n",
      "train loss on epoch 2118 : 0.237\n",
      "train accuracy on epoch 2118: 0.889\n",
      "test loss on epoch 2118: 0.358\n",
      "test accuracy on epoch 2118: 0.769\n",
      "train loss on epoch 2119 : 0.199\n",
      "train accuracy on epoch 2119: 0.944\n",
      "test loss on epoch 2119: 0.353\n",
      "test accuracy on epoch 2119: 0.769\n",
      "train loss on epoch 2120 : 0.094\n",
      "train accuracy on epoch 2120: 0.944\n",
      "test loss on epoch 2120: 0.363\n",
      "test accuracy on epoch 2120: 0.769\n",
      "train loss on epoch 2121 : 0.140\n",
      "train accuracy on epoch 2121: 0.889\n",
      "test loss on epoch 2121: 0.359\n",
      "test accuracy on epoch 2121: 0.769\n",
      "train loss on epoch 2122 : 0.152\n",
      "train accuracy on epoch 2122: 0.889\n",
      "test loss on epoch 2122: 0.367\n",
      "test accuracy on epoch 2122: 0.769\n",
      "train loss on epoch 2123 : 0.150\n",
      "train accuracy on epoch 2123: 0.889\n",
      "test loss on epoch 2123: 0.375\n",
      "test accuracy on epoch 2123: 0.769\n",
      "train loss on epoch 2124 : 0.145\n",
      "train accuracy on epoch 2124: 0.944\n",
      "test loss on epoch 2124: 0.373\n",
      "test accuracy on epoch 2124: 0.769\n",
      "train loss on epoch 2125 : 0.089\n",
      "train accuracy on epoch 2125: 1.000\n",
      "test loss on epoch 2125: 0.369\n",
      "test accuracy on epoch 2125: 0.769\n",
      "train loss on epoch 2126 : 0.175\n",
      "train accuracy on epoch 2126: 0.889\n",
      "test loss on epoch 2126: 0.368\n",
      "test accuracy on epoch 2126: 0.769\n",
      "train loss on epoch 2127 : 0.316\n",
      "train accuracy on epoch 2127: 0.889\n",
      "test loss on epoch 2127: 0.370\n",
      "test accuracy on epoch 2127: 0.769\n",
      "train loss on epoch 2128 : 0.081\n",
      "train accuracy on epoch 2128: 1.000\n",
      "test loss on epoch 2128: 0.375\n",
      "test accuracy on epoch 2128: 0.769\n",
      "train loss on epoch 2129 : 0.584\n",
      "train accuracy on epoch 2129: 0.778\n",
      "test loss on epoch 2129: 0.377\n",
      "test accuracy on epoch 2129: 0.769\n",
      "train loss on epoch 2130 : 0.368\n",
      "train accuracy on epoch 2130: 0.889\n",
      "test loss on epoch 2130: 0.377\n",
      "test accuracy on epoch 2130: 0.769\n",
      "train loss on epoch 2131 : 0.227\n",
      "train accuracy on epoch 2131: 0.889\n",
      "test loss on epoch 2131: 0.379\n",
      "test accuracy on epoch 2131: 0.769\n",
      "train loss on epoch 2132 : 0.059\n",
      "train accuracy on epoch 2132: 1.000\n",
      "test loss on epoch 2132: 0.374\n",
      "test accuracy on epoch 2132: 0.769\n",
      "train loss on epoch 2133 : 0.075\n",
      "train accuracy on epoch 2133: 0.944\n",
      "test loss on epoch 2133: 0.378\n",
      "test accuracy on epoch 2133: 0.769\n",
      "train loss on epoch 2134 : 0.226\n",
      "train accuracy on epoch 2134: 0.944\n",
      "test loss on epoch 2134: 0.365\n",
      "test accuracy on epoch 2134: 0.769\n",
      "train loss on epoch 2135 : 0.212\n",
      "train accuracy on epoch 2135: 0.944\n",
      "test loss on epoch 2135: 0.365\n",
      "test accuracy on epoch 2135: 0.769\n",
      "train loss on epoch 2136 : 0.105\n",
      "train accuracy on epoch 2136: 0.889\n",
      "test loss on epoch 2136: 0.348\n",
      "test accuracy on epoch 2136: 0.769\n",
      "train loss on epoch 2137 : 0.070\n",
      "train accuracy on epoch 2137: 1.000\n",
      "test loss on epoch 2137: 0.345\n",
      "test accuracy on epoch 2137: 0.846\n",
      "train loss on epoch 2138 : 0.203\n",
      "train accuracy on epoch 2138: 0.889\n",
      "test loss on epoch 2138: 0.363\n",
      "test accuracy on epoch 2138: 0.692\n",
      "train loss on epoch 2139 : 0.266\n",
      "train accuracy on epoch 2139: 0.889\n",
      "test loss on epoch 2139: 0.361\n",
      "test accuracy on epoch 2139: 0.769\n",
      "train loss on epoch 2140 : 0.077\n",
      "train accuracy on epoch 2140: 0.944\n",
      "test loss on epoch 2140: 0.350\n",
      "test accuracy on epoch 2140: 0.769\n",
      "train loss on epoch 2141 : 0.123\n",
      "train accuracy on epoch 2141: 0.944\n",
      "test loss on epoch 2141: 0.362\n",
      "test accuracy on epoch 2141: 0.769\n",
      "train loss on epoch 2142 : 0.089\n",
      "train accuracy on epoch 2142: 0.944\n",
      "test loss on epoch 2142: 0.361\n",
      "test accuracy on epoch 2142: 0.692\n",
      "train loss on epoch 2143 : 0.319\n",
      "train accuracy on epoch 2143: 0.889\n",
      "test loss on epoch 2143: 0.354\n",
      "test accuracy on epoch 2143: 0.769\n",
      "train loss on epoch 2144 : 0.152\n",
      "train accuracy on epoch 2144: 0.944\n",
      "test loss on epoch 2144: 0.357\n",
      "test accuracy on epoch 2144: 0.769\n",
      "train loss on epoch 2145 : 0.162\n",
      "train accuracy on epoch 2145: 0.889\n",
      "test loss on epoch 2145: 0.357\n",
      "test accuracy on epoch 2145: 0.769\n",
      "train loss on epoch 2146 : 0.092\n",
      "train accuracy on epoch 2146: 0.944\n",
      "test loss on epoch 2146: 0.351\n",
      "test accuracy on epoch 2146: 0.769\n",
      "train loss on epoch 2147 : 0.231\n",
      "train accuracy on epoch 2147: 0.833\n",
      "test loss on epoch 2147: 0.357\n",
      "test accuracy on epoch 2147: 0.769\n",
      "train loss on epoch 2148 : 0.135\n",
      "train accuracy on epoch 2148: 0.944\n",
      "test loss on epoch 2148: 0.365\n",
      "test accuracy on epoch 2148: 0.692\n",
      "train loss on epoch 2149 : 0.036\n",
      "train accuracy on epoch 2149: 1.000\n",
      "test loss on epoch 2149: 0.362\n",
      "test accuracy on epoch 2149: 0.692\n",
      "train loss on epoch 2150 : 0.086\n",
      "train accuracy on epoch 2150: 0.944\n",
      "test loss on epoch 2150: 0.357\n",
      "test accuracy on epoch 2150: 0.769\n",
      "train loss on epoch 2151 : 0.241\n",
      "train accuracy on epoch 2151: 0.944\n",
      "test loss on epoch 2151: 0.345\n",
      "test accuracy on epoch 2151: 0.846\n",
      "train loss on epoch 2152 : 0.137\n",
      "train accuracy on epoch 2152: 0.889\n",
      "test loss on epoch 2152: 0.366\n",
      "test accuracy on epoch 2152: 0.692\n",
      "train loss on epoch 2153 : 0.284\n",
      "train accuracy on epoch 2153: 0.889\n",
      "test loss on epoch 2153: 0.368\n",
      "test accuracy on epoch 2153: 0.769\n",
      "train loss on epoch 2154 : 0.336\n",
      "train accuracy on epoch 2154: 0.889\n",
      "test loss on epoch 2154: 0.362\n",
      "test accuracy on epoch 2154: 0.769\n",
      "train loss on epoch 2155 : 0.419\n",
      "train accuracy on epoch 2155: 0.889\n",
      "test loss on epoch 2155: 0.362\n",
      "test accuracy on epoch 2155: 0.769\n",
      "train loss on epoch 2156 : 0.127\n",
      "train accuracy on epoch 2156: 0.944\n",
      "test loss on epoch 2156: 0.372\n",
      "test accuracy on epoch 2156: 0.769\n",
      "train loss on epoch 2157 : 0.127\n",
      "train accuracy on epoch 2157: 0.944\n",
      "test loss on epoch 2157: 0.360\n",
      "test accuracy on epoch 2157: 0.769\n",
      "train loss on epoch 2158 : 0.053\n",
      "train accuracy on epoch 2158: 1.000\n",
      "test loss on epoch 2158: 0.355\n",
      "test accuracy on epoch 2158: 0.769\n",
      "train loss on epoch 2159 : 0.207\n",
      "train accuracy on epoch 2159: 0.889\n",
      "test loss on epoch 2159: 0.364\n",
      "test accuracy on epoch 2159: 0.769\n",
      "train loss on epoch 2160 : 0.082\n",
      "train accuracy on epoch 2160: 1.000\n",
      "test loss on epoch 2160: 0.367\n",
      "test accuracy on epoch 2160: 0.769\n",
      "train loss on epoch 2161 : 0.198\n",
      "train accuracy on epoch 2161: 0.889\n",
      "test loss on epoch 2161: 0.367\n",
      "test accuracy on epoch 2161: 0.769\n",
      "train loss on epoch 2162 : 0.215\n",
      "train accuracy on epoch 2162: 0.944\n",
      "test loss on epoch 2162: 0.361\n",
      "test accuracy on epoch 2162: 0.769\n",
      "train loss on epoch 2163 : 0.196\n",
      "train accuracy on epoch 2163: 0.944\n",
      "test loss on epoch 2163: 0.348\n",
      "test accuracy on epoch 2163: 0.769\n",
      "train loss on epoch 2164 : 0.300\n",
      "train accuracy on epoch 2164: 0.889\n",
      "test loss on epoch 2164: 0.372\n",
      "test accuracy on epoch 2164: 0.769\n",
      "train loss on epoch 2165 : 0.054\n",
      "train accuracy on epoch 2165: 1.000\n",
      "test loss on epoch 2165: 0.346\n",
      "test accuracy on epoch 2165: 0.769\n",
      "train loss on epoch 2166 : 0.208\n",
      "train accuracy on epoch 2166: 0.889\n",
      "test loss on epoch 2166: 0.351\n",
      "test accuracy on epoch 2166: 0.769\n",
      "train loss on epoch 2167 : 0.100\n",
      "train accuracy on epoch 2167: 1.000\n",
      "test loss on epoch 2167: 0.374\n",
      "test accuracy on epoch 2167: 0.769\n",
      "train loss on epoch 2168 : 0.074\n",
      "train accuracy on epoch 2168: 0.944\n",
      "test loss on epoch 2168: 0.375\n",
      "test accuracy on epoch 2168: 0.769\n",
      "train loss on epoch 2169 : 0.205\n",
      "train accuracy on epoch 2169: 0.944\n",
      "test loss on epoch 2169: 0.351\n",
      "test accuracy on epoch 2169: 0.769\n",
      "train loss on epoch 2170 : 0.428\n",
      "train accuracy on epoch 2170: 0.889\n",
      "test loss on epoch 2170: 0.352\n",
      "test accuracy on epoch 2170: 0.769\n",
      "train loss on epoch 2171 : 0.074\n",
      "train accuracy on epoch 2171: 0.944\n",
      "test loss on epoch 2171: 0.364\n",
      "test accuracy on epoch 2171: 0.769\n",
      "train loss on epoch 2172 : 0.094\n",
      "train accuracy on epoch 2172: 0.944\n",
      "test loss on epoch 2172: 0.346\n",
      "test accuracy on epoch 2172: 0.769\n",
      "train loss on epoch 2173 : 0.116\n",
      "train accuracy on epoch 2173: 0.944\n",
      "test loss on epoch 2173: 0.354\n",
      "test accuracy on epoch 2173: 0.769\n",
      "train loss on epoch 2174 : 0.120\n",
      "train accuracy on epoch 2174: 0.944\n",
      "test loss on epoch 2174: 0.362\n",
      "test accuracy on epoch 2174: 0.769\n",
      "train loss on epoch 2175 : 0.158\n",
      "train accuracy on epoch 2175: 0.944\n",
      "test loss on epoch 2175: 0.343\n",
      "test accuracy on epoch 2175: 0.769\n",
      "train loss on epoch 2176 : 0.029\n",
      "train accuracy on epoch 2176: 1.000\n",
      "test loss on epoch 2176: 0.346\n",
      "test accuracy on epoch 2176: 0.769\n",
      "train loss on epoch 2177 : 0.109\n",
      "train accuracy on epoch 2177: 0.944\n",
      "test loss on epoch 2177: 0.344\n",
      "test accuracy on epoch 2177: 0.769\n",
      "train loss on epoch 2178 : 0.328\n",
      "train accuracy on epoch 2178: 0.889\n",
      "test loss on epoch 2178: 0.340\n",
      "test accuracy on epoch 2178: 0.769\n",
      "train loss on epoch 2179 : 0.181\n",
      "train accuracy on epoch 2179: 0.889\n",
      "test loss on epoch 2179: 0.350\n",
      "test accuracy on epoch 2179: 0.769\n",
      "train loss on epoch 2180 : 0.232\n",
      "train accuracy on epoch 2180: 0.944\n",
      "test loss on epoch 2180: 0.360\n",
      "test accuracy on epoch 2180: 0.769\n",
      "train loss on epoch 2181 : 0.201\n",
      "train accuracy on epoch 2181: 0.944\n",
      "test loss on epoch 2181: 0.353\n",
      "test accuracy on epoch 2181: 0.769\n",
      "train loss on epoch 2182 : 0.144\n",
      "train accuracy on epoch 2182: 0.944\n",
      "test loss on epoch 2182: 0.358\n",
      "test accuracy on epoch 2182: 0.692\n",
      "train loss on epoch 2183 : 0.188\n",
      "train accuracy on epoch 2183: 0.889\n",
      "test loss on epoch 2183: 0.340\n",
      "test accuracy on epoch 2183: 0.846\n",
      "train loss on epoch 2184 : 0.221\n",
      "train accuracy on epoch 2184: 0.889\n",
      "test loss on epoch 2184: 0.359\n",
      "test accuracy on epoch 2184: 0.692\n",
      "train loss on epoch 2185 : 0.049\n",
      "train accuracy on epoch 2185: 1.000\n",
      "test loss on epoch 2185: 0.353\n",
      "test accuracy on epoch 2185: 0.769\n",
      "train loss on epoch 2186 : 0.160\n",
      "train accuracy on epoch 2186: 0.889\n",
      "test loss on epoch 2186: 0.346\n",
      "test accuracy on epoch 2186: 0.846\n",
      "train loss on epoch 2187 : 0.116\n",
      "train accuracy on epoch 2187: 0.944\n",
      "test loss on epoch 2187: 0.343\n",
      "test accuracy on epoch 2187: 0.846\n",
      "train loss on epoch 2188 : 0.196\n",
      "train accuracy on epoch 2188: 0.944\n",
      "test loss on epoch 2188: 0.351\n",
      "test accuracy on epoch 2188: 0.769\n",
      "train loss on epoch 2189 : 0.122\n",
      "train accuracy on epoch 2189: 0.944\n",
      "test loss on epoch 2189: 0.345\n",
      "test accuracy on epoch 2189: 0.846\n",
      "train loss on epoch 2190 : 0.208\n",
      "train accuracy on epoch 2190: 0.944\n",
      "test loss on epoch 2190: 0.344\n",
      "test accuracy on epoch 2190: 0.846\n",
      "train loss on epoch 2191 : 0.030\n",
      "train accuracy on epoch 2191: 1.000\n",
      "test loss on epoch 2191: 0.355\n",
      "test accuracy on epoch 2191: 0.692\n",
      "train loss on epoch 2192 : 0.196\n",
      "train accuracy on epoch 2192: 0.889\n",
      "test loss on epoch 2192: 0.356\n",
      "test accuracy on epoch 2192: 0.769\n",
      "train loss on epoch 2193 : 0.217\n",
      "train accuracy on epoch 2193: 0.944\n",
      "test loss on epoch 2193: 0.346\n",
      "test accuracy on epoch 2193: 0.769\n",
      "train loss on epoch 2194 : 0.098\n",
      "train accuracy on epoch 2194: 0.944\n",
      "test loss on epoch 2194: 0.352\n",
      "test accuracy on epoch 2194: 0.769\n",
      "train loss on epoch 2195 : 0.224\n",
      "train accuracy on epoch 2195: 0.889\n",
      "test loss on epoch 2195: 0.360\n",
      "test accuracy on epoch 2195: 0.769\n",
      "train loss on epoch 2196 : 0.456\n",
      "train accuracy on epoch 2196: 0.889\n",
      "test loss on epoch 2196: 0.362\n",
      "test accuracy on epoch 2196: 0.769\n",
      "train loss on epoch 2197 : 0.270\n",
      "train accuracy on epoch 2197: 0.944\n",
      "test loss on epoch 2197: 0.354\n",
      "test accuracy on epoch 2197: 0.769\n",
      "train loss on epoch 2198 : 0.145\n",
      "train accuracy on epoch 2198: 0.889\n",
      "test loss on epoch 2198: 0.367\n",
      "test accuracy on epoch 2198: 0.692\n",
      "train loss on epoch 2199 : 0.138\n",
      "train accuracy on epoch 2199: 0.889\n",
      "test loss on epoch 2199: 0.348\n",
      "test accuracy on epoch 2199: 0.846\n",
      "train loss on epoch 2200 : 0.190\n",
      "train accuracy on epoch 2200: 0.944\n",
      "test loss on epoch 2200: 0.358\n",
      "test accuracy on epoch 2200: 0.769\n",
      "train loss on epoch 2201 : 0.100\n",
      "train accuracy on epoch 2201: 1.000\n",
      "test loss on epoch 2201: 0.346\n",
      "test accuracy on epoch 2201: 0.846\n",
      "train loss on epoch 2202 : 0.305\n",
      "train accuracy on epoch 2202: 0.833\n",
      "test loss on epoch 2202: 0.342\n",
      "test accuracy on epoch 2202: 0.769\n",
      "train loss on epoch 2203 : 0.057\n",
      "train accuracy on epoch 2203: 1.000\n",
      "test loss on epoch 2203: 0.367\n",
      "test accuracy on epoch 2203: 0.769\n",
      "train loss on epoch 2204 : 0.352\n",
      "train accuracy on epoch 2204: 0.889\n",
      "test loss on epoch 2204: 0.340\n",
      "test accuracy on epoch 2204: 0.769\n",
      "train loss on epoch 2205 : 0.267\n",
      "train accuracy on epoch 2205: 0.944\n",
      "test loss on epoch 2205: 0.350\n",
      "test accuracy on epoch 2205: 0.769\n",
      "train loss on epoch 2206 : 0.382\n",
      "train accuracy on epoch 2206: 0.889\n",
      "test loss on epoch 2206: 0.341\n",
      "test accuracy on epoch 2206: 0.769\n",
      "train loss on epoch 2207 : 0.205\n",
      "train accuracy on epoch 2207: 0.889\n",
      "test loss on epoch 2207: 0.343\n",
      "test accuracy on epoch 2207: 0.769\n",
      "train loss on epoch 2208 : 0.411\n",
      "train accuracy on epoch 2208: 0.833\n",
      "test loss on epoch 2208: 0.365\n",
      "test accuracy on epoch 2208: 0.692\n",
      "train loss on epoch 2209 : 0.147\n",
      "train accuracy on epoch 2209: 0.944\n",
      "test loss on epoch 2209: 0.345\n",
      "test accuracy on epoch 2209: 0.846\n",
      "train loss on epoch 2210 : 0.162\n",
      "train accuracy on epoch 2210: 0.944\n",
      "test loss on epoch 2210: 0.361\n",
      "test accuracy on epoch 2210: 0.769\n",
      "train loss on epoch 2211 : 0.193\n",
      "train accuracy on epoch 2211: 0.833\n",
      "test loss on epoch 2211: 0.361\n",
      "test accuracy on epoch 2211: 0.769\n",
      "train loss on epoch 2212 : 0.256\n",
      "train accuracy on epoch 2212: 0.889\n",
      "test loss on epoch 2212: 0.347\n",
      "test accuracy on epoch 2212: 0.769\n",
      "train loss on epoch 2213 : 0.269\n",
      "train accuracy on epoch 2213: 0.833\n",
      "test loss on epoch 2213: 0.340\n",
      "test accuracy on epoch 2213: 0.769\n",
      "train loss on epoch 2214 : 0.120\n",
      "train accuracy on epoch 2214: 0.944\n",
      "test loss on epoch 2214: 0.349\n",
      "test accuracy on epoch 2214: 0.769\n",
      "train loss on epoch 2215 : 0.370\n",
      "train accuracy on epoch 2215: 0.833\n",
      "test loss on epoch 2215: 0.357\n",
      "test accuracy on epoch 2215: 0.769\n",
      "train loss on epoch 2216 : 0.197\n",
      "train accuracy on epoch 2216: 0.889\n",
      "test loss on epoch 2216: 0.347\n",
      "test accuracy on epoch 2216: 0.769\n",
      "train loss on epoch 2217 : 0.347\n",
      "train accuracy on epoch 2217: 0.889\n",
      "test loss on epoch 2217: 0.348\n",
      "test accuracy on epoch 2217: 0.769\n",
      "train loss on epoch 2218 : 0.060\n",
      "train accuracy on epoch 2218: 0.944\n",
      "test loss on epoch 2218: 0.358\n",
      "test accuracy on epoch 2218: 0.769\n",
      "train loss on epoch 2219 : 0.114\n",
      "train accuracy on epoch 2219: 0.889\n",
      "test loss on epoch 2219: 0.346\n",
      "test accuracy on epoch 2219: 0.846\n",
      "train loss on epoch 2220 : 0.290\n",
      "train accuracy on epoch 2220: 0.944\n",
      "test loss on epoch 2220: 0.370\n",
      "test accuracy on epoch 2220: 0.769\n",
      "train loss on epoch 2221 : 0.208\n",
      "train accuracy on epoch 2221: 0.889\n",
      "test loss on epoch 2221: 0.348\n",
      "test accuracy on epoch 2221: 0.769\n",
      "train loss on epoch 2222 : 0.194\n",
      "train accuracy on epoch 2222: 0.889\n",
      "test loss on epoch 2222: 0.344\n",
      "test accuracy on epoch 2222: 0.769\n",
      "train loss on epoch 2223 : 0.118\n",
      "train accuracy on epoch 2223: 0.944\n",
      "test loss on epoch 2223: 0.348\n",
      "test accuracy on epoch 2223: 0.769\n",
      "train loss on epoch 2224 : 0.154\n",
      "train accuracy on epoch 2224: 0.944\n",
      "test loss on epoch 2224: 0.360\n",
      "test accuracy on epoch 2224: 0.769\n",
      "train loss on epoch 2225 : 0.120\n",
      "train accuracy on epoch 2225: 1.000\n",
      "test loss on epoch 2225: 0.372\n",
      "test accuracy on epoch 2225: 0.769\n",
      "train loss on epoch 2226 : 0.104\n",
      "train accuracy on epoch 2226: 0.944\n",
      "test loss on epoch 2226: 0.370\n",
      "test accuracy on epoch 2226: 0.692\n",
      "train loss on epoch 2227 : 0.188\n",
      "train accuracy on epoch 2227: 0.889\n",
      "test loss on epoch 2227: 0.355\n",
      "test accuracy on epoch 2227: 0.846\n",
      "train loss on epoch 2228 : 0.357\n",
      "train accuracy on epoch 2228: 0.889\n",
      "test loss on epoch 2228: 0.381\n",
      "test accuracy on epoch 2228: 0.769\n",
      "train loss on epoch 2229 : 0.274\n",
      "train accuracy on epoch 2229: 0.889\n",
      "test loss on epoch 2229: 0.362\n",
      "test accuracy on epoch 2229: 0.769\n",
      "train loss on epoch 2230 : 0.263\n",
      "train accuracy on epoch 2230: 0.889\n",
      "test loss on epoch 2230: 0.362\n",
      "test accuracy on epoch 2230: 0.769\n",
      "train loss on epoch 2231 : 0.129\n",
      "train accuracy on epoch 2231: 0.944\n",
      "test loss on epoch 2231: 0.360\n",
      "test accuracy on epoch 2231: 0.769\n",
      "train loss on epoch 2232 : 0.181\n",
      "train accuracy on epoch 2232: 0.944\n",
      "test loss on epoch 2232: 0.355\n",
      "test accuracy on epoch 2232: 0.846\n",
      "train loss on epoch 2233 : 0.406\n",
      "train accuracy on epoch 2233: 0.889\n",
      "test loss on epoch 2233: 0.377\n",
      "test accuracy on epoch 2233: 0.692\n",
      "train loss on epoch 2234 : 0.335\n",
      "train accuracy on epoch 2234: 0.944\n",
      "test loss on epoch 2234: 0.362\n",
      "test accuracy on epoch 2234: 0.769\n",
      "train loss on epoch 2235 : 0.154\n",
      "train accuracy on epoch 2235: 0.889\n",
      "test loss on epoch 2235: 0.363\n",
      "test accuracy on epoch 2235: 0.769\n",
      "train loss on epoch 2236 : 0.199\n",
      "train accuracy on epoch 2236: 0.944\n",
      "test loss on epoch 2236: 0.372\n",
      "test accuracy on epoch 2236: 0.769\n",
      "train loss on epoch 2237 : 0.438\n",
      "train accuracy on epoch 2237: 0.833\n",
      "test loss on epoch 2237: 0.358\n",
      "test accuracy on epoch 2237: 0.769\n",
      "train loss on epoch 2238 : 0.116\n",
      "train accuracy on epoch 2238: 0.944\n",
      "test loss on epoch 2238: 0.348\n",
      "test accuracy on epoch 2238: 0.769\n",
      "train loss on epoch 2239 : 0.296\n",
      "train accuracy on epoch 2239: 0.889\n",
      "test loss on epoch 2239: 0.367\n",
      "test accuracy on epoch 2239: 0.769\n",
      "train loss on epoch 2240 : 0.130\n",
      "train accuracy on epoch 2240: 0.944\n",
      "test loss on epoch 2240: 0.353\n",
      "test accuracy on epoch 2240: 0.769\n",
      "train loss on epoch 2241 : 0.171\n",
      "train accuracy on epoch 2241: 0.944\n",
      "test loss on epoch 2241: 0.358\n",
      "test accuracy on epoch 2241: 0.769\n",
      "train loss on epoch 2242 : 0.247\n",
      "train accuracy on epoch 2242: 0.833\n",
      "test loss on epoch 2242: 0.364\n",
      "test accuracy on epoch 2242: 0.769\n",
      "train loss on epoch 2243 : 0.072\n",
      "train accuracy on epoch 2243: 1.000\n",
      "test loss on epoch 2243: 0.355\n",
      "test accuracy on epoch 2243: 0.846\n",
      "train loss on epoch 2244 : 0.085\n",
      "train accuracy on epoch 2244: 0.944\n",
      "test loss on epoch 2244: 0.374\n",
      "test accuracy on epoch 2244: 0.692\n",
      "train loss on epoch 2245 : 0.163\n",
      "train accuracy on epoch 2245: 0.833\n",
      "test loss on epoch 2245: 0.366\n",
      "test accuracy on epoch 2245: 0.769\n",
      "train loss on epoch 2246 : 0.295\n",
      "train accuracy on epoch 2246: 0.889\n",
      "test loss on epoch 2246: 0.372\n",
      "test accuracy on epoch 2246: 0.769\n",
      "train loss on epoch 2247 : 0.104\n",
      "train accuracy on epoch 2247: 1.000\n",
      "test loss on epoch 2247: 0.371\n",
      "test accuracy on epoch 2247: 0.769\n",
      "train loss on epoch 2248 : 0.079\n",
      "train accuracy on epoch 2248: 1.000\n",
      "test loss on epoch 2248: 0.372\n",
      "test accuracy on epoch 2248: 0.769\n",
      "train loss on epoch 2249 : 0.125\n",
      "train accuracy on epoch 2249: 0.944\n",
      "test loss on epoch 2249: 0.356\n",
      "test accuracy on epoch 2249: 0.769\n",
      "train loss on epoch 2250 : 0.194\n",
      "train accuracy on epoch 2250: 0.889\n",
      "test loss on epoch 2250: 0.360\n",
      "test accuracy on epoch 2250: 0.769\n",
      "train loss on epoch 2251 : 0.130\n",
      "train accuracy on epoch 2251: 0.944\n",
      "test loss on epoch 2251: 0.364\n",
      "test accuracy on epoch 2251: 0.769\n",
      "train loss on epoch 2252 : 0.205\n",
      "train accuracy on epoch 2252: 0.944\n",
      "test loss on epoch 2252: 0.364\n",
      "test accuracy on epoch 2252: 0.769\n",
      "train loss on epoch 2253 : 0.414\n",
      "train accuracy on epoch 2253: 0.778\n",
      "test loss on epoch 2253: 0.346\n",
      "test accuracy on epoch 2253: 0.846\n",
      "train loss on epoch 2254 : 0.072\n",
      "train accuracy on epoch 2254: 0.944\n",
      "test loss on epoch 2254: 0.352\n",
      "test accuracy on epoch 2254: 0.769\n",
      "train loss on epoch 2255 : 0.225\n",
      "train accuracy on epoch 2255: 0.944\n",
      "test loss on epoch 2255: 0.347\n",
      "test accuracy on epoch 2255: 0.769\n",
      "train loss on epoch 2256 : 0.137\n",
      "train accuracy on epoch 2256: 0.944\n",
      "test loss on epoch 2256: 0.357\n",
      "test accuracy on epoch 2256: 0.769\n",
      "train loss on epoch 2257 : 0.055\n",
      "train accuracy on epoch 2257: 1.000\n",
      "test loss on epoch 2257: 0.365\n",
      "test accuracy on epoch 2257: 0.769\n",
      "train loss on epoch 2258 : 0.092\n",
      "train accuracy on epoch 2258: 0.944\n",
      "test loss on epoch 2258: 0.359\n",
      "test accuracy on epoch 2258: 0.769\n",
      "train loss on epoch 2259 : 0.162\n",
      "train accuracy on epoch 2259: 0.889\n",
      "test loss on epoch 2259: 0.368\n",
      "test accuracy on epoch 2259: 0.769\n",
      "train loss on epoch 2260 : 0.440\n",
      "train accuracy on epoch 2260: 0.889\n",
      "test loss on epoch 2260: 0.363\n",
      "test accuracy on epoch 2260: 0.769\n",
      "train loss on epoch 2261 : 0.244\n",
      "train accuracy on epoch 2261: 0.833\n",
      "test loss on epoch 2261: 0.369\n",
      "test accuracy on epoch 2261: 0.769\n",
      "train loss on epoch 2262 : 0.220\n",
      "train accuracy on epoch 2262: 0.889\n",
      "test loss on epoch 2262: 0.357\n",
      "test accuracy on epoch 2262: 0.769\n",
      "train loss on epoch 2263 : 0.237\n",
      "train accuracy on epoch 2263: 0.889\n",
      "test loss on epoch 2263: 0.362\n",
      "test accuracy on epoch 2263: 0.769\n",
      "train loss on epoch 2264 : 0.189\n",
      "train accuracy on epoch 2264: 0.889\n",
      "test loss on epoch 2264: 0.361\n",
      "test accuracy on epoch 2264: 0.692\n",
      "train loss on epoch 2265 : 0.070\n",
      "train accuracy on epoch 2265: 1.000\n",
      "test loss on epoch 2265: 0.363\n",
      "test accuracy on epoch 2265: 0.692\n",
      "train loss on epoch 2266 : 0.203\n",
      "train accuracy on epoch 2266: 0.889\n",
      "test loss on epoch 2266: 0.345\n",
      "test accuracy on epoch 2266: 0.846\n",
      "train loss on epoch 2267 : 0.163\n",
      "train accuracy on epoch 2267: 0.889\n",
      "test loss on epoch 2267: 0.346\n",
      "test accuracy on epoch 2267: 0.846\n",
      "train loss on epoch 2268 : 0.169\n",
      "train accuracy on epoch 2268: 0.944\n",
      "test loss on epoch 2268: 0.360\n",
      "test accuracy on epoch 2268: 0.769\n",
      "train loss on epoch 2269 : 0.125\n",
      "train accuracy on epoch 2269: 0.944\n",
      "test loss on epoch 2269: 0.358\n",
      "test accuracy on epoch 2269: 0.769\n",
      "train loss on epoch 2270 : 0.080\n",
      "train accuracy on epoch 2270: 1.000\n",
      "test loss on epoch 2270: 0.368\n",
      "test accuracy on epoch 2270: 0.769\n",
      "train loss on epoch 2271 : 0.102\n",
      "train accuracy on epoch 2271: 1.000\n",
      "test loss on epoch 2271: 0.365\n",
      "test accuracy on epoch 2271: 0.769\n",
      "train loss on epoch 2272 : 0.212\n",
      "train accuracy on epoch 2272: 0.889\n",
      "test loss on epoch 2272: 0.375\n",
      "test accuracy on epoch 2272: 0.769\n",
      "train loss on epoch 2273 : 0.084\n",
      "train accuracy on epoch 2273: 0.944\n",
      "test loss on epoch 2273: 0.372\n",
      "test accuracy on epoch 2273: 0.769\n",
      "train loss on epoch 2274 : 0.067\n",
      "train accuracy on epoch 2274: 1.000\n",
      "test loss on epoch 2274: 0.379\n",
      "test accuracy on epoch 2274: 0.769\n",
      "train loss on epoch 2275 : 0.184\n",
      "train accuracy on epoch 2275: 0.889\n",
      "test loss on epoch 2275: 0.366\n",
      "test accuracy on epoch 2275: 0.769\n",
      "train loss on epoch 2276 : 0.258\n",
      "train accuracy on epoch 2276: 0.833\n",
      "test loss on epoch 2276: 0.370\n",
      "test accuracy on epoch 2276: 0.769\n",
      "train loss on epoch 2277 : 0.319\n",
      "train accuracy on epoch 2277: 0.889\n",
      "test loss on epoch 2277: 0.371\n",
      "test accuracy on epoch 2277: 0.769\n",
      "train loss on epoch 2278 : 0.293\n",
      "train accuracy on epoch 2278: 0.944\n",
      "test loss on epoch 2278: 0.371\n",
      "test accuracy on epoch 2278: 0.769\n",
      "train loss on epoch 2279 : 0.188\n",
      "train accuracy on epoch 2279: 0.889\n",
      "test loss on epoch 2279: 0.361\n",
      "test accuracy on epoch 2279: 0.769\n",
      "train loss on epoch 2280 : 0.201\n",
      "train accuracy on epoch 2280: 0.833\n",
      "test loss on epoch 2280: 0.351\n",
      "test accuracy on epoch 2280: 0.769\n",
      "train loss on epoch 2281 : 0.045\n",
      "train accuracy on epoch 2281: 1.000\n",
      "test loss on epoch 2281: 0.346\n",
      "test accuracy on epoch 2281: 0.846\n",
      "train loss on epoch 2282 : 0.283\n",
      "train accuracy on epoch 2282: 0.889\n",
      "test loss on epoch 2282: 0.344\n",
      "test accuracy on epoch 2282: 0.846\n",
      "train loss on epoch 2283 : 0.257\n",
      "train accuracy on epoch 2283: 0.889\n",
      "test loss on epoch 2283: 0.361\n",
      "test accuracy on epoch 2283: 0.692\n",
      "train loss on epoch 2284 : 0.061\n",
      "train accuracy on epoch 2284: 1.000\n",
      "test loss on epoch 2284: 0.338\n",
      "test accuracy on epoch 2284: 0.846\n",
      "train loss on epoch 2285 : 0.059\n",
      "train accuracy on epoch 2285: 1.000\n",
      "test loss on epoch 2285: 0.336\n",
      "test accuracy on epoch 2285: 0.846\n",
      "train loss on epoch 2286 : 0.068\n",
      "train accuracy on epoch 2286: 1.000\n",
      "test loss on epoch 2286: 0.337\n",
      "test accuracy on epoch 2286: 0.769\n",
      "train loss on epoch 2287 : 0.076\n",
      "train accuracy on epoch 2287: 0.944\n",
      "test loss on epoch 2287: 0.337\n",
      "test accuracy on epoch 2287: 0.769\n",
      "train loss on epoch 2288 : 0.113\n",
      "train accuracy on epoch 2288: 0.944\n",
      "test loss on epoch 2288: 0.359\n",
      "test accuracy on epoch 2288: 0.692\n",
      "train loss on epoch 2289 : 0.150\n",
      "train accuracy on epoch 2289: 0.889\n",
      "test loss on epoch 2289: 0.356\n",
      "test accuracy on epoch 2289: 0.769\n",
      "train loss on epoch 2290 : 0.084\n",
      "train accuracy on epoch 2290: 1.000\n",
      "test loss on epoch 2290: 0.333\n",
      "test accuracy on epoch 2290: 0.769\n",
      "train loss on epoch 2291 : 0.189\n",
      "train accuracy on epoch 2291: 0.944\n",
      "test loss on epoch 2291: 0.359\n",
      "test accuracy on epoch 2291: 0.769\n",
      "train loss on epoch 2292 : 0.315\n",
      "train accuracy on epoch 2292: 0.944\n",
      "test loss on epoch 2292: 0.358\n",
      "test accuracy on epoch 2292: 0.692\n",
      "train loss on epoch 2293 : 0.411\n",
      "train accuracy on epoch 2293: 0.889\n",
      "test loss on epoch 2293: 0.335\n",
      "test accuracy on epoch 2293: 0.769\n",
      "train loss on epoch 2294 : 0.091\n",
      "train accuracy on epoch 2294: 0.944\n",
      "test loss on epoch 2294: 0.347\n",
      "test accuracy on epoch 2294: 0.769\n",
      "train loss on epoch 2295 : 0.113\n",
      "train accuracy on epoch 2295: 0.944\n",
      "test loss on epoch 2295: 0.347\n",
      "test accuracy on epoch 2295: 0.769\n",
      "train loss on epoch 2296 : 0.058\n",
      "train accuracy on epoch 2296: 1.000\n",
      "test loss on epoch 2296: 0.344\n",
      "test accuracy on epoch 2296: 0.769\n",
      "train loss on epoch 2297 : 0.084\n",
      "train accuracy on epoch 2297: 1.000\n",
      "test loss on epoch 2297: 0.337\n",
      "test accuracy on epoch 2297: 0.846\n",
      "train loss on epoch 2298 : 0.206\n",
      "train accuracy on epoch 2298: 0.833\n",
      "test loss on epoch 2298: 0.356\n",
      "test accuracy on epoch 2298: 0.692\n",
      "train loss on epoch 2299 : 0.320\n",
      "train accuracy on epoch 2299: 0.889\n",
      "test loss on epoch 2299: 0.342\n",
      "test accuracy on epoch 2299: 0.769\n",
      "train loss on epoch 2300 : 0.122\n",
      "train accuracy on epoch 2300: 0.944\n",
      "test loss on epoch 2300: 0.344\n",
      "test accuracy on epoch 2300: 0.769\n",
      "train loss on epoch 2301 : 0.068\n",
      "train accuracy on epoch 2301: 1.000\n",
      "test loss on epoch 2301: 0.348\n",
      "test accuracy on epoch 2301: 0.769\n",
      "train loss on epoch 2302 : 0.222\n",
      "train accuracy on epoch 2302: 0.889\n",
      "test loss on epoch 2302: 0.355\n",
      "test accuracy on epoch 2302: 0.769\n",
      "train loss on epoch 2303 : 0.282\n",
      "train accuracy on epoch 2303: 0.833\n",
      "test loss on epoch 2303: 0.367\n",
      "test accuracy on epoch 2303: 0.769\n",
      "train loss on epoch 2304 : 0.477\n",
      "train accuracy on epoch 2304: 0.833\n",
      "test loss on epoch 2304: 0.369\n",
      "test accuracy on epoch 2304: 0.769\n",
      "train loss on epoch 2305 : 0.277\n",
      "train accuracy on epoch 2305: 0.833\n",
      "test loss on epoch 2305: 0.377\n",
      "test accuracy on epoch 2305: 0.769\n",
      "train loss on epoch 2306 : 0.210\n",
      "train accuracy on epoch 2306: 0.889\n",
      "test loss on epoch 2306: 0.366\n",
      "test accuracy on epoch 2306: 0.769\n",
      "train loss on epoch 2307 : 0.218\n",
      "train accuracy on epoch 2307: 0.944\n",
      "test loss on epoch 2307: 0.380\n",
      "test accuracy on epoch 2307: 0.769\n",
      "train loss on epoch 2308 : 0.145\n",
      "train accuracy on epoch 2308: 0.944\n",
      "test loss on epoch 2308: 0.376\n",
      "test accuracy on epoch 2308: 0.769\n",
      "train loss on epoch 2309 : 0.115\n",
      "train accuracy on epoch 2309: 0.944\n",
      "test loss on epoch 2309: 0.371\n",
      "test accuracy on epoch 2309: 0.769\n",
      "train loss on epoch 2310 : 0.125\n",
      "train accuracy on epoch 2310: 0.889\n",
      "test loss on epoch 2310: 0.373\n",
      "test accuracy on epoch 2310: 0.769\n",
      "train loss on epoch 2311 : 0.057\n",
      "train accuracy on epoch 2311: 1.000\n",
      "test loss on epoch 2311: 0.366\n",
      "test accuracy on epoch 2311: 0.769\n",
      "train loss on epoch 2312 : 0.203\n",
      "train accuracy on epoch 2312: 0.889\n",
      "test loss on epoch 2312: 0.364\n",
      "test accuracy on epoch 2312: 0.769\n",
      "train loss on epoch 2313 : 0.076\n",
      "train accuracy on epoch 2313: 1.000\n",
      "test loss on epoch 2313: 0.367\n",
      "test accuracy on epoch 2313: 0.769\n",
      "train loss on epoch 2314 : 0.072\n",
      "train accuracy on epoch 2314: 1.000\n",
      "test loss on epoch 2314: 0.360\n",
      "test accuracy on epoch 2314: 0.769\n",
      "train loss on epoch 2315 : 0.414\n",
      "train accuracy on epoch 2315: 0.778\n",
      "test loss on epoch 2315: 0.366\n",
      "test accuracy on epoch 2315: 0.769\n",
      "train loss on epoch 2316 : 0.088\n",
      "train accuracy on epoch 2316: 0.944\n",
      "test loss on epoch 2316: 0.361\n",
      "test accuracy on epoch 2316: 0.769\n",
      "train loss on epoch 2317 : 0.121\n",
      "train accuracy on epoch 2317: 0.944\n",
      "test loss on epoch 2317: 0.365\n",
      "test accuracy on epoch 2317: 0.769\n",
      "train loss on epoch 2318 : 0.238\n",
      "train accuracy on epoch 2318: 0.944\n",
      "test loss on epoch 2318: 0.356\n",
      "test accuracy on epoch 2318: 0.769\n",
      "train loss on epoch 2319 : 0.214\n",
      "train accuracy on epoch 2319: 0.889\n",
      "test loss on epoch 2319: 0.363\n",
      "test accuracy on epoch 2319: 0.769\n",
      "train loss on epoch 2320 : 0.127\n",
      "train accuracy on epoch 2320: 0.944\n",
      "test loss on epoch 2320: 0.365\n",
      "test accuracy on epoch 2320: 0.769\n",
      "train loss on epoch 2321 : 0.162\n",
      "train accuracy on epoch 2321: 0.944\n",
      "test loss on epoch 2321: 0.365\n",
      "test accuracy on epoch 2321: 0.769\n",
      "train loss on epoch 2322 : 0.396\n",
      "train accuracy on epoch 2322: 0.889\n",
      "test loss on epoch 2322: 0.372\n",
      "test accuracy on epoch 2322: 0.769\n",
      "train loss on epoch 2323 : 0.246\n",
      "train accuracy on epoch 2323: 0.944\n",
      "test loss on epoch 2323: 0.368\n",
      "test accuracy on epoch 2323: 0.769\n",
      "train loss on epoch 2324 : 0.204\n",
      "train accuracy on epoch 2324: 0.944\n",
      "test loss on epoch 2324: 0.366\n",
      "test accuracy on epoch 2324: 0.769\n",
      "train loss on epoch 2325 : 0.350\n",
      "train accuracy on epoch 2325: 0.889\n",
      "test loss on epoch 2325: 0.356\n",
      "test accuracy on epoch 2325: 0.769\n",
      "train loss on epoch 2326 : 0.078\n",
      "train accuracy on epoch 2326: 1.000\n",
      "test loss on epoch 2326: 0.356\n",
      "test accuracy on epoch 2326: 0.769\n",
      "train loss on epoch 2327 : 0.150\n",
      "train accuracy on epoch 2327: 0.944\n",
      "test loss on epoch 2327: 0.357\n",
      "test accuracy on epoch 2327: 0.769\n",
      "train loss on epoch 2328 : 0.316\n",
      "train accuracy on epoch 2328: 0.833\n",
      "test loss on epoch 2328: 0.353\n",
      "test accuracy on epoch 2328: 0.769\n",
      "train loss on epoch 2329 : 0.153\n",
      "train accuracy on epoch 2329: 0.944\n",
      "test loss on epoch 2329: 0.356\n",
      "test accuracy on epoch 2329: 0.769\n",
      "train loss on epoch 2330 : 0.071\n",
      "train accuracy on epoch 2330: 1.000\n",
      "test loss on epoch 2330: 0.355\n",
      "test accuracy on epoch 2330: 0.769\n",
      "train loss on epoch 2331 : 0.309\n",
      "train accuracy on epoch 2331: 0.833\n",
      "test loss on epoch 2331: 0.361\n",
      "test accuracy on epoch 2331: 0.769\n",
      "train loss on epoch 2332 : 0.081\n",
      "train accuracy on epoch 2332: 1.000\n",
      "test loss on epoch 2332: 0.358\n",
      "test accuracy on epoch 2332: 0.769\n",
      "train loss on epoch 2333 : 0.194\n",
      "train accuracy on epoch 2333: 0.889\n",
      "test loss on epoch 2333: 0.357\n",
      "test accuracy on epoch 2333: 0.769\n",
      "train loss on epoch 2334 : 0.124\n",
      "train accuracy on epoch 2334: 0.944\n",
      "test loss on epoch 2334: 0.351\n",
      "test accuracy on epoch 2334: 0.769\n",
      "train loss on epoch 2335 : 0.180\n",
      "train accuracy on epoch 2335: 0.944\n",
      "test loss on epoch 2335: 0.363\n",
      "test accuracy on epoch 2335: 0.769\n",
      "train loss on epoch 2336 : 0.060\n",
      "train accuracy on epoch 2336: 0.944\n",
      "test loss on epoch 2336: 0.361\n",
      "test accuracy on epoch 2336: 0.769\n",
      "train loss on epoch 2337 : 0.129\n",
      "train accuracy on epoch 2337: 0.944\n",
      "test loss on epoch 2337: 0.355\n",
      "test accuracy on epoch 2337: 0.769\n",
      "train loss on epoch 2338 : 0.114\n",
      "train accuracy on epoch 2338: 1.000\n",
      "test loss on epoch 2338: 0.351\n",
      "test accuracy on epoch 2338: 0.769\n",
      "train loss on epoch 2339 : 0.211\n",
      "train accuracy on epoch 2339: 0.889\n",
      "test loss on epoch 2339: 0.361\n",
      "test accuracy on epoch 2339: 0.769\n",
      "train loss on epoch 2340 : 0.131\n",
      "train accuracy on epoch 2340: 0.944\n",
      "test loss on epoch 2340: 0.358\n",
      "test accuracy on epoch 2340: 0.769\n",
      "train loss on epoch 2341 : 0.292\n",
      "train accuracy on epoch 2341: 0.889\n",
      "test loss on epoch 2341: 0.363\n",
      "test accuracy on epoch 2341: 0.769\n",
      "train loss on epoch 2342 : 0.223\n",
      "train accuracy on epoch 2342: 0.944\n",
      "test loss on epoch 2342: 0.368\n",
      "test accuracy on epoch 2342: 0.769\n",
      "train loss on epoch 2343 : 0.143\n",
      "train accuracy on epoch 2343: 1.000\n",
      "test loss on epoch 2343: 0.369\n",
      "test accuracy on epoch 2343: 0.769\n",
      "train loss on epoch 2344 : 0.183\n",
      "train accuracy on epoch 2344: 0.889\n",
      "test loss on epoch 2344: 0.374\n",
      "test accuracy on epoch 2344: 0.769\n",
      "train loss on epoch 2345 : 0.326\n",
      "train accuracy on epoch 2345: 0.889\n",
      "test loss on epoch 2345: 0.367\n",
      "test accuracy on epoch 2345: 0.769\n",
      "train loss on epoch 2346 : 0.340\n",
      "train accuracy on epoch 2346: 0.833\n",
      "test loss on epoch 2346: 0.358\n",
      "test accuracy on epoch 2346: 0.769\n",
      "train loss on epoch 2347 : 0.201\n",
      "train accuracy on epoch 2347: 0.944\n",
      "test loss on epoch 2347: 0.361\n",
      "test accuracy on epoch 2347: 0.769\n",
      "train loss on epoch 2348 : 0.195\n",
      "train accuracy on epoch 2348: 0.944\n",
      "test loss on epoch 2348: 0.367\n",
      "test accuracy on epoch 2348: 0.769\n",
      "train loss on epoch 2349 : 0.251\n",
      "train accuracy on epoch 2349: 0.889\n",
      "test loss on epoch 2349: 0.370\n",
      "test accuracy on epoch 2349: 0.769\n",
      "train loss on epoch 2350 : 0.418\n",
      "train accuracy on epoch 2350: 0.778\n",
      "test loss on epoch 2350: 0.371\n",
      "test accuracy on epoch 2350: 0.769\n",
      "train loss on epoch 2351 : 0.125\n",
      "train accuracy on epoch 2351: 0.944\n",
      "test loss on epoch 2351: 0.376\n",
      "test accuracy on epoch 2351: 0.769\n",
      "train loss on epoch 2352 : 0.148\n",
      "train accuracy on epoch 2352: 0.889\n",
      "test loss on epoch 2352: 0.377\n",
      "test accuracy on epoch 2352: 0.769\n",
      "train loss on epoch 2353 : 0.350\n",
      "train accuracy on epoch 2353: 0.833\n",
      "test loss on epoch 2353: 0.379\n",
      "test accuracy on epoch 2353: 0.769\n",
      "train loss on epoch 2354 : 0.171\n",
      "train accuracy on epoch 2354: 0.944\n",
      "test loss on epoch 2354: 0.368\n",
      "test accuracy on epoch 2354: 0.769\n",
      "train loss on epoch 2355 : 0.077\n",
      "train accuracy on epoch 2355: 1.000\n",
      "test loss on epoch 2355: 0.364\n",
      "test accuracy on epoch 2355: 0.769\n",
      "train loss on epoch 2356 : 0.327\n",
      "train accuracy on epoch 2356: 0.889\n",
      "test loss on epoch 2356: 0.361\n",
      "test accuracy on epoch 2356: 0.769\n",
      "train loss on epoch 2357 : 0.352\n",
      "train accuracy on epoch 2357: 0.889\n",
      "test loss on epoch 2357: 0.354\n",
      "test accuracy on epoch 2357: 0.769\n",
      "train loss on epoch 2358 : 0.123\n",
      "train accuracy on epoch 2358: 0.944\n",
      "test loss on epoch 2358: 0.354\n",
      "test accuracy on epoch 2358: 0.769\n",
      "train loss on epoch 2359 : 0.121\n",
      "train accuracy on epoch 2359: 0.944\n",
      "test loss on epoch 2359: 0.356\n",
      "test accuracy on epoch 2359: 0.769\n",
      "train loss on epoch 2360 : 0.254\n",
      "train accuracy on epoch 2360: 0.889\n",
      "test loss on epoch 2360: 0.355\n",
      "test accuracy on epoch 2360: 0.769\n",
      "train loss on epoch 2361 : 0.188\n",
      "train accuracy on epoch 2361: 0.944\n",
      "test loss on epoch 2361: 0.359\n",
      "test accuracy on epoch 2361: 0.769\n",
      "train loss on epoch 2362 : 0.096\n",
      "train accuracy on epoch 2362: 0.944\n",
      "test loss on epoch 2362: 0.366\n",
      "test accuracy on epoch 2362: 0.769\n",
      "train loss on epoch 2363 : 0.429\n",
      "train accuracy on epoch 2363: 0.889\n",
      "test loss on epoch 2363: 0.366\n",
      "test accuracy on epoch 2363: 0.769\n",
      "train loss on epoch 2364 : 0.234\n",
      "train accuracy on epoch 2364: 0.944\n",
      "test loss on epoch 2364: 0.369\n",
      "test accuracy on epoch 2364: 0.769\n",
      "train loss on epoch 2365 : 0.095\n",
      "train accuracy on epoch 2365: 1.000\n",
      "test loss on epoch 2365: 0.369\n",
      "test accuracy on epoch 2365: 0.769\n",
      "train loss on epoch 2366 : 0.343\n",
      "train accuracy on epoch 2366: 0.833\n",
      "test loss on epoch 2366: 0.364\n",
      "test accuracy on epoch 2366: 0.769\n",
      "train loss on epoch 2367 : 0.293\n",
      "train accuracy on epoch 2367: 0.889\n",
      "test loss on epoch 2367: 0.362\n",
      "test accuracy on epoch 2367: 0.769\n",
      "train loss on epoch 2368 : 0.125\n",
      "train accuracy on epoch 2368: 0.944\n",
      "test loss on epoch 2368: 0.353\n",
      "test accuracy on epoch 2368: 0.769\n",
      "train loss on epoch 2369 : 0.222\n",
      "train accuracy on epoch 2369: 0.889\n",
      "test loss on epoch 2369: 0.354\n",
      "test accuracy on epoch 2369: 0.769\n",
      "train loss on epoch 2370 : 0.677\n",
      "train accuracy on epoch 2370: 0.667\n",
      "test loss on epoch 2370: 0.349\n",
      "test accuracy on epoch 2370: 0.769\n",
      "train loss on epoch 2371 : 0.135\n",
      "train accuracy on epoch 2371: 0.889\n",
      "test loss on epoch 2371: 0.346\n",
      "test accuracy on epoch 2371: 0.769\n",
      "train loss on epoch 2372 : 0.410\n",
      "train accuracy on epoch 2372: 0.833\n",
      "test loss on epoch 2372: 0.346\n",
      "test accuracy on epoch 2372: 0.769\n",
      "train loss on epoch 2373 : 0.349\n",
      "train accuracy on epoch 2373: 0.833\n",
      "test loss on epoch 2373: 0.352\n",
      "test accuracy on epoch 2373: 0.769\n",
      "train loss on epoch 2374 : 0.146\n",
      "train accuracy on epoch 2374: 0.944\n",
      "test loss on epoch 2374: 0.345\n",
      "test accuracy on epoch 2374: 0.769\n",
      "train loss on epoch 2375 : 0.126\n",
      "train accuracy on epoch 2375: 0.944\n",
      "test loss on epoch 2375: 0.355\n",
      "test accuracy on epoch 2375: 0.769\n",
      "train loss on epoch 2376 : 0.150\n",
      "train accuracy on epoch 2376: 0.944\n",
      "test loss on epoch 2376: 0.354\n",
      "test accuracy on epoch 2376: 0.769\n",
      "train loss on epoch 2377 : 0.154\n",
      "train accuracy on epoch 2377: 0.944\n",
      "test loss on epoch 2377: 0.363\n",
      "test accuracy on epoch 2377: 0.769\n",
      "train loss on epoch 2378 : 0.262\n",
      "train accuracy on epoch 2378: 0.944\n",
      "test loss on epoch 2378: 0.363\n",
      "test accuracy on epoch 2378: 0.769\n",
      "train loss on epoch 2379 : 0.111\n",
      "train accuracy on epoch 2379: 0.944\n",
      "test loss on epoch 2379: 0.379\n",
      "test accuracy on epoch 2379: 0.769\n",
      "train loss on epoch 2380 : 0.254\n",
      "train accuracy on epoch 2380: 0.944\n",
      "test loss on epoch 2380: 0.388\n",
      "test accuracy on epoch 2380: 0.769\n",
      "train loss on epoch 2381 : 0.044\n",
      "train accuracy on epoch 2381: 1.000\n",
      "test loss on epoch 2381: 0.389\n",
      "test accuracy on epoch 2381: 0.769\n",
      "train loss on epoch 2382 : 0.069\n",
      "train accuracy on epoch 2382: 1.000\n",
      "test loss on epoch 2382: 0.389\n",
      "test accuracy on epoch 2382: 0.769\n",
      "train loss on epoch 2383 : 0.084\n",
      "train accuracy on epoch 2383: 1.000\n",
      "test loss on epoch 2383: 0.390\n",
      "test accuracy on epoch 2383: 0.769\n",
      "train loss on epoch 2384 : 0.228\n",
      "train accuracy on epoch 2384: 0.889\n",
      "test loss on epoch 2384: 0.398\n",
      "test accuracy on epoch 2384: 0.769\n",
      "train loss on epoch 2385 : 0.097\n",
      "train accuracy on epoch 2385: 0.944\n",
      "test loss on epoch 2385: 0.404\n",
      "test accuracy on epoch 2385: 0.769\n",
      "train loss on epoch 2386 : 0.183\n",
      "train accuracy on epoch 2386: 0.889\n",
      "test loss on epoch 2386: 0.399\n",
      "test accuracy on epoch 2386: 0.769\n",
      "train loss on epoch 2387 : 0.087\n",
      "train accuracy on epoch 2387: 0.944\n",
      "test loss on epoch 2387: 0.390\n",
      "test accuracy on epoch 2387: 0.769\n",
      "train loss on epoch 2388 : 0.244\n",
      "train accuracy on epoch 2388: 0.944\n",
      "test loss on epoch 2388: 0.379\n",
      "test accuracy on epoch 2388: 0.769\n",
      "train loss on epoch 2389 : 0.226\n",
      "train accuracy on epoch 2389: 0.889\n",
      "test loss on epoch 2389: 0.376\n",
      "test accuracy on epoch 2389: 0.769\n",
      "train loss on epoch 2390 : 0.286\n",
      "train accuracy on epoch 2390: 0.833\n",
      "test loss on epoch 2390: 0.374\n",
      "test accuracy on epoch 2390: 0.769\n",
      "train loss on epoch 2391 : 0.067\n",
      "train accuracy on epoch 2391: 1.000\n",
      "test loss on epoch 2391: 0.378\n",
      "test accuracy on epoch 2391: 0.769\n",
      "train loss on epoch 2392 : 0.362\n",
      "train accuracy on epoch 2392: 0.889\n",
      "test loss on epoch 2392: 0.368\n",
      "test accuracy on epoch 2392: 0.769\n",
      "train loss on epoch 2393 : 0.051\n",
      "train accuracy on epoch 2393: 1.000\n",
      "test loss on epoch 2393: 0.364\n",
      "test accuracy on epoch 2393: 0.769\n",
      "train loss on epoch 2394 : 0.209\n",
      "train accuracy on epoch 2394: 0.944\n",
      "test loss on epoch 2394: 0.364\n",
      "test accuracy on epoch 2394: 0.769\n",
      "train loss on epoch 2395 : 0.104\n",
      "train accuracy on epoch 2395: 0.944\n",
      "test loss on epoch 2395: 0.367\n",
      "test accuracy on epoch 2395: 0.769\n",
      "train loss on epoch 2396 : 0.135\n",
      "train accuracy on epoch 2396: 0.944\n",
      "test loss on epoch 2396: 0.367\n",
      "test accuracy on epoch 2396: 0.769\n",
      "train loss on epoch 2397 : 0.312\n",
      "train accuracy on epoch 2397: 0.944\n",
      "test loss on epoch 2397: 0.368\n",
      "test accuracy on epoch 2397: 0.769\n",
      "train loss on epoch 2398 : 0.171\n",
      "train accuracy on epoch 2398: 0.889\n",
      "test loss on epoch 2398: 0.369\n",
      "test accuracy on epoch 2398: 0.769\n",
      "train loss on epoch 2399 : 0.277\n",
      "train accuracy on epoch 2399: 0.889\n",
      "test loss on epoch 2399: 0.372\n",
      "test accuracy on epoch 2399: 0.769\n",
      "train loss on epoch 2400 : 0.105\n",
      "train accuracy on epoch 2400: 0.889\n",
      "test loss on epoch 2400: 0.376\n",
      "test accuracy on epoch 2400: 0.769\n",
      "train loss on epoch 2401 : 0.080\n",
      "train accuracy on epoch 2401: 0.944\n",
      "test loss on epoch 2401: 0.376\n",
      "test accuracy on epoch 2401: 0.769\n",
      "train loss on epoch 2402 : 0.128\n",
      "train accuracy on epoch 2402: 0.944\n",
      "test loss on epoch 2402: 0.374\n",
      "test accuracy on epoch 2402: 0.769\n",
      "train loss on epoch 2403 : 0.264\n",
      "train accuracy on epoch 2403: 0.944\n",
      "test loss on epoch 2403: 0.374\n",
      "test accuracy on epoch 2403: 0.769\n",
      "train loss on epoch 2404 : 0.225\n",
      "train accuracy on epoch 2404: 0.833\n",
      "test loss on epoch 2404: 0.385\n",
      "test accuracy on epoch 2404: 0.769\n",
      "train loss on epoch 2405 : 0.245\n",
      "train accuracy on epoch 2405: 0.889\n",
      "test loss on epoch 2405: 0.389\n",
      "test accuracy on epoch 2405: 0.769\n",
      "train loss on epoch 2406 : 0.148\n",
      "train accuracy on epoch 2406: 0.944\n",
      "test loss on epoch 2406: 0.383\n",
      "test accuracy on epoch 2406: 0.769\n",
      "train loss on epoch 2407 : 0.145\n",
      "train accuracy on epoch 2407: 0.944\n",
      "test loss on epoch 2407: 0.387\n",
      "test accuracy on epoch 2407: 0.769\n",
      "train loss on epoch 2408 : 0.364\n",
      "train accuracy on epoch 2408: 0.889\n",
      "test loss on epoch 2408: 0.390\n",
      "test accuracy on epoch 2408: 0.769\n",
      "train loss on epoch 2409 : 0.174\n",
      "train accuracy on epoch 2409: 0.944\n",
      "test loss on epoch 2409: 0.379\n",
      "test accuracy on epoch 2409: 0.769\n",
      "train loss on epoch 2410 : 0.100\n",
      "train accuracy on epoch 2410: 0.944\n",
      "test loss on epoch 2410: 0.370\n",
      "test accuracy on epoch 2410: 0.769\n",
      "train loss on epoch 2411 : 0.247\n",
      "train accuracy on epoch 2411: 0.889\n",
      "test loss on epoch 2411: 0.369\n",
      "test accuracy on epoch 2411: 0.769\n",
      "train loss on epoch 2412 : 0.173\n",
      "train accuracy on epoch 2412: 0.944\n",
      "test loss on epoch 2412: 0.368\n",
      "test accuracy on epoch 2412: 0.769\n",
      "train loss on epoch 2413 : 0.272\n",
      "train accuracy on epoch 2413: 0.889\n",
      "test loss on epoch 2413: 0.364\n",
      "test accuracy on epoch 2413: 0.769\n",
      "train loss on epoch 2414 : 0.271\n",
      "train accuracy on epoch 2414: 0.778\n",
      "test loss on epoch 2414: 0.348\n",
      "test accuracy on epoch 2414: 0.769\n",
      "train loss on epoch 2415 : 0.198\n",
      "train accuracy on epoch 2415: 0.889\n",
      "test loss on epoch 2415: 0.349\n",
      "test accuracy on epoch 2415: 0.769\n",
      "train loss on epoch 2416 : 0.219\n",
      "train accuracy on epoch 2416: 0.889\n",
      "test loss on epoch 2416: 0.336\n",
      "test accuracy on epoch 2416: 0.846\n",
      "train loss on epoch 2417 : 0.185\n",
      "train accuracy on epoch 2417: 0.944\n",
      "test loss on epoch 2417: 0.356\n",
      "test accuracy on epoch 2417: 0.769\n",
      "train loss on epoch 2418 : 0.106\n",
      "train accuracy on epoch 2418: 1.000\n",
      "test loss on epoch 2418: 0.358\n",
      "test accuracy on epoch 2418: 0.769\n",
      "train loss on epoch 2419 : 0.187\n",
      "train accuracy on epoch 2419: 0.833\n",
      "test loss on epoch 2419: 0.339\n",
      "test accuracy on epoch 2419: 0.769\n",
      "train loss on epoch 2420 : 0.126\n",
      "train accuracy on epoch 2420: 0.889\n",
      "test loss on epoch 2420: 0.353\n",
      "test accuracy on epoch 2420: 0.769\n",
      "train loss on epoch 2421 : 0.249\n",
      "train accuracy on epoch 2421: 0.833\n",
      "test loss on epoch 2421: 0.339\n",
      "test accuracy on epoch 2421: 0.769\n",
      "train loss on epoch 2422 : 0.081\n",
      "train accuracy on epoch 2422: 1.000\n",
      "test loss on epoch 2422: 0.357\n",
      "test accuracy on epoch 2422: 0.769\n",
      "train loss on epoch 2423 : 0.099\n",
      "train accuracy on epoch 2423: 0.944\n",
      "test loss on epoch 2423: 0.358\n",
      "test accuracy on epoch 2423: 0.769\n",
      "train loss on epoch 2424 : 0.124\n",
      "train accuracy on epoch 2424: 0.944\n",
      "test loss on epoch 2424: 0.355\n",
      "test accuracy on epoch 2424: 0.692\n",
      "train loss on epoch 2425 : 0.180\n",
      "train accuracy on epoch 2425: 0.944\n",
      "test loss on epoch 2425: 0.350\n",
      "test accuracy on epoch 2425: 0.769\n",
      "train loss on epoch 2426 : 0.156\n",
      "train accuracy on epoch 2426: 0.944\n",
      "test loss on epoch 2426: 0.350\n",
      "test accuracy on epoch 2426: 0.769\n",
      "train loss on epoch 2427 : 0.192\n",
      "train accuracy on epoch 2427: 0.944\n",
      "test loss on epoch 2427: 0.363\n",
      "test accuracy on epoch 2427: 0.769\n",
      "train loss on epoch 2428 : 0.177\n",
      "train accuracy on epoch 2428: 0.944\n",
      "test loss on epoch 2428: 0.364\n",
      "test accuracy on epoch 2428: 0.769\n",
      "train loss on epoch 2429 : 0.129\n",
      "train accuracy on epoch 2429: 1.000\n",
      "test loss on epoch 2429: 0.369\n",
      "test accuracy on epoch 2429: 0.769\n",
      "train loss on epoch 2430 : 0.097\n",
      "train accuracy on epoch 2430: 0.944\n",
      "test loss on epoch 2430: 0.370\n",
      "test accuracy on epoch 2430: 0.769\n",
      "train loss on epoch 2431 : 0.247\n",
      "train accuracy on epoch 2431: 0.833\n",
      "test loss on epoch 2431: 0.369\n",
      "test accuracy on epoch 2431: 0.769\n",
      "train loss on epoch 2432 : 0.405\n",
      "train accuracy on epoch 2432: 0.722\n",
      "test loss on epoch 2432: 0.358\n",
      "test accuracy on epoch 2432: 0.769\n",
      "train loss on epoch 2433 : 0.141\n",
      "train accuracy on epoch 2433: 0.944\n",
      "test loss on epoch 2433: 0.353\n",
      "test accuracy on epoch 2433: 0.769\n",
      "train loss on epoch 2434 : 0.076\n",
      "train accuracy on epoch 2434: 1.000\n",
      "test loss on epoch 2434: 0.356\n",
      "test accuracy on epoch 2434: 0.769\n",
      "train loss on epoch 2435 : 0.050\n",
      "train accuracy on epoch 2435: 1.000\n",
      "test loss on epoch 2435: 0.354\n",
      "test accuracy on epoch 2435: 0.769\n",
      "train loss on epoch 2436 : 0.273\n",
      "train accuracy on epoch 2436: 0.889\n",
      "test loss on epoch 2436: 0.346\n",
      "test accuracy on epoch 2436: 0.769\n",
      "train loss on epoch 2437 : 0.093\n",
      "train accuracy on epoch 2437: 0.944\n",
      "test loss on epoch 2437: 0.344\n",
      "test accuracy on epoch 2437: 0.769\n",
      "train loss on epoch 2438 : 0.273\n",
      "train accuracy on epoch 2438: 0.944\n",
      "test loss on epoch 2438: 0.341\n",
      "test accuracy on epoch 2438: 0.769\n",
      "train loss on epoch 2439 : 0.160\n",
      "train accuracy on epoch 2439: 0.889\n",
      "test loss on epoch 2439: 0.347\n",
      "test accuracy on epoch 2439: 0.769\n",
      "train loss on epoch 2440 : 0.152\n",
      "train accuracy on epoch 2440: 0.944\n",
      "test loss on epoch 2440: 0.348\n",
      "test accuracy on epoch 2440: 0.769\n",
      "train loss on epoch 2441 : 0.193\n",
      "train accuracy on epoch 2441: 0.889\n",
      "test loss on epoch 2441: 0.356\n",
      "test accuracy on epoch 2441: 0.769\n",
      "train loss on epoch 2442 : 0.161\n",
      "train accuracy on epoch 2442: 0.944\n",
      "test loss on epoch 2442: 0.354\n",
      "test accuracy on epoch 2442: 0.769\n",
      "train loss on epoch 2443 : 0.142\n",
      "train accuracy on epoch 2443: 0.944\n",
      "test loss on epoch 2443: 0.360\n",
      "test accuracy on epoch 2443: 0.769\n",
      "train loss on epoch 2444 : 0.100\n",
      "train accuracy on epoch 2444: 0.944\n",
      "test loss on epoch 2444: 0.364\n",
      "test accuracy on epoch 2444: 0.769\n",
      "train loss on epoch 2445 : 0.097\n",
      "train accuracy on epoch 2445: 0.944\n",
      "test loss on epoch 2445: 0.358\n",
      "test accuracy on epoch 2445: 0.769\n",
      "train loss on epoch 2446 : 0.182\n",
      "train accuracy on epoch 2446: 0.944\n",
      "test loss on epoch 2446: 0.361\n",
      "test accuracy on epoch 2446: 0.769\n",
      "train loss on epoch 2447 : 0.165\n",
      "train accuracy on epoch 2447: 0.944\n",
      "test loss on epoch 2447: 0.357\n",
      "test accuracy on epoch 2447: 0.769\n",
      "train loss on epoch 2448 : 0.059\n",
      "train accuracy on epoch 2448: 1.000\n",
      "test loss on epoch 2448: 0.351\n",
      "test accuracy on epoch 2448: 0.769\n",
      "train loss on epoch 2449 : 0.072\n",
      "train accuracy on epoch 2449: 1.000\n",
      "test loss on epoch 2449: 0.346\n",
      "test accuracy on epoch 2449: 0.769\n",
      "train loss on epoch 2450 : 0.150\n",
      "train accuracy on epoch 2450: 0.889\n",
      "test loss on epoch 2450: 0.351\n",
      "test accuracy on epoch 2450: 0.769\n",
      "train loss on epoch 2451 : 0.043\n",
      "train accuracy on epoch 2451: 1.000\n",
      "test loss on epoch 2451: 0.347\n",
      "test accuracy on epoch 2451: 0.769\n",
      "train loss on epoch 2452 : 0.098\n",
      "train accuracy on epoch 2452: 1.000\n",
      "test loss on epoch 2452: 0.345\n",
      "test accuracy on epoch 2452: 0.769\n",
      "train loss on epoch 2453 : 0.299\n",
      "train accuracy on epoch 2453: 0.889\n",
      "test loss on epoch 2453: 0.346\n",
      "test accuracy on epoch 2453: 0.769\n",
      "train loss on epoch 2454 : 0.152\n",
      "train accuracy on epoch 2454: 0.944\n",
      "test loss on epoch 2454: 0.338\n",
      "test accuracy on epoch 2454: 0.769\n",
      "train loss on epoch 2455 : 0.295\n",
      "train accuracy on epoch 2455: 0.944\n",
      "test loss on epoch 2455: 0.345\n",
      "test accuracy on epoch 2455: 0.769\n",
      "train loss on epoch 2456 : 0.382\n",
      "train accuracy on epoch 2456: 0.944\n",
      "test loss on epoch 2456: 0.355\n",
      "test accuracy on epoch 2456: 0.769\n",
      "train loss on epoch 2457 : 0.350\n",
      "train accuracy on epoch 2457: 0.889\n",
      "test loss on epoch 2457: 0.358\n",
      "test accuracy on epoch 2457: 0.769\n",
      "train loss on epoch 2458 : 0.212\n",
      "train accuracy on epoch 2458: 0.889\n",
      "test loss on epoch 2458: 0.359\n",
      "test accuracy on epoch 2458: 0.769\n",
      "train loss on epoch 2459 : 0.037\n",
      "train accuracy on epoch 2459: 1.000\n",
      "test loss on epoch 2459: 0.365\n",
      "test accuracy on epoch 2459: 0.769\n",
      "train loss on epoch 2460 : 0.137\n",
      "train accuracy on epoch 2460: 0.944\n",
      "test loss on epoch 2460: 0.373\n",
      "test accuracy on epoch 2460: 0.769\n",
      "train loss on epoch 2461 : 0.253\n",
      "train accuracy on epoch 2461: 0.833\n",
      "test loss on epoch 2461: 0.371\n",
      "test accuracy on epoch 2461: 0.769\n",
      "train loss on epoch 2462 : 0.120\n",
      "train accuracy on epoch 2462: 0.944\n",
      "test loss on epoch 2462: 0.373\n",
      "test accuracy on epoch 2462: 0.769\n",
      "train loss on epoch 2463 : 0.138\n",
      "train accuracy on epoch 2463: 0.944\n",
      "test loss on epoch 2463: 0.371\n",
      "test accuracy on epoch 2463: 0.769\n",
      "train loss on epoch 2464 : 0.074\n",
      "train accuracy on epoch 2464: 1.000\n",
      "test loss on epoch 2464: 0.368\n",
      "test accuracy on epoch 2464: 0.769\n",
      "train loss on epoch 2465 : 0.154\n",
      "train accuracy on epoch 2465: 0.889\n",
      "test loss on epoch 2465: 0.367\n",
      "test accuracy on epoch 2465: 0.769\n",
      "train loss on epoch 2466 : 0.129\n",
      "train accuracy on epoch 2466: 0.944\n",
      "test loss on epoch 2466: 0.372\n",
      "test accuracy on epoch 2466: 0.769\n",
      "train loss on epoch 2467 : 0.143\n",
      "train accuracy on epoch 2467: 0.889\n",
      "test loss on epoch 2467: 0.381\n",
      "test accuracy on epoch 2467: 0.769\n",
      "train loss on epoch 2468 : 0.113\n",
      "train accuracy on epoch 2468: 0.944\n",
      "test loss on epoch 2468: 0.386\n",
      "test accuracy on epoch 2468: 0.769\n",
      "train loss on epoch 2469 : 0.161\n",
      "train accuracy on epoch 2469: 0.944\n",
      "test loss on epoch 2469: 0.383\n",
      "test accuracy on epoch 2469: 0.769\n",
      "train loss on epoch 2470 : 0.386\n",
      "train accuracy on epoch 2470: 0.889\n",
      "test loss on epoch 2470: 0.375\n",
      "test accuracy on epoch 2470: 0.769\n",
      "train loss on epoch 2471 : 0.204\n",
      "train accuracy on epoch 2471: 0.833\n",
      "test loss on epoch 2471: 0.372\n",
      "test accuracy on epoch 2471: 0.769\n",
      "train loss on epoch 2472 : 0.195\n",
      "train accuracy on epoch 2472: 0.944\n",
      "test loss on epoch 2472: 0.372\n",
      "test accuracy on epoch 2472: 0.769\n",
      "train loss on epoch 2473 : 0.185\n",
      "train accuracy on epoch 2473: 0.944\n",
      "test loss on epoch 2473: 0.370\n",
      "test accuracy on epoch 2473: 0.769\n",
      "train loss on epoch 2474 : 0.199\n",
      "train accuracy on epoch 2474: 0.889\n",
      "test loss on epoch 2474: 0.359\n",
      "test accuracy on epoch 2474: 0.769\n",
      "train loss on epoch 2475 : 0.322\n",
      "train accuracy on epoch 2475: 0.889\n",
      "test loss on epoch 2475: 0.352\n",
      "test accuracy on epoch 2475: 0.769\n",
      "train loss on epoch 2476 : 0.439\n",
      "train accuracy on epoch 2476: 0.833\n",
      "test loss on epoch 2476: 0.356\n",
      "test accuracy on epoch 2476: 0.769\n",
      "train loss on epoch 2477 : 0.068\n",
      "train accuracy on epoch 2477: 0.944\n",
      "test loss on epoch 2477: 0.361\n",
      "test accuracy on epoch 2477: 0.769\n",
      "train loss on epoch 2478 : 0.036\n",
      "train accuracy on epoch 2478: 1.000\n",
      "test loss on epoch 2478: 0.354\n",
      "test accuracy on epoch 2478: 0.769\n",
      "train loss on epoch 2479 : 0.238\n",
      "train accuracy on epoch 2479: 0.889\n",
      "test loss on epoch 2479: 0.362\n",
      "test accuracy on epoch 2479: 0.769\n",
      "train loss on epoch 2480 : 0.258\n",
      "train accuracy on epoch 2480: 0.889\n",
      "test loss on epoch 2480: 0.360\n",
      "test accuracy on epoch 2480: 0.769\n",
      "train loss on epoch 2481 : 0.381\n",
      "train accuracy on epoch 2481: 0.833\n",
      "test loss on epoch 2481: 0.362\n",
      "test accuracy on epoch 2481: 0.769\n",
      "train loss on epoch 2482 : 0.104\n",
      "train accuracy on epoch 2482: 1.000\n",
      "test loss on epoch 2482: 0.356\n",
      "test accuracy on epoch 2482: 0.769\n",
      "train loss on epoch 2483 : 0.179\n",
      "train accuracy on epoch 2483: 0.944\n",
      "test loss on epoch 2483: 0.361\n",
      "test accuracy on epoch 2483: 0.769\n",
      "train loss on epoch 2484 : 0.324\n",
      "train accuracy on epoch 2484: 0.833\n",
      "test loss on epoch 2484: 0.351\n",
      "test accuracy on epoch 2484: 0.769\n",
      "train loss on epoch 2485 : 0.174\n",
      "train accuracy on epoch 2485: 0.889\n",
      "test loss on epoch 2485: 0.351\n",
      "test accuracy on epoch 2485: 0.769\n",
      "train loss on epoch 2486 : 0.065\n",
      "train accuracy on epoch 2486: 1.000\n",
      "test loss on epoch 2486: 0.357\n",
      "test accuracy on epoch 2486: 0.769\n",
      "train loss on epoch 2487 : 0.109\n",
      "train accuracy on epoch 2487: 0.944\n",
      "test loss on epoch 2487: 0.347\n",
      "test accuracy on epoch 2487: 0.769\n",
      "train loss on epoch 2488 : 0.125\n",
      "train accuracy on epoch 2488: 0.944\n",
      "test loss on epoch 2488: 0.346\n",
      "test accuracy on epoch 2488: 0.769\n",
      "train loss on epoch 2489 : 0.062\n",
      "train accuracy on epoch 2489: 1.000\n",
      "test loss on epoch 2489: 0.344\n",
      "test accuracy on epoch 2489: 0.846\n",
      "train loss on epoch 2490 : 0.096\n",
      "train accuracy on epoch 2490: 0.944\n",
      "test loss on epoch 2490: 0.356\n",
      "test accuracy on epoch 2490: 0.692\n",
      "train loss on epoch 2491 : 0.378\n",
      "train accuracy on epoch 2491: 0.889\n",
      "test loss on epoch 2491: 0.361\n",
      "test accuracy on epoch 2491: 0.692\n",
      "train loss on epoch 2492 : 0.038\n",
      "train accuracy on epoch 2492: 1.000\n",
      "test loss on epoch 2492: 0.361\n",
      "test accuracy on epoch 2492: 0.692\n",
      "train loss on epoch 2493 : 0.405\n",
      "train accuracy on epoch 2493: 0.889\n",
      "test loss on epoch 2493: 0.363\n",
      "test accuracy on epoch 2493: 0.692\n",
      "train loss on epoch 2494 : 0.518\n",
      "train accuracy on epoch 2494: 0.889\n",
      "test loss on epoch 2494: 0.348\n",
      "test accuracy on epoch 2494: 0.769\n",
      "train loss on epoch 2495 : 0.164\n",
      "train accuracy on epoch 2495: 0.944\n",
      "test loss on epoch 2495: 0.360\n",
      "test accuracy on epoch 2495: 0.769\n",
      "train loss on epoch 2496 : 0.113\n",
      "train accuracy on epoch 2496: 0.944\n",
      "test loss on epoch 2496: 0.368\n",
      "test accuracy on epoch 2496: 0.692\n",
      "train loss on epoch 2497 : 0.432\n",
      "train accuracy on epoch 2497: 0.833\n",
      "test loss on epoch 2497: 0.370\n",
      "test accuracy on epoch 2497: 0.692\n",
      "train loss on epoch 2498 : 0.055\n",
      "train accuracy on epoch 2498: 1.000\n",
      "test loss on epoch 2498: 0.371\n",
      "test accuracy on epoch 2498: 0.692\n",
      "train loss on epoch 2499 : 0.141\n",
      "train accuracy on epoch 2499: 0.944\n",
      "test loss on epoch 2499: 0.350\n",
      "test accuracy on epoch 2499: 0.769\n",
      "train loss on epoch 2500 : 0.172\n",
      "train accuracy on epoch 2500: 0.889\n",
      "test loss on epoch 2500: 0.371\n",
      "test accuracy on epoch 2500: 0.769\n",
      "train loss on epoch 2501 : 0.372\n",
      "train accuracy on epoch 2501: 0.722\n",
      "test loss on epoch 2501: 0.354\n",
      "test accuracy on epoch 2501: 0.769\n",
      "train loss on epoch 2502 : 0.273\n",
      "train accuracy on epoch 2502: 0.833\n",
      "test loss on epoch 2502: 0.373\n",
      "test accuracy on epoch 2502: 0.692\n",
      "train loss on epoch 2503 : 0.069\n",
      "train accuracy on epoch 2503: 1.000\n",
      "test loss on epoch 2503: 0.353\n",
      "test accuracy on epoch 2503: 0.846\n",
      "train loss on epoch 2504 : 0.111\n",
      "train accuracy on epoch 2504: 1.000\n",
      "test loss on epoch 2504: 0.358\n",
      "test accuracy on epoch 2504: 0.769\n",
      "train loss on epoch 2505 : 0.069\n",
      "train accuracy on epoch 2505: 0.944\n",
      "test loss on epoch 2505: 0.375\n",
      "test accuracy on epoch 2505: 0.769\n",
      "train loss on epoch 2506 : 0.197\n",
      "train accuracy on epoch 2506: 0.889\n",
      "test loss on epoch 2506: 0.362\n",
      "test accuracy on epoch 2506: 0.769\n",
      "train loss on epoch 2507 : 0.316\n",
      "train accuracy on epoch 2507: 0.944\n",
      "test loss on epoch 2507: 0.372\n",
      "test accuracy on epoch 2507: 0.769\n",
      "train loss on epoch 2508 : 0.147\n",
      "train accuracy on epoch 2508: 0.944\n",
      "test loss on epoch 2508: 0.382\n",
      "test accuracy on epoch 2508: 0.769\n",
      "train loss on epoch 2509 : 0.179\n",
      "train accuracy on epoch 2509: 0.944\n",
      "test loss on epoch 2509: 0.388\n",
      "test accuracy on epoch 2509: 0.769\n",
      "train loss on epoch 2510 : 0.145\n",
      "train accuracy on epoch 2510: 0.889\n",
      "test loss on epoch 2510: 0.387\n",
      "test accuracy on epoch 2510: 0.769\n",
      "train loss on epoch 2511 : 0.151\n",
      "train accuracy on epoch 2511: 0.944\n",
      "test loss on epoch 2511: 0.380\n",
      "test accuracy on epoch 2511: 0.769\n",
      "train loss on epoch 2512 : 0.054\n",
      "train accuracy on epoch 2512: 1.000\n",
      "test loss on epoch 2512: 0.382\n",
      "test accuracy on epoch 2512: 0.769\n",
      "train loss on epoch 2513 : 0.174\n",
      "train accuracy on epoch 2513: 0.944\n",
      "test loss on epoch 2513: 0.374\n",
      "test accuracy on epoch 2513: 0.769\n",
      "train loss on epoch 2514 : 0.036\n",
      "train accuracy on epoch 2514: 1.000\n",
      "test loss on epoch 2514: 0.375\n",
      "test accuracy on epoch 2514: 0.769\n",
      "train loss on epoch 2515 : 0.185\n",
      "train accuracy on epoch 2515: 0.944\n",
      "test loss on epoch 2515: 0.361\n",
      "test accuracy on epoch 2515: 0.769\n",
      "train loss on epoch 2516 : 0.229\n",
      "train accuracy on epoch 2516: 0.889\n",
      "test loss on epoch 2516: 0.374\n",
      "test accuracy on epoch 2516: 0.769\n",
      "train loss on epoch 2517 : 0.130\n",
      "train accuracy on epoch 2517: 0.944\n",
      "test loss on epoch 2517: 0.356\n",
      "test accuracy on epoch 2517: 0.769\n",
      "train loss on epoch 2518 : 0.217\n",
      "train accuracy on epoch 2518: 0.833\n",
      "test loss on epoch 2518: 0.374\n",
      "test accuracy on epoch 2518: 0.769\n",
      "train loss on epoch 2519 : 0.289\n",
      "train accuracy on epoch 2519: 0.889\n",
      "test loss on epoch 2519: 0.364\n",
      "test accuracy on epoch 2519: 0.769\n",
      "train loss on epoch 2520 : 0.295\n",
      "train accuracy on epoch 2520: 0.944\n",
      "test loss on epoch 2520: 0.358\n",
      "test accuracy on epoch 2520: 0.769\n",
      "train loss on epoch 2521 : 0.094\n",
      "train accuracy on epoch 2521: 1.000\n",
      "test loss on epoch 2521: 0.367\n",
      "test accuracy on epoch 2521: 0.769\n",
      "train loss on epoch 2522 : 0.303\n",
      "train accuracy on epoch 2522: 0.833\n",
      "test loss on epoch 2522: 0.379\n",
      "test accuracy on epoch 2522: 0.769\n",
      "train loss on epoch 2523 : 0.348\n",
      "train accuracy on epoch 2523: 0.833\n",
      "test loss on epoch 2523: 0.377\n",
      "test accuracy on epoch 2523: 0.769\n",
      "train loss on epoch 2524 : 0.155\n",
      "train accuracy on epoch 2524: 0.944\n",
      "test loss on epoch 2524: 0.385\n",
      "test accuracy on epoch 2524: 0.769\n",
      "train loss on epoch 2525 : 0.150\n",
      "train accuracy on epoch 2525: 0.889\n",
      "test loss on epoch 2525: 0.388\n",
      "test accuracy on epoch 2525: 0.769\n",
      "train loss on epoch 2526 : 0.127\n",
      "train accuracy on epoch 2526: 0.944\n",
      "test loss on epoch 2526: 0.392\n",
      "test accuracy on epoch 2526: 0.769\n",
      "train loss on epoch 2527 : 0.294\n",
      "train accuracy on epoch 2527: 0.944\n",
      "test loss on epoch 2527: 0.386\n",
      "test accuracy on epoch 2527: 0.769\n",
      "train loss on epoch 2528 : 0.415\n",
      "train accuracy on epoch 2528: 0.833\n",
      "test loss on epoch 2528: 0.368\n",
      "test accuracy on epoch 2528: 0.769\n",
      "train loss on epoch 2529 : 0.117\n",
      "train accuracy on epoch 2529: 0.889\n",
      "test loss on epoch 2529: 0.373\n",
      "test accuracy on epoch 2529: 0.769\n",
      "train loss on epoch 2530 : 0.247\n",
      "train accuracy on epoch 2530: 0.778\n",
      "test loss on epoch 2530: 0.362\n",
      "test accuracy on epoch 2530: 0.769\n",
      "train loss on epoch 2531 : 0.082\n",
      "train accuracy on epoch 2531: 0.944\n",
      "test loss on epoch 2531: 0.351\n",
      "test accuracy on epoch 2531: 0.846\n",
      "train loss on epoch 2532 : 0.158\n",
      "train accuracy on epoch 2532: 0.944\n",
      "test loss on epoch 2532: 0.363\n",
      "test accuracy on epoch 2532: 0.769\n",
      "train loss on epoch 2533 : 0.251\n",
      "train accuracy on epoch 2533: 0.833\n",
      "test loss on epoch 2533: 0.357\n",
      "test accuracy on epoch 2533: 0.769\n",
      "train loss on epoch 2534 : 0.192\n",
      "train accuracy on epoch 2534: 0.889\n",
      "test loss on epoch 2534: 0.355\n",
      "test accuracy on epoch 2534: 0.769\n",
      "train loss on epoch 2535 : 0.105\n",
      "train accuracy on epoch 2535: 1.000\n",
      "test loss on epoch 2535: 0.349\n",
      "test accuracy on epoch 2535: 0.846\n",
      "train loss on epoch 2536 : 0.469\n",
      "train accuracy on epoch 2536: 0.889\n",
      "test loss on epoch 2536: 0.361\n",
      "test accuracy on epoch 2536: 0.769\n",
      "train loss on epoch 2537 : 0.091\n",
      "train accuracy on epoch 2537: 1.000\n",
      "test loss on epoch 2537: 0.370\n",
      "test accuracy on epoch 2537: 0.769\n",
      "train loss on epoch 2538 : 0.248\n",
      "train accuracy on epoch 2538: 0.889\n",
      "test loss on epoch 2538: 0.373\n",
      "test accuracy on epoch 2538: 0.769\n",
      "train loss on epoch 2539 : 0.190\n",
      "train accuracy on epoch 2539: 0.833\n",
      "test loss on epoch 2539: 0.376\n",
      "test accuracy on epoch 2539: 0.769\n",
      "train loss on epoch 2540 : 0.135\n",
      "train accuracy on epoch 2540: 0.944\n",
      "test loss on epoch 2540: 0.383\n",
      "test accuracy on epoch 2540: 0.769\n",
      "train loss on epoch 2541 : 0.170\n",
      "train accuracy on epoch 2541: 0.889\n",
      "test loss on epoch 2541: 0.384\n",
      "test accuracy on epoch 2541: 0.769\n",
      "train loss on epoch 2542 : 0.231\n",
      "train accuracy on epoch 2542: 0.944\n",
      "test loss on epoch 2542: 0.392\n",
      "test accuracy on epoch 2542: 0.769\n",
      "train loss on epoch 2543 : 0.133\n",
      "train accuracy on epoch 2543: 0.944\n",
      "test loss on epoch 2543: 0.396\n",
      "test accuracy on epoch 2543: 0.769\n",
      "train loss on epoch 2544 : 0.308\n",
      "train accuracy on epoch 2544: 0.944\n",
      "test loss on epoch 2544: 0.392\n",
      "test accuracy on epoch 2544: 0.769\n",
      "train loss on epoch 2545 : 0.171\n",
      "train accuracy on epoch 2545: 0.944\n",
      "test loss on epoch 2545: 0.383\n",
      "test accuracy on epoch 2545: 0.769\n",
      "train loss on epoch 2546 : 0.132\n",
      "train accuracy on epoch 2546: 0.944\n",
      "test loss on epoch 2546: 0.369\n",
      "test accuracy on epoch 2546: 0.769\n",
      "train loss on epoch 2547 : 0.214\n",
      "train accuracy on epoch 2547: 0.889\n",
      "test loss on epoch 2547: 0.361\n",
      "test accuracy on epoch 2547: 0.769\n",
      "train loss on epoch 2548 : 0.108\n",
      "train accuracy on epoch 2548: 1.000\n",
      "test loss on epoch 2548: 0.363\n",
      "test accuracy on epoch 2548: 0.769\n",
      "train loss on epoch 2549 : 0.047\n",
      "train accuracy on epoch 2549: 1.000\n",
      "test loss on epoch 2549: 0.355\n",
      "test accuracy on epoch 2549: 0.769\n",
      "train loss on epoch 2550 : 0.162\n",
      "train accuracy on epoch 2550: 0.889\n",
      "test loss on epoch 2550: 0.348\n",
      "test accuracy on epoch 2550: 0.769\n",
      "train loss on epoch 2551 : 0.254\n",
      "train accuracy on epoch 2551: 0.778\n",
      "test loss on epoch 2551: 0.363\n",
      "test accuracy on epoch 2551: 0.769\n",
      "train loss on epoch 2552 : 0.064\n",
      "train accuracy on epoch 2552: 1.000\n",
      "test loss on epoch 2552: 0.358\n",
      "test accuracy on epoch 2552: 0.769\n",
      "train loss on epoch 2553 : 0.048\n",
      "train accuracy on epoch 2553: 1.000\n",
      "test loss on epoch 2553: 0.359\n",
      "test accuracy on epoch 2553: 0.769\n",
      "train loss on epoch 2554 : 0.244\n",
      "train accuracy on epoch 2554: 0.889\n",
      "test loss on epoch 2554: 0.367\n",
      "test accuracy on epoch 2554: 0.769\n",
      "train loss on epoch 2555 : 0.053\n",
      "train accuracy on epoch 2555: 1.000\n",
      "test loss on epoch 2555: 0.369\n",
      "test accuracy on epoch 2555: 0.769\n",
      "train loss on epoch 2556 : 0.166\n",
      "train accuracy on epoch 2556: 0.944\n",
      "test loss on epoch 2556: 0.370\n",
      "test accuracy on epoch 2556: 0.769\n",
      "train loss on epoch 2557 : 0.299\n",
      "train accuracy on epoch 2557: 0.833\n",
      "test loss on epoch 2557: 0.367\n",
      "test accuracy on epoch 2557: 0.769\n",
      "train loss on epoch 2558 : 0.249\n",
      "train accuracy on epoch 2558: 0.889\n",
      "test loss on epoch 2558: 0.373\n",
      "test accuracy on epoch 2558: 0.769\n",
      "train loss on epoch 2559 : 0.235\n",
      "train accuracy on epoch 2559: 0.889\n",
      "test loss on epoch 2559: 0.367\n",
      "test accuracy on epoch 2559: 0.769\n",
      "train loss on epoch 2560 : 0.055\n",
      "train accuracy on epoch 2560: 1.000\n",
      "test loss on epoch 2560: 0.366\n",
      "test accuracy on epoch 2560: 0.769\n",
      "train loss on epoch 2561 : 0.098\n",
      "train accuracy on epoch 2561: 0.944\n",
      "test loss on epoch 2561: 0.364\n",
      "test accuracy on epoch 2561: 0.769\n",
      "train loss on epoch 2562 : 0.121\n",
      "train accuracy on epoch 2562: 0.944\n",
      "test loss on epoch 2562: 0.361\n",
      "test accuracy on epoch 2562: 0.769\n",
      "train loss on epoch 2563 : 0.198\n",
      "train accuracy on epoch 2563: 0.944\n",
      "test loss on epoch 2563: 0.365\n",
      "test accuracy on epoch 2563: 0.769\n",
      "train loss on epoch 2564 : 0.141\n",
      "train accuracy on epoch 2564: 0.889\n",
      "test loss on epoch 2564: 0.371\n",
      "test accuracy on epoch 2564: 0.769\n",
      "train loss on epoch 2565 : 0.080\n",
      "train accuracy on epoch 2565: 0.944\n",
      "test loss on epoch 2565: 0.369\n",
      "test accuracy on epoch 2565: 0.769\n",
      "train loss on epoch 2566 : 0.211\n",
      "train accuracy on epoch 2566: 0.889\n",
      "test loss on epoch 2566: 0.362\n",
      "test accuracy on epoch 2566: 0.769\n",
      "train loss on epoch 2567 : 0.218\n",
      "train accuracy on epoch 2567: 0.889\n",
      "test loss on epoch 2567: 0.377\n",
      "test accuracy on epoch 2567: 0.769\n",
      "train loss on epoch 2568 : 0.117\n",
      "train accuracy on epoch 2568: 0.944\n",
      "test loss on epoch 2568: 0.374\n",
      "test accuracy on epoch 2568: 0.769\n",
      "train loss on epoch 2569 : 0.184\n",
      "train accuracy on epoch 2569: 0.889\n",
      "test loss on epoch 2569: 0.372\n",
      "test accuracy on epoch 2569: 0.769\n",
      "train loss on epoch 2570 : 0.148\n",
      "train accuracy on epoch 2570: 0.944\n",
      "test loss on epoch 2570: 0.364\n",
      "test accuracy on epoch 2570: 0.769\n",
      "train loss on epoch 2571 : 0.066\n",
      "train accuracy on epoch 2571: 1.000\n",
      "test loss on epoch 2571: 0.367\n",
      "test accuracy on epoch 2571: 0.769\n",
      "train loss on epoch 2572 : 0.198\n",
      "train accuracy on epoch 2572: 0.889\n",
      "test loss on epoch 2572: 0.377\n",
      "test accuracy on epoch 2572: 0.769\n",
      "train loss on epoch 2573 : 0.383\n",
      "train accuracy on epoch 2573: 0.889\n",
      "test loss on epoch 2573: 0.377\n",
      "test accuracy on epoch 2573: 0.769\n",
      "train loss on epoch 2574 : 0.039\n",
      "train accuracy on epoch 2574: 1.000\n",
      "test loss on epoch 2574: 0.374\n",
      "test accuracy on epoch 2574: 0.769\n",
      "train loss on epoch 2575 : 0.149\n",
      "train accuracy on epoch 2575: 0.944\n",
      "test loss on epoch 2575: 0.366\n",
      "test accuracy on epoch 2575: 0.769\n",
      "train loss on epoch 2576 : 0.115\n",
      "train accuracy on epoch 2576: 0.944\n",
      "test loss on epoch 2576: 0.365\n",
      "test accuracy on epoch 2576: 0.769\n",
      "train loss on epoch 2577 : 0.245\n",
      "train accuracy on epoch 2577: 0.889\n",
      "test loss on epoch 2577: 0.361\n",
      "test accuracy on epoch 2577: 0.769\n",
      "train loss on epoch 2578 : 0.308\n",
      "train accuracy on epoch 2578: 0.889\n",
      "test loss on epoch 2578: 0.361\n",
      "test accuracy on epoch 2578: 0.769\n",
      "train loss on epoch 2579 : 0.144\n",
      "train accuracy on epoch 2579: 0.944\n",
      "test loss on epoch 2579: 0.359\n",
      "test accuracy on epoch 2579: 0.769\n",
      "train loss on epoch 2580 : 0.286\n",
      "train accuracy on epoch 2580: 0.833\n",
      "test loss on epoch 2580: 0.361\n",
      "test accuracy on epoch 2580: 0.769\n",
      "train loss on epoch 2581 : 0.192\n",
      "train accuracy on epoch 2581: 0.889\n",
      "test loss on epoch 2581: 0.359\n",
      "test accuracy on epoch 2581: 0.769\n",
      "train loss on epoch 2582 : 0.211\n",
      "train accuracy on epoch 2582: 0.944\n",
      "test loss on epoch 2582: 0.366\n",
      "test accuracy on epoch 2582: 0.769\n",
      "train loss on epoch 2583 : 0.129\n",
      "train accuracy on epoch 2583: 0.944\n",
      "test loss on epoch 2583: 0.363\n",
      "test accuracy on epoch 2583: 0.769\n",
      "train loss on epoch 2584 : 0.062\n",
      "train accuracy on epoch 2584: 0.944\n",
      "test loss on epoch 2584: 0.361\n",
      "test accuracy on epoch 2584: 0.769\n",
      "train loss on epoch 2585 : 0.123\n",
      "train accuracy on epoch 2585: 0.944\n",
      "test loss on epoch 2585: 0.367\n",
      "test accuracy on epoch 2585: 0.769\n",
      "train loss on epoch 2586 : 0.409\n",
      "train accuracy on epoch 2586: 0.778\n",
      "test loss on epoch 2586: 0.367\n",
      "test accuracy on epoch 2586: 0.769\n",
      "train loss on epoch 2587 : 0.200\n",
      "train accuracy on epoch 2587: 0.889\n",
      "test loss on epoch 2587: 0.374\n",
      "test accuracy on epoch 2587: 0.769\n",
      "train loss on epoch 2588 : 0.251\n",
      "train accuracy on epoch 2588: 0.833\n",
      "test loss on epoch 2588: 0.377\n",
      "test accuracy on epoch 2588: 0.769\n",
      "train loss on epoch 2589 : 0.148\n",
      "train accuracy on epoch 2589: 0.889\n",
      "test loss on epoch 2589: 0.381\n",
      "test accuracy on epoch 2589: 0.769\n",
      "train loss on epoch 2590 : 0.373\n",
      "train accuracy on epoch 2590: 0.944\n",
      "test loss on epoch 2590: 0.379\n",
      "test accuracy on epoch 2590: 0.769\n",
      "train loss on epoch 2591 : 0.454\n",
      "train accuracy on epoch 2591: 0.889\n",
      "test loss on epoch 2591: 0.383\n",
      "test accuracy on epoch 2591: 0.769\n",
      "train loss on epoch 2592 : 0.116\n",
      "train accuracy on epoch 2592: 0.944\n",
      "test loss on epoch 2592: 0.388\n",
      "test accuracy on epoch 2592: 0.769\n",
      "train loss on epoch 2593 : 0.120\n",
      "train accuracy on epoch 2593: 0.944\n",
      "test loss on epoch 2593: 0.400\n",
      "test accuracy on epoch 2593: 0.769\n",
      "train loss on epoch 2594 : 0.141\n",
      "train accuracy on epoch 2594: 0.944\n",
      "test loss on epoch 2594: 0.405\n",
      "test accuracy on epoch 2594: 0.769\n",
      "train loss on epoch 2595 : 0.267\n",
      "train accuracy on epoch 2595: 0.889\n",
      "test loss on epoch 2595: 0.405\n",
      "test accuracy on epoch 2595: 0.769\n",
      "train loss on epoch 2596 : 0.101\n",
      "train accuracy on epoch 2596: 0.944\n",
      "test loss on epoch 2596: 0.412\n",
      "test accuracy on epoch 2596: 0.769\n",
      "train loss on epoch 2597 : 0.272\n",
      "train accuracy on epoch 2597: 0.889\n",
      "test loss on epoch 2597: 0.405\n",
      "test accuracy on epoch 2597: 0.769\n",
      "train loss on epoch 2598 : 0.131\n",
      "train accuracy on epoch 2598: 0.889\n",
      "test loss on epoch 2598: 0.400\n",
      "test accuracy on epoch 2598: 0.769\n",
      "train loss on epoch 2599 : 0.400\n",
      "train accuracy on epoch 2599: 0.944\n",
      "test loss on epoch 2599: 0.385\n",
      "test accuracy on epoch 2599: 0.769\n",
      "train loss on epoch 2600 : 0.036\n",
      "train accuracy on epoch 2600: 1.000\n",
      "test loss on epoch 2600: 0.373\n",
      "test accuracy on epoch 2600: 0.769\n",
      "train loss on epoch 2601 : 0.267\n",
      "train accuracy on epoch 2601: 0.833\n",
      "test loss on epoch 2601: 0.362\n",
      "test accuracy on epoch 2601: 0.769\n",
      "train loss on epoch 2602 : 0.080\n",
      "train accuracy on epoch 2602: 0.944\n",
      "test loss on epoch 2602: 0.344\n",
      "test accuracy on epoch 2602: 0.769\n",
      "train loss on epoch 2603 : 0.106\n",
      "train accuracy on epoch 2603: 0.944\n",
      "test loss on epoch 2603: 0.345\n",
      "test accuracy on epoch 2603: 0.769\n",
      "train loss on epoch 2604 : 0.046\n",
      "train accuracy on epoch 2604: 1.000\n",
      "test loss on epoch 2604: 0.342\n",
      "test accuracy on epoch 2604: 0.769\n",
      "train loss on epoch 2605 : 0.170\n",
      "train accuracy on epoch 2605: 0.944\n",
      "test loss on epoch 2605: 0.336\n",
      "test accuracy on epoch 2605: 0.769\n",
      "train loss on epoch 2606 : 0.455\n",
      "train accuracy on epoch 2606: 0.889\n",
      "test loss on epoch 2606: 0.347\n",
      "test accuracy on epoch 2606: 0.769\n",
      "train loss on epoch 2607 : 0.078\n",
      "train accuracy on epoch 2607: 1.000\n",
      "test loss on epoch 2607: 0.344\n",
      "test accuracy on epoch 2607: 0.769\n",
      "train loss on epoch 2608 : 0.050\n",
      "train accuracy on epoch 2608: 1.000\n",
      "test loss on epoch 2608: 0.333\n",
      "test accuracy on epoch 2608: 0.769\n",
      "train loss on epoch 2609 : 0.223\n",
      "train accuracy on epoch 2609: 0.944\n",
      "test loss on epoch 2609: 0.350\n",
      "test accuracy on epoch 2609: 0.769\n",
      "train loss on epoch 2610 : 0.091\n",
      "train accuracy on epoch 2610: 0.944\n",
      "test loss on epoch 2610: 0.351\n",
      "test accuracy on epoch 2610: 0.769\n",
      "train loss on epoch 2611 : 0.048\n",
      "train accuracy on epoch 2611: 0.944\n",
      "test loss on epoch 2611: 0.356\n",
      "test accuracy on epoch 2611: 0.769\n",
      "train loss on epoch 2612 : 0.064\n",
      "train accuracy on epoch 2612: 1.000\n",
      "test loss on epoch 2612: 0.366\n",
      "test accuracy on epoch 2612: 0.769\n",
      "train loss on epoch 2613 : 0.298\n",
      "train accuracy on epoch 2613: 0.889\n",
      "test loss on epoch 2613: 0.369\n",
      "test accuracy on epoch 2613: 0.769\n",
      "train loss on epoch 2614 : 0.301\n",
      "train accuracy on epoch 2614: 0.833\n",
      "test loss on epoch 2614: 0.380\n",
      "test accuracy on epoch 2614: 0.769\n",
      "train loss on epoch 2615 : 0.131\n",
      "train accuracy on epoch 2615: 0.944\n",
      "test loss on epoch 2615: 0.378\n",
      "test accuracy on epoch 2615: 0.769\n",
      "train loss on epoch 2616 : 0.334\n",
      "train accuracy on epoch 2616: 0.889\n",
      "test loss on epoch 2616: 0.369\n",
      "test accuracy on epoch 2616: 0.769\n",
      "train loss on epoch 2617 : 0.146\n",
      "train accuracy on epoch 2617: 0.944\n",
      "test loss on epoch 2617: 0.370\n",
      "test accuracy on epoch 2617: 0.769\n",
      "train loss on epoch 2618 : 0.130\n",
      "train accuracy on epoch 2618: 0.944\n",
      "test loss on epoch 2618: 0.360\n",
      "test accuracy on epoch 2618: 0.769\n",
      "train loss on epoch 2619 : 0.217\n",
      "train accuracy on epoch 2619: 0.889\n",
      "test loss on epoch 2619: 0.344\n",
      "test accuracy on epoch 2619: 0.769\n",
      "train loss on epoch 2620 : 0.295\n",
      "train accuracy on epoch 2620: 0.833\n",
      "test loss on epoch 2620: 0.351\n",
      "test accuracy on epoch 2620: 0.769\n",
      "train loss on epoch 2621 : 0.140\n",
      "train accuracy on epoch 2621: 0.944\n",
      "test loss on epoch 2621: 0.332\n",
      "test accuracy on epoch 2621: 0.769\n",
      "train loss on epoch 2622 : 0.211\n",
      "train accuracy on epoch 2622: 0.889\n",
      "test loss on epoch 2622: 0.341\n",
      "test accuracy on epoch 2622: 0.769\n",
      "train loss on epoch 2623 : 0.104\n",
      "train accuracy on epoch 2623: 1.000\n",
      "test loss on epoch 2623: 0.335\n",
      "test accuracy on epoch 2623: 0.769\n",
      "train loss on epoch 2624 : 0.145\n",
      "train accuracy on epoch 2624: 0.944\n",
      "test loss on epoch 2624: 0.336\n",
      "test accuracy on epoch 2624: 0.769\n",
      "train loss on epoch 2625 : 0.073\n",
      "train accuracy on epoch 2625: 1.000\n",
      "test loss on epoch 2625: 0.337\n",
      "test accuracy on epoch 2625: 0.769\n",
      "train loss on epoch 2626 : 0.186\n",
      "train accuracy on epoch 2626: 0.889\n",
      "test loss on epoch 2626: 0.338\n",
      "test accuracy on epoch 2626: 0.769\n",
      "train loss on epoch 2627 : 0.206\n",
      "train accuracy on epoch 2627: 0.944\n",
      "test loss on epoch 2627: 0.342\n",
      "test accuracy on epoch 2627: 0.769\n",
      "train loss on epoch 2628 : 0.295\n",
      "train accuracy on epoch 2628: 0.944\n",
      "test loss on epoch 2628: 0.348\n",
      "test accuracy on epoch 2628: 0.769\n",
      "train loss on epoch 2629 : 0.214\n",
      "train accuracy on epoch 2629: 0.944\n",
      "test loss on epoch 2629: 0.352\n",
      "test accuracy on epoch 2629: 0.769\n",
      "train loss on epoch 2630 : 0.114\n",
      "train accuracy on epoch 2630: 0.944\n",
      "test loss on epoch 2630: 0.356\n",
      "test accuracy on epoch 2630: 0.769\n",
      "train loss on epoch 2631 : 0.239\n",
      "train accuracy on epoch 2631: 0.889\n",
      "test loss on epoch 2631: 0.353\n",
      "test accuracy on epoch 2631: 0.769\n",
      "train loss on epoch 2632 : 0.163\n",
      "train accuracy on epoch 2632: 0.944\n",
      "test loss on epoch 2632: 0.340\n",
      "test accuracy on epoch 2632: 0.769\n",
      "train loss on epoch 2633 : 0.099\n",
      "train accuracy on epoch 2633: 1.000\n",
      "test loss on epoch 2633: 0.344\n",
      "test accuracy on epoch 2633: 0.769\n",
      "train loss on epoch 2634 : 0.263\n",
      "train accuracy on epoch 2634: 0.889\n",
      "test loss on epoch 2634: 0.351\n",
      "test accuracy on epoch 2634: 0.769\n",
      "train loss on epoch 2635 : 0.066\n",
      "train accuracy on epoch 2635: 1.000\n",
      "test loss on epoch 2635: 0.352\n",
      "test accuracy on epoch 2635: 0.769\n",
      "train loss on epoch 2636 : 0.127\n",
      "train accuracy on epoch 2636: 0.944\n",
      "test loss on epoch 2636: 0.354\n",
      "test accuracy on epoch 2636: 0.769\n",
      "train loss on epoch 2637 : 0.335\n",
      "train accuracy on epoch 2637: 0.889\n",
      "test loss on epoch 2637: 0.352\n",
      "test accuracy on epoch 2637: 0.769\n",
      "train loss on epoch 2638 : 0.081\n",
      "train accuracy on epoch 2638: 1.000\n",
      "test loss on epoch 2638: 0.355\n",
      "test accuracy on epoch 2638: 0.769\n",
      "train loss on epoch 2639 : 0.307\n",
      "train accuracy on epoch 2639: 0.944\n",
      "test loss on epoch 2639: 0.356\n",
      "test accuracy on epoch 2639: 0.769\n",
      "train loss on epoch 2640 : 0.089\n",
      "train accuracy on epoch 2640: 0.944\n",
      "test loss on epoch 2640: 0.351\n",
      "test accuracy on epoch 2640: 0.769\n",
      "train loss on epoch 2641 : 0.106\n",
      "train accuracy on epoch 2641: 0.944\n",
      "test loss on epoch 2641: 0.352\n",
      "test accuracy on epoch 2641: 0.769\n",
      "train loss on epoch 2642 : 0.184\n",
      "train accuracy on epoch 2642: 0.944\n",
      "test loss on epoch 2642: 0.339\n",
      "test accuracy on epoch 2642: 0.769\n",
      "train loss on epoch 2643 : 0.463\n",
      "train accuracy on epoch 2643: 0.833\n",
      "test loss on epoch 2643: 0.339\n",
      "test accuracy on epoch 2643: 0.769\n",
      "train loss on epoch 2644 : 0.253\n",
      "train accuracy on epoch 2644: 0.889\n",
      "test loss on epoch 2644: 0.348\n",
      "test accuracy on epoch 2644: 0.769\n",
      "train loss on epoch 2645 : 0.194\n",
      "train accuracy on epoch 2645: 0.889\n",
      "test loss on epoch 2645: 0.352\n",
      "test accuracy on epoch 2645: 0.769\n",
      "train loss on epoch 2646 : 0.121\n",
      "train accuracy on epoch 2646: 0.889\n",
      "test loss on epoch 2646: 0.345\n",
      "test accuracy on epoch 2646: 0.769\n",
      "train loss on epoch 2647 : 0.375\n",
      "train accuracy on epoch 2647: 0.889\n",
      "test loss on epoch 2647: 0.351\n",
      "test accuracy on epoch 2647: 0.769\n",
      "train loss on epoch 2648 : 0.103\n",
      "train accuracy on epoch 2648: 0.944\n",
      "test loss on epoch 2648: 0.354\n",
      "test accuracy on epoch 2648: 0.769\n",
      "train loss on epoch 2649 : 0.082\n",
      "train accuracy on epoch 2649: 0.944\n",
      "test loss on epoch 2649: 0.359\n",
      "test accuracy on epoch 2649: 0.769\n",
      "train loss on epoch 2650 : 0.049\n",
      "train accuracy on epoch 2650: 1.000\n",
      "test loss on epoch 2650: 0.362\n",
      "test accuracy on epoch 2650: 0.769\n",
      "train loss on epoch 2651 : 0.081\n",
      "train accuracy on epoch 2651: 0.944\n",
      "test loss on epoch 2651: 0.363\n",
      "test accuracy on epoch 2651: 0.769\n",
      "train loss on epoch 2652 : 0.253\n",
      "train accuracy on epoch 2652: 0.944\n",
      "test loss on epoch 2652: 0.358\n",
      "test accuracy on epoch 2652: 0.769\n",
      "train loss on epoch 2653 : 0.126\n",
      "train accuracy on epoch 2653: 0.889\n",
      "test loss on epoch 2653: 0.356\n",
      "test accuracy on epoch 2653: 0.769\n",
      "train loss on epoch 2654 : 0.354\n",
      "train accuracy on epoch 2654: 0.889\n",
      "test loss on epoch 2654: 0.359\n",
      "test accuracy on epoch 2654: 0.769\n",
      "train loss on epoch 2655 : 0.128\n",
      "train accuracy on epoch 2655: 0.944\n",
      "test loss on epoch 2655: 0.351\n",
      "test accuracy on epoch 2655: 0.769\n",
      "train loss on epoch 2656 : 0.349\n",
      "train accuracy on epoch 2656: 0.833\n",
      "test loss on epoch 2656: 0.357\n",
      "test accuracy on epoch 2656: 0.769\n",
      "train loss on epoch 2657 : 0.282\n",
      "train accuracy on epoch 2657: 0.889\n",
      "test loss on epoch 2657: 0.361\n",
      "test accuracy on epoch 2657: 0.769\n",
      "train loss on epoch 2658 : 0.252\n",
      "train accuracy on epoch 2658: 0.944\n",
      "test loss on epoch 2658: 0.360\n",
      "test accuracy on epoch 2658: 0.769\n",
      "train loss on epoch 2659 : 0.389\n",
      "train accuracy on epoch 2659: 0.889\n",
      "test loss on epoch 2659: 0.355\n",
      "test accuracy on epoch 2659: 0.769\n",
      "train loss on epoch 2660 : 0.184\n",
      "train accuracy on epoch 2660: 0.944\n",
      "test loss on epoch 2660: 0.352\n",
      "test accuracy on epoch 2660: 0.769\n",
      "train loss on epoch 2661 : 0.052\n",
      "train accuracy on epoch 2661: 0.944\n",
      "test loss on epoch 2661: 0.345\n",
      "test accuracy on epoch 2661: 0.769\n",
      "train loss on epoch 2662 : 0.133\n",
      "train accuracy on epoch 2662: 0.944\n",
      "test loss on epoch 2662: 0.352\n",
      "test accuracy on epoch 2662: 0.769\n",
      "train loss on epoch 2663 : 0.281\n",
      "train accuracy on epoch 2663: 0.889\n",
      "test loss on epoch 2663: 0.352\n",
      "test accuracy on epoch 2663: 0.769\n",
      "train loss on epoch 2664 : 0.090\n",
      "train accuracy on epoch 2664: 1.000\n",
      "test loss on epoch 2664: 0.347\n",
      "test accuracy on epoch 2664: 0.769\n",
      "train loss on epoch 2665 : 0.224\n",
      "train accuracy on epoch 2665: 0.889\n",
      "test loss on epoch 2665: 0.351\n",
      "test accuracy on epoch 2665: 0.769\n",
      "train loss on epoch 2666 : 0.207\n",
      "train accuracy on epoch 2666: 0.944\n",
      "test loss on epoch 2666: 0.351\n",
      "test accuracy on epoch 2666: 0.769\n",
      "train loss on epoch 2667 : 0.043\n",
      "train accuracy on epoch 2667: 1.000\n",
      "test loss on epoch 2667: 0.348\n",
      "test accuracy on epoch 2667: 0.769\n",
      "train loss on epoch 2668 : 0.061\n",
      "train accuracy on epoch 2668: 1.000\n",
      "test loss on epoch 2668: 0.357\n",
      "test accuracy on epoch 2668: 0.769\n",
      "train loss on epoch 2669 : 0.122\n",
      "train accuracy on epoch 2669: 0.944\n",
      "test loss on epoch 2669: 0.359\n",
      "test accuracy on epoch 2669: 0.769\n",
      "train loss on epoch 2670 : 0.079\n",
      "train accuracy on epoch 2670: 1.000\n",
      "test loss on epoch 2670: 0.359\n",
      "test accuracy on epoch 2670: 0.769\n",
      "train loss on epoch 2671 : 0.102\n",
      "train accuracy on epoch 2671: 1.000\n",
      "test loss on epoch 2671: 0.358\n",
      "test accuracy on epoch 2671: 0.769\n",
      "train loss on epoch 2672 : 0.326\n",
      "train accuracy on epoch 2672: 0.889\n",
      "test loss on epoch 2672: 0.356\n",
      "test accuracy on epoch 2672: 0.769\n",
      "train loss on epoch 2673 : 0.175\n",
      "train accuracy on epoch 2673: 0.889\n",
      "test loss on epoch 2673: 0.347\n",
      "test accuracy on epoch 2673: 0.769\n",
      "train loss on epoch 2674 : 0.323\n",
      "train accuracy on epoch 2674: 0.889\n",
      "test loss on epoch 2674: 0.353\n",
      "test accuracy on epoch 2674: 0.769\n",
      "train loss on epoch 2675 : 0.222\n",
      "train accuracy on epoch 2675: 0.889\n",
      "test loss on epoch 2675: 0.357\n",
      "test accuracy on epoch 2675: 0.769\n",
      "train loss on epoch 2676 : 0.103\n",
      "train accuracy on epoch 2676: 0.944\n",
      "test loss on epoch 2676: 0.357\n",
      "test accuracy on epoch 2676: 0.769\n",
      "train loss on epoch 2677 : 0.356\n",
      "train accuracy on epoch 2677: 0.889\n",
      "test loss on epoch 2677: 0.344\n",
      "test accuracy on epoch 2677: 0.769\n",
      "train loss on epoch 2678 : 0.173\n",
      "train accuracy on epoch 2678: 0.889\n",
      "test loss on epoch 2678: 0.348\n",
      "test accuracy on epoch 2678: 0.769\n",
      "train loss on epoch 2679 : 0.053\n",
      "train accuracy on epoch 2679: 1.000\n",
      "test loss on epoch 2679: 0.359\n",
      "test accuracy on epoch 2679: 0.769\n",
      "train loss on epoch 2680 : 0.054\n",
      "train accuracy on epoch 2680: 1.000\n",
      "test loss on epoch 2680: 0.356\n",
      "test accuracy on epoch 2680: 0.769\n",
      "train loss on epoch 2681 : 0.074\n",
      "train accuracy on epoch 2681: 1.000\n",
      "test loss on epoch 2681: 0.359\n",
      "test accuracy on epoch 2681: 0.769\n",
      "train loss on epoch 2682 : 0.247\n",
      "train accuracy on epoch 2682: 0.833\n",
      "test loss on epoch 2682: 0.367\n",
      "test accuracy on epoch 2682: 0.769\n",
      "train loss on epoch 2683 : 0.102\n",
      "train accuracy on epoch 2683: 0.944\n",
      "test loss on epoch 2683: 0.372\n",
      "test accuracy on epoch 2683: 0.769\n",
      "train loss on epoch 2684 : 0.251\n",
      "train accuracy on epoch 2684: 0.889\n",
      "test loss on epoch 2684: 0.378\n",
      "test accuracy on epoch 2684: 0.769\n",
      "train loss on epoch 2685 : 0.167\n",
      "train accuracy on epoch 2685: 0.889\n",
      "test loss on epoch 2685: 0.373\n",
      "test accuracy on epoch 2685: 0.769\n",
      "train loss on epoch 2686 : 0.266\n",
      "train accuracy on epoch 2686: 0.944\n",
      "test loss on epoch 2686: 0.375\n",
      "test accuracy on epoch 2686: 0.769\n",
      "train loss on epoch 2687 : 0.244\n",
      "train accuracy on epoch 2687: 0.944\n",
      "test loss on epoch 2687: 0.367\n",
      "test accuracy on epoch 2687: 0.769\n",
      "train loss on epoch 2688 : 0.155\n",
      "train accuracy on epoch 2688: 0.889\n",
      "test loss on epoch 2688: 0.367\n",
      "test accuracy on epoch 2688: 0.769\n",
      "train loss on epoch 2689 : 0.224\n",
      "train accuracy on epoch 2689: 0.889\n",
      "test loss on epoch 2689: 0.364\n",
      "test accuracy on epoch 2689: 0.769\n",
      "train loss on epoch 2690 : 0.222\n",
      "train accuracy on epoch 2690: 0.889\n",
      "test loss on epoch 2690: 0.359\n",
      "test accuracy on epoch 2690: 0.769\n",
      "train loss on epoch 2691 : 0.128\n",
      "train accuracy on epoch 2691: 0.944\n",
      "test loss on epoch 2691: 0.358\n",
      "test accuracy on epoch 2691: 0.769\n",
      "train loss on epoch 2692 : 0.115\n",
      "train accuracy on epoch 2692: 0.944\n",
      "test loss on epoch 2692: 0.356\n",
      "test accuracy on epoch 2692: 0.769\n",
      "train loss on epoch 2693 : 0.342\n",
      "train accuracy on epoch 2693: 0.889\n",
      "test loss on epoch 2693: 0.349\n",
      "test accuracy on epoch 2693: 0.769\n",
      "train loss on epoch 2694 : 0.132\n",
      "train accuracy on epoch 2694: 0.944\n",
      "test loss on epoch 2694: 0.340\n",
      "test accuracy on epoch 2694: 0.769\n",
      "train loss on epoch 2695 : 0.279\n",
      "train accuracy on epoch 2695: 0.889\n",
      "test loss on epoch 2695: 0.337\n",
      "test accuracy on epoch 2695: 0.769\n",
      "train loss on epoch 2696 : 0.243\n",
      "train accuracy on epoch 2696: 0.889\n",
      "test loss on epoch 2696: 0.338\n",
      "test accuracy on epoch 2696: 0.846\n",
      "train loss on epoch 2697 : 0.237\n",
      "train accuracy on epoch 2697: 0.889\n",
      "test loss on epoch 2697: 0.350\n",
      "test accuracy on epoch 2697: 0.769\n",
      "train loss on epoch 2698 : 0.143\n",
      "train accuracy on epoch 2698: 0.944\n",
      "test loss on epoch 2698: 0.344\n",
      "test accuracy on epoch 2698: 0.769\n",
      "train loss on epoch 2699 : 0.053\n",
      "train accuracy on epoch 2699: 1.000\n",
      "test loss on epoch 2699: 0.334\n",
      "test accuracy on epoch 2699: 0.769\n",
      "train loss on epoch 2700 : 0.186\n",
      "train accuracy on epoch 2700: 0.944\n",
      "test loss on epoch 2700: 0.334\n",
      "test accuracy on epoch 2700: 0.769\n",
      "train loss on epoch 2701 : 0.163\n",
      "train accuracy on epoch 2701: 0.944\n",
      "test loss on epoch 2701: 0.347\n",
      "test accuracy on epoch 2701: 0.692\n",
      "train loss on epoch 2702 : 0.147\n",
      "train accuracy on epoch 2702: 0.889\n",
      "test loss on epoch 2702: 0.347\n",
      "test accuracy on epoch 2702: 0.692\n",
      "train loss on epoch 2703 : 0.243\n",
      "train accuracy on epoch 2703: 0.889\n",
      "test loss on epoch 2703: 0.333\n",
      "test accuracy on epoch 2703: 0.846\n",
      "train loss on epoch 2704 : 0.199\n",
      "train accuracy on epoch 2704: 0.889\n",
      "test loss on epoch 2704: 0.335\n",
      "test accuracy on epoch 2704: 0.769\n",
      "train loss on epoch 2705 : 0.584\n",
      "train accuracy on epoch 2705: 0.889\n",
      "test loss on epoch 2705: 0.337\n",
      "test accuracy on epoch 2705: 0.769\n",
      "train loss on epoch 2706 : 0.137\n",
      "train accuracy on epoch 2706: 0.889\n",
      "test loss on epoch 2706: 0.346\n",
      "test accuracy on epoch 2706: 0.769\n",
      "train loss on epoch 2707 : 0.193\n",
      "train accuracy on epoch 2707: 0.889\n",
      "test loss on epoch 2707: 0.353\n",
      "test accuracy on epoch 2707: 0.769\n",
      "train loss on epoch 2708 : 0.072\n",
      "train accuracy on epoch 2708: 1.000\n",
      "test loss on epoch 2708: 0.342\n",
      "test accuracy on epoch 2708: 0.769\n",
      "train loss on epoch 2709 : 0.166\n",
      "train accuracy on epoch 2709: 0.889\n",
      "test loss on epoch 2709: 0.340\n",
      "test accuracy on epoch 2709: 0.769\n",
      "train loss on epoch 2710 : 0.044\n",
      "train accuracy on epoch 2710: 1.000\n",
      "test loss on epoch 2710: 0.349\n",
      "test accuracy on epoch 2710: 0.769\n",
      "train loss on epoch 2711 : 0.178\n",
      "train accuracy on epoch 2711: 0.889\n",
      "test loss on epoch 2711: 0.348\n",
      "test accuracy on epoch 2711: 0.769\n",
      "train loss on epoch 2712 : 0.169\n",
      "train accuracy on epoch 2712: 0.889\n",
      "test loss on epoch 2712: 0.356\n",
      "test accuracy on epoch 2712: 0.769\n",
      "train loss on epoch 2713 : 0.222\n",
      "train accuracy on epoch 2713: 0.833\n",
      "test loss on epoch 2713: 0.347\n",
      "test accuracy on epoch 2713: 0.769\n",
      "train loss on epoch 2714 : 0.229\n",
      "train accuracy on epoch 2714: 0.889\n",
      "test loss on epoch 2714: 0.350\n",
      "test accuracy on epoch 2714: 0.769\n",
      "train loss on epoch 2715 : 0.211\n",
      "train accuracy on epoch 2715: 0.944\n",
      "test loss on epoch 2715: 0.351\n",
      "test accuracy on epoch 2715: 0.769\n",
      "train loss on epoch 2716 : 0.246\n",
      "train accuracy on epoch 2716: 0.944\n",
      "test loss on epoch 2716: 0.363\n",
      "test accuracy on epoch 2716: 0.769\n",
      "train loss on epoch 2717 : 0.105\n",
      "train accuracy on epoch 2717: 0.944\n",
      "test loss on epoch 2717: 0.365\n",
      "test accuracy on epoch 2717: 0.769\n",
      "train loss on epoch 2718 : 0.241\n",
      "train accuracy on epoch 2718: 0.944\n",
      "test loss on epoch 2718: 0.369\n",
      "test accuracy on epoch 2718: 0.769\n",
      "train loss on epoch 2719 : 0.374\n",
      "train accuracy on epoch 2719: 0.889\n",
      "test loss on epoch 2719: 0.356\n",
      "test accuracy on epoch 2719: 0.769\n",
      "train loss on epoch 2720 : 0.219\n",
      "train accuracy on epoch 2720: 0.944\n",
      "test loss on epoch 2720: 0.357\n",
      "test accuracy on epoch 2720: 0.769\n",
      "train loss on epoch 2721 : 0.123\n",
      "train accuracy on epoch 2721: 0.944\n",
      "test loss on epoch 2721: 0.347\n",
      "test accuracy on epoch 2721: 0.769\n",
      "train loss on epoch 2722 : 0.227\n",
      "train accuracy on epoch 2722: 0.889\n",
      "test loss on epoch 2722: 0.350\n",
      "test accuracy on epoch 2722: 0.769\n",
      "train loss on epoch 2723 : 0.080\n",
      "train accuracy on epoch 2723: 1.000\n",
      "test loss on epoch 2723: 0.350\n",
      "test accuracy on epoch 2723: 0.769\n",
      "train loss on epoch 2724 : 0.141\n",
      "train accuracy on epoch 2724: 0.944\n",
      "test loss on epoch 2724: 0.362\n",
      "test accuracy on epoch 2724: 0.769\n",
      "train loss on epoch 2725 : 0.076\n",
      "train accuracy on epoch 2725: 0.944\n",
      "test loss on epoch 2725: 0.364\n",
      "test accuracy on epoch 2725: 0.769\n",
      "train loss on epoch 2726 : 0.251\n",
      "train accuracy on epoch 2726: 0.944\n",
      "test loss on epoch 2726: 0.363\n",
      "test accuracy on epoch 2726: 0.769\n",
      "train loss on epoch 2727 : 0.437\n",
      "train accuracy on epoch 2727: 0.889\n",
      "test loss on epoch 2727: 0.351\n",
      "test accuracy on epoch 2727: 0.769\n",
      "train loss on epoch 2728 : 0.146\n",
      "train accuracy on epoch 2728: 0.889\n",
      "test loss on epoch 2728: 0.344\n",
      "test accuracy on epoch 2728: 0.846\n",
      "train loss on epoch 2729 : 0.168\n",
      "train accuracy on epoch 2729: 0.944\n",
      "test loss on epoch 2729: 0.350\n",
      "test accuracy on epoch 2729: 0.846\n",
      "train loss on epoch 2730 : 0.108\n",
      "train accuracy on epoch 2730: 0.944\n",
      "test loss on epoch 2730: 0.359\n",
      "test accuracy on epoch 2730: 0.769\n",
      "train loss on epoch 2731 : 0.184\n",
      "train accuracy on epoch 2731: 0.889\n",
      "test loss on epoch 2731: 0.367\n",
      "test accuracy on epoch 2731: 0.769\n",
      "train loss on epoch 2732 : 0.244\n",
      "train accuracy on epoch 2732: 0.889\n",
      "test loss on epoch 2732: 0.359\n",
      "test accuracy on epoch 2732: 0.769\n",
      "train loss on epoch 2733 : 0.224\n",
      "train accuracy on epoch 2733: 0.944\n",
      "test loss on epoch 2733: 0.360\n",
      "test accuracy on epoch 2733: 0.692\n",
      "train loss on epoch 2734 : 0.106\n",
      "train accuracy on epoch 2734: 0.944\n",
      "test loss on epoch 2734: 0.338\n",
      "test accuracy on epoch 2734: 0.769\n",
      "train loss on epoch 2735 : 0.280\n",
      "train accuracy on epoch 2735: 0.944\n",
      "test loss on epoch 2735: 0.357\n",
      "test accuracy on epoch 2735: 0.769\n",
      "train loss on epoch 2736 : 0.304\n",
      "train accuracy on epoch 2736: 0.889\n",
      "test loss on epoch 2736: 0.357\n",
      "test accuracy on epoch 2736: 0.692\n",
      "train loss on epoch 2737 : 0.157\n",
      "train accuracy on epoch 2737: 0.944\n",
      "test loss on epoch 2737: 0.358\n",
      "test accuracy on epoch 2737: 0.769\n",
      "train loss on epoch 2738 : 0.097\n",
      "train accuracy on epoch 2738: 1.000\n",
      "test loss on epoch 2738: 0.346\n",
      "test accuracy on epoch 2738: 0.769\n",
      "train loss on epoch 2739 : 0.111\n",
      "train accuracy on epoch 2739: 0.944\n",
      "test loss on epoch 2739: 0.336\n",
      "test accuracy on epoch 2739: 0.769\n",
      "train loss on epoch 2740 : 0.167\n",
      "train accuracy on epoch 2740: 0.889\n",
      "test loss on epoch 2740: 0.357\n",
      "test accuracy on epoch 2740: 0.769\n",
      "train loss on epoch 2741 : 0.161\n",
      "train accuracy on epoch 2741: 0.889\n",
      "test loss on epoch 2741: 0.360\n",
      "test accuracy on epoch 2741: 0.692\n",
      "train loss on epoch 2742 : 0.070\n",
      "train accuracy on epoch 2742: 1.000\n",
      "test loss on epoch 2742: 0.344\n",
      "test accuracy on epoch 2742: 0.769\n",
      "train loss on epoch 2743 : 0.239\n",
      "train accuracy on epoch 2743: 0.833\n",
      "test loss on epoch 2743: 0.362\n",
      "test accuracy on epoch 2743: 0.769\n",
      "train loss on epoch 2744 : 0.183\n",
      "train accuracy on epoch 2744: 0.889\n",
      "test loss on epoch 2744: 0.357\n",
      "test accuracy on epoch 2744: 0.769\n",
      "train loss on epoch 2745 : 0.056\n",
      "train accuracy on epoch 2745: 1.000\n",
      "test loss on epoch 2745: 0.356\n",
      "test accuracy on epoch 2745: 0.769\n",
      "train loss on epoch 2746 : 0.148\n",
      "train accuracy on epoch 2746: 0.889\n",
      "test loss on epoch 2746: 0.349\n",
      "test accuracy on epoch 2746: 0.769\n",
      "train loss on epoch 2747 : 0.150\n",
      "train accuracy on epoch 2747: 0.889\n",
      "test loss on epoch 2747: 0.350\n",
      "test accuracy on epoch 2747: 0.769\n",
      "train loss on epoch 2748 : 0.276\n",
      "train accuracy on epoch 2748: 0.889\n",
      "test loss on epoch 2748: 0.358\n",
      "test accuracy on epoch 2748: 0.769\n",
      "train loss on epoch 2749 : 0.234\n",
      "train accuracy on epoch 2749: 0.833\n",
      "test loss on epoch 2749: 0.357\n",
      "test accuracy on epoch 2749: 0.769\n",
      "train loss on epoch 2750 : 0.235\n",
      "train accuracy on epoch 2750: 0.833\n",
      "test loss on epoch 2750: 0.356\n",
      "test accuracy on epoch 2750: 0.769\n",
      "train loss on epoch 2751 : 0.185\n",
      "train accuracy on epoch 2751: 0.944\n",
      "test loss on epoch 2751: 0.372\n",
      "test accuracy on epoch 2751: 0.769\n",
      "train loss on epoch 2752 : 0.127\n",
      "train accuracy on epoch 2752: 1.000\n",
      "test loss on epoch 2752: 0.363\n",
      "test accuracy on epoch 2752: 0.769\n",
      "train loss on epoch 2753 : 0.100\n",
      "train accuracy on epoch 2753: 0.944\n",
      "test loss on epoch 2753: 0.363\n",
      "test accuracy on epoch 2753: 0.769\n",
      "train loss on epoch 2754 : 0.183\n",
      "train accuracy on epoch 2754: 0.944\n",
      "test loss on epoch 2754: 0.346\n",
      "test accuracy on epoch 2754: 0.769\n",
      "train loss on epoch 2755 : 0.113\n",
      "train accuracy on epoch 2755: 0.944\n",
      "test loss on epoch 2755: 0.345\n",
      "test accuracy on epoch 2755: 0.769\n",
      "train loss on epoch 2756 : 0.173\n",
      "train accuracy on epoch 2756: 0.889\n",
      "test loss on epoch 2756: 0.333\n",
      "test accuracy on epoch 2756: 0.769\n",
      "train loss on epoch 2757 : 0.143\n",
      "train accuracy on epoch 2757: 0.944\n",
      "test loss on epoch 2757: 0.350\n",
      "test accuracy on epoch 2757: 0.769\n",
      "train loss on epoch 2758 : 0.107\n",
      "train accuracy on epoch 2758: 0.944\n",
      "test loss on epoch 2758: 0.339\n",
      "test accuracy on epoch 2758: 0.769\n",
      "train loss on epoch 2759 : 0.237\n",
      "train accuracy on epoch 2759: 0.889\n",
      "test loss on epoch 2759: 0.346\n",
      "test accuracy on epoch 2759: 0.769\n",
      "train loss on epoch 2760 : 0.254\n",
      "train accuracy on epoch 2760: 0.889\n",
      "test loss on epoch 2760: 0.339\n",
      "test accuracy on epoch 2760: 0.769\n",
      "train loss on epoch 2761 : 0.127\n",
      "train accuracy on epoch 2761: 0.944\n",
      "test loss on epoch 2761: 0.355\n",
      "test accuracy on epoch 2761: 0.769\n",
      "train loss on epoch 2762 : 0.223\n",
      "train accuracy on epoch 2762: 0.889\n",
      "test loss on epoch 2762: 0.347\n",
      "test accuracy on epoch 2762: 0.769\n",
      "train loss on epoch 2763 : 0.118\n",
      "train accuracy on epoch 2763: 0.944\n",
      "test loss on epoch 2763: 0.355\n",
      "test accuracy on epoch 2763: 0.769\n",
      "train loss on epoch 2764 : 0.119\n",
      "train accuracy on epoch 2764: 1.000\n",
      "test loss on epoch 2764: 0.339\n",
      "test accuracy on epoch 2764: 0.769\n",
      "train loss on epoch 2765 : 0.214\n",
      "train accuracy on epoch 2765: 0.889\n",
      "test loss on epoch 2765: 0.360\n",
      "test accuracy on epoch 2765: 0.769\n",
      "train loss on epoch 2766 : 0.223\n",
      "train accuracy on epoch 2766: 0.833\n",
      "test loss on epoch 2766: 0.363\n",
      "test accuracy on epoch 2766: 0.769\n",
      "train loss on epoch 2767 : 0.268\n",
      "train accuracy on epoch 2767: 0.889\n",
      "test loss on epoch 2767: 0.368\n",
      "test accuracy on epoch 2767: 0.769\n",
      "train loss on epoch 2768 : 0.084\n",
      "train accuracy on epoch 2768: 0.944\n",
      "test loss on epoch 2768: 0.370\n",
      "test accuracy on epoch 2768: 0.769\n",
      "train loss on epoch 2769 : 0.328\n",
      "train accuracy on epoch 2769: 0.833\n",
      "test loss on epoch 2769: 0.358\n",
      "test accuracy on epoch 2769: 0.769\n",
      "train loss on epoch 2770 : 0.129\n",
      "train accuracy on epoch 2770: 0.889\n",
      "test loss on epoch 2770: 0.355\n",
      "test accuracy on epoch 2770: 0.769\n",
      "train loss on epoch 2771 : 0.372\n",
      "train accuracy on epoch 2771: 0.889\n",
      "test loss on epoch 2771: 0.359\n",
      "test accuracy on epoch 2771: 0.769\n",
      "train loss on epoch 2772 : 0.104\n",
      "train accuracy on epoch 2772: 0.944\n",
      "test loss on epoch 2772: 0.366\n",
      "test accuracy on epoch 2772: 0.769\n",
      "train loss on epoch 2773 : 0.088\n",
      "train accuracy on epoch 2773: 0.944\n",
      "test loss on epoch 2773: 0.361\n",
      "test accuracy on epoch 2773: 0.769\n",
      "train loss on epoch 2774 : 0.204\n",
      "train accuracy on epoch 2774: 0.944\n",
      "test loss on epoch 2774: 0.355\n",
      "test accuracy on epoch 2774: 0.769\n",
      "train loss on epoch 2775 : 0.262\n",
      "train accuracy on epoch 2775: 0.944\n",
      "test loss on epoch 2775: 0.358\n",
      "test accuracy on epoch 2775: 0.769\n",
      "train loss on epoch 2776 : 0.303\n",
      "train accuracy on epoch 2776: 0.833\n",
      "test loss on epoch 2776: 0.355\n",
      "test accuracy on epoch 2776: 0.769\n",
      "train loss on epoch 2777 : 0.159\n",
      "train accuracy on epoch 2777: 0.944\n",
      "test loss on epoch 2777: 0.354\n",
      "test accuracy on epoch 2777: 0.769\n",
      "train loss on epoch 2778 : 0.054\n",
      "train accuracy on epoch 2778: 1.000\n",
      "test loss on epoch 2778: 0.346\n",
      "test accuracy on epoch 2778: 0.769\n",
      "train loss on epoch 2779 : 0.136\n",
      "train accuracy on epoch 2779: 0.944\n",
      "test loss on epoch 2779: 0.349\n",
      "test accuracy on epoch 2779: 0.769\n",
      "train loss on epoch 2780 : 0.188\n",
      "train accuracy on epoch 2780: 0.944\n",
      "test loss on epoch 2780: 0.357\n",
      "test accuracy on epoch 2780: 0.769\n",
      "train loss on epoch 2781 : 0.239\n",
      "train accuracy on epoch 2781: 0.833\n",
      "test loss on epoch 2781: 0.345\n",
      "test accuracy on epoch 2781: 0.769\n",
      "train loss on epoch 2782 : 0.257\n",
      "train accuracy on epoch 2782: 0.889\n",
      "test loss on epoch 2782: 0.362\n",
      "test accuracy on epoch 2782: 0.769\n",
      "train loss on epoch 2783 : 0.208\n",
      "train accuracy on epoch 2783: 0.889\n",
      "test loss on epoch 2783: 0.348\n",
      "test accuracy on epoch 2783: 0.769\n",
      "train loss on epoch 2784 : 0.253\n",
      "train accuracy on epoch 2784: 0.944\n",
      "test loss on epoch 2784: 0.358\n",
      "test accuracy on epoch 2784: 0.769\n",
      "train loss on epoch 2785 : 0.192\n",
      "train accuracy on epoch 2785: 0.944\n",
      "test loss on epoch 2785: 0.360\n",
      "test accuracy on epoch 2785: 0.769\n",
      "train loss on epoch 2786 : 0.098\n",
      "train accuracy on epoch 2786: 1.000\n",
      "test loss on epoch 2786: 0.350\n",
      "test accuracy on epoch 2786: 0.769\n",
      "train loss on epoch 2787 : 0.406\n",
      "train accuracy on epoch 2787: 0.889\n",
      "test loss on epoch 2787: 0.358\n",
      "test accuracy on epoch 2787: 0.769\n",
      "train loss on epoch 2788 : 0.220\n",
      "train accuracy on epoch 2788: 0.944\n",
      "test loss on epoch 2788: 0.350\n",
      "test accuracy on epoch 2788: 0.769\n",
      "train loss on epoch 2789 : 0.392\n",
      "train accuracy on epoch 2789: 0.889\n",
      "test loss on epoch 2789: 0.367\n",
      "test accuracy on epoch 2789: 0.769\n",
      "train loss on epoch 2790 : 0.317\n",
      "train accuracy on epoch 2790: 0.889\n",
      "test loss on epoch 2790: 0.364\n",
      "test accuracy on epoch 2790: 0.769\n",
      "train loss on epoch 2791 : 0.236\n",
      "train accuracy on epoch 2791: 0.833\n",
      "test loss on epoch 2791: 0.374\n",
      "test accuracy on epoch 2791: 0.769\n",
      "train loss on epoch 2792 : 0.256\n",
      "train accuracy on epoch 2792: 0.889\n",
      "test loss on epoch 2792: 0.377\n",
      "test accuracy on epoch 2792: 0.769\n",
      "train loss on epoch 2793 : 0.093\n",
      "train accuracy on epoch 2793: 1.000\n",
      "test loss on epoch 2793: 0.361\n",
      "test accuracy on epoch 2793: 0.769\n",
      "train loss on epoch 2794 : 0.130\n",
      "train accuracy on epoch 2794: 0.944\n",
      "test loss on epoch 2794: 0.365\n",
      "test accuracy on epoch 2794: 0.769\n",
      "train loss on epoch 2795 : 0.122\n",
      "train accuracy on epoch 2795: 0.944\n",
      "test loss on epoch 2795: 0.368\n",
      "test accuracy on epoch 2795: 0.769\n",
      "train loss on epoch 2796 : 0.181\n",
      "train accuracy on epoch 2796: 0.889\n",
      "test loss on epoch 2796: 0.375\n",
      "test accuracy on epoch 2796: 0.769\n",
      "train loss on epoch 2797 : 0.164\n",
      "train accuracy on epoch 2797: 0.944\n",
      "test loss on epoch 2797: 0.367\n",
      "test accuracy on epoch 2797: 0.769\n",
      "train loss on epoch 2798 : 0.143\n",
      "train accuracy on epoch 2798: 0.889\n",
      "test loss on epoch 2798: 0.365\n",
      "test accuracy on epoch 2798: 0.692\n",
      "train loss on epoch 2799 : 0.216\n",
      "train accuracy on epoch 2799: 0.944\n",
      "test loss on epoch 2799: 0.349\n",
      "test accuracy on epoch 2799: 0.769\n",
      "train loss on epoch 2800 : 0.290\n",
      "train accuracy on epoch 2800: 0.889\n",
      "test loss on epoch 2800: 0.360\n",
      "test accuracy on epoch 2800: 0.692\n",
      "train loss on epoch 2801 : 0.206\n",
      "train accuracy on epoch 2801: 0.944\n",
      "test loss on epoch 2801: 0.334\n",
      "test accuracy on epoch 2801: 0.769\n",
      "train loss on epoch 2802 : 0.191\n",
      "train accuracy on epoch 2802: 0.889\n",
      "test loss on epoch 2802: 0.335\n",
      "test accuracy on epoch 2802: 0.769\n",
      "train loss on epoch 2803 : 0.133\n",
      "train accuracy on epoch 2803: 0.944\n",
      "test loss on epoch 2803: 0.348\n",
      "test accuracy on epoch 2803: 0.769\n",
      "train loss on epoch 2804 : 0.194\n",
      "train accuracy on epoch 2804: 0.944\n",
      "test loss on epoch 2804: 0.366\n",
      "test accuracy on epoch 2804: 0.692\n",
      "train loss on epoch 2805 : 0.214\n",
      "train accuracy on epoch 2805: 0.889\n",
      "test loss on epoch 2805: 0.371\n",
      "test accuracy on epoch 2805: 0.769\n",
      "train loss on epoch 2806 : 0.207\n",
      "train accuracy on epoch 2806: 0.944\n",
      "test loss on epoch 2806: 0.354\n",
      "test accuracy on epoch 2806: 0.769\n",
      "train loss on epoch 2807 : 0.243\n",
      "train accuracy on epoch 2807: 0.889\n",
      "test loss on epoch 2807: 0.377\n",
      "test accuracy on epoch 2807: 0.769\n",
      "train loss on epoch 2808 : 0.161\n",
      "train accuracy on epoch 2808: 0.944\n",
      "test loss on epoch 2808: 0.370\n",
      "test accuracy on epoch 2808: 0.769\n",
      "train loss on epoch 2809 : 0.062\n",
      "train accuracy on epoch 2809: 0.944\n",
      "test loss on epoch 2809: 0.366\n",
      "test accuracy on epoch 2809: 0.769\n",
      "train loss on epoch 2810 : 0.196\n",
      "train accuracy on epoch 2810: 0.944\n",
      "test loss on epoch 2810: 0.357\n",
      "test accuracy on epoch 2810: 0.769\n",
      "train loss on epoch 2811 : 0.213\n",
      "train accuracy on epoch 2811: 0.889\n",
      "test loss on epoch 2811: 0.360\n",
      "test accuracy on epoch 2811: 0.769\n",
      "train loss on epoch 2812 : 0.270\n",
      "train accuracy on epoch 2812: 0.889\n",
      "test loss on epoch 2812: 0.374\n",
      "test accuracy on epoch 2812: 0.769\n",
      "train loss on epoch 2813 : 0.076\n",
      "train accuracy on epoch 2813: 1.000\n",
      "test loss on epoch 2813: 0.372\n",
      "test accuracy on epoch 2813: 0.769\n",
      "train loss on epoch 2814 : 0.120\n",
      "train accuracy on epoch 2814: 0.944\n",
      "test loss on epoch 2814: 0.373\n",
      "test accuracy on epoch 2814: 0.769\n",
      "train loss on epoch 2815 : 0.149\n",
      "train accuracy on epoch 2815: 0.944\n",
      "test loss on epoch 2815: 0.361\n",
      "test accuracy on epoch 2815: 0.769\n",
      "train loss on epoch 2816 : 0.222\n",
      "train accuracy on epoch 2816: 0.889\n",
      "test loss on epoch 2816: 0.364\n",
      "test accuracy on epoch 2816: 0.769\n",
      "train loss on epoch 2817 : 0.223\n",
      "train accuracy on epoch 2817: 0.889\n",
      "test loss on epoch 2817: 0.359\n",
      "test accuracy on epoch 2817: 0.769\n",
      "train loss on epoch 2818 : 0.118\n",
      "train accuracy on epoch 2818: 0.944\n",
      "test loss on epoch 2818: 0.349\n",
      "test accuracy on epoch 2818: 0.769\n",
      "train loss on epoch 2819 : 0.071\n",
      "train accuracy on epoch 2819: 0.944\n",
      "test loss on epoch 2819: 0.334\n",
      "test accuracy on epoch 2819: 0.769\n",
      "train loss on epoch 2820 : 0.287\n",
      "train accuracy on epoch 2820: 0.889\n",
      "test loss on epoch 2820: 0.358\n",
      "test accuracy on epoch 2820: 0.769\n",
      "train loss on epoch 2821 : 0.263\n",
      "train accuracy on epoch 2821: 0.944\n",
      "test loss on epoch 2821: 0.351\n",
      "test accuracy on epoch 2821: 0.769\n",
      "train loss on epoch 2822 : 0.473\n",
      "train accuracy on epoch 2822: 0.833\n",
      "test loss on epoch 2822: 0.355\n",
      "test accuracy on epoch 2822: 0.769\n",
      "train loss on epoch 2823 : 0.133\n",
      "train accuracy on epoch 2823: 0.889\n",
      "test loss on epoch 2823: 0.367\n",
      "test accuracy on epoch 2823: 0.769\n",
      "train loss on epoch 2824 : 0.168\n",
      "train accuracy on epoch 2824: 0.889\n",
      "test loss on epoch 2824: 0.369\n",
      "test accuracy on epoch 2824: 0.769\n",
      "train loss on epoch 2825 : 0.233\n",
      "train accuracy on epoch 2825: 0.944\n",
      "test loss on epoch 2825: 0.359\n",
      "test accuracy on epoch 2825: 0.769\n",
      "train loss on epoch 2826 : 0.076\n",
      "train accuracy on epoch 2826: 1.000\n",
      "test loss on epoch 2826: 0.367\n",
      "test accuracy on epoch 2826: 0.769\n",
      "train loss on epoch 2827 : 0.152\n",
      "train accuracy on epoch 2827: 0.889\n",
      "test loss on epoch 2827: 0.369\n",
      "test accuracy on epoch 2827: 0.769\n",
      "train loss on epoch 2828 : 0.128\n",
      "train accuracy on epoch 2828: 0.944\n",
      "test loss on epoch 2828: 0.364\n",
      "test accuracy on epoch 2828: 0.769\n",
      "train loss on epoch 2829 : 0.088\n",
      "train accuracy on epoch 2829: 0.944\n",
      "test loss on epoch 2829: 0.358\n",
      "test accuracy on epoch 2829: 0.692\n",
      "train loss on epoch 2830 : 0.351\n",
      "train accuracy on epoch 2830: 0.833\n",
      "test loss on epoch 2830: 0.334\n",
      "test accuracy on epoch 2830: 0.846\n",
      "train loss on epoch 2831 : 0.205\n",
      "train accuracy on epoch 2831: 0.944\n",
      "test loss on epoch 2831: 0.362\n",
      "test accuracy on epoch 2831: 0.692\n",
      "train loss on epoch 2832 : 0.118\n",
      "train accuracy on epoch 2832: 0.944\n",
      "test loss on epoch 2832: 0.361\n",
      "test accuracy on epoch 2832: 0.692\n",
      "train loss on epoch 2833 : 0.232\n",
      "train accuracy on epoch 2833: 0.889\n",
      "test loss on epoch 2833: 0.343\n",
      "test accuracy on epoch 2833: 0.769\n",
      "train loss on epoch 2834 : 0.136\n",
      "train accuracy on epoch 2834: 0.944\n",
      "test loss on epoch 2834: 0.355\n",
      "test accuracy on epoch 2834: 0.769\n",
      "train loss on epoch 2835 : 0.060\n",
      "train accuracy on epoch 2835: 1.000\n",
      "test loss on epoch 2835: 0.347\n",
      "test accuracy on epoch 2835: 0.769\n",
      "train loss on epoch 2836 : 0.233\n",
      "train accuracy on epoch 2836: 0.833\n",
      "test loss on epoch 2836: 0.349\n",
      "test accuracy on epoch 2836: 0.769\n",
      "train loss on epoch 2837 : 0.317\n",
      "train accuracy on epoch 2837: 0.889\n",
      "test loss on epoch 2837: 0.358\n",
      "test accuracy on epoch 2837: 0.769\n",
      "train loss on epoch 2838 : 0.213\n",
      "train accuracy on epoch 2838: 0.944\n",
      "test loss on epoch 2838: 0.353\n",
      "test accuracy on epoch 2838: 0.769\n",
      "train loss on epoch 2839 : 0.216\n",
      "train accuracy on epoch 2839: 0.889\n",
      "test loss on epoch 2839: 0.352\n",
      "test accuracy on epoch 2839: 0.769\n",
      "train loss on epoch 2840 : 0.171\n",
      "train accuracy on epoch 2840: 0.944\n",
      "test loss on epoch 2840: 0.354\n",
      "test accuracy on epoch 2840: 0.769\n",
      "train loss on epoch 2841 : 0.164\n",
      "train accuracy on epoch 2841: 0.944\n",
      "test loss on epoch 2841: 0.354\n",
      "test accuracy on epoch 2841: 0.769\n",
      "train loss on epoch 2842 : 0.114\n",
      "train accuracy on epoch 2842: 1.000\n",
      "test loss on epoch 2842: 0.365\n",
      "test accuracy on epoch 2842: 0.692\n",
      "train loss on epoch 2843 : 0.143\n",
      "train accuracy on epoch 2843: 0.889\n",
      "test loss on epoch 2843: 0.347\n",
      "test accuracy on epoch 2843: 0.846\n",
      "train loss on epoch 2844 : 0.145\n",
      "train accuracy on epoch 2844: 0.889\n",
      "test loss on epoch 2844: 0.362\n",
      "test accuracy on epoch 2844: 0.769\n",
      "train loss on epoch 2845 : 0.100\n",
      "train accuracy on epoch 2845: 0.944\n",
      "test loss on epoch 2845: 0.362\n",
      "test accuracy on epoch 2845: 0.769\n",
      "train loss on epoch 2846 : 0.225\n",
      "train accuracy on epoch 2846: 0.944\n",
      "test loss on epoch 2846: 0.370\n",
      "test accuracy on epoch 2846: 0.769\n",
      "train loss on epoch 2847 : 0.048\n",
      "train accuracy on epoch 2847: 1.000\n",
      "test loss on epoch 2847: 0.392\n",
      "test accuracy on epoch 2847: 0.769\n",
      "train loss on epoch 2848 : 0.223\n",
      "train accuracy on epoch 2848: 0.833\n",
      "test loss on epoch 2848: 0.395\n",
      "test accuracy on epoch 2848: 0.769\n",
      "train loss on epoch 2849 : 0.069\n",
      "train accuracy on epoch 2849: 1.000\n",
      "test loss on epoch 2849: 0.390\n",
      "test accuracy on epoch 2849: 0.769\n",
      "train loss on epoch 2850 : 0.163\n",
      "train accuracy on epoch 2850: 0.889\n",
      "test loss on epoch 2850: 0.393\n",
      "test accuracy on epoch 2850: 0.769\n",
      "train loss on epoch 2851 : 0.079\n",
      "train accuracy on epoch 2851: 1.000\n",
      "test loss on epoch 2851: 0.371\n",
      "test accuracy on epoch 2851: 0.769\n",
      "train loss on epoch 2852 : 0.056\n",
      "train accuracy on epoch 2852: 1.000\n",
      "test loss on epoch 2852: 0.368\n",
      "test accuracy on epoch 2852: 0.769\n",
      "train loss on epoch 2853 : 0.144\n",
      "train accuracy on epoch 2853: 0.944\n",
      "test loss on epoch 2853: 0.364\n",
      "test accuracy on epoch 2853: 0.769\n",
      "train loss on epoch 2854 : 0.226\n",
      "train accuracy on epoch 2854: 0.944\n",
      "test loss on epoch 2854: 0.374\n",
      "test accuracy on epoch 2854: 0.769\n",
      "train loss on epoch 2855 : 0.228\n",
      "train accuracy on epoch 2855: 0.889\n",
      "test loss on epoch 2855: 0.379\n",
      "test accuracy on epoch 2855: 0.769\n",
      "train loss on epoch 2856 : 0.094\n",
      "train accuracy on epoch 2856: 0.944\n",
      "test loss on epoch 2856: 0.360\n",
      "test accuracy on epoch 2856: 0.769\n",
      "train loss on epoch 2857 : 0.035\n",
      "train accuracy on epoch 2857: 1.000\n",
      "test loss on epoch 2857: 0.374\n",
      "test accuracy on epoch 2857: 0.769\n",
      "train loss on epoch 2858 : 0.324\n",
      "train accuracy on epoch 2858: 0.778\n",
      "test loss on epoch 2858: 0.347\n",
      "test accuracy on epoch 2858: 0.846\n",
      "train loss on epoch 2859 : 0.055\n",
      "train accuracy on epoch 2859: 1.000\n",
      "test loss on epoch 2859: 0.343\n",
      "test accuracy on epoch 2859: 0.846\n",
      "train loss on epoch 2860 : 0.197\n",
      "train accuracy on epoch 2860: 0.889\n",
      "test loss on epoch 2860: 0.345\n",
      "test accuracy on epoch 2860: 0.846\n",
      "train loss on epoch 2861 : 0.270\n",
      "train accuracy on epoch 2861: 0.889\n",
      "test loss on epoch 2861: 0.357\n",
      "test accuracy on epoch 2861: 0.769\n",
      "train loss on epoch 2862 : 0.117\n",
      "train accuracy on epoch 2862: 0.889\n",
      "test loss on epoch 2862: 0.357\n",
      "test accuracy on epoch 2862: 0.769\n",
      "train loss on epoch 2863 : 0.141\n",
      "train accuracy on epoch 2863: 0.944\n",
      "test loss on epoch 2863: 0.365\n",
      "test accuracy on epoch 2863: 0.769\n",
      "train loss on epoch 2864 : 0.074\n",
      "train accuracy on epoch 2864: 1.000\n",
      "test loss on epoch 2864: 0.346\n",
      "test accuracy on epoch 2864: 0.769\n",
      "train loss on epoch 2865 : 0.366\n",
      "train accuracy on epoch 2865: 0.833\n",
      "test loss on epoch 2865: 0.361\n",
      "test accuracy on epoch 2865: 0.769\n",
      "train loss on epoch 2866 : 0.070\n",
      "train accuracy on epoch 2866: 1.000\n",
      "test loss on epoch 2866: 0.351\n",
      "test accuracy on epoch 2866: 0.769\n",
      "train loss on epoch 2867 : 0.225\n",
      "train accuracy on epoch 2867: 0.944\n",
      "test loss on epoch 2867: 0.368\n",
      "test accuracy on epoch 2867: 0.769\n",
      "train loss on epoch 2868 : 0.195\n",
      "train accuracy on epoch 2868: 0.889\n",
      "test loss on epoch 2868: 0.364\n",
      "test accuracy on epoch 2868: 0.769\n",
      "train loss on epoch 2869 : 0.197\n",
      "train accuracy on epoch 2869: 0.944\n",
      "test loss on epoch 2869: 0.370\n",
      "test accuracy on epoch 2869: 0.769\n",
      "train loss on epoch 2870 : 0.309\n",
      "train accuracy on epoch 2870: 0.944\n",
      "test loss on epoch 2870: 0.365\n",
      "test accuracy on epoch 2870: 0.769\n",
      "train loss on epoch 2871 : 0.170\n",
      "train accuracy on epoch 2871: 0.944\n",
      "test loss on epoch 2871: 0.354\n",
      "test accuracy on epoch 2871: 0.769\n",
      "train loss on epoch 2872 : 0.311\n",
      "train accuracy on epoch 2872: 0.944\n",
      "test loss on epoch 2872: 0.343\n",
      "test accuracy on epoch 2872: 0.846\n",
      "train loss on epoch 2873 : 0.234\n",
      "train accuracy on epoch 2873: 0.944\n",
      "test loss on epoch 2873: 0.349\n",
      "test accuracy on epoch 2873: 0.846\n",
      "train loss on epoch 2874 : 0.139\n",
      "train accuracy on epoch 2874: 0.944\n",
      "test loss on epoch 2874: 0.353\n",
      "test accuracy on epoch 2874: 0.769\n",
      "train loss on epoch 2875 : 0.404\n",
      "train accuracy on epoch 2875: 0.889\n",
      "test loss on epoch 2875: 0.374\n",
      "test accuracy on epoch 2875: 0.769\n",
      "train loss on epoch 2876 : 0.343\n",
      "train accuracy on epoch 2876: 0.833\n",
      "test loss on epoch 2876: 0.350\n",
      "test accuracy on epoch 2876: 0.846\n",
      "train loss on epoch 2877 : 0.101\n",
      "train accuracy on epoch 2877: 0.944\n",
      "test loss on epoch 2877: 0.368\n",
      "test accuracy on epoch 2877: 0.769\n",
      "train loss on epoch 2878 : 0.227\n",
      "train accuracy on epoch 2878: 0.944\n",
      "test loss on epoch 2878: 0.343\n",
      "test accuracy on epoch 2878: 0.769\n",
      "train loss on epoch 2879 : 0.099\n",
      "train accuracy on epoch 2879: 0.944\n",
      "test loss on epoch 2879: 0.360\n",
      "test accuracy on epoch 2879: 0.769\n",
      "train loss on epoch 2880 : 0.243\n",
      "train accuracy on epoch 2880: 0.944\n",
      "test loss on epoch 2880: 0.371\n",
      "test accuracy on epoch 2880: 0.769\n",
      "train loss on epoch 2881 : 0.143\n",
      "train accuracy on epoch 2881: 0.944\n",
      "test loss on epoch 2881: 0.363\n",
      "test accuracy on epoch 2881: 0.769\n",
      "train loss on epoch 2882 : 0.116\n",
      "train accuracy on epoch 2882: 0.944\n",
      "test loss on epoch 2882: 0.373\n",
      "test accuracy on epoch 2882: 0.692\n",
      "train loss on epoch 2883 : 0.169\n",
      "train accuracy on epoch 2883: 0.944\n",
      "test loss on epoch 2883: 0.371\n",
      "test accuracy on epoch 2883: 0.769\n",
      "train loss on epoch 2884 : 0.105\n",
      "train accuracy on epoch 2884: 0.944\n",
      "test loss on epoch 2884: 0.347\n",
      "test accuracy on epoch 2884: 0.769\n",
      "train loss on epoch 2885 : 0.154\n",
      "train accuracy on epoch 2885: 0.944\n",
      "test loss on epoch 2885: 0.349\n",
      "test accuracy on epoch 2885: 0.769\n",
      "train loss on epoch 2886 : 0.055\n",
      "train accuracy on epoch 2886: 1.000\n",
      "test loss on epoch 2886: 0.358\n",
      "test accuracy on epoch 2886: 0.769\n",
      "train loss on epoch 2887 : 0.149\n",
      "train accuracy on epoch 2887: 0.944\n",
      "test loss on epoch 2887: 0.375\n",
      "test accuracy on epoch 2887: 0.769\n",
      "train loss on epoch 2888 : 0.113\n",
      "train accuracy on epoch 2888: 0.944\n",
      "test loss on epoch 2888: 0.353\n",
      "test accuracy on epoch 2888: 0.769\n",
      "train loss on epoch 2889 : 0.129\n",
      "train accuracy on epoch 2889: 0.944\n",
      "test loss on epoch 2889: 0.380\n",
      "test accuracy on epoch 2889: 0.692\n",
      "train loss on epoch 2890 : 0.300\n",
      "train accuracy on epoch 2890: 0.833\n",
      "test loss on epoch 2890: 0.358\n",
      "test accuracy on epoch 2890: 0.846\n",
      "train loss on epoch 2891 : 0.164\n",
      "train accuracy on epoch 2891: 0.944\n",
      "test loss on epoch 2891: 0.352\n",
      "test accuracy on epoch 2891: 0.769\n",
      "train loss on epoch 2892 : 0.369\n",
      "train accuracy on epoch 2892: 0.889\n",
      "test loss on epoch 2892: 0.369\n",
      "test accuracy on epoch 2892: 0.769\n",
      "train loss on epoch 2893 : 0.115\n",
      "train accuracy on epoch 2893: 0.944\n",
      "test loss on epoch 2893: 0.380\n",
      "test accuracy on epoch 2893: 0.769\n",
      "train loss on epoch 2894 : 0.103\n",
      "train accuracy on epoch 2894: 0.944\n",
      "test loss on epoch 2894: 0.377\n",
      "test accuracy on epoch 2894: 0.769\n",
      "train loss on epoch 2895 : 0.318\n",
      "train accuracy on epoch 2895: 0.889\n",
      "test loss on epoch 2895: 0.375\n",
      "test accuracy on epoch 2895: 0.769\n",
      "train loss on epoch 2896 : 0.400\n",
      "train accuracy on epoch 2896: 0.833\n",
      "test loss on epoch 2896: 0.376\n",
      "test accuracy on epoch 2896: 0.769\n",
      "train loss on epoch 2897 : 0.114\n",
      "train accuracy on epoch 2897: 0.944\n",
      "test loss on epoch 2897: 0.352\n",
      "test accuracy on epoch 2897: 0.769\n",
      "train loss on epoch 2898 : 0.133\n",
      "train accuracy on epoch 2898: 0.944\n",
      "test loss on epoch 2898: 0.350\n",
      "test accuracy on epoch 2898: 0.769\n",
      "train loss on epoch 2899 : 0.055\n",
      "train accuracy on epoch 2899: 1.000\n",
      "test loss on epoch 2899: 0.366\n",
      "test accuracy on epoch 2899: 0.769\n",
      "train loss on epoch 2900 : 0.116\n",
      "train accuracy on epoch 2900: 0.944\n",
      "test loss on epoch 2900: 0.348\n",
      "test accuracy on epoch 2900: 0.769\n",
      "train loss on epoch 2901 : 0.292\n",
      "train accuracy on epoch 2901: 0.889\n",
      "test loss on epoch 2901: 0.376\n",
      "test accuracy on epoch 2901: 0.769\n",
      "train loss on epoch 2902 : 0.054\n",
      "train accuracy on epoch 2902: 1.000\n",
      "test loss on epoch 2902: 0.376\n",
      "test accuracy on epoch 2902: 0.769\n",
      "train loss on epoch 2903 : 0.025\n",
      "train accuracy on epoch 2903: 1.000\n",
      "test loss on epoch 2903: 0.369\n",
      "test accuracy on epoch 2903: 0.769\n",
      "train loss on epoch 2904 : 0.359\n",
      "train accuracy on epoch 2904: 0.889\n",
      "test loss on epoch 2904: 0.380\n",
      "test accuracy on epoch 2904: 0.769\n",
      "train loss on epoch 2905 : 0.296\n",
      "train accuracy on epoch 2905: 0.889\n",
      "test loss on epoch 2905: 0.382\n",
      "test accuracy on epoch 2905: 0.769\n",
      "train loss on epoch 2906 : 0.322\n",
      "train accuracy on epoch 2906: 0.889\n",
      "test loss on epoch 2906: 0.364\n",
      "test accuracy on epoch 2906: 0.769\n",
      "train loss on epoch 2907 : 0.157\n",
      "train accuracy on epoch 2907: 0.889\n",
      "test loss on epoch 2907: 0.363\n",
      "test accuracy on epoch 2907: 0.769\n",
      "train loss on epoch 2908 : 0.316\n",
      "train accuracy on epoch 2908: 0.944\n",
      "test loss on epoch 2908: 0.380\n",
      "test accuracy on epoch 2908: 0.769\n",
      "train loss on epoch 2909 : 0.186\n",
      "train accuracy on epoch 2909: 0.944\n",
      "test loss on epoch 2909: 0.375\n",
      "test accuracy on epoch 2909: 0.769\n",
      "train loss on epoch 2910 : 0.113\n",
      "train accuracy on epoch 2910: 1.000\n",
      "test loss on epoch 2910: 0.387\n",
      "test accuracy on epoch 2910: 0.769\n",
      "train loss on epoch 2911 : 0.037\n",
      "train accuracy on epoch 2911: 1.000\n",
      "test loss on epoch 2911: 0.359\n",
      "test accuracy on epoch 2911: 0.769\n",
      "train loss on epoch 2912 : 0.089\n",
      "train accuracy on epoch 2912: 1.000\n",
      "test loss on epoch 2912: 0.367\n",
      "test accuracy on epoch 2912: 0.769\n",
      "train loss on epoch 2913 : 0.304\n",
      "train accuracy on epoch 2913: 0.889\n",
      "test loss on epoch 2913: 0.357\n",
      "test accuracy on epoch 2913: 0.769\n",
      "train loss on epoch 2914 : 0.205\n",
      "train accuracy on epoch 2914: 0.889\n",
      "test loss on epoch 2914: 0.381\n",
      "test accuracy on epoch 2914: 0.769\n",
      "train loss on epoch 2915 : 0.246\n",
      "train accuracy on epoch 2915: 0.889\n",
      "test loss on epoch 2915: 0.357\n",
      "test accuracy on epoch 2915: 0.769\n",
      "train loss on epoch 2916 : 0.173\n",
      "train accuracy on epoch 2916: 0.889\n",
      "test loss on epoch 2916: 0.363\n",
      "test accuracy on epoch 2916: 0.769\n",
      "train loss on epoch 2917 : 0.410\n",
      "train accuracy on epoch 2917: 0.889\n",
      "test loss on epoch 2917: 0.383\n",
      "test accuracy on epoch 2917: 0.769\n",
      "train loss on epoch 2918 : 0.231\n",
      "train accuracy on epoch 2918: 0.889\n",
      "test loss on epoch 2918: 0.382\n",
      "test accuracy on epoch 2918: 0.769\n",
      "train loss on epoch 2919 : 0.122\n",
      "train accuracy on epoch 2919: 0.944\n",
      "test loss on epoch 2919: 0.381\n",
      "test accuracy on epoch 2919: 0.769\n",
      "train loss on epoch 2920 : 0.158\n",
      "train accuracy on epoch 2920: 0.944\n",
      "test loss on epoch 2920: 0.361\n",
      "test accuracy on epoch 2920: 0.769\n",
      "train loss on epoch 2921 : 0.101\n",
      "train accuracy on epoch 2921: 1.000\n",
      "test loss on epoch 2921: 0.381\n",
      "test accuracy on epoch 2921: 0.769\n",
      "train loss on epoch 2922 : 0.164\n",
      "train accuracy on epoch 2922: 0.889\n",
      "test loss on epoch 2922: 0.372\n",
      "test accuracy on epoch 2922: 0.769\n",
      "train loss on epoch 2923 : 0.080\n",
      "train accuracy on epoch 2923: 0.944\n",
      "test loss on epoch 2923: 0.356\n",
      "test accuracy on epoch 2923: 0.769\n",
      "train loss on epoch 2924 : 0.113\n",
      "train accuracy on epoch 2924: 0.944\n",
      "test loss on epoch 2924: 0.379\n",
      "test accuracy on epoch 2924: 0.769\n",
      "train loss on epoch 2925 : 0.092\n",
      "train accuracy on epoch 2925: 1.000\n",
      "test loss on epoch 2925: 0.353\n",
      "test accuracy on epoch 2925: 0.769\n",
      "train loss on epoch 2926 : 0.056\n",
      "train accuracy on epoch 2926: 1.000\n",
      "test loss on epoch 2926: 0.378\n",
      "test accuracy on epoch 2926: 0.769\n",
      "train loss on epoch 2927 : 0.078\n",
      "train accuracy on epoch 2927: 1.000\n",
      "test loss on epoch 2927: 0.351\n",
      "test accuracy on epoch 2927: 0.769\n",
      "train loss on epoch 2928 : 0.351\n",
      "train accuracy on epoch 2928: 0.889\n",
      "test loss on epoch 2928: 0.354\n",
      "test accuracy on epoch 2928: 0.769\n",
      "train loss on epoch 2929 : 0.072\n",
      "train accuracy on epoch 2929: 1.000\n",
      "test loss on epoch 2929: 0.375\n",
      "test accuracy on epoch 2929: 0.769\n",
      "train loss on epoch 2930 : 0.463\n",
      "train accuracy on epoch 2930: 0.833\n",
      "test loss on epoch 2930: 0.360\n",
      "test accuracy on epoch 2930: 0.769\n",
      "train loss on epoch 2931 : 0.362\n",
      "train accuracy on epoch 2931: 0.833\n",
      "test loss on epoch 2931: 0.377\n",
      "test accuracy on epoch 2931: 0.769\n",
      "train loss on epoch 2932 : 0.152\n",
      "train accuracy on epoch 2932: 0.944\n",
      "test loss on epoch 2932: 0.366\n",
      "test accuracy on epoch 2932: 0.769\n",
      "train loss on epoch 2933 : 0.197\n",
      "train accuracy on epoch 2933: 0.944\n",
      "test loss on epoch 2933: 0.370\n",
      "test accuracy on epoch 2933: 0.769\n",
      "train loss on epoch 2934 : 0.092\n",
      "train accuracy on epoch 2934: 0.944\n",
      "test loss on epoch 2934: 0.374\n",
      "test accuracy on epoch 2934: 0.769\n",
      "train loss on epoch 2935 : 0.172\n",
      "train accuracy on epoch 2935: 0.944\n",
      "test loss on epoch 2935: 0.371\n",
      "test accuracy on epoch 2935: 0.769\n",
      "train loss on epoch 2936 : 0.253\n",
      "train accuracy on epoch 2936: 0.889\n",
      "test loss on epoch 2936: 0.354\n",
      "test accuracy on epoch 2936: 0.769\n",
      "train loss on epoch 2937 : 0.166\n",
      "train accuracy on epoch 2937: 0.889\n",
      "test loss on epoch 2937: 0.370\n",
      "test accuracy on epoch 2937: 0.769\n",
      "train loss on epoch 2938 : 0.047\n",
      "train accuracy on epoch 2938: 1.000\n",
      "test loss on epoch 2938: 0.350\n",
      "test accuracy on epoch 2938: 0.769\n",
      "train loss on epoch 2939 : 0.336\n",
      "train accuracy on epoch 2939: 0.889\n",
      "test loss on epoch 2939: 0.369\n",
      "test accuracy on epoch 2939: 0.769\n",
      "train loss on epoch 2940 : 0.108\n",
      "train accuracy on epoch 2940: 0.944\n",
      "test loss on epoch 2940: 0.366\n",
      "test accuracy on epoch 2940: 0.769\n",
      "train loss on epoch 2941 : 0.197\n",
      "train accuracy on epoch 2941: 0.889\n",
      "test loss on epoch 2941: 0.347\n",
      "test accuracy on epoch 2941: 0.769\n",
      "train loss on epoch 2942 : 0.119\n",
      "train accuracy on epoch 2942: 0.944\n",
      "test loss on epoch 2942: 0.356\n",
      "test accuracy on epoch 2942: 0.769\n",
      "train loss on epoch 2943 : 0.116\n",
      "train accuracy on epoch 2943: 0.944\n",
      "test loss on epoch 2943: 0.349\n",
      "test accuracy on epoch 2943: 0.769\n",
      "train loss on epoch 2944 : 0.172\n",
      "train accuracy on epoch 2944: 0.944\n",
      "test loss on epoch 2944: 0.354\n",
      "test accuracy on epoch 2944: 0.769\n",
      "train loss on epoch 2945 : 0.179\n",
      "train accuracy on epoch 2945: 0.944\n",
      "test loss on epoch 2945: 0.350\n",
      "test accuracy on epoch 2945: 0.769\n",
      "train loss on epoch 2946 : 0.246\n",
      "train accuracy on epoch 2946: 0.833\n",
      "test loss on epoch 2946: 0.345\n",
      "test accuracy on epoch 2946: 0.769\n",
      "train loss on epoch 2947 : 0.085\n",
      "train accuracy on epoch 2947: 1.000\n",
      "test loss on epoch 2947: 0.341\n",
      "test accuracy on epoch 2947: 0.769\n",
      "train loss on epoch 2948 : 0.119\n",
      "train accuracy on epoch 2948: 0.944\n",
      "test loss on epoch 2948: 0.364\n",
      "test accuracy on epoch 2948: 0.769\n",
      "train loss on epoch 2949 : 0.164\n",
      "train accuracy on epoch 2949: 0.889\n",
      "test loss on epoch 2949: 0.366\n",
      "test accuracy on epoch 2949: 0.769\n",
      "train loss on epoch 2950 : 0.205\n",
      "train accuracy on epoch 2950: 0.889\n",
      "test loss on epoch 2950: 0.359\n",
      "test accuracy on epoch 2950: 0.769\n",
      "train loss on epoch 2951 : 0.220\n",
      "train accuracy on epoch 2951: 0.889\n",
      "test loss on epoch 2951: 0.339\n",
      "test accuracy on epoch 2951: 0.769\n",
      "train loss on epoch 2952 : 0.634\n",
      "train accuracy on epoch 2952: 0.833\n",
      "test loss on epoch 2952: 0.357\n",
      "test accuracy on epoch 2952: 0.769\n",
      "train loss on epoch 2953 : 0.217\n",
      "train accuracy on epoch 2953: 0.944\n",
      "test loss on epoch 2953: 0.355\n",
      "test accuracy on epoch 2953: 0.769\n",
      "train loss on epoch 2954 : 0.108\n",
      "train accuracy on epoch 2954: 0.944\n",
      "test loss on epoch 2954: 0.354\n",
      "test accuracy on epoch 2954: 0.769\n",
      "train loss on epoch 2955 : 0.063\n",
      "train accuracy on epoch 2955: 1.000\n",
      "test loss on epoch 2955: 0.346\n",
      "test accuracy on epoch 2955: 0.769\n",
      "train loss on epoch 2956 : 0.282\n",
      "train accuracy on epoch 2956: 0.889\n",
      "test loss on epoch 2956: 0.351\n",
      "test accuracy on epoch 2956: 0.769\n",
      "train loss on epoch 2957 : 0.062\n",
      "train accuracy on epoch 2957: 1.000\n",
      "test loss on epoch 2957: 0.348\n",
      "test accuracy on epoch 2957: 0.769\n",
      "train loss on epoch 2958 : 0.342\n",
      "train accuracy on epoch 2958: 0.889\n",
      "test loss on epoch 2958: 0.330\n",
      "test accuracy on epoch 2958: 0.769\n",
      "train loss on epoch 2959 : 0.131\n",
      "train accuracy on epoch 2959: 0.944\n",
      "test loss on epoch 2959: 0.328\n",
      "test accuracy on epoch 2959: 0.769\n",
      "train loss on epoch 2960 : 0.329\n",
      "train accuracy on epoch 2960: 0.889\n",
      "test loss on epoch 2960: 0.345\n",
      "test accuracy on epoch 2960: 0.769\n",
      "train loss on epoch 2961 : 0.052\n",
      "train accuracy on epoch 2961: 1.000\n",
      "test loss on epoch 2961: 0.346\n",
      "test accuracy on epoch 2961: 0.769\n",
      "train loss on epoch 2962 : 0.358\n",
      "train accuracy on epoch 2962: 0.889\n",
      "test loss on epoch 2962: 0.340\n",
      "test accuracy on epoch 2962: 0.769\n",
      "train loss on epoch 2963 : 0.307\n",
      "train accuracy on epoch 2963: 0.944\n",
      "test loss on epoch 2963: 0.342\n",
      "test accuracy on epoch 2963: 0.769\n",
      "train loss on epoch 2964 : 0.200\n",
      "train accuracy on epoch 2964: 0.944\n",
      "test loss on epoch 2964: 0.348\n",
      "test accuracy on epoch 2964: 0.769\n",
      "train loss on epoch 2965 : 0.361\n",
      "train accuracy on epoch 2965: 0.833\n",
      "test loss on epoch 2965: 0.346\n",
      "test accuracy on epoch 2965: 0.769\n",
      "train loss on epoch 2966 : 0.221\n",
      "train accuracy on epoch 2966: 0.833\n",
      "test loss on epoch 2966: 0.325\n",
      "test accuracy on epoch 2966: 0.769\n",
      "train loss on epoch 2967 : 0.041\n",
      "train accuracy on epoch 2967: 1.000\n",
      "test loss on epoch 2967: 0.335\n",
      "test accuracy on epoch 2967: 0.769\n",
      "train loss on epoch 2968 : 0.188\n",
      "train accuracy on epoch 2968: 0.944\n",
      "test loss on epoch 2968: 0.346\n",
      "test accuracy on epoch 2968: 0.692\n",
      "train loss on epoch 2969 : 0.049\n",
      "train accuracy on epoch 2969: 1.000\n",
      "test loss on epoch 2969: 0.330\n",
      "test accuracy on epoch 2969: 0.846\n",
      "train loss on epoch 2970 : 0.265\n",
      "train accuracy on epoch 2970: 0.944\n",
      "test loss on epoch 2970: 0.342\n",
      "test accuracy on epoch 2970: 0.769\n",
      "train loss on epoch 2971 : 0.230\n",
      "train accuracy on epoch 2971: 0.889\n",
      "test loss on epoch 2971: 0.350\n",
      "test accuracy on epoch 2971: 0.769\n",
      "train loss on epoch 2972 : 0.469\n",
      "train accuracy on epoch 2972: 0.833\n",
      "test loss on epoch 2972: 0.365\n",
      "test accuracy on epoch 2972: 0.769\n",
      "train loss on epoch 2973 : 0.342\n",
      "train accuracy on epoch 2973: 0.944\n",
      "test loss on epoch 2973: 0.362\n",
      "test accuracy on epoch 2973: 0.769\n",
      "train loss on epoch 2974 : 0.149\n",
      "train accuracy on epoch 2974: 0.944\n",
      "test loss on epoch 2974: 0.353\n",
      "test accuracy on epoch 2974: 0.769\n",
      "train loss on epoch 2975 : 0.323\n",
      "train accuracy on epoch 2975: 0.944\n",
      "test loss on epoch 2975: 0.338\n",
      "test accuracy on epoch 2975: 0.769\n",
      "train loss on epoch 2976 : 0.066\n",
      "train accuracy on epoch 2976: 1.000\n",
      "test loss on epoch 2976: 0.332\n",
      "test accuracy on epoch 2976: 0.769\n",
      "train loss on epoch 2977 : 0.131\n",
      "train accuracy on epoch 2977: 1.000\n",
      "test loss on epoch 2977: 0.331\n",
      "test accuracy on epoch 2977: 0.769\n",
      "train loss on epoch 2978 : 0.245\n",
      "train accuracy on epoch 2978: 0.889\n",
      "test loss on epoch 2978: 0.335\n",
      "test accuracy on epoch 2978: 0.769\n",
      "train loss on epoch 2979 : 0.280\n",
      "train accuracy on epoch 2979: 0.889\n",
      "test loss on epoch 2979: 0.324\n",
      "test accuracy on epoch 2979: 0.846\n",
      "train loss on epoch 2980 : 0.465\n",
      "train accuracy on epoch 2980: 0.889\n",
      "test loss on epoch 2980: 0.340\n",
      "test accuracy on epoch 2980: 0.692\n",
      "train loss on epoch 2981 : 0.147\n",
      "train accuracy on epoch 2981: 0.944\n",
      "test loss on epoch 2981: 0.333\n",
      "test accuracy on epoch 2981: 0.769\n",
      "train loss on epoch 2982 : 0.090\n",
      "train accuracy on epoch 2982: 0.944\n",
      "test loss on epoch 2982: 0.337\n",
      "test accuracy on epoch 2982: 0.769\n",
      "train loss on epoch 2983 : 0.190\n",
      "train accuracy on epoch 2983: 0.944\n",
      "test loss on epoch 2983: 0.336\n",
      "test accuracy on epoch 2983: 0.769\n",
      "train loss on epoch 2984 : 0.165\n",
      "train accuracy on epoch 2984: 0.944\n",
      "test loss on epoch 2984: 0.326\n",
      "test accuracy on epoch 2984: 0.846\n",
      "train loss on epoch 2985 : 0.137\n",
      "train accuracy on epoch 2985: 0.944\n",
      "test loss on epoch 2985: 0.336\n",
      "test accuracy on epoch 2985: 0.769\n",
      "train loss on epoch 2986 : 0.186\n",
      "train accuracy on epoch 2986: 0.889\n",
      "test loss on epoch 2986: 0.325\n",
      "test accuracy on epoch 2986: 0.769\n",
      "train loss on epoch 2987 : 0.101\n",
      "train accuracy on epoch 2987: 1.000\n",
      "test loss on epoch 2987: 0.328\n",
      "test accuracy on epoch 2987: 0.769\n",
      "train loss on epoch 2988 : 0.087\n",
      "train accuracy on epoch 2988: 1.000\n",
      "test loss on epoch 2988: 0.348\n",
      "test accuracy on epoch 2988: 0.769\n",
      "train loss on epoch 2989 : 0.167\n",
      "train accuracy on epoch 2989: 0.944\n",
      "test loss on epoch 2989: 0.343\n",
      "test accuracy on epoch 2989: 0.769\n",
      "train loss on epoch 2990 : 0.217\n",
      "train accuracy on epoch 2990: 0.889\n",
      "test loss on epoch 2990: 0.325\n",
      "test accuracy on epoch 2990: 0.769\n",
      "train loss on epoch 2991 : 0.210\n",
      "train accuracy on epoch 2991: 0.889\n",
      "test loss on epoch 2991: 0.327\n",
      "test accuracy on epoch 2991: 0.769\n",
      "train loss on epoch 2992 : 0.255\n",
      "train accuracy on epoch 2992: 0.889\n",
      "test loss on epoch 2992: 0.329\n",
      "test accuracy on epoch 2992: 0.769\n",
      "train loss on epoch 2993 : 0.362\n",
      "train accuracy on epoch 2993: 0.889\n",
      "test loss on epoch 2993: 0.329\n",
      "test accuracy on epoch 2993: 0.769\n",
      "train loss on epoch 2994 : 0.173\n",
      "train accuracy on epoch 2994: 0.889\n",
      "test loss on epoch 2994: 0.330\n",
      "test accuracy on epoch 2994: 0.769\n",
      "train loss on epoch 2995 : 0.238\n",
      "train accuracy on epoch 2995: 0.889\n",
      "test loss on epoch 2995: 0.349\n",
      "test accuracy on epoch 2995: 0.769\n",
      "train loss on epoch 2996 : 0.238\n",
      "train accuracy on epoch 2996: 0.944\n",
      "test loss on epoch 2996: 0.351\n",
      "test accuracy on epoch 2996: 0.769\n",
      "train loss on epoch 2997 : 0.074\n",
      "train accuracy on epoch 2997: 1.000\n",
      "test loss on epoch 2997: 0.351\n",
      "test accuracy on epoch 2997: 0.769\n",
      "train loss on epoch 2998 : 0.052\n",
      "train accuracy on epoch 2998: 1.000\n",
      "test loss on epoch 2998: 0.326\n",
      "test accuracy on epoch 2998: 0.769\n",
      "train loss on epoch 2999 : 0.093\n",
      "train accuracy on epoch 2999: 0.944\n",
      "test loss on epoch 2999: 0.337\n",
      "test accuracy on epoch 2999: 0.769\n",
      "train loss on epoch 3000 : 0.382\n",
      "train accuracy on epoch 3000: 0.833\n",
      "test loss on epoch 3000: 0.329\n",
      "test accuracy on epoch 3000: 0.846\n",
      "train loss on epoch 3001 : 0.246\n",
      "train accuracy on epoch 3001: 0.889\n",
      "test loss on epoch 3001: 0.330\n",
      "test accuracy on epoch 3001: 0.846\n",
      "train loss on epoch 3002 : 0.354\n",
      "train accuracy on epoch 3002: 0.889\n",
      "test loss on epoch 3002: 0.350\n",
      "test accuracy on epoch 3002: 0.692\n",
      "train loss on epoch 3003 : 0.184\n",
      "train accuracy on epoch 3003: 0.889\n",
      "test loss on epoch 3003: 0.344\n",
      "test accuracy on epoch 3003: 0.769\n",
      "train loss on epoch 3004 : 0.112\n",
      "train accuracy on epoch 3004: 0.944\n",
      "test loss on epoch 3004: 0.343\n",
      "test accuracy on epoch 3004: 0.769\n",
      "train loss on epoch 3005 : 0.176\n",
      "train accuracy on epoch 3005: 0.944\n",
      "test loss on epoch 3005: 0.349\n",
      "test accuracy on epoch 3005: 0.692\n",
      "train loss on epoch 3006 : 0.245\n",
      "train accuracy on epoch 3006: 0.889\n",
      "test loss on epoch 3006: 0.349\n",
      "test accuracy on epoch 3006: 0.692\n",
      "train loss on epoch 3007 : 0.437\n",
      "train accuracy on epoch 3007: 0.889\n",
      "test loss on epoch 3007: 0.347\n",
      "test accuracy on epoch 3007: 0.769\n",
      "train loss on epoch 3008 : 0.141\n",
      "train accuracy on epoch 3008: 0.944\n",
      "test loss on epoch 3008: 0.328\n",
      "test accuracy on epoch 3008: 0.769\n",
      "train loss on epoch 3009 : 0.314\n",
      "train accuracy on epoch 3009: 0.889\n",
      "test loss on epoch 3009: 0.350\n",
      "test accuracy on epoch 3009: 0.769\n",
      "train loss on epoch 3010 : 0.127\n",
      "train accuracy on epoch 3010: 0.944\n",
      "test loss on epoch 3010: 0.350\n",
      "test accuracy on epoch 3010: 0.769\n",
      "train loss on epoch 3011 : 0.134\n",
      "train accuracy on epoch 3011: 1.000\n",
      "test loss on epoch 3011: 0.325\n",
      "test accuracy on epoch 3011: 0.769\n",
      "train loss on epoch 3012 : 0.133\n",
      "train accuracy on epoch 3012: 0.944\n",
      "test loss on epoch 3012: 0.325\n",
      "test accuracy on epoch 3012: 0.769\n",
      "train loss on epoch 3013 : 0.190\n",
      "train accuracy on epoch 3013: 0.944\n",
      "test loss on epoch 3013: 0.349\n",
      "test accuracy on epoch 3013: 0.769\n",
      "train loss on epoch 3014 : 0.193\n",
      "train accuracy on epoch 3014: 0.944\n",
      "test loss on epoch 3014: 0.328\n",
      "test accuracy on epoch 3014: 0.846\n",
      "train loss on epoch 3015 : 0.103\n",
      "train accuracy on epoch 3015: 1.000\n",
      "test loss on epoch 3015: 0.330\n",
      "test accuracy on epoch 3015: 0.846\n",
      "train loss on epoch 3016 : 0.169\n",
      "train accuracy on epoch 3016: 0.944\n",
      "test loss on epoch 3016: 0.341\n",
      "test accuracy on epoch 3016: 0.769\n",
      "train loss on epoch 3017 : 0.061\n",
      "train accuracy on epoch 3017: 1.000\n",
      "test loss on epoch 3017: 0.333\n",
      "test accuracy on epoch 3017: 0.846\n",
      "train loss on epoch 3018 : 0.064\n",
      "train accuracy on epoch 3018: 1.000\n",
      "test loss on epoch 3018: 0.343\n",
      "test accuracy on epoch 3018: 0.769\n",
      "train loss on epoch 3019 : 0.227\n",
      "train accuracy on epoch 3019: 0.944\n",
      "test loss on epoch 3019: 0.339\n",
      "test accuracy on epoch 3019: 0.769\n",
      "train loss on epoch 3020 : 0.387\n",
      "train accuracy on epoch 3020: 0.889\n",
      "test loss on epoch 3020: 0.342\n",
      "test accuracy on epoch 3020: 0.769\n",
      "train loss on epoch 3021 : 0.394\n",
      "train accuracy on epoch 3021: 0.889\n",
      "test loss on epoch 3021: 0.364\n",
      "test accuracy on epoch 3021: 0.769\n",
      "train loss on epoch 3022 : 0.156\n",
      "train accuracy on epoch 3022: 0.944\n",
      "test loss on epoch 3022: 0.352\n",
      "test accuracy on epoch 3022: 0.769\n",
      "train loss on epoch 3023 : 0.104\n",
      "train accuracy on epoch 3023: 1.000\n",
      "test loss on epoch 3023: 0.354\n",
      "test accuracy on epoch 3023: 0.769\n",
      "train loss on epoch 3024 : 0.280\n",
      "train accuracy on epoch 3024: 0.889\n",
      "test loss on epoch 3024: 0.357\n",
      "test accuracy on epoch 3024: 0.692\n",
      "train loss on epoch 3025 : 0.052\n",
      "train accuracy on epoch 3025: 1.000\n",
      "test loss on epoch 3025: 0.338\n",
      "test accuracy on epoch 3025: 0.769\n",
      "train loss on epoch 3026 : 0.282\n",
      "train accuracy on epoch 3026: 0.889\n",
      "test loss on epoch 3026: 0.347\n",
      "test accuracy on epoch 3026: 0.769\n",
      "train loss on epoch 3027 : 0.136\n",
      "train accuracy on epoch 3027: 0.944\n",
      "test loss on epoch 3027: 0.353\n",
      "test accuracy on epoch 3027: 0.692\n",
      "train loss on epoch 3028 : 0.161\n",
      "train accuracy on epoch 3028: 0.944\n",
      "test loss on epoch 3028: 0.353\n",
      "test accuracy on epoch 3028: 0.692\n",
      "train loss on epoch 3029 : 0.414\n",
      "train accuracy on epoch 3029: 0.833\n",
      "test loss on epoch 3029: 0.331\n",
      "test accuracy on epoch 3029: 0.846\n",
      "train loss on epoch 3030 : 0.299\n",
      "train accuracy on epoch 3030: 0.889\n",
      "test loss on epoch 3030: 0.345\n",
      "test accuracy on epoch 3030: 0.769\n",
      "train loss on epoch 3031 : 0.074\n",
      "train accuracy on epoch 3031: 1.000\n",
      "test loss on epoch 3031: 0.343\n",
      "test accuracy on epoch 3031: 0.769\n",
      "train loss on epoch 3032 : 0.147\n",
      "train accuracy on epoch 3032: 0.944\n",
      "test loss on epoch 3032: 0.337\n",
      "test accuracy on epoch 3032: 0.846\n",
      "train loss on epoch 3033 : 0.293\n",
      "train accuracy on epoch 3033: 0.889\n",
      "test loss on epoch 3033: 0.334\n",
      "test accuracy on epoch 3033: 0.846\n",
      "train loss on epoch 3034 : 0.172\n",
      "train accuracy on epoch 3034: 0.944\n",
      "test loss on epoch 3034: 0.338\n",
      "test accuracy on epoch 3034: 0.846\n",
      "train loss on epoch 3035 : 0.286\n",
      "train accuracy on epoch 3035: 0.889\n",
      "test loss on epoch 3035: 0.339\n",
      "test accuracy on epoch 3035: 0.846\n",
      "train loss on epoch 3036 : 0.155\n",
      "train accuracy on epoch 3036: 0.889\n",
      "test loss on epoch 3036: 0.341\n",
      "test accuracy on epoch 3036: 0.846\n",
      "train loss on epoch 3037 : 0.112\n",
      "train accuracy on epoch 3037: 0.889\n",
      "test loss on epoch 3037: 0.364\n",
      "test accuracy on epoch 3037: 0.769\n",
      "train loss on epoch 3038 : 0.169\n",
      "train accuracy on epoch 3038: 0.944\n",
      "test loss on epoch 3038: 0.344\n",
      "test accuracy on epoch 3038: 0.769\n",
      "train loss on epoch 3039 : 0.100\n",
      "train accuracy on epoch 3039: 1.000\n",
      "test loss on epoch 3039: 0.364\n",
      "test accuracy on epoch 3039: 0.769\n",
      "train loss on epoch 3040 : 0.184\n",
      "train accuracy on epoch 3040: 0.889\n",
      "test loss on epoch 3040: 0.346\n",
      "test accuracy on epoch 3040: 0.769\n",
      "train loss on epoch 3041 : 0.062\n",
      "train accuracy on epoch 3041: 1.000\n",
      "test loss on epoch 3041: 0.350\n",
      "test accuracy on epoch 3041: 0.769\n",
      "train loss on epoch 3042 : 0.097\n",
      "train accuracy on epoch 3042: 1.000\n",
      "test loss on epoch 3042: 0.356\n",
      "test accuracy on epoch 3042: 0.769\n",
      "train loss on epoch 3043 : 0.125\n",
      "train accuracy on epoch 3043: 0.944\n",
      "test loss on epoch 3043: 0.369\n",
      "test accuracy on epoch 3043: 0.769\n",
      "train loss on epoch 3044 : 0.137\n",
      "train accuracy on epoch 3044: 0.944\n",
      "test loss on epoch 3044: 0.368\n",
      "test accuracy on epoch 3044: 0.769\n",
      "train loss on epoch 3045 : 0.261\n",
      "train accuracy on epoch 3045: 0.889\n",
      "test loss on epoch 3045: 0.352\n",
      "test accuracy on epoch 3045: 0.769\n",
      "train loss on epoch 3046 : 0.133\n",
      "train accuracy on epoch 3046: 0.889\n",
      "test loss on epoch 3046: 0.362\n",
      "test accuracy on epoch 3046: 0.769\n",
      "train loss on epoch 3047 : 0.067\n",
      "train accuracy on epoch 3047: 1.000\n",
      "test loss on epoch 3047: 0.365\n",
      "test accuracy on epoch 3047: 0.769\n",
      "train loss on epoch 3048 : 0.034\n",
      "train accuracy on epoch 3048: 1.000\n",
      "test loss on epoch 3048: 0.337\n",
      "test accuracy on epoch 3048: 0.769\n",
      "train loss on epoch 3049 : 0.088\n",
      "train accuracy on epoch 3049: 0.944\n",
      "test loss on epoch 3049: 0.347\n",
      "test accuracy on epoch 3049: 0.769\n",
      "train loss on epoch 3050 : 0.237\n",
      "train accuracy on epoch 3050: 0.889\n",
      "test loss on epoch 3050: 0.337\n",
      "test accuracy on epoch 3050: 0.769\n",
      "train loss on epoch 3051 : 0.415\n",
      "train accuracy on epoch 3051: 0.833\n",
      "test loss on epoch 3051: 0.337\n",
      "test accuracy on epoch 3051: 0.769\n",
      "train loss on epoch 3052 : 0.137\n",
      "train accuracy on epoch 3052: 0.944\n",
      "test loss on epoch 3052: 0.362\n",
      "test accuracy on epoch 3052: 0.769\n",
      "train loss on epoch 3053 : 0.141\n",
      "train accuracy on epoch 3053: 0.944\n",
      "test loss on epoch 3053: 0.339\n",
      "test accuracy on epoch 3053: 0.769\n",
      "train loss on epoch 3054 : 0.046\n",
      "train accuracy on epoch 3054: 1.000\n",
      "test loss on epoch 3054: 0.362\n",
      "test accuracy on epoch 3054: 0.692\n",
      "train loss on epoch 3055 : 0.226\n",
      "train accuracy on epoch 3055: 0.944\n",
      "test loss on epoch 3055: 0.362\n",
      "test accuracy on epoch 3055: 0.692\n",
      "train loss on epoch 3056 : 0.255\n",
      "train accuracy on epoch 3056: 0.889\n",
      "test loss on epoch 3056: 0.351\n",
      "test accuracy on epoch 3056: 0.769\n",
      "train loss on epoch 3057 : 0.152\n",
      "train accuracy on epoch 3057: 0.944\n",
      "test loss on epoch 3057: 0.340\n",
      "test accuracy on epoch 3057: 0.846\n",
      "train loss on epoch 3058 : 0.098\n",
      "train accuracy on epoch 3058: 0.944\n",
      "test loss on epoch 3058: 0.354\n",
      "test accuracy on epoch 3058: 0.769\n",
      "train loss on epoch 3059 : 0.145\n",
      "train accuracy on epoch 3059: 0.944\n",
      "test loss on epoch 3059: 0.354\n",
      "test accuracy on epoch 3059: 0.769\n",
      "train loss on epoch 3060 : 0.192\n",
      "train accuracy on epoch 3060: 0.889\n",
      "test loss on epoch 3060: 0.344\n",
      "test accuracy on epoch 3060: 0.846\n",
      "train loss on epoch 3061 : 0.377\n",
      "train accuracy on epoch 3061: 0.833\n",
      "test loss on epoch 3061: 0.342\n",
      "test accuracy on epoch 3061: 0.846\n",
      "train loss on epoch 3062 : 0.253\n",
      "train accuracy on epoch 3062: 0.889\n",
      "test loss on epoch 3062: 0.366\n",
      "test accuracy on epoch 3062: 0.769\n",
      "train loss on epoch 3063 : 0.143\n",
      "train accuracy on epoch 3063: 1.000\n",
      "test loss on epoch 3063: 0.369\n",
      "test accuracy on epoch 3063: 0.769\n",
      "train loss on epoch 3064 : 0.084\n",
      "train accuracy on epoch 3064: 1.000\n",
      "test loss on epoch 3064: 0.364\n",
      "test accuracy on epoch 3064: 0.769\n",
      "train loss on epoch 3065 : 0.172\n",
      "train accuracy on epoch 3065: 0.889\n",
      "test loss on epoch 3065: 0.340\n",
      "test accuracy on epoch 3065: 0.769\n",
      "train loss on epoch 3066 : 0.169\n",
      "train accuracy on epoch 3066: 0.944\n",
      "test loss on epoch 3066: 0.339\n",
      "test accuracy on epoch 3066: 0.769\n",
      "train loss on epoch 3067 : 0.202\n",
      "train accuracy on epoch 3067: 0.889\n",
      "test loss on epoch 3067: 0.339\n",
      "test accuracy on epoch 3067: 0.769\n",
      "train loss on epoch 3068 : 0.066\n",
      "train accuracy on epoch 3068: 1.000\n",
      "test loss on epoch 3068: 0.341\n",
      "test accuracy on epoch 3068: 0.769\n",
      "train loss on epoch 3069 : 0.382\n",
      "train accuracy on epoch 3069: 0.889\n",
      "test loss on epoch 3069: 0.354\n",
      "test accuracy on epoch 3069: 0.769\n",
      "train loss on epoch 3070 : 0.295\n",
      "train accuracy on epoch 3070: 0.833\n",
      "test loss on epoch 3070: 0.344\n",
      "test accuracy on epoch 3070: 0.846\n",
      "train loss on epoch 3071 : 0.307\n",
      "train accuracy on epoch 3071: 0.889\n",
      "test loss on epoch 3071: 0.362\n",
      "test accuracy on epoch 3071: 0.692\n",
      "train loss on epoch 3072 : 0.033\n",
      "train accuracy on epoch 3072: 1.000\n",
      "test loss on epoch 3072: 0.365\n",
      "test accuracy on epoch 3072: 0.692\n",
      "train loss on epoch 3073 : 0.179\n",
      "train accuracy on epoch 3073: 0.944\n",
      "test loss on epoch 3073: 0.366\n",
      "test accuracy on epoch 3073: 0.692\n",
      "train loss on epoch 3074 : 0.157\n",
      "train accuracy on epoch 3074: 0.944\n",
      "test loss on epoch 3074: 0.341\n",
      "test accuracy on epoch 3074: 0.769\n",
      "train loss on epoch 3075 : 0.064\n",
      "train accuracy on epoch 3075: 1.000\n",
      "test loss on epoch 3075: 0.343\n",
      "test accuracy on epoch 3075: 0.769\n",
      "train loss on epoch 3076 : 0.157\n",
      "train accuracy on epoch 3076: 0.889\n",
      "test loss on epoch 3076: 0.362\n",
      "test accuracy on epoch 3076: 0.769\n",
      "train loss on epoch 3077 : 0.091\n",
      "train accuracy on epoch 3077: 0.944\n",
      "test loss on epoch 3077: 0.355\n",
      "test accuracy on epoch 3077: 0.769\n",
      "train loss on epoch 3078 : 0.249\n",
      "train accuracy on epoch 3078: 0.944\n",
      "test loss on epoch 3078: 0.351\n",
      "test accuracy on epoch 3078: 0.769\n",
      "train loss on epoch 3079 : 0.144\n",
      "train accuracy on epoch 3079: 0.944\n",
      "test loss on epoch 3079: 0.362\n",
      "test accuracy on epoch 3079: 0.692\n",
      "train loss on epoch 3080 : 0.107\n",
      "train accuracy on epoch 3080: 0.944\n",
      "test loss on epoch 3080: 0.365\n",
      "test accuracy on epoch 3080: 0.692\n",
      "train loss on epoch 3081 : 0.052\n",
      "train accuracy on epoch 3081: 1.000\n",
      "test loss on epoch 3081: 0.370\n",
      "test accuracy on epoch 3081: 0.769\n",
      "train loss on epoch 3082 : 0.304\n",
      "train accuracy on epoch 3082: 0.889\n",
      "test loss on epoch 3082: 0.361\n",
      "test accuracy on epoch 3082: 0.769\n",
      "train loss on epoch 3083 : 0.236\n",
      "train accuracy on epoch 3083: 0.889\n",
      "test loss on epoch 3083: 0.349\n",
      "test accuracy on epoch 3083: 0.846\n",
      "train loss on epoch 3084 : 0.179\n",
      "train accuracy on epoch 3084: 0.889\n",
      "test loss on epoch 3084: 0.370\n",
      "test accuracy on epoch 3084: 0.692\n",
      "train loss on epoch 3085 : 0.069\n",
      "train accuracy on epoch 3085: 1.000\n",
      "test loss on epoch 3085: 0.367\n",
      "test accuracy on epoch 3085: 0.692\n",
      "train loss on epoch 3086 : 0.115\n",
      "train accuracy on epoch 3086: 0.944\n",
      "test loss on epoch 3086: 0.358\n",
      "test accuracy on epoch 3086: 0.769\n",
      "train loss on epoch 3087 : 0.082\n",
      "train accuracy on epoch 3087: 0.944\n",
      "test loss on epoch 3087: 0.358\n",
      "test accuracy on epoch 3087: 0.769\n",
      "train loss on epoch 3088 : 0.163\n",
      "train accuracy on epoch 3088: 0.944\n",
      "test loss on epoch 3088: 0.355\n",
      "test accuracy on epoch 3088: 0.769\n",
      "train loss on epoch 3089 : 0.211\n",
      "train accuracy on epoch 3089: 0.833\n",
      "test loss on epoch 3089: 0.347\n",
      "test accuracy on epoch 3089: 0.846\n",
      "train loss on epoch 3090 : 0.161\n",
      "train accuracy on epoch 3090: 0.944\n",
      "test loss on epoch 3090: 0.367\n",
      "test accuracy on epoch 3090: 0.769\n",
      "train loss on epoch 3091 : 0.118\n",
      "train accuracy on epoch 3091: 0.944\n",
      "test loss on epoch 3091: 0.361\n",
      "test accuracy on epoch 3091: 0.769\n",
      "train loss on epoch 3092 : 0.052\n",
      "train accuracy on epoch 3092: 1.000\n",
      "test loss on epoch 3092: 0.346\n",
      "test accuracy on epoch 3092: 0.769\n",
      "train loss on epoch 3093 : 0.266\n",
      "train accuracy on epoch 3093: 0.944\n",
      "test loss on epoch 3093: 0.373\n",
      "test accuracy on epoch 3093: 0.769\n",
      "train loss on epoch 3094 : 0.072\n",
      "train accuracy on epoch 3094: 1.000\n",
      "test loss on epoch 3094: 0.373\n",
      "test accuracy on epoch 3094: 0.769\n",
      "train loss on epoch 3095 : 0.080\n",
      "train accuracy on epoch 3095: 1.000\n",
      "test loss on epoch 3095: 0.364\n",
      "test accuracy on epoch 3095: 0.769\n",
      "train loss on epoch 3096 : 0.116\n",
      "train accuracy on epoch 3096: 0.944\n",
      "test loss on epoch 3096: 0.370\n",
      "test accuracy on epoch 3096: 0.769\n",
      "train loss on epoch 3097 : 0.115\n",
      "train accuracy on epoch 3097: 0.944\n",
      "test loss on epoch 3097: 0.363\n",
      "test accuracy on epoch 3097: 0.769\n",
      "train loss on epoch 3098 : 0.138\n",
      "train accuracy on epoch 3098: 0.944\n",
      "test loss on epoch 3098: 0.373\n",
      "test accuracy on epoch 3098: 0.769\n",
      "train loss on epoch 3099 : 0.588\n",
      "train accuracy on epoch 3099: 0.833\n",
      "test loss on epoch 3099: 0.348\n",
      "test accuracy on epoch 3099: 0.769\n",
      "train loss on epoch 3100 : 0.143\n",
      "train accuracy on epoch 3100: 0.889\n",
      "test loss on epoch 3100: 0.346\n",
      "test accuracy on epoch 3100: 0.769\n",
      "train loss on epoch 3101 : 0.214\n",
      "train accuracy on epoch 3101: 0.889\n",
      "test loss on epoch 3101: 0.356\n",
      "test accuracy on epoch 3101: 0.769\n",
      "train loss on epoch 3102 : 0.069\n",
      "train accuracy on epoch 3102: 1.000\n",
      "test loss on epoch 3102: 0.351\n",
      "test accuracy on epoch 3102: 0.769\n",
      "train loss on epoch 3103 : 0.373\n",
      "train accuracy on epoch 3103: 0.833\n",
      "test loss on epoch 3103: 0.347\n",
      "test accuracy on epoch 3103: 0.769\n",
      "train loss on epoch 3104 : 0.154\n",
      "train accuracy on epoch 3104: 0.889\n",
      "test loss on epoch 3104: 0.350\n",
      "test accuracy on epoch 3104: 0.769\n",
      "train loss on epoch 3105 : 0.128\n",
      "train accuracy on epoch 3105: 0.944\n",
      "test loss on epoch 3105: 0.349\n",
      "test accuracy on epoch 3105: 0.769\n",
      "train loss on epoch 3106 : 0.329\n",
      "train accuracy on epoch 3106: 0.889\n",
      "test loss on epoch 3106: 0.351\n",
      "test accuracy on epoch 3106: 0.846\n",
      "train loss on epoch 3107 : 0.091\n",
      "train accuracy on epoch 3107: 1.000\n",
      "test loss on epoch 3107: 0.363\n",
      "test accuracy on epoch 3107: 0.769\n",
      "train loss on epoch 3108 : 0.238\n",
      "train accuracy on epoch 3108: 0.889\n",
      "test loss on epoch 3108: 0.375\n",
      "test accuracy on epoch 3108: 0.692\n",
      "train loss on epoch 3109 : 0.206\n",
      "train accuracy on epoch 3109: 0.889\n",
      "test loss on epoch 3109: 0.366\n",
      "test accuracy on epoch 3109: 0.769\n",
      "train loss on epoch 3110 : 0.129\n",
      "train accuracy on epoch 3110: 0.889\n",
      "test loss on epoch 3110: 0.368\n",
      "test accuracy on epoch 3110: 0.769\n",
      "train loss on epoch 3111 : 0.225\n",
      "train accuracy on epoch 3111: 0.889\n",
      "test loss on epoch 3111: 0.371\n",
      "test accuracy on epoch 3111: 0.769\n",
      "train loss on epoch 3112 : 0.181\n",
      "train accuracy on epoch 3112: 0.889\n",
      "test loss on epoch 3112: 0.377\n",
      "test accuracy on epoch 3112: 0.769\n",
      "train loss on epoch 3113 : 0.404\n",
      "train accuracy on epoch 3113: 0.889\n",
      "test loss on epoch 3113: 0.354\n",
      "test accuracy on epoch 3113: 0.769\n",
      "train loss on epoch 3114 : 0.186\n",
      "train accuracy on epoch 3114: 0.889\n",
      "test loss on epoch 3114: 0.364\n",
      "test accuracy on epoch 3114: 0.769\n",
      "train loss on epoch 3115 : 0.252\n",
      "train accuracy on epoch 3115: 0.889\n",
      "test loss on epoch 3115: 0.381\n",
      "test accuracy on epoch 3115: 0.769\n",
      "train loss on epoch 3116 : 0.186\n",
      "train accuracy on epoch 3116: 0.944\n",
      "test loss on epoch 3116: 0.372\n",
      "test accuracy on epoch 3116: 0.769\n",
      "train loss on epoch 3117 : 0.353\n",
      "train accuracy on epoch 3117: 0.833\n",
      "test loss on epoch 3117: 0.360\n",
      "test accuracy on epoch 3117: 0.846\n",
      "train loss on epoch 3118 : 0.084\n",
      "train accuracy on epoch 3118: 1.000\n",
      "test loss on epoch 3118: 0.379\n",
      "test accuracy on epoch 3118: 0.692\n",
      "train loss on epoch 3119 : 0.042\n",
      "train accuracy on epoch 3119: 1.000\n",
      "test loss on epoch 3119: 0.370\n",
      "test accuracy on epoch 3119: 0.769\n",
      "train loss on epoch 3120 : 0.202\n",
      "train accuracy on epoch 3120: 0.889\n",
      "test loss on epoch 3120: 0.369\n",
      "test accuracy on epoch 3120: 0.769\n",
      "train loss on epoch 3121 : 0.236\n",
      "train accuracy on epoch 3121: 0.833\n",
      "test loss on epoch 3121: 0.360\n",
      "test accuracy on epoch 3121: 0.846\n",
      "train loss on epoch 3122 : 0.222\n",
      "train accuracy on epoch 3122: 0.889\n",
      "test loss on epoch 3122: 0.357\n",
      "test accuracy on epoch 3122: 0.846\n",
      "train loss on epoch 3123 : 0.154\n",
      "train accuracy on epoch 3123: 0.889\n",
      "test loss on epoch 3123: 0.376\n",
      "test accuracy on epoch 3123: 0.769\n",
      "train loss on epoch 3124 : 0.195\n",
      "train accuracy on epoch 3124: 0.889\n",
      "test loss on epoch 3124: 0.368\n",
      "test accuracy on epoch 3124: 0.769\n",
      "train loss on epoch 3125 : 0.248\n",
      "train accuracy on epoch 3125: 0.944\n",
      "test loss on epoch 3125: 0.355\n",
      "test accuracy on epoch 3125: 0.769\n",
      "train loss on epoch 3126 : 0.105\n",
      "train accuracy on epoch 3126: 1.000\n",
      "test loss on epoch 3126: 0.380\n",
      "test accuracy on epoch 3126: 0.769\n",
      "train loss on epoch 3127 : 0.221\n",
      "train accuracy on epoch 3127: 0.889\n",
      "test loss on epoch 3127: 0.379\n",
      "test accuracy on epoch 3127: 0.769\n",
      "train loss on epoch 3128 : 0.120\n",
      "train accuracy on epoch 3128: 0.944\n",
      "test loss on epoch 3128: 0.384\n",
      "test accuracy on epoch 3128: 0.769\n",
      "train loss on epoch 3129 : 0.197\n",
      "train accuracy on epoch 3129: 0.889\n",
      "test loss on epoch 3129: 0.381\n",
      "test accuracy on epoch 3129: 0.769\n",
      "train loss on epoch 3130 : 0.145\n",
      "train accuracy on epoch 3130: 0.944\n",
      "test loss on epoch 3130: 0.380\n",
      "test accuracy on epoch 3130: 0.769\n",
      "train loss on epoch 3131 : 0.230\n",
      "train accuracy on epoch 3131: 0.944\n",
      "test loss on epoch 3131: 0.371\n",
      "test accuracy on epoch 3131: 0.769\n",
      "train loss on epoch 3132 : 0.305\n",
      "train accuracy on epoch 3132: 0.889\n",
      "test loss on epoch 3132: 0.379\n",
      "test accuracy on epoch 3132: 0.769\n",
      "train loss on epoch 3133 : 0.201\n",
      "train accuracy on epoch 3133: 0.889\n",
      "test loss on epoch 3133: 0.358\n",
      "test accuracy on epoch 3133: 0.769\n",
      "train loss on epoch 3134 : 0.211\n",
      "train accuracy on epoch 3134: 0.944\n",
      "test loss on epoch 3134: 0.380\n",
      "test accuracy on epoch 3134: 0.769\n",
      "train loss on epoch 3135 : 0.112\n",
      "train accuracy on epoch 3135: 0.944\n",
      "test loss on epoch 3135: 0.363\n",
      "test accuracy on epoch 3135: 0.846\n",
      "train loss on epoch 3136 : 0.227\n",
      "train accuracy on epoch 3136: 0.944\n",
      "test loss on epoch 3136: 0.382\n",
      "test accuracy on epoch 3136: 0.692\n",
      "train loss on epoch 3137 : 0.244\n",
      "train accuracy on epoch 3137: 0.833\n",
      "test loss on epoch 3137: 0.365\n",
      "test accuracy on epoch 3137: 0.846\n",
      "train loss on epoch 3138 : 0.091\n",
      "train accuracy on epoch 3138: 0.944\n",
      "test loss on epoch 3138: 0.380\n",
      "test accuracy on epoch 3138: 0.692\n",
      "train loss on epoch 3139 : 0.338\n",
      "train accuracy on epoch 3139: 0.889\n",
      "test loss on epoch 3139: 0.370\n",
      "test accuracy on epoch 3139: 0.769\n",
      "train loss on epoch 3140 : 0.128\n",
      "train accuracy on epoch 3140: 0.944\n",
      "test loss on epoch 3140: 0.378\n",
      "test accuracy on epoch 3140: 0.769\n",
      "train loss on epoch 3141 : 0.175\n",
      "train accuracy on epoch 3141: 0.889\n",
      "test loss on epoch 3141: 0.364\n",
      "test accuracy on epoch 3141: 0.769\n",
      "train loss on epoch 3142 : 0.068\n",
      "train accuracy on epoch 3142: 1.000\n",
      "test loss on epoch 3142: 0.378\n",
      "test accuracy on epoch 3142: 0.769\n",
      "train loss on epoch 3143 : 0.159\n",
      "train accuracy on epoch 3143: 0.889\n",
      "test loss on epoch 3143: 0.358\n",
      "test accuracy on epoch 3143: 0.846\n",
      "train loss on epoch 3144 : 0.180\n",
      "train accuracy on epoch 3144: 0.944\n",
      "test loss on epoch 3144: 0.354\n",
      "test accuracy on epoch 3144: 0.846\n",
      "train loss on epoch 3145 : 0.367\n",
      "train accuracy on epoch 3145: 0.778\n",
      "test loss on epoch 3145: 0.362\n",
      "test accuracy on epoch 3145: 0.769\n",
      "train loss on epoch 3146 : 0.316\n",
      "train accuracy on epoch 3146: 0.889\n",
      "test loss on epoch 3146: 0.362\n",
      "test accuracy on epoch 3146: 0.769\n",
      "train loss on epoch 3147 : 0.100\n",
      "train accuracy on epoch 3147: 1.000\n",
      "test loss on epoch 3147: 0.372\n",
      "test accuracy on epoch 3147: 0.769\n",
      "train loss on epoch 3148 : 0.142\n",
      "train accuracy on epoch 3148: 0.944\n",
      "test loss on epoch 3148: 0.364\n",
      "test accuracy on epoch 3148: 0.769\n",
      "train loss on epoch 3149 : 0.111\n",
      "train accuracy on epoch 3149: 0.944\n",
      "test loss on epoch 3149: 0.367\n",
      "test accuracy on epoch 3149: 0.692\n",
      "train loss on epoch 3150 : 0.158\n",
      "train accuracy on epoch 3150: 0.944\n",
      "test loss on epoch 3150: 0.360\n",
      "test accuracy on epoch 3150: 0.769\n",
      "train loss on epoch 3151 : 0.213\n",
      "train accuracy on epoch 3151: 0.889\n",
      "test loss on epoch 3151: 0.358\n",
      "test accuracy on epoch 3151: 0.769\n",
      "train loss on epoch 3152 : 0.203\n",
      "train accuracy on epoch 3152: 0.889\n",
      "test loss on epoch 3152: 0.362\n",
      "test accuracy on epoch 3152: 0.769\n",
      "train loss on epoch 3153 : 0.035\n",
      "train accuracy on epoch 3153: 1.000\n",
      "test loss on epoch 3153: 0.359\n",
      "test accuracy on epoch 3153: 0.769\n",
      "train loss on epoch 3154 : 0.203\n",
      "train accuracy on epoch 3154: 0.889\n",
      "test loss on epoch 3154: 0.360\n",
      "test accuracy on epoch 3154: 0.769\n",
      "train loss on epoch 3155 : 0.313\n",
      "train accuracy on epoch 3155: 0.889\n",
      "test loss on epoch 3155: 0.363\n",
      "test accuracy on epoch 3155: 0.769\n",
      "train loss on epoch 3156 : 0.067\n",
      "train accuracy on epoch 3156: 1.000\n",
      "test loss on epoch 3156: 0.358\n",
      "test accuracy on epoch 3156: 0.769\n",
      "train loss on epoch 3157 : 0.343\n",
      "train accuracy on epoch 3157: 0.833\n",
      "test loss on epoch 3157: 0.368\n",
      "test accuracy on epoch 3157: 0.769\n",
      "train loss on epoch 3158 : 0.061\n",
      "train accuracy on epoch 3158: 1.000\n",
      "test loss on epoch 3158: 0.369\n",
      "test accuracy on epoch 3158: 0.769\n",
      "train loss on epoch 3159 : 0.290\n",
      "train accuracy on epoch 3159: 0.889\n",
      "test loss on epoch 3159: 0.357\n",
      "test accuracy on epoch 3159: 0.769\n",
      "train loss on epoch 3160 : 0.370\n",
      "train accuracy on epoch 3160: 0.833\n",
      "test loss on epoch 3160: 0.363\n",
      "test accuracy on epoch 3160: 0.769\n",
      "train loss on epoch 3161 : 0.130\n",
      "train accuracy on epoch 3161: 0.944\n",
      "test loss on epoch 3161: 0.365\n",
      "test accuracy on epoch 3161: 0.769\n",
      "train loss on epoch 3162 : 0.035\n",
      "train accuracy on epoch 3162: 1.000\n",
      "test loss on epoch 3162: 0.347\n",
      "test accuracy on epoch 3162: 0.769\n",
      "train loss on epoch 3163 : 0.324\n",
      "train accuracy on epoch 3163: 0.833\n",
      "test loss on epoch 3163: 0.350\n",
      "test accuracy on epoch 3163: 0.769\n",
      "train loss on epoch 3164 : 0.146\n",
      "train accuracy on epoch 3164: 0.944\n",
      "test loss on epoch 3164: 0.351\n",
      "test accuracy on epoch 3164: 0.769\n",
      "train loss on epoch 3165 : 0.108\n",
      "train accuracy on epoch 3165: 0.944\n",
      "test loss on epoch 3165: 0.346\n",
      "test accuracy on epoch 3165: 0.846\n",
      "train loss on epoch 3166 : 0.127\n",
      "train accuracy on epoch 3166: 1.000\n",
      "test loss on epoch 3166: 0.345\n",
      "test accuracy on epoch 3166: 0.846\n",
      "train loss on epoch 3167 : 0.296\n",
      "train accuracy on epoch 3167: 0.833\n",
      "test loss on epoch 3167: 0.360\n",
      "test accuracy on epoch 3167: 0.769\n",
      "train loss on epoch 3168 : 0.108\n",
      "train accuracy on epoch 3168: 0.944\n",
      "test loss on epoch 3168: 0.347\n",
      "test accuracy on epoch 3168: 0.769\n",
      "train loss on epoch 3169 : 0.206\n",
      "train accuracy on epoch 3169: 0.889\n",
      "test loss on epoch 3169: 0.359\n",
      "test accuracy on epoch 3169: 0.692\n",
      "train loss on epoch 3170 : 0.265\n",
      "train accuracy on epoch 3170: 0.778\n",
      "test loss on epoch 3170: 0.358\n",
      "test accuracy on epoch 3170: 0.769\n",
      "train loss on epoch 3171 : 0.195\n",
      "train accuracy on epoch 3171: 0.944\n",
      "test loss on epoch 3171: 0.356\n",
      "test accuracy on epoch 3171: 0.769\n",
      "train loss on epoch 3172 : 0.136\n",
      "train accuracy on epoch 3172: 0.944\n",
      "test loss on epoch 3172: 0.356\n",
      "test accuracy on epoch 3172: 0.769\n",
      "train loss on epoch 3173 : 0.135\n",
      "train accuracy on epoch 3173: 0.944\n",
      "test loss on epoch 3173: 0.370\n",
      "test accuracy on epoch 3173: 0.769\n",
      "train loss on epoch 3174 : 0.112\n",
      "train accuracy on epoch 3174: 0.944\n",
      "test loss on epoch 3174: 0.374\n",
      "test accuracy on epoch 3174: 0.769\n",
      "train loss on epoch 3175 : 0.088\n",
      "train accuracy on epoch 3175: 1.000\n",
      "test loss on epoch 3175: 0.379\n",
      "test accuracy on epoch 3175: 0.769\n",
      "train loss on epoch 3176 : 0.236\n",
      "train accuracy on epoch 3176: 0.889\n",
      "test loss on epoch 3176: 0.373\n",
      "test accuracy on epoch 3176: 0.769\n",
      "train loss on epoch 3177 : 0.037\n",
      "train accuracy on epoch 3177: 1.000\n",
      "test loss on epoch 3177: 0.373\n",
      "test accuracy on epoch 3177: 0.769\n",
      "train loss on epoch 3178 : 0.071\n",
      "train accuracy on epoch 3178: 1.000\n",
      "test loss on epoch 3178: 0.375\n",
      "test accuracy on epoch 3178: 0.769\n",
      "train loss on epoch 3179 : 0.066\n",
      "train accuracy on epoch 3179: 1.000\n",
      "test loss on epoch 3179: 0.368\n",
      "test accuracy on epoch 3179: 0.769\n",
      "train loss on epoch 3180 : 0.112\n",
      "train accuracy on epoch 3180: 0.944\n",
      "test loss on epoch 3180: 0.367\n",
      "test accuracy on epoch 3180: 0.769\n",
      "train loss on epoch 3181 : 0.128\n",
      "train accuracy on epoch 3181: 0.944\n",
      "test loss on epoch 3181: 0.367\n",
      "test accuracy on epoch 3181: 0.769\n",
      "train loss on epoch 3182 : 0.280\n",
      "train accuracy on epoch 3182: 0.889\n",
      "test loss on epoch 3182: 0.363\n",
      "test accuracy on epoch 3182: 0.769\n",
      "train loss on epoch 3183 : 0.402\n",
      "train accuracy on epoch 3183: 0.889\n",
      "test loss on epoch 3183: 0.367\n",
      "test accuracy on epoch 3183: 0.769\n",
      "train loss on epoch 3184 : 0.116\n",
      "train accuracy on epoch 3184: 0.944\n",
      "test loss on epoch 3184: 0.366\n",
      "test accuracy on epoch 3184: 0.769\n",
      "train loss on epoch 3185 : 0.165\n",
      "train accuracy on epoch 3185: 0.889\n",
      "test loss on epoch 3185: 0.358\n",
      "test accuracy on epoch 3185: 0.769\n",
      "train loss on epoch 3186 : 0.143\n",
      "train accuracy on epoch 3186: 0.944\n",
      "test loss on epoch 3186: 0.364\n",
      "test accuracy on epoch 3186: 0.769\n",
      "train loss on epoch 3187 : 0.337\n",
      "train accuracy on epoch 3187: 0.889\n",
      "test loss on epoch 3187: 0.383\n",
      "test accuracy on epoch 3187: 0.769\n",
      "train loss on epoch 3188 : 0.081\n",
      "train accuracy on epoch 3188: 0.944\n",
      "test loss on epoch 3188: 0.391\n",
      "test accuracy on epoch 3188: 0.769\n",
      "train loss on epoch 3189 : 0.106\n",
      "train accuracy on epoch 3189: 0.944\n",
      "test loss on epoch 3189: 0.400\n",
      "test accuracy on epoch 3189: 0.769\n",
      "train loss on epoch 3190 : 0.208\n",
      "train accuracy on epoch 3190: 0.889\n",
      "test loss on epoch 3190: 0.394\n",
      "test accuracy on epoch 3190: 0.769\n",
      "train loss on epoch 3191 : 0.200\n",
      "train accuracy on epoch 3191: 0.944\n",
      "test loss on epoch 3191: 0.383\n",
      "test accuracy on epoch 3191: 0.769\n",
      "train loss on epoch 3192 : 0.287\n",
      "train accuracy on epoch 3192: 0.778\n",
      "test loss on epoch 3192: 0.376\n",
      "test accuracy on epoch 3192: 0.769\n",
      "train loss on epoch 3193 : 0.196\n",
      "train accuracy on epoch 3193: 0.889\n",
      "test loss on epoch 3193: 0.366\n",
      "test accuracy on epoch 3193: 0.769\n",
      "train loss on epoch 3194 : 0.138\n",
      "train accuracy on epoch 3194: 0.944\n",
      "test loss on epoch 3194: 0.359\n",
      "test accuracy on epoch 3194: 0.769\n",
      "train loss on epoch 3195 : 0.157\n",
      "train accuracy on epoch 3195: 0.944\n",
      "test loss on epoch 3195: 0.356\n",
      "test accuracy on epoch 3195: 0.769\n",
      "train loss on epoch 3196 : 0.124\n",
      "train accuracy on epoch 3196: 0.944\n",
      "test loss on epoch 3196: 0.355\n",
      "test accuracy on epoch 3196: 0.769\n",
      "train loss on epoch 3197 : 0.207\n",
      "train accuracy on epoch 3197: 0.889\n",
      "test loss on epoch 3197: 0.355\n",
      "test accuracy on epoch 3197: 0.769\n",
      "train loss on epoch 3198 : 0.329\n",
      "train accuracy on epoch 3198: 0.889\n",
      "test loss on epoch 3198: 0.364\n",
      "test accuracy on epoch 3198: 0.769\n",
      "train loss on epoch 3199 : 0.273\n",
      "train accuracy on epoch 3199: 0.889\n",
      "test loss on epoch 3199: 0.363\n",
      "test accuracy on epoch 3199: 0.769\n",
      "train loss on epoch 3200 : 0.142\n",
      "train accuracy on epoch 3200: 0.944\n",
      "test loss on epoch 3200: 0.358\n",
      "test accuracy on epoch 3200: 0.769\n",
      "train loss on epoch 3201 : 0.132\n",
      "train accuracy on epoch 3201: 0.944\n",
      "test loss on epoch 3201: 0.357\n",
      "test accuracy on epoch 3201: 0.769\n",
      "train loss on epoch 3202 : 0.350\n",
      "train accuracy on epoch 3202: 0.889\n",
      "test loss on epoch 3202: 0.353\n",
      "test accuracy on epoch 3202: 0.769\n",
      "train loss on epoch 3203 : 0.133\n",
      "train accuracy on epoch 3203: 0.889\n",
      "test loss on epoch 3203: 0.351\n",
      "test accuracy on epoch 3203: 0.769\n",
      "train loss on epoch 3204 : 0.023\n",
      "train accuracy on epoch 3204: 1.000\n",
      "test loss on epoch 3204: 0.346\n",
      "test accuracy on epoch 3204: 0.769\n",
      "train loss on epoch 3205 : 0.173\n",
      "train accuracy on epoch 3205: 0.889\n",
      "test loss on epoch 3205: 0.346\n",
      "test accuracy on epoch 3205: 0.769\n",
      "train loss on epoch 3206 : 0.511\n",
      "train accuracy on epoch 3206: 0.889\n",
      "test loss on epoch 3206: 0.353\n",
      "test accuracy on epoch 3206: 0.769\n",
      "train loss on epoch 3207 : 0.267\n",
      "train accuracy on epoch 3207: 0.833\n",
      "test loss on epoch 3207: 0.353\n",
      "test accuracy on epoch 3207: 0.769\n",
      "train loss on epoch 3208 : 0.195\n",
      "train accuracy on epoch 3208: 0.889\n",
      "test loss on epoch 3208: 0.351\n",
      "test accuracy on epoch 3208: 0.769\n",
      "train loss on epoch 3209 : 0.056\n",
      "train accuracy on epoch 3209: 1.000\n",
      "test loss on epoch 3209: 0.350\n",
      "test accuracy on epoch 3209: 0.769\n",
      "train loss on epoch 3210 : 0.201\n",
      "train accuracy on epoch 3210: 0.889\n",
      "test loss on epoch 3210: 0.354\n",
      "test accuracy on epoch 3210: 0.769\n",
      "train loss on epoch 3211 : 0.107\n",
      "train accuracy on epoch 3211: 0.944\n",
      "test loss on epoch 3211: 0.360\n",
      "test accuracy on epoch 3211: 0.769\n",
      "train loss on epoch 3212 : 0.214\n",
      "train accuracy on epoch 3212: 0.889\n",
      "test loss on epoch 3212: 0.360\n",
      "test accuracy on epoch 3212: 0.769\n",
      "train loss on epoch 3213 : 0.185\n",
      "train accuracy on epoch 3213: 0.944\n",
      "test loss on epoch 3213: 0.356\n",
      "test accuracy on epoch 3213: 0.769\n",
      "train loss on epoch 3214 : 0.163\n",
      "train accuracy on epoch 3214: 0.889\n",
      "test loss on epoch 3214: 0.354\n",
      "test accuracy on epoch 3214: 0.769\n",
      "train loss on epoch 3215 : 0.281\n",
      "train accuracy on epoch 3215: 0.889\n",
      "test loss on epoch 3215: 0.355\n",
      "test accuracy on epoch 3215: 0.769\n",
      "train loss on epoch 3216 : 0.151\n",
      "train accuracy on epoch 3216: 0.889\n",
      "test loss on epoch 3216: 0.358\n",
      "test accuracy on epoch 3216: 0.769\n",
      "train loss on epoch 3217 : 0.097\n",
      "train accuracy on epoch 3217: 1.000\n",
      "test loss on epoch 3217: 0.358\n",
      "test accuracy on epoch 3217: 0.769\n",
      "train loss on epoch 3218 : 0.235\n",
      "train accuracy on epoch 3218: 0.944\n",
      "test loss on epoch 3218: 0.358\n",
      "test accuracy on epoch 3218: 0.769\n",
      "train loss on epoch 3219 : 0.205\n",
      "train accuracy on epoch 3219: 0.889\n",
      "test loss on epoch 3219: 0.363\n",
      "test accuracy on epoch 3219: 0.769\n",
      "train loss on epoch 3220 : 0.119\n",
      "train accuracy on epoch 3220: 0.944\n",
      "test loss on epoch 3220: 0.364\n",
      "test accuracy on epoch 3220: 0.769\n",
      "train loss on epoch 3221 : 0.165\n",
      "train accuracy on epoch 3221: 0.944\n",
      "test loss on epoch 3221: 0.367\n",
      "test accuracy on epoch 3221: 0.769\n",
      "train loss on epoch 3222 : 0.080\n",
      "train accuracy on epoch 3222: 1.000\n",
      "test loss on epoch 3222: 0.368\n",
      "test accuracy on epoch 3222: 0.769\n",
      "train loss on epoch 3223 : 0.211\n",
      "train accuracy on epoch 3223: 0.889\n",
      "test loss on epoch 3223: 0.361\n",
      "test accuracy on epoch 3223: 0.769\n",
      "train loss on epoch 3224 : 0.215\n",
      "train accuracy on epoch 3224: 0.889\n",
      "test loss on epoch 3224: 0.357\n",
      "test accuracy on epoch 3224: 0.769\n",
      "train loss on epoch 3225 : 0.111\n",
      "train accuracy on epoch 3225: 0.944\n",
      "test loss on epoch 3225: 0.352\n",
      "test accuracy on epoch 3225: 0.769\n",
      "train loss on epoch 3226 : 0.211\n",
      "train accuracy on epoch 3226: 0.889\n",
      "test loss on epoch 3226: 0.357\n",
      "test accuracy on epoch 3226: 0.769\n",
      "train loss on epoch 3227 : 0.157\n",
      "train accuracy on epoch 3227: 0.944\n",
      "test loss on epoch 3227: 0.364\n",
      "test accuracy on epoch 3227: 0.769\n",
      "train loss on epoch 3228 : 0.167\n",
      "train accuracy on epoch 3228: 0.889\n",
      "test loss on epoch 3228: 0.365\n",
      "test accuracy on epoch 3228: 0.769\n",
      "train loss on epoch 3229 : 0.493\n",
      "train accuracy on epoch 3229: 0.833\n",
      "test loss on epoch 3229: 0.365\n",
      "test accuracy on epoch 3229: 0.769\n",
      "train loss on epoch 3230 : 0.137\n",
      "train accuracy on epoch 3230: 0.889\n",
      "test loss on epoch 3230: 0.362\n",
      "test accuracy on epoch 3230: 0.769\n",
      "train loss on epoch 3231 : 0.211\n",
      "train accuracy on epoch 3231: 0.833\n",
      "test loss on epoch 3231: 0.362\n",
      "test accuracy on epoch 3231: 0.769\n",
      "train loss on epoch 3232 : 0.045\n",
      "train accuracy on epoch 3232: 1.000\n",
      "test loss on epoch 3232: 0.355\n",
      "test accuracy on epoch 3232: 0.769\n",
      "train loss on epoch 3233 : 0.102\n",
      "train accuracy on epoch 3233: 0.944\n",
      "test loss on epoch 3233: 0.356\n",
      "test accuracy on epoch 3233: 0.769\n",
      "train loss on epoch 3234 : 0.110\n",
      "train accuracy on epoch 3234: 0.944\n",
      "test loss on epoch 3234: 0.355\n",
      "test accuracy on epoch 3234: 0.769\n",
      "train loss on epoch 3235 : 0.222\n",
      "train accuracy on epoch 3235: 0.944\n",
      "test loss on epoch 3235: 0.357\n",
      "test accuracy on epoch 3235: 0.769\n",
      "train loss on epoch 3236 : 0.070\n",
      "train accuracy on epoch 3236: 1.000\n",
      "test loss on epoch 3236: 0.353\n",
      "test accuracy on epoch 3236: 0.769\n",
      "train loss on epoch 3237 : 0.326\n",
      "train accuracy on epoch 3237: 0.833\n",
      "test loss on epoch 3237: 0.359\n",
      "test accuracy on epoch 3237: 0.769\n",
      "train loss on epoch 3238 : 0.114\n",
      "train accuracy on epoch 3238: 0.944\n",
      "test loss on epoch 3238: 0.365\n",
      "test accuracy on epoch 3238: 0.769\n",
      "train loss on epoch 3239 : 0.387\n",
      "train accuracy on epoch 3239: 0.944\n",
      "test loss on epoch 3239: 0.357\n",
      "test accuracy on epoch 3239: 0.769\n",
      "train loss on epoch 3240 : 0.302\n",
      "train accuracy on epoch 3240: 0.889\n",
      "test loss on epoch 3240: 0.361\n",
      "test accuracy on epoch 3240: 0.769\n",
      "train loss on epoch 3241 : 0.125\n",
      "train accuracy on epoch 3241: 0.944\n",
      "test loss on epoch 3241: 0.358\n",
      "test accuracy on epoch 3241: 0.769\n",
      "train loss on epoch 3242 : 0.101\n",
      "train accuracy on epoch 3242: 1.000\n",
      "test loss on epoch 3242: 0.360\n",
      "test accuracy on epoch 3242: 0.769\n",
      "train loss on epoch 3243 : 0.157\n",
      "train accuracy on epoch 3243: 0.889\n",
      "test loss on epoch 3243: 0.361\n",
      "test accuracy on epoch 3243: 0.769\n",
      "train loss on epoch 3244 : 0.290\n",
      "train accuracy on epoch 3244: 0.944\n",
      "test loss on epoch 3244: 0.365\n",
      "test accuracy on epoch 3244: 0.769\n",
      "train loss on epoch 3245 : 0.158\n",
      "train accuracy on epoch 3245: 0.944\n",
      "test loss on epoch 3245: 0.364\n",
      "test accuracy on epoch 3245: 0.769\n",
      "train loss on epoch 3246 : 0.098\n",
      "train accuracy on epoch 3246: 0.944\n",
      "test loss on epoch 3246: 0.360\n",
      "test accuracy on epoch 3246: 0.769\n",
      "train loss on epoch 3247 : 0.039\n",
      "train accuracy on epoch 3247: 1.000\n",
      "test loss on epoch 3247: 0.362\n",
      "test accuracy on epoch 3247: 0.769\n",
      "train loss on epoch 3248 : 0.148\n",
      "train accuracy on epoch 3248: 0.944\n",
      "test loss on epoch 3248: 0.356\n",
      "test accuracy on epoch 3248: 0.769\n",
      "train loss on epoch 3249 : 0.052\n",
      "train accuracy on epoch 3249: 1.000\n",
      "test loss on epoch 3249: 0.354\n",
      "test accuracy on epoch 3249: 0.769\n",
      "train loss on epoch 3250 : 0.175\n",
      "train accuracy on epoch 3250: 0.944\n",
      "test loss on epoch 3250: 0.350\n",
      "test accuracy on epoch 3250: 0.769\n",
      "train loss on epoch 3251 : 0.258\n",
      "train accuracy on epoch 3251: 0.944\n",
      "test loss on epoch 3251: 0.346\n",
      "test accuracy on epoch 3251: 0.769\n",
      "train loss on epoch 3252 : 0.210\n",
      "train accuracy on epoch 3252: 0.889\n",
      "test loss on epoch 3252: 0.344\n",
      "test accuracy on epoch 3252: 0.769\n",
      "train loss on epoch 3253 : 0.167\n",
      "train accuracy on epoch 3253: 0.944\n",
      "test loss on epoch 3253: 0.352\n",
      "test accuracy on epoch 3253: 0.769\n",
      "train loss on epoch 3254 : 0.196\n",
      "train accuracy on epoch 3254: 0.944\n",
      "test loss on epoch 3254: 0.345\n",
      "test accuracy on epoch 3254: 0.769\n",
      "train loss on epoch 3255 : 0.166\n",
      "train accuracy on epoch 3255: 0.944\n",
      "test loss on epoch 3255: 0.350\n",
      "test accuracy on epoch 3255: 0.769\n",
      "train loss on epoch 3256 : 0.203\n",
      "train accuracy on epoch 3256: 0.889\n",
      "test loss on epoch 3256: 0.352\n",
      "test accuracy on epoch 3256: 0.769\n",
      "train loss on epoch 3257 : 0.108\n",
      "train accuracy on epoch 3257: 0.944\n",
      "test loss on epoch 3257: 0.352\n",
      "test accuracy on epoch 3257: 0.769\n",
      "train loss on epoch 3258 : 0.137\n",
      "train accuracy on epoch 3258: 0.944\n",
      "test loss on epoch 3258: 0.355\n",
      "test accuracy on epoch 3258: 0.769\n",
      "train loss on epoch 3259 : 0.198\n",
      "train accuracy on epoch 3259: 0.944\n",
      "test loss on epoch 3259: 0.359\n",
      "test accuracy on epoch 3259: 0.769\n",
      "train loss on epoch 3260 : 0.070\n",
      "train accuracy on epoch 3260: 1.000\n",
      "test loss on epoch 3260: 0.362\n",
      "test accuracy on epoch 3260: 0.769\n",
      "train loss on epoch 3261 : 0.215\n",
      "train accuracy on epoch 3261: 0.889\n",
      "test loss on epoch 3261: 0.362\n",
      "test accuracy on epoch 3261: 0.769\n",
      "train loss on epoch 3262 : 0.330\n",
      "train accuracy on epoch 3262: 0.944\n",
      "test loss on epoch 3262: 0.354\n",
      "test accuracy on epoch 3262: 0.769\n",
      "train loss on epoch 3263 : 0.068\n",
      "train accuracy on epoch 3263: 1.000\n",
      "test loss on epoch 3263: 0.347\n",
      "test accuracy on epoch 3263: 0.769\n",
      "train loss on epoch 3264 : 0.163\n",
      "train accuracy on epoch 3264: 0.889\n",
      "test loss on epoch 3264: 0.354\n",
      "test accuracy on epoch 3264: 0.769\n",
      "train loss on epoch 3265 : 0.166\n",
      "train accuracy on epoch 3265: 0.944\n",
      "test loss on epoch 3265: 0.356\n",
      "test accuracy on epoch 3265: 0.769\n",
      "train loss on epoch 3266 : 0.071\n",
      "train accuracy on epoch 3266: 1.000\n",
      "test loss on epoch 3266: 0.359\n",
      "test accuracy on epoch 3266: 0.769\n",
      "train loss on epoch 3267 : 0.162\n",
      "train accuracy on epoch 3267: 0.944\n",
      "test loss on epoch 3267: 0.351\n",
      "test accuracy on epoch 3267: 0.769\n",
      "train loss on epoch 3268 : 0.189\n",
      "train accuracy on epoch 3268: 0.944\n",
      "test loss on epoch 3268: 0.347\n",
      "test accuracy on epoch 3268: 0.769\n",
      "train loss on epoch 3269 : 0.056\n",
      "train accuracy on epoch 3269: 1.000\n",
      "test loss on epoch 3269: 0.344\n",
      "test accuracy on epoch 3269: 0.769\n",
      "train loss on epoch 3270 : 0.260\n",
      "train accuracy on epoch 3270: 0.889\n",
      "test loss on epoch 3270: 0.353\n",
      "test accuracy on epoch 3270: 0.692\n",
      "train loss on epoch 3271 : 0.291\n",
      "train accuracy on epoch 3271: 0.889\n",
      "test loss on epoch 3271: 0.340\n",
      "test accuracy on epoch 3271: 0.846\n",
      "train loss on epoch 3272 : 0.027\n",
      "train accuracy on epoch 3272: 1.000\n",
      "test loss on epoch 3272: 0.345\n",
      "test accuracy on epoch 3272: 0.769\n",
      "train loss on epoch 3273 : 0.145\n",
      "train accuracy on epoch 3273: 0.889\n",
      "test loss on epoch 3273: 0.347\n",
      "test accuracy on epoch 3273: 0.769\n",
      "train loss on epoch 3274 : 0.264\n",
      "train accuracy on epoch 3274: 0.889\n",
      "test loss on epoch 3274: 0.348\n",
      "test accuracy on epoch 3274: 0.769\n",
      "train loss on epoch 3275 : 0.245\n",
      "train accuracy on epoch 3275: 0.889\n",
      "test loss on epoch 3275: 0.353\n",
      "test accuracy on epoch 3275: 0.769\n",
      "train loss on epoch 3276 : 0.248\n",
      "train accuracy on epoch 3276: 0.944\n",
      "test loss on epoch 3276: 0.351\n",
      "test accuracy on epoch 3276: 0.769\n",
      "train loss on epoch 3277 : 0.325\n",
      "train accuracy on epoch 3277: 0.889\n",
      "test loss on epoch 3277: 0.354\n",
      "test accuracy on epoch 3277: 0.769\n",
      "train loss on epoch 3278 : 0.232\n",
      "train accuracy on epoch 3278: 0.944\n",
      "test loss on epoch 3278: 0.352\n",
      "test accuracy on epoch 3278: 0.769\n",
      "train loss on epoch 3279 : 0.221\n",
      "train accuracy on epoch 3279: 0.833\n",
      "test loss on epoch 3279: 0.359\n",
      "test accuracy on epoch 3279: 0.769\n",
      "train loss on epoch 3280 : 0.141\n",
      "train accuracy on epoch 3280: 1.000\n",
      "test loss on epoch 3280: 0.354\n",
      "test accuracy on epoch 3280: 0.769\n",
      "train loss on epoch 3281 : 0.105\n",
      "train accuracy on epoch 3281: 0.944\n",
      "test loss on epoch 3281: 0.354\n",
      "test accuracy on epoch 3281: 0.769\n",
      "train loss on epoch 3282 : 0.093\n",
      "train accuracy on epoch 3282: 0.944\n",
      "test loss on epoch 3282: 0.352\n",
      "test accuracy on epoch 3282: 0.769\n",
      "train loss on epoch 3283 : 0.032\n",
      "train accuracy on epoch 3283: 1.000\n",
      "test loss on epoch 3283: 0.355\n",
      "test accuracy on epoch 3283: 0.769\n",
      "train loss on epoch 3284 : 0.352\n",
      "train accuracy on epoch 3284: 0.889\n",
      "test loss on epoch 3284: 0.357\n",
      "test accuracy on epoch 3284: 0.769\n",
      "train loss on epoch 3285 : 0.185\n",
      "train accuracy on epoch 3285: 0.889\n",
      "test loss on epoch 3285: 0.347\n",
      "test accuracy on epoch 3285: 0.769\n",
      "train loss on epoch 3286 : 0.306\n",
      "train accuracy on epoch 3286: 0.889\n",
      "test loss on epoch 3286: 0.359\n",
      "test accuracy on epoch 3286: 0.769\n",
      "train loss on epoch 3287 : 0.067\n",
      "train accuracy on epoch 3287: 1.000\n",
      "test loss on epoch 3287: 0.356\n",
      "test accuracy on epoch 3287: 0.769\n",
      "train loss on epoch 3288 : 0.141\n",
      "train accuracy on epoch 3288: 0.889\n",
      "test loss on epoch 3288: 0.354\n",
      "test accuracy on epoch 3288: 0.769\n",
      "train loss on epoch 3289 : 0.119\n",
      "train accuracy on epoch 3289: 0.944\n",
      "test loss on epoch 3289: 0.357\n",
      "test accuracy on epoch 3289: 0.769\n",
      "train loss on epoch 3290 : 0.085\n",
      "train accuracy on epoch 3290: 1.000\n",
      "test loss on epoch 3290: 0.357\n",
      "test accuracy on epoch 3290: 0.769\n",
      "train loss on epoch 3291 : 0.106\n",
      "train accuracy on epoch 3291: 0.944\n",
      "test loss on epoch 3291: 0.358\n",
      "test accuracy on epoch 3291: 0.769\n",
      "train loss on epoch 3292 : 0.137\n",
      "train accuracy on epoch 3292: 0.889\n",
      "test loss on epoch 3292: 0.356\n",
      "test accuracy on epoch 3292: 0.769\n",
      "train loss on epoch 3293 : 0.308\n",
      "train accuracy on epoch 3293: 0.889\n",
      "test loss on epoch 3293: 0.363\n",
      "test accuracy on epoch 3293: 0.769\n",
      "train loss on epoch 3294 : 0.310\n",
      "train accuracy on epoch 3294: 0.889\n",
      "test loss on epoch 3294: 0.362\n",
      "test accuracy on epoch 3294: 0.769\n",
      "train loss on epoch 3295 : 0.306\n",
      "train accuracy on epoch 3295: 0.889\n",
      "test loss on epoch 3295: 0.359\n",
      "test accuracy on epoch 3295: 0.769\n",
      "train loss on epoch 3296 : 0.338\n",
      "train accuracy on epoch 3296: 0.833\n",
      "test loss on epoch 3296: 0.345\n",
      "test accuracy on epoch 3296: 0.769\n",
      "train loss on epoch 3297 : 0.204\n",
      "train accuracy on epoch 3297: 0.944\n",
      "test loss on epoch 3297: 0.350\n",
      "test accuracy on epoch 3297: 0.769\n",
      "train loss on epoch 3298 : 0.301\n",
      "train accuracy on epoch 3298: 0.889\n",
      "test loss on epoch 3298: 0.336\n",
      "test accuracy on epoch 3298: 0.846\n",
      "train loss on epoch 3299 : 0.074\n",
      "train accuracy on epoch 3299: 1.000\n",
      "test loss on epoch 3299: 0.352\n",
      "test accuracy on epoch 3299: 0.769\n",
      "train loss on epoch 3300 : 0.168\n",
      "train accuracy on epoch 3300: 0.944\n",
      "test loss on epoch 3300: 0.348\n",
      "test accuracy on epoch 3300: 0.769\n",
      "train loss on epoch 3301 : 0.419\n",
      "train accuracy on epoch 3301: 0.833\n",
      "test loss on epoch 3301: 0.341\n",
      "test accuracy on epoch 3301: 0.846\n",
      "train loss on epoch 3302 : 0.041\n",
      "train accuracy on epoch 3302: 1.000\n",
      "test loss on epoch 3302: 0.348\n",
      "test accuracy on epoch 3302: 0.769\n",
      "train loss on epoch 3303 : 0.361\n",
      "train accuracy on epoch 3303: 0.833\n",
      "test loss on epoch 3303: 0.345\n",
      "test accuracy on epoch 3303: 0.769\n",
      "train loss on epoch 3304 : 0.032\n",
      "train accuracy on epoch 3304: 1.000\n",
      "test loss on epoch 3304: 0.350\n",
      "test accuracy on epoch 3304: 0.769\n",
      "train loss on epoch 3305 : 0.203\n",
      "train accuracy on epoch 3305: 0.889\n",
      "test loss on epoch 3305: 0.349\n",
      "test accuracy on epoch 3305: 0.769\n",
      "train loss on epoch 3306 : 0.270\n",
      "train accuracy on epoch 3306: 0.833\n",
      "test loss on epoch 3306: 0.341\n",
      "test accuracy on epoch 3306: 0.769\n",
      "train loss on epoch 3307 : 0.056\n",
      "train accuracy on epoch 3307: 0.944\n",
      "test loss on epoch 3307: 0.339\n",
      "test accuracy on epoch 3307: 0.769\n",
      "train loss on epoch 3308 : 0.192\n",
      "train accuracy on epoch 3308: 0.889\n",
      "test loss on epoch 3308: 0.349\n",
      "test accuracy on epoch 3308: 0.769\n",
      "train loss on epoch 3309 : 0.181\n",
      "train accuracy on epoch 3309: 0.889\n",
      "test loss on epoch 3309: 0.345\n",
      "test accuracy on epoch 3309: 0.769\n",
      "train loss on epoch 3310 : 0.130\n",
      "train accuracy on epoch 3310: 1.000\n",
      "test loss on epoch 3310: 0.358\n",
      "test accuracy on epoch 3310: 0.769\n",
      "train loss on epoch 3311 : 0.045\n",
      "train accuracy on epoch 3311: 1.000\n",
      "test loss on epoch 3311: 0.359\n",
      "test accuracy on epoch 3311: 0.769\n",
      "train loss on epoch 3312 : 0.143\n",
      "train accuracy on epoch 3312: 0.944\n",
      "test loss on epoch 3312: 0.359\n",
      "test accuracy on epoch 3312: 0.769\n",
      "train loss on epoch 3313 : 0.117\n",
      "train accuracy on epoch 3313: 0.944\n",
      "test loss on epoch 3313: 0.358\n",
      "test accuracy on epoch 3313: 0.769\n",
      "train loss on epoch 3314 : 0.147\n",
      "train accuracy on epoch 3314: 0.889\n",
      "test loss on epoch 3314: 0.340\n",
      "test accuracy on epoch 3314: 0.769\n",
      "train loss on epoch 3315 : 0.221\n",
      "train accuracy on epoch 3315: 0.889\n",
      "test loss on epoch 3315: 0.355\n",
      "test accuracy on epoch 3315: 0.769\n",
      "train loss on epoch 3316 : 0.214\n",
      "train accuracy on epoch 3316: 0.889\n",
      "test loss on epoch 3316: 0.351\n",
      "test accuracy on epoch 3316: 0.692\n",
      "train loss on epoch 3317 : 0.236\n",
      "train accuracy on epoch 3317: 0.889\n",
      "test loss on epoch 3317: 0.350\n",
      "test accuracy on epoch 3317: 0.692\n",
      "train loss on epoch 3318 : 0.105\n",
      "train accuracy on epoch 3318: 0.944\n",
      "test loss on epoch 3318: 0.336\n",
      "test accuracy on epoch 3318: 0.769\n",
      "train loss on epoch 3319 : 0.122\n",
      "train accuracy on epoch 3319: 0.944\n",
      "test loss on epoch 3319: 0.332\n",
      "test accuracy on epoch 3319: 0.846\n",
      "train loss on epoch 3320 : 0.129\n",
      "train accuracy on epoch 3320: 1.000\n",
      "test loss on epoch 3320: 0.344\n",
      "test accuracy on epoch 3320: 0.769\n",
      "train loss on epoch 3321 : 0.172\n",
      "train accuracy on epoch 3321: 0.944\n",
      "test loss on epoch 3321: 0.349\n",
      "test accuracy on epoch 3321: 0.692\n",
      "train loss on epoch 3322 : 0.184\n",
      "train accuracy on epoch 3322: 0.889\n",
      "test loss on epoch 3322: 0.351\n",
      "test accuracy on epoch 3322: 0.692\n",
      "train loss on epoch 3323 : 0.301\n",
      "train accuracy on epoch 3323: 0.833\n",
      "test loss on epoch 3323: 0.332\n",
      "test accuracy on epoch 3323: 0.769\n",
      "train loss on epoch 3324 : 0.336\n",
      "train accuracy on epoch 3324: 0.889\n",
      "test loss on epoch 3324: 0.333\n",
      "test accuracy on epoch 3324: 0.769\n",
      "train loss on epoch 3325 : 0.147\n",
      "train accuracy on epoch 3325: 0.944\n",
      "test loss on epoch 3325: 0.357\n",
      "test accuracy on epoch 3325: 0.769\n",
      "train loss on epoch 3326 : 0.156\n",
      "train accuracy on epoch 3326: 0.944\n",
      "test loss on epoch 3326: 0.354\n",
      "test accuracy on epoch 3326: 0.769\n",
      "train loss on epoch 3327 : 0.068\n",
      "train accuracy on epoch 3327: 1.000\n",
      "test loss on epoch 3327: 0.330\n",
      "test accuracy on epoch 3327: 0.769\n",
      "train loss on epoch 3328 : 0.411\n",
      "train accuracy on epoch 3328: 0.778\n",
      "test loss on epoch 3328: 0.329\n",
      "test accuracy on epoch 3328: 0.846\n",
      "train loss on epoch 3329 : 0.273\n",
      "train accuracy on epoch 3329: 0.944\n",
      "test loss on epoch 3329: 0.353\n",
      "test accuracy on epoch 3329: 0.692\n",
      "train loss on epoch 3330 : 0.132\n",
      "train accuracy on epoch 3330: 0.944\n",
      "test loss on epoch 3330: 0.330\n",
      "test accuracy on epoch 3330: 0.846\n",
      "train loss on epoch 3331 : 0.258\n",
      "train accuracy on epoch 3331: 0.944\n",
      "test loss on epoch 3331: 0.329\n",
      "test accuracy on epoch 3331: 0.769\n",
      "train loss on epoch 3332 : 0.443\n",
      "train accuracy on epoch 3332: 0.833\n",
      "test loss on epoch 3332: 0.350\n",
      "test accuracy on epoch 3332: 0.692\n",
      "train loss on epoch 3333 : 0.130\n",
      "train accuracy on epoch 3333: 0.944\n",
      "test loss on epoch 3333: 0.351\n",
      "test accuracy on epoch 3333: 0.692\n",
      "train loss on epoch 3334 : 0.089\n",
      "train accuracy on epoch 3334: 0.944\n",
      "test loss on epoch 3334: 0.327\n",
      "test accuracy on epoch 3334: 0.846\n",
      "train loss on epoch 3335 : 0.434\n",
      "train accuracy on epoch 3335: 0.833\n",
      "test loss on epoch 3335: 0.340\n",
      "test accuracy on epoch 3335: 0.769\n",
      "train loss on epoch 3336 : 0.178\n",
      "train accuracy on epoch 3336: 0.944\n",
      "test loss on epoch 3336: 0.340\n",
      "test accuracy on epoch 3336: 0.769\n",
      "train loss on epoch 3337 : 0.135\n",
      "train accuracy on epoch 3337: 0.889\n",
      "test loss on epoch 3337: 0.348\n",
      "test accuracy on epoch 3337: 0.769\n",
      "train loss on epoch 3338 : 0.243\n",
      "train accuracy on epoch 3338: 0.944\n",
      "test loss on epoch 3338: 0.343\n",
      "test accuracy on epoch 3338: 0.769\n",
      "train loss on epoch 3339 : 0.265\n",
      "train accuracy on epoch 3339: 0.889\n",
      "test loss on epoch 3339: 0.355\n",
      "test accuracy on epoch 3339: 0.769\n",
      "train loss on epoch 3340 : 0.179\n",
      "train accuracy on epoch 3340: 0.944\n",
      "test loss on epoch 3340: 0.336\n",
      "test accuracy on epoch 3340: 0.769\n",
      "train loss on epoch 3341 : 0.123\n",
      "train accuracy on epoch 3341: 0.944\n",
      "test loss on epoch 3341: 0.352\n",
      "test accuracy on epoch 3341: 0.769\n",
      "train loss on epoch 3342 : 0.100\n",
      "train accuracy on epoch 3342: 1.000\n",
      "test loss on epoch 3342: 0.343\n",
      "test accuracy on epoch 3342: 0.769\n",
      "train loss on epoch 3343 : 0.124\n",
      "train accuracy on epoch 3343: 0.944\n",
      "test loss on epoch 3343: 0.339\n",
      "test accuracy on epoch 3343: 0.769\n",
      "train loss on epoch 3344 : 0.429\n",
      "train accuracy on epoch 3344: 0.889\n",
      "test loss on epoch 3344: 0.338\n",
      "test accuracy on epoch 3344: 0.769\n",
      "train loss on epoch 3345 : 0.113\n",
      "train accuracy on epoch 3345: 0.944\n",
      "test loss on epoch 3345: 0.337\n",
      "test accuracy on epoch 3345: 0.769\n",
      "train loss on epoch 3346 : 0.142\n",
      "train accuracy on epoch 3346: 1.000\n",
      "test loss on epoch 3346: 0.336\n",
      "test accuracy on epoch 3346: 0.769\n",
      "train loss on epoch 3347 : 0.091\n",
      "train accuracy on epoch 3347: 0.944\n",
      "test loss on epoch 3347: 0.343\n",
      "test accuracy on epoch 3347: 0.769\n",
      "train loss on epoch 3348 : 0.260\n",
      "train accuracy on epoch 3348: 0.944\n",
      "test loss on epoch 3348: 0.326\n",
      "test accuracy on epoch 3348: 0.769\n",
      "train loss on epoch 3349 : 0.112\n",
      "train accuracy on epoch 3349: 0.944\n",
      "test loss on epoch 3349: 0.346\n",
      "test accuracy on epoch 3349: 0.769\n",
      "train loss on epoch 3350 : 0.048\n",
      "train accuracy on epoch 3350: 1.000\n",
      "test loss on epoch 3350: 0.327\n",
      "test accuracy on epoch 3350: 0.769\n",
      "train loss on epoch 3351 : 0.298\n",
      "train accuracy on epoch 3351: 0.944\n",
      "test loss on epoch 3351: 0.331\n",
      "test accuracy on epoch 3351: 0.769\n",
      "train loss on epoch 3352 : 0.186\n",
      "train accuracy on epoch 3352: 0.889\n",
      "test loss on epoch 3352: 0.338\n",
      "test accuracy on epoch 3352: 0.769\n",
      "train loss on epoch 3353 : 0.206\n",
      "train accuracy on epoch 3353: 0.944\n",
      "test loss on epoch 3353: 0.340\n",
      "test accuracy on epoch 3353: 0.769\n",
      "train loss on epoch 3354 : 0.151\n",
      "train accuracy on epoch 3354: 0.944\n",
      "test loss on epoch 3354: 0.337\n",
      "test accuracy on epoch 3354: 0.769\n",
      "train loss on epoch 3355 : 0.052\n",
      "train accuracy on epoch 3355: 1.000\n",
      "test loss on epoch 3355: 0.342\n",
      "test accuracy on epoch 3355: 0.769\n",
      "train loss on epoch 3356 : 0.149\n",
      "train accuracy on epoch 3356: 0.889\n",
      "test loss on epoch 3356: 0.337\n",
      "test accuracy on epoch 3356: 0.769\n",
      "train loss on epoch 3357 : 0.315\n",
      "train accuracy on epoch 3357: 0.722\n",
      "test loss on epoch 3357: 0.350\n",
      "test accuracy on epoch 3357: 0.769\n",
      "train loss on epoch 3358 : 0.180\n",
      "train accuracy on epoch 3358: 0.944\n",
      "test loss on epoch 3358: 0.339\n",
      "test accuracy on epoch 3358: 0.769\n",
      "train loss on epoch 3359 : 0.032\n",
      "train accuracy on epoch 3359: 1.000\n",
      "test loss on epoch 3359: 0.335\n",
      "test accuracy on epoch 3359: 0.769\n",
      "train loss on epoch 3360 : 0.062\n",
      "train accuracy on epoch 3360: 1.000\n",
      "test loss on epoch 3360: 0.349\n",
      "test accuracy on epoch 3360: 0.769\n",
      "train loss on epoch 3361 : 0.094\n",
      "train accuracy on epoch 3361: 0.944\n",
      "test loss on epoch 3361: 0.333\n",
      "test accuracy on epoch 3361: 0.769\n",
      "train loss on epoch 3362 : 0.136\n",
      "train accuracy on epoch 3362: 0.889\n",
      "test loss on epoch 3362: 0.349\n",
      "test accuracy on epoch 3362: 0.769\n",
      "train loss on epoch 3363 : 0.060\n",
      "train accuracy on epoch 3363: 1.000\n",
      "test loss on epoch 3363: 0.343\n",
      "test accuracy on epoch 3363: 0.769\n",
      "train loss on epoch 3364 : 0.330\n",
      "train accuracy on epoch 3364: 0.889\n",
      "test loss on epoch 3364: 0.348\n",
      "test accuracy on epoch 3364: 0.769\n",
      "train loss on epoch 3365 : 0.107\n",
      "train accuracy on epoch 3365: 1.000\n",
      "test loss on epoch 3365: 0.356\n",
      "test accuracy on epoch 3365: 0.769\n",
      "train loss on epoch 3366 : 0.274\n",
      "train accuracy on epoch 3366: 0.889\n",
      "test loss on epoch 3366: 0.346\n",
      "test accuracy on epoch 3366: 0.769\n",
      "train loss on epoch 3367 : 0.324\n",
      "train accuracy on epoch 3367: 0.889\n",
      "test loss on epoch 3367: 0.347\n",
      "test accuracy on epoch 3367: 0.769\n",
      "train loss on epoch 3368 : 0.067\n",
      "train accuracy on epoch 3368: 0.944\n",
      "test loss on epoch 3368: 0.348\n",
      "test accuracy on epoch 3368: 0.769\n",
      "train loss on epoch 3369 : 0.057\n",
      "train accuracy on epoch 3369: 1.000\n",
      "test loss on epoch 3369: 0.348\n",
      "test accuracy on epoch 3369: 0.692\n",
      "train loss on epoch 3370 : 0.125\n",
      "train accuracy on epoch 3370: 0.944\n",
      "test loss on epoch 3370: 0.349\n",
      "test accuracy on epoch 3370: 0.692\n",
      "train loss on epoch 3371 : 0.168\n",
      "train accuracy on epoch 3371: 0.889\n",
      "test loss on epoch 3371: 0.332\n",
      "test accuracy on epoch 3371: 0.846\n",
      "train loss on epoch 3372 : 0.231\n",
      "train accuracy on epoch 3372: 0.889\n",
      "test loss on epoch 3372: 0.351\n",
      "test accuracy on epoch 3372: 0.769\n",
      "train loss on epoch 3373 : 0.137\n",
      "train accuracy on epoch 3373: 0.889\n",
      "test loss on epoch 3373: 0.333\n",
      "test accuracy on epoch 3373: 0.846\n",
      "train loss on epoch 3374 : 0.152\n",
      "train accuracy on epoch 3374: 0.944\n",
      "test loss on epoch 3374: 0.351\n",
      "test accuracy on epoch 3374: 0.769\n",
      "train loss on epoch 3375 : 0.184\n",
      "train accuracy on epoch 3375: 0.944\n",
      "test loss on epoch 3375: 0.353\n",
      "test accuracy on epoch 3375: 0.769\n",
      "train loss on epoch 3376 : 0.129\n",
      "train accuracy on epoch 3376: 0.944\n",
      "test loss on epoch 3376: 0.356\n",
      "test accuracy on epoch 3376: 0.769\n",
      "train loss on epoch 3377 : 0.111\n",
      "train accuracy on epoch 3377: 0.944\n",
      "test loss on epoch 3377: 0.359\n",
      "test accuracy on epoch 3377: 0.769\n",
      "train loss on epoch 3378 : 0.100\n",
      "train accuracy on epoch 3378: 1.000\n",
      "test loss on epoch 3378: 0.354\n",
      "test accuracy on epoch 3378: 0.769\n",
      "train loss on epoch 3379 : 0.125\n",
      "train accuracy on epoch 3379: 0.944\n",
      "test loss on epoch 3379: 0.358\n",
      "test accuracy on epoch 3379: 0.769\n",
      "train loss on epoch 3380 : 0.103\n",
      "train accuracy on epoch 3380: 0.944\n",
      "test loss on epoch 3380: 0.360\n",
      "test accuracy on epoch 3380: 0.769\n",
      "train loss on epoch 3381 : 0.191\n",
      "train accuracy on epoch 3381: 0.944\n",
      "test loss on epoch 3381: 0.339\n",
      "test accuracy on epoch 3381: 0.769\n",
      "train loss on epoch 3382 : 0.162\n",
      "train accuracy on epoch 3382: 0.889\n",
      "test loss on epoch 3382: 0.351\n",
      "test accuracy on epoch 3382: 0.692\n",
      "train loss on epoch 3383 : 0.053\n",
      "train accuracy on epoch 3383: 1.000\n",
      "test loss on epoch 3383: 0.344\n",
      "test accuracy on epoch 3383: 0.769\n",
      "train loss on epoch 3384 : 0.259\n",
      "train accuracy on epoch 3384: 0.944\n",
      "test loss on epoch 3384: 0.330\n",
      "test accuracy on epoch 3384: 0.769\n",
      "train loss on epoch 3385 : 0.373\n",
      "train accuracy on epoch 3385: 0.833\n",
      "test loss on epoch 3385: 0.333\n",
      "test accuracy on epoch 3385: 0.846\n",
      "train loss on epoch 3386 : 0.138\n",
      "train accuracy on epoch 3386: 0.944\n",
      "test loss on epoch 3386: 0.345\n",
      "test accuracy on epoch 3386: 0.769\n",
      "train loss on epoch 3387 : 0.152\n",
      "train accuracy on epoch 3387: 0.944\n",
      "test loss on epoch 3387: 0.356\n",
      "test accuracy on epoch 3387: 0.769\n",
      "train loss on epoch 3388 : 0.242\n",
      "train accuracy on epoch 3388: 0.944\n",
      "test loss on epoch 3388: 0.359\n",
      "test accuracy on epoch 3388: 0.769\n",
      "train loss on epoch 3389 : 0.110\n",
      "train accuracy on epoch 3389: 0.944\n",
      "test loss on epoch 3389: 0.361\n",
      "test accuracy on epoch 3389: 0.769\n",
      "train loss on epoch 3390 : 0.163\n",
      "train accuracy on epoch 3390: 0.944\n",
      "test loss on epoch 3390: 0.369\n",
      "test accuracy on epoch 3390: 0.769\n",
      "train loss on epoch 3391 : 0.071\n",
      "train accuracy on epoch 3391: 1.000\n",
      "test loss on epoch 3391: 0.376\n",
      "test accuracy on epoch 3391: 0.769\n",
      "train loss on epoch 3392 : 0.123\n",
      "train accuracy on epoch 3392: 0.944\n",
      "test loss on epoch 3392: 0.379\n",
      "test accuracy on epoch 3392: 0.769\n",
      "train loss on epoch 3393 : 0.202\n",
      "train accuracy on epoch 3393: 0.889\n",
      "test loss on epoch 3393: 0.371\n",
      "test accuracy on epoch 3393: 0.769\n",
      "train loss on epoch 3394 : 0.219\n",
      "train accuracy on epoch 3394: 0.889\n",
      "test loss on epoch 3394: 0.366\n",
      "test accuracy on epoch 3394: 0.769\n",
      "train loss on epoch 3395 : 0.085\n",
      "train accuracy on epoch 3395: 1.000\n",
      "test loss on epoch 3395: 0.355\n",
      "test accuracy on epoch 3395: 0.769\n",
      "train loss on epoch 3396 : 0.092\n",
      "train accuracy on epoch 3396: 0.944\n",
      "test loss on epoch 3396: 0.335\n",
      "test accuracy on epoch 3396: 0.769\n",
      "train loss on epoch 3397 : 0.088\n",
      "train accuracy on epoch 3397: 1.000\n",
      "test loss on epoch 3397: 0.333\n",
      "test accuracy on epoch 3397: 0.769\n",
      "train loss on epoch 3398 : 0.289\n",
      "train accuracy on epoch 3398: 0.889\n",
      "test loss on epoch 3398: 0.329\n",
      "test accuracy on epoch 3398: 0.846\n",
      "train loss on epoch 3399 : 0.118\n",
      "train accuracy on epoch 3399: 0.944\n",
      "test loss on epoch 3399: 0.331\n",
      "test accuracy on epoch 3399: 0.769\n",
      "train loss on epoch 3400 : 0.128\n",
      "train accuracy on epoch 3400: 0.889\n",
      "test loss on epoch 3400: 0.350\n",
      "test accuracy on epoch 3400: 0.769\n",
      "train loss on epoch 3401 : 0.042\n",
      "train accuracy on epoch 3401: 1.000\n",
      "test loss on epoch 3401: 0.347\n",
      "test accuracy on epoch 3401: 0.769\n",
      "train loss on epoch 3402 : 0.231\n",
      "train accuracy on epoch 3402: 0.889\n",
      "test loss on epoch 3402: 0.326\n",
      "test accuracy on epoch 3402: 0.769\n",
      "train loss on epoch 3403 : 0.058\n",
      "train accuracy on epoch 3403: 1.000\n",
      "test loss on epoch 3403: 0.345\n",
      "test accuracy on epoch 3403: 0.769\n",
      "train loss on epoch 3404 : 0.153\n",
      "train accuracy on epoch 3404: 0.944\n",
      "test loss on epoch 3404: 0.329\n",
      "test accuracy on epoch 3404: 0.846\n",
      "train loss on epoch 3405 : 0.222\n",
      "train accuracy on epoch 3405: 0.944\n",
      "test loss on epoch 3405: 0.333\n",
      "test accuracy on epoch 3405: 0.769\n",
      "train loss on epoch 3406 : 0.235\n",
      "train accuracy on epoch 3406: 0.889\n",
      "test loss on epoch 3406: 0.341\n",
      "test accuracy on epoch 3406: 0.769\n",
      "train loss on epoch 3407 : 0.315\n",
      "train accuracy on epoch 3407: 0.889\n",
      "test loss on epoch 3407: 0.338\n",
      "test accuracy on epoch 3407: 0.769\n",
      "train loss on epoch 3408 : 0.170\n",
      "train accuracy on epoch 3408: 0.944\n",
      "test loss on epoch 3408: 0.348\n",
      "test accuracy on epoch 3408: 0.692\n",
      "train loss on epoch 3409 : 0.161\n",
      "train accuracy on epoch 3409: 0.889\n",
      "test loss on epoch 3409: 0.340\n",
      "test accuracy on epoch 3409: 0.769\n",
      "train loss on epoch 3410 : 0.081\n",
      "train accuracy on epoch 3410: 1.000\n",
      "test loss on epoch 3410: 0.348\n",
      "test accuracy on epoch 3410: 0.769\n",
      "train loss on epoch 3411 : 0.263\n",
      "train accuracy on epoch 3411: 0.944\n",
      "test loss on epoch 3411: 0.327\n",
      "test accuracy on epoch 3411: 0.846\n",
      "train loss on epoch 3412 : 0.086\n",
      "train accuracy on epoch 3412: 0.944\n",
      "test loss on epoch 3412: 0.336\n",
      "test accuracy on epoch 3412: 0.769\n",
      "train loss on epoch 3413 : 0.149\n",
      "train accuracy on epoch 3413: 0.944\n",
      "test loss on epoch 3413: 0.339\n",
      "test accuracy on epoch 3413: 0.769\n",
      "train loss on epoch 3414 : 0.072\n",
      "train accuracy on epoch 3414: 0.944\n",
      "test loss on epoch 3414: 0.340\n",
      "test accuracy on epoch 3414: 0.769\n",
      "train loss on epoch 3415 : 0.177\n",
      "train accuracy on epoch 3415: 0.944\n",
      "test loss on epoch 3415: 0.331\n",
      "test accuracy on epoch 3415: 0.846\n",
      "train loss on epoch 3416 : 0.168\n",
      "train accuracy on epoch 3416: 0.944\n",
      "test loss on epoch 3416: 0.325\n",
      "test accuracy on epoch 3416: 0.769\n",
      "train loss on epoch 3417 : 0.117\n",
      "train accuracy on epoch 3417: 0.944\n",
      "test loss on epoch 3417: 0.336\n",
      "test accuracy on epoch 3417: 0.769\n",
      "train loss on epoch 3418 : 0.094\n",
      "train accuracy on epoch 3418: 1.000\n",
      "test loss on epoch 3418: 0.328\n",
      "test accuracy on epoch 3418: 0.769\n",
      "train loss on epoch 3419 : 0.083\n",
      "train accuracy on epoch 3419: 0.944\n",
      "test loss on epoch 3419: 0.330\n",
      "test accuracy on epoch 3419: 0.769\n",
      "train loss on epoch 3420 : 0.258\n",
      "train accuracy on epoch 3420: 0.889\n",
      "test loss on epoch 3420: 0.328\n",
      "test accuracy on epoch 3420: 0.769\n",
      "train loss on epoch 3421 : 0.341\n",
      "train accuracy on epoch 3421: 0.889\n",
      "test loss on epoch 3421: 0.346\n",
      "test accuracy on epoch 3421: 0.769\n",
      "train loss on epoch 3422 : 0.163\n",
      "train accuracy on epoch 3422: 0.944\n",
      "test loss on epoch 3422: 0.334\n",
      "test accuracy on epoch 3422: 0.769\n",
      "train loss on epoch 3423 : 0.250\n",
      "train accuracy on epoch 3423: 0.833\n",
      "test loss on epoch 3423: 0.352\n",
      "test accuracy on epoch 3423: 0.769\n",
      "train loss on epoch 3424 : 0.310\n",
      "train accuracy on epoch 3424: 0.833\n",
      "test loss on epoch 3424: 0.346\n",
      "test accuracy on epoch 3424: 0.769\n",
      "train loss on epoch 3425 : 0.137\n",
      "train accuracy on epoch 3425: 0.944\n",
      "test loss on epoch 3425: 0.334\n",
      "test accuracy on epoch 3425: 0.769\n",
      "train loss on epoch 3426 : 0.081\n",
      "train accuracy on epoch 3426: 0.944\n",
      "test loss on epoch 3426: 0.332\n",
      "test accuracy on epoch 3426: 0.769\n",
      "train loss on epoch 3427 : 0.119\n",
      "train accuracy on epoch 3427: 0.944\n",
      "test loss on epoch 3427: 0.328\n",
      "test accuracy on epoch 3427: 0.769\n",
      "train loss on epoch 3428 : 0.043\n",
      "train accuracy on epoch 3428: 1.000\n",
      "test loss on epoch 3428: 0.330\n",
      "test accuracy on epoch 3428: 0.769\n",
      "train loss on epoch 3429 : 0.167\n",
      "train accuracy on epoch 3429: 0.944\n",
      "test loss on epoch 3429: 0.349\n",
      "test accuracy on epoch 3429: 0.769\n",
      "train loss on epoch 3430 : 0.124\n",
      "train accuracy on epoch 3430: 0.944\n",
      "test loss on epoch 3430: 0.336\n",
      "test accuracy on epoch 3430: 0.846\n",
      "train loss on epoch 3431 : 0.188\n",
      "train accuracy on epoch 3431: 0.944\n",
      "test loss on epoch 3431: 0.358\n",
      "test accuracy on epoch 3431: 0.769\n",
      "train loss on epoch 3432 : 0.298\n",
      "train accuracy on epoch 3432: 0.889\n",
      "test loss on epoch 3432: 0.352\n",
      "test accuracy on epoch 3432: 0.769\n",
      "train loss on epoch 3433 : 0.319\n",
      "train accuracy on epoch 3433: 0.944\n",
      "test loss on epoch 3433: 0.365\n",
      "test accuracy on epoch 3433: 0.769\n",
      "train loss on epoch 3434 : 0.298\n",
      "train accuracy on epoch 3434: 0.833\n",
      "test loss on epoch 3434: 0.365\n",
      "test accuracy on epoch 3434: 0.769\n",
      "train loss on epoch 3435 : 0.376\n",
      "train accuracy on epoch 3435: 0.833\n",
      "test loss on epoch 3435: 0.359\n",
      "test accuracy on epoch 3435: 0.769\n",
      "train loss on epoch 3436 : 0.117\n",
      "train accuracy on epoch 3436: 0.944\n",
      "test loss on epoch 3436: 0.354\n",
      "test accuracy on epoch 3436: 0.769\n",
      "train loss on epoch 3437 : 0.137\n",
      "train accuracy on epoch 3437: 0.944\n",
      "test loss on epoch 3437: 0.355\n",
      "test accuracy on epoch 3437: 0.769\n",
      "train loss on epoch 3438 : 0.257\n",
      "train accuracy on epoch 3438: 0.833\n",
      "test loss on epoch 3438: 0.350\n",
      "test accuracy on epoch 3438: 0.769\n",
      "train loss on epoch 3439 : 0.064\n",
      "train accuracy on epoch 3439: 1.000\n",
      "test loss on epoch 3439: 0.349\n",
      "test accuracy on epoch 3439: 0.769\n",
      "train loss on epoch 3440 : 0.276\n",
      "train accuracy on epoch 3440: 0.889\n",
      "test loss on epoch 3440: 0.350\n",
      "test accuracy on epoch 3440: 0.769\n",
      "train loss on epoch 3441 : 0.243\n",
      "train accuracy on epoch 3441: 0.944\n",
      "test loss on epoch 3441: 0.350\n",
      "test accuracy on epoch 3441: 0.769\n",
      "train loss on epoch 3442 : 0.128\n",
      "train accuracy on epoch 3442: 0.944\n",
      "test loss on epoch 3442: 0.342\n",
      "test accuracy on epoch 3442: 0.769\n",
      "train loss on epoch 3443 : 0.072\n",
      "train accuracy on epoch 3443: 0.944\n",
      "test loss on epoch 3443: 0.341\n",
      "test accuracy on epoch 3443: 0.769\n",
      "train loss on epoch 3444 : 0.157\n",
      "train accuracy on epoch 3444: 0.944\n",
      "test loss on epoch 3444: 0.347\n",
      "test accuracy on epoch 3444: 0.692\n",
      "train loss on epoch 3445 : 0.255\n",
      "train accuracy on epoch 3445: 0.889\n",
      "test loss on epoch 3445: 0.346\n",
      "test accuracy on epoch 3445: 0.692\n",
      "train loss on epoch 3446 : 0.149\n",
      "train accuracy on epoch 3446: 0.944\n",
      "test loss on epoch 3446: 0.347\n",
      "test accuracy on epoch 3446: 0.692\n",
      "train loss on epoch 3447 : 0.124\n",
      "train accuracy on epoch 3447: 0.889\n",
      "test loss on epoch 3447: 0.344\n",
      "test accuracy on epoch 3447: 0.769\n",
      "train loss on epoch 3448 : 0.190\n",
      "train accuracy on epoch 3448: 0.889\n",
      "test loss on epoch 3448: 0.344\n",
      "test accuracy on epoch 3448: 0.769\n",
      "train loss on epoch 3449 : 0.085\n",
      "train accuracy on epoch 3449: 0.944\n",
      "test loss on epoch 3449: 0.339\n",
      "test accuracy on epoch 3449: 0.769\n",
      "train loss on epoch 3450 : 0.092\n",
      "train accuracy on epoch 3450: 0.944\n",
      "test loss on epoch 3450: 0.344\n",
      "test accuracy on epoch 3450: 0.769\n",
      "train loss on epoch 3451 : 0.261\n",
      "train accuracy on epoch 3451: 0.944\n",
      "test loss on epoch 3451: 0.347\n",
      "test accuracy on epoch 3451: 0.769\n",
      "train loss on epoch 3452 : 0.072\n",
      "train accuracy on epoch 3452: 0.944\n",
      "test loss on epoch 3452: 0.328\n",
      "test accuracy on epoch 3452: 0.769\n",
      "train loss on epoch 3453 : 0.178\n",
      "train accuracy on epoch 3453: 0.944\n",
      "test loss on epoch 3453: 0.329\n",
      "test accuracy on epoch 3453: 0.769\n",
      "train loss on epoch 3454 : 0.104\n",
      "train accuracy on epoch 3454: 0.944\n",
      "test loss on epoch 3454: 0.331\n",
      "test accuracy on epoch 3454: 0.769\n",
      "train loss on epoch 3455 : 0.199\n",
      "train accuracy on epoch 3455: 0.889\n",
      "test loss on epoch 3455: 0.341\n",
      "test accuracy on epoch 3455: 0.692\n",
      "train loss on epoch 3456 : 0.297\n",
      "train accuracy on epoch 3456: 0.889\n",
      "test loss on epoch 3456: 0.341\n",
      "test accuracy on epoch 3456: 0.769\n",
      "train loss on epoch 3457 : 0.253\n",
      "train accuracy on epoch 3457: 0.889\n",
      "test loss on epoch 3457: 0.326\n",
      "test accuracy on epoch 3457: 0.846\n",
      "train loss on epoch 3458 : 0.166\n",
      "train accuracy on epoch 3458: 0.944\n",
      "test loss on epoch 3458: 0.338\n",
      "test accuracy on epoch 3458: 0.692\n",
      "train loss on epoch 3459 : 0.059\n",
      "train accuracy on epoch 3459: 0.944\n",
      "test loss on epoch 3459: 0.333\n",
      "test accuracy on epoch 3459: 0.769\n",
      "train loss on epoch 3460 : 0.148\n",
      "train accuracy on epoch 3460: 0.944\n",
      "test loss on epoch 3460: 0.338\n",
      "test accuracy on epoch 3460: 0.769\n",
      "train loss on epoch 3461 : 0.072\n",
      "train accuracy on epoch 3461: 0.944\n",
      "test loss on epoch 3461: 0.329\n",
      "test accuracy on epoch 3461: 0.769\n",
      "train loss on epoch 3462 : 0.149\n",
      "train accuracy on epoch 3462: 0.944\n",
      "test loss on epoch 3462: 0.334\n",
      "test accuracy on epoch 3462: 0.769\n",
      "train loss on epoch 3463 : 0.208\n",
      "train accuracy on epoch 3463: 0.889\n",
      "test loss on epoch 3463: 0.355\n",
      "test accuracy on epoch 3463: 0.769\n",
      "train loss on epoch 3464 : 0.220\n",
      "train accuracy on epoch 3464: 0.889\n",
      "test loss on epoch 3464: 0.360\n",
      "test accuracy on epoch 3464: 0.769\n",
      "train loss on epoch 3465 : 0.256\n",
      "train accuracy on epoch 3465: 0.889\n",
      "test loss on epoch 3465: 0.363\n",
      "test accuracy on epoch 3465: 0.769\n",
      "train loss on epoch 3466 : 0.207\n",
      "train accuracy on epoch 3466: 0.889\n",
      "test loss on epoch 3466: 0.353\n",
      "test accuracy on epoch 3466: 0.769\n",
      "train loss on epoch 3467 : 0.074\n",
      "train accuracy on epoch 3467: 0.944\n",
      "test loss on epoch 3467: 0.353\n",
      "test accuracy on epoch 3467: 0.769\n",
      "train loss on epoch 3468 : 0.223\n",
      "train accuracy on epoch 3468: 0.944\n",
      "test loss on epoch 3468: 0.347\n",
      "test accuracy on epoch 3468: 0.769\n",
      "train loss on epoch 3469 : 0.077\n",
      "train accuracy on epoch 3469: 0.944\n",
      "test loss on epoch 3469: 0.354\n",
      "test accuracy on epoch 3469: 0.769\n",
      "train loss on epoch 3470 : 0.370\n",
      "train accuracy on epoch 3470: 0.889\n",
      "test loss on epoch 3470: 0.341\n",
      "test accuracy on epoch 3470: 0.769\n",
      "train loss on epoch 3471 : 0.335\n",
      "train accuracy on epoch 3471: 0.889\n",
      "test loss on epoch 3471: 0.344\n",
      "test accuracy on epoch 3471: 0.769\n",
      "train loss on epoch 3472 : 0.088\n",
      "train accuracy on epoch 3472: 0.944\n",
      "test loss on epoch 3472: 0.344\n",
      "test accuracy on epoch 3472: 0.692\n",
      "train loss on epoch 3473 : 0.124\n",
      "train accuracy on epoch 3473: 0.944\n",
      "test loss on epoch 3473: 0.334\n",
      "test accuracy on epoch 3473: 0.769\n",
      "train loss on epoch 3474 : 0.045\n",
      "train accuracy on epoch 3474: 1.000\n",
      "test loss on epoch 3474: 0.340\n",
      "test accuracy on epoch 3474: 0.769\n",
      "train loss on epoch 3475 : 0.240\n",
      "train accuracy on epoch 3475: 0.889\n",
      "test loss on epoch 3475: 0.327\n",
      "test accuracy on epoch 3475: 0.769\n",
      "train loss on epoch 3476 : 0.111\n",
      "train accuracy on epoch 3476: 0.944\n",
      "test loss on epoch 3476: 0.335\n",
      "test accuracy on epoch 3476: 0.769\n",
      "train loss on epoch 3477 : 0.102\n",
      "train accuracy on epoch 3477: 0.944\n",
      "test loss on epoch 3477: 0.331\n",
      "test accuracy on epoch 3477: 0.846\n",
      "train loss on epoch 3478 : 0.127\n",
      "train accuracy on epoch 3478: 0.944\n",
      "test loss on epoch 3478: 0.349\n",
      "test accuracy on epoch 3478: 0.769\n",
      "train loss on epoch 3479 : 0.146\n",
      "train accuracy on epoch 3479: 0.944\n",
      "test loss on epoch 3479: 0.349\n",
      "test accuracy on epoch 3479: 0.769\n",
      "train loss on epoch 3480 : 0.074\n",
      "train accuracy on epoch 3480: 1.000\n",
      "test loss on epoch 3480: 0.342\n",
      "test accuracy on epoch 3480: 0.769\n",
      "train loss on epoch 3481 : 0.377\n",
      "train accuracy on epoch 3481: 0.889\n",
      "test loss on epoch 3481: 0.336\n",
      "test accuracy on epoch 3481: 0.769\n",
      "train loss on epoch 3482 : 0.091\n",
      "train accuracy on epoch 3482: 0.944\n",
      "test loss on epoch 3482: 0.339\n",
      "test accuracy on epoch 3482: 0.769\n",
      "train loss on epoch 3483 : 0.127\n",
      "train accuracy on epoch 3483: 0.944\n",
      "test loss on epoch 3483: 0.347\n",
      "test accuracy on epoch 3483: 0.769\n",
      "train loss on epoch 3484 : 0.137\n",
      "train accuracy on epoch 3484: 0.944\n",
      "test loss on epoch 3484: 0.338\n",
      "test accuracy on epoch 3484: 0.769\n",
      "train loss on epoch 3485 : 0.377\n",
      "train accuracy on epoch 3485: 0.833\n",
      "test loss on epoch 3485: 0.335\n",
      "test accuracy on epoch 3485: 0.769\n",
      "train loss on epoch 3486 : 0.060\n",
      "train accuracy on epoch 3486: 1.000\n",
      "test loss on epoch 3486: 0.341\n",
      "test accuracy on epoch 3486: 0.769\n",
      "train loss on epoch 3487 : 0.201\n",
      "train accuracy on epoch 3487: 0.889\n",
      "test loss on epoch 3487: 0.332\n",
      "test accuracy on epoch 3487: 0.769\n",
      "train loss on epoch 3488 : 0.053\n",
      "train accuracy on epoch 3488: 1.000\n",
      "test loss on epoch 3488: 0.347\n",
      "test accuracy on epoch 3488: 0.692\n",
      "train loss on epoch 3489 : 0.139\n",
      "train accuracy on epoch 3489: 0.944\n",
      "test loss on epoch 3489: 0.350\n",
      "test accuracy on epoch 3489: 0.769\n",
      "train loss on epoch 3490 : 0.050\n",
      "train accuracy on epoch 3490: 1.000\n",
      "test loss on epoch 3490: 0.353\n",
      "test accuracy on epoch 3490: 0.769\n",
      "train loss on epoch 3491 : 0.113\n",
      "train accuracy on epoch 3491: 0.944\n",
      "test loss on epoch 3491: 0.352\n",
      "test accuracy on epoch 3491: 0.769\n",
      "train loss on epoch 3492 : 0.209\n",
      "train accuracy on epoch 3492: 0.944\n",
      "test loss on epoch 3492: 0.351\n",
      "test accuracy on epoch 3492: 0.769\n",
      "train loss on epoch 3493 : 0.163\n",
      "train accuracy on epoch 3493: 0.944\n",
      "test loss on epoch 3493: 0.342\n",
      "test accuracy on epoch 3493: 0.769\n",
      "train loss on epoch 3494 : 0.292\n",
      "train accuracy on epoch 3494: 0.778\n",
      "test loss on epoch 3494: 0.340\n",
      "test accuracy on epoch 3494: 0.769\n",
      "train loss on epoch 3495 : 0.274\n",
      "train accuracy on epoch 3495: 0.889\n",
      "test loss on epoch 3495: 0.346\n",
      "test accuracy on epoch 3495: 0.769\n",
      "train loss on epoch 3496 : 0.296\n",
      "train accuracy on epoch 3496: 0.889\n",
      "test loss on epoch 3496: 0.337\n",
      "test accuracy on epoch 3496: 0.769\n",
      "train loss on epoch 3497 : 0.091\n",
      "train accuracy on epoch 3497: 1.000\n",
      "test loss on epoch 3497: 0.337\n",
      "test accuracy on epoch 3497: 0.769\n",
      "train loss on epoch 3498 : 0.250\n",
      "train accuracy on epoch 3498: 0.833\n",
      "test loss on epoch 3498: 0.341\n",
      "test accuracy on epoch 3498: 0.769\n",
      "train loss on epoch 3499 : 0.252\n",
      "train accuracy on epoch 3499: 0.889\n",
      "test loss on epoch 3499: 0.339\n",
      "test accuracy on epoch 3499: 0.769\n",
      "train loss on epoch 3500 : 0.354\n",
      "train accuracy on epoch 3500: 0.778\n",
      "test loss on epoch 3500: 0.329\n",
      "test accuracy on epoch 3500: 0.769\n",
      "train loss on epoch 3501 : 0.206\n",
      "train accuracy on epoch 3501: 0.944\n",
      "test loss on epoch 3501: 0.333\n",
      "test accuracy on epoch 3501: 0.769\n",
      "train loss on epoch 3502 : 0.063\n",
      "train accuracy on epoch 3502: 1.000\n",
      "test loss on epoch 3502: 0.331\n",
      "test accuracy on epoch 3502: 0.769\n",
      "train loss on epoch 3503 : 0.111\n",
      "train accuracy on epoch 3503: 1.000\n",
      "test loss on epoch 3503: 0.332\n",
      "test accuracy on epoch 3503: 0.769\n",
      "train loss on epoch 3504 : 0.160\n",
      "train accuracy on epoch 3504: 0.944\n",
      "test loss on epoch 3504: 0.333\n",
      "test accuracy on epoch 3504: 0.769\n",
      "train loss on epoch 3505 : 0.157\n",
      "train accuracy on epoch 3505: 0.944\n",
      "test loss on epoch 3505: 0.340\n",
      "test accuracy on epoch 3505: 0.769\n",
      "train loss on epoch 3506 : 0.220\n",
      "train accuracy on epoch 3506: 0.944\n",
      "test loss on epoch 3506: 0.341\n",
      "test accuracy on epoch 3506: 0.769\n",
      "train loss on epoch 3507 : 0.184\n",
      "train accuracy on epoch 3507: 0.889\n",
      "test loss on epoch 3507: 0.345\n",
      "test accuracy on epoch 3507: 0.769\n",
      "train loss on epoch 3508 : 0.137\n",
      "train accuracy on epoch 3508: 0.944\n",
      "test loss on epoch 3508: 0.345\n",
      "test accuracy on epoch 3508: 0.769\n",
      "train loss on epoch 3509 : 0.236\n",
      "train accuracy on epoch 3509: 0.889\n",
      "test loss on epoch 3509: 0.347\n",
      "test accuracy on epoch 3509: 0.769\n",
      "train loss on epoch 3510 : 0.089\n",
      "train accuracy on epoch 3510: 1.000\n",
      "test loss on epoch 3510: 0.341\n",
      "test accuracy on epoch 3510: 0.769\n",
      "train loss on epoch 3511 : 0.393\n",
      "train accuracy on epoch 3511: 0.889\n",
      "test loss on epoch 3511: 0.346\n",
      "test accuracy on epoch 3511: 0.769\n",
      "train loss on epoch 3512 : 0.041\n",
      "train accuracy on epoch 3512: 1.000\n",
      "test loss on epoch 3512: 0.350\n",
      "test accuracy on epoch 3512: 0.769\n",
      "train loss on epoch 3513 : 0.074\n",
      "train accuracy on epoch 3513: 1.000\n",
      "test loss on epoch 3513: 0.340\n",
      "test accuracy on epoch 3513: 0.769\n",
      "train loss on epoch 3514 : 0.344\n",
      "train accuracy on epoch 3514: 0.889\n",
      "test loss on epoch 3514: 0.349\n",
      "test accuracy on epoch 3514: 0.769\n",
      "train loss on epoch 3515 : 0.252\n",
      "train accuracy on epoch 3515: 0.889\n",
      "test loss on epoch 3515: 0.345\n",
      "test accuracy on epoch 3515: 0.769\n",
      "train loss on epoch 3516 : 0.210\n",
      "train accuracy on epoch 3516: 0.889\n",
      "test loss on epoch 3516: 0.348\n",
      "test accuracy on epoch 3516: 0.769\n",
      "train loss on epoch 3517 : 0.170\n",
      "train accuracy on epoch 3517: 0.889\n",
      "test loss on epoch 3517: 0.353\n",
      "test accuracy on epoch 3517: 0.769\n",
      "train loss on epoch 3518 : 0.283\n",
      "train accuracy on epoch 3518: 0.889\n",
      "test loss on epoch 3518: 0.357\n",
      "test accuracy on epoch 3518: 0.769\n",
      "train loss on epoch 3519 : 0.220\n",
      "train accuracy on epoch 3519: 0.889\n",
      "test loss on epoch 3519: 0.358\n",
      "test accuracy on epoch 3519: 0.769\n",
      "train loss on epoch 3520 : 0.179\n",
      "train accuracy on epoch 3520: 0.889\n",
      "test loss on epoch 3520: 0.358\n",
      "test accuracy on epoch 3520: 0.769\n",
      "train loss on epoch 3521 : 0.495\n",
      "train accuracy on epoch 3521: 0.833\n",
      "test loss on epoch 3521: 0.360\n",
      "test accuracy on epoch 3521: 0.769\n",
      "train loss on epoch 3522 : 0.112\n",
      "train accuracy on epoch 3522: 0.944\n",
      "test loss on epoch 3522: 0.348\n",
      "test accuracy on epoch 3522: 0.769\n",
      "train loss on epoch 3523 : 0.175\n",
      "train accuracy on epoch 3523: 0.889\n",
      "test loss on epoch 3523: 0.354\n",
      "test accuracy on epoch 3523: 0.769\n",
      "train loss on epoch 3524 : 0.257\n",
      "train accuracy on epoch 3524: 0.889\n",
      "test loss on epoch 3524: 0.347\n",
      "test accuracy on epoch 3524: 0.769\n",
      "train loss on epoch 3525 : 0.131\n",
      "train accuracy on epoch 3525: 0.889\n",
      "test loss on epoch 3525: 0.351\n",
      "test accuracy on epoch 3525: 0.769\n",
      "train loss on epoch 3526 : 0.109\n",
      "train accuracy on epoch 3526: 0.944\n",
      "test loss on epoch 3526: 0.349\n",
      "test accuracy on epoch 3526: 0.769\n",
      "train loss on epoch 3527 : 0.094\n",
      "train accuracy on epoch 3527: 0.944\n",
      "test loss on epoch 3527: 0.350\n",
      "test accuracy on epoch 3527: 0.769\n",
      "train loss on epoch 3528 : 0.121\n",
      "train accuracy on epoch 3528: 0.944\n",
      "test loss on epoch 3528: 0.346\n",
      "test accuracy on epoch 3528: 0.769\n",
      "train loss on epoch 3529 : 0.212\n",
      "train accuracy on epoch 3529: 0.889\n",
      "test loss on epoch 3529: 0.348\n",
      "test accuracy on epoch 3529: 0.769\n",
      "train loss on epoch 3530 : 0.191\n",
      "train accuracy on epoch 3530: 0.889\n",
      "test loss on epoch 3530: 0.354\n",
      "test accuracy on epoch 3530: 0.769\n",
      "train loss on epoch 3531 : 0.345\n",
      "train accuracy on epoch 3531: 0.944\n",
      "test loss on epoch 3531: 0.370\n",
      "test accuracy on epoch 3531: 0.769\n",
      "train loss on epoch 3532 : 0.101\n",
      "train accuracy on epoch 3532: 0.944\n",
      "test loss on epoch 3532: 0.378\n",
      "test accuracy on epoch 3532: 0.769\n",
      "train loss on epoch 3533 : 0.061\n",
      "train accuracy on epoch 3533: 1.000\n",
      "test loss on epoch 3533: 0.380\n",
      "test accuracy on epoch 3533: 0.769\n",
      "train loss on epoch 3534 : 0.196\n",
      "train accuracy on epoch 3534: 0.944\n",
      "test loss on epoch 3534: 0.389\n",
      "test accuracy on epoch 3534: 0.769\n",
      "train loss on epoch 3535 : 0.147\n",
      "train accuracy on epoch 3535: 0.944\n",
      "test loss on epoch 3535: 0.385\n",
      "test accuracy on epoch 3535: 0.769\n",
      "train loss on epoch 3536 : 0.446\n",
      "train accuracy on epoch 3536: 0.778\n",
      "test loss on epoch 3536: 0.379\n",
      "test accuracy on epoch 3536: 0.769\n",
      "train loss on epoch 3537 : 0.077\n",
      "train accuracy on epoch 3537: 0.944\n",
      "test loss on epoch 3537: 0.375\n",
      "test accuracy on epoch 3537: 0.769\n",
      "train loss on epoch 3538 : 0.142\n",
      "train accuracy on epoch 3538: 0.944\n",
      "test loss on epoch 3538: 0.369\n",
      "test accuracy on epoch 3538: 0.769\n",
      "train loss on epoch 3539 : 0.286\n",
      "train accuracy on epoch 3539: 0.889\n",
      "test loss on epoch 3539: 0.359\n",
      "test accuracy on epoch 3539: 0.769\n",
      "train loss on epoch 3540 : 0.125\n",
      "train accuracy on epoch 3540: 0.944\n",
      "test loss on epoch 3540: 0.352\n",
      "test accuracy on epoch 3540: 0.769\n",
      "train loss on epoch 3541 : 0.305\n",
      "train accuracy on epoch 3541: 0.944\n",
      "test loss on epoch 3541: 0.345\n",
      "test accuracy on epoch 3541: 0.769\n",
      "train loss on epoch 3542 : 0.126\n",
      "train accuracy on epoch 3542: 0.944\n",
      "test loss on epoch 3542: 0.334\n",
      "test accuracy on epoch 3542: 0.769\n",
      "train loss on epoch 3543 : 0.283\n",
      "train accuracy on epoch 3543: 0.889\n",
      "test loss on epoch 3543: 0.328\n",
      "test accuracy on epoch 3543: 0.769\n",
      "train loss on epoch 3544 : 0.082\n",
      "train accuracy on epoch 3544: 0.944\n",
      "test loss on epoch 3544: 0.333\n",
      "test accuracy on epoch 3544: 0.769\n",
      "train loss on epoch 3545 : 0.035\n",
      "train accuracy on epoch 3545: 1.000\n",
      "test loss on epoch 3545: 0.330\n",
      "test accuracy on epoch 3545: 0.769\n",
      "train loss on epoch 3546 : 0.181\n",
      "train accuracy on epoch 3546: 0.889\n",
      "test loss on epoch 3546: 0.339\n",
      "test accuracy on epoch 3546: 0.769\n",
      "train loss on epoch 3547 : 0.297\n",
      "train accuracy on epoch 3547: 0.944\n",
      "test loss on epoch 3547: 0.337\n",
      "test accuracy on epoch 3547: 0.769\n",
      "train loss on epoch 3548 : 0.319\n",
      "train accuracy on epoch 3548: 0.889\n",
      "test loss on epoch 3548: 0.329\n",
      "test accuracy on epoch 3548: 0.769\n",
      "train loss on epoch 3549 : 0.193\n",
      "train accuracy on epoch 3549: 0.944\n",
      "test loss on epoch 3549: 0.322\n",
      "test accuracy on epoch 3549: 0.769\n",
      "train loss on epoch 3550 : 0.194\n",
      "train accuracy on epoch 3550: 0.833\n",
      "test loss on epoch 3550: 0.330\n",
      "test accuracy on epoch 3550: 0.769\n",
      "train loss on epoch 3551 : 0.247\n",
      "train accuracy on epoch 3551: 0.944\n",
      "test loss on epoch 3551: 0.335\n",
      "test accuracy on epoch 3551: 0.769\n",
      "train loss on epoch 3552 : 0.074\n",
      "train accuracy on epoch 3552: 0.944\n",
      "test loss on epoch 3552: 0.340\n",
      "test accuracy on epoch 3552: 0.769\n",
      "train loss on epoch 3553 : 0.209\n",
      "train accuracy on epoch 3553: 0.889\n",
      "test loss on epoch 3553: 0.340\n",
      "test accuracy on epoch 3553: 0.769\n",
      "train loss on epoch 3554 : 0.136\n",
      "train accuracy on epoch 3554: 0.889\n",
      "test loss on epoch 3554: 0.339\n",
      "test accuracy on epoch 3554: 0.769\n",
      "train loss on epoch 3555 : 0.160\n",
      "train accuracy on epoch 3555: 0.944\n",
      "test loss on epoch 3555: 0.346\n",
      "test accuracy on epoch 3555: 0.769\n",
      "train loss on epoch 3556 : 0.073\n",
      "train accuracy on epoch 3556: 0.944\n",
      "test loss on epoch 3556: 0.343\n",
      "test accuracy on epoch 3556: 0.769\n",
      "train loss on epoch 3557 : 0.154\n",
      "train accuracy on epoch 3557: 0.944\n",
      "test loss on epoch 3557: 0.341\n",
      "test accuracy on epoch 3557: 0.692\n",
      "train loss on epoch 3558 : 0.174\n",
      "train accuracy on epoch 3558: 0.889\n",
      "test loss on epoch 3558: 0.338\n",
      "test accuracy on epoch 3558: 0.769\n",
      "train loss on epoch 3559 : 0.099\n",
      "train accuracy on epoch 3559: 0.944\n",
      "test loss on epoch 3559: 0.338\n",
      "test accuracy on epoch 3559: 0.769\n",
      "train loss on epoch 3560 : 0.250\n",
      "train accuracy on epoch 3560: 0.944\n",
      "test loss on epoch 3560: 0.345\n",
      "test accuracy on epoch 3560: 0.769\n",
      "train loss on epoch 3561 : 0.094\n",
      "train accuracy on epoch 3561: 0.944\n",
      "test loss on epoch 3561: 0.354\n",
      "test accuracy on epoch 3561: 0.769\n",
      "train loss on epoch 3562 : 0.084\n",
      "train accuracy on epoch 3562: 1.000\n",
      "test loss on epoch 3562: 0.346\n",
      "test accuracy on epoch 3562: 0.769\n",
      "train loss on epoch 3563 : 0.160\n",
      "train accuracy on epoch 3563: 0.944\n",
      "test loss on epoch 3563: 0.346\n",
      "test accuracy on epoch 3563: 0.769\n",
      "train loss on epoch 3564 : 0.276\n",
      "train accuracy on epoch 3564: 0.944\n",
      "test loss on epoch 3564: 0.350\n",
      "test accuracy on epoch 3564: 0.769\n",
      "train loss on epoch 3565 : 0.380\n",
      "train accuracy on epoch 3565: 0.889\n",
      "test loss on epoch 3565: 0.340\n",
      "test accuracy on epoch 3565: 0.769\n",
      "train loss on epoch 3566 : 0.250\n",
      "train accuracy on epoch 3566: 0.889\n",
      "test loss on epoch 3566: 0.337\n",
      "test accuracy on epoch 3566: 0.769\n",
      "train loss on epoch 3567 : 0.132\n",
      "train accuracy on epoch 3567: 0.944\n",
      "test loss on epoch 3567: 0.340\n",
      "test accuracy on epoch 3567: 0.769\n",
      "train loss on epoch 3568 : 0.167\n",
      "train accuracy on epoch 3568: 0.944\n",
      "test loss on epoch 3568: 0.339\n",
      "test accuracy on epoch 3568: 0.769\n",
      "train loss on epoch 3569 : 0.333\n",
      "train accuracy on epoch 3569: 0.944\n",
      "test loss on epoch 3569: 0.342\n",
      "test accuracy on epoch 3569: 0.769\n",
      "train loss on epoch 3570 : 0.221\n",
      "train accuracy on epoch 3570: 0.889\n",
      "test loss on epoch 3570: 0.339\n",
      "test accuracy on epoch 3570: 0.769\n",
      "train loss on epoch 3571 : 0.340\n",
      "train accuracy on epoch 3571: 0.778\n",
      "test loss on epoch 3571: 0.347\n",
      "test accuracy on epoch 3571: 0.769\n",
      "train loss on epoch 3572 : 0.197\n",
      "train accuracy on epoch 3572: 0.944\n",
      "test loss on epoch 3572: 0.363\n",
      "test accuracy on epoch 3572: 0.769\n",
      "train loss on epoch 3573 : 0.209\n",
      "train accuracy on epoch 3573: 0.889\n",
      "test loss on epoch 3573: 0.380\n",
      "test accuracy on epoch 3573: 0.769\n",
      "train loss on epoch 3574 : 0.212\n",
      "train accuracy on epoch 3574: 0.889\n",
      "test loss on epoch 3574: 0.381\n",
      "test accuracy on epoch 3574: 0.769\n",
      "train loss on epoch 3575 : 0.193\n",
      "train accuracy on epoch 3575: 0.944\n",
      "test loss on epoch 3575: 0.384\n",
      "test accuracy on epoch 3575: 0.769\n",
      "train loss on epoch 3576 : 0.522\n",
      "train accuracy on epoch 3576: 0.833\n",
      "test loss on epoch 3576: 0.386\n",
      "test accuracy on epoch 3576: 0.769\n",
      "train loss on epoch 3577 : 0.231\n",
      "train accuracy on epoch 3577: 0.944\n",
      "test loss on epoch 3577: 0.373\n",
      "test accuracy on epoch 3577: 0.769\n",
      "train loss on epoch 3578 : 0.272\n",
      "train accuracy on epoch 3578: 0.889\n",
      "test loss on epoch 3578: 0.364\n",
      "test accuracy on epoch 3578: 0.769\n",
      "train loss on epoch 3579 : 0.257\n",
      "train accuracy on epoch 3579: 0.889\n",
      "test loss on epoch 3579: 0.361\n",
      "test accuracy on epoch 3579: 0.769\n",
      "train loss on epoch 3580 : 0.120\n",
      "train accuracy on epoch 3580: 0.944\n",
      "test loss on epoch 3580: 0.362\n",
      "test accuracy on epoch 3580: 0.769\n",
      "train loss on epoch 3581 : 0.072\n",
      "train accuracy on epoch 3581: 1.000\n",
      "test loss on epoch 3581: 0.363\n",
      "test accuracy on epoch 3581: 0.769\n",
      "train loss on epoch 3582 : 0.089\n",
      "train accuracy on epoch 3582: 0.944\n",
      "test loss on epoch 3582: 0.369\n",
      "test accuracy on epoch 3582: 0.769\n",
      "train loss on epoch 3583 : 0.067\n",
      "train accuracy on epoch 3583: 0.944\n",
      "test loss on epoch 3583: 0.378\n",
      "test accuracy on epoch 3583: 0.769\n",
      "train loss on epoch 3584 : 0.153\n",
      "train accuracy on epoch 3584: 0.944\n",
      "test loss on epoch 3584: 0.380\n",
      "test accuracy on epoch 3584: 0.769\n",
      "train loss on epoch 3585 : 0.204\n",
      "train accuracy on epoch 3585: 0.889\n",
      "test loss on epoch 3585: 0.378\n",
      "test accuracy on epoch 3585: 0.769\n",
      "train loss on epoch 3586 : 0.410\n",
      "train accuracy on epoch 3586: 0.889\n",
      "test loss on epoch 3586: 0.381\n",
      "test accuracy on epoch 3586: 0.769\n",
      "train loss on epoch 3587 : 0.199\n",
      "train accuracy on epoch 3587: 0.889\n",
      "test loss on epoch 3587: 0.381\n",
      "test accuracy on epoch 3587: 0.769\n",
      "train loss on epoch 3588 : 0.150\n",
      "train accuracy on epoch 3588: 0.889\n",
      "test loss on epoch 3588: 0.378\n",
      "test accuracy on epoch 3588: 0.769\n",
      "train loss on epoch 3589 : 0.114\n",
      "train accuracy on epoch 3589: 0.944\n",
      "test loss on epoch 3589: 0.379\n",
      "test accuracy on epoch 3589: 0.769\n",
      "train loss on epoch 3590 : 0.162\n",
      "train accuracy on epoch 3590: 0.944\n",
      "test loss on epoch 3590: 0.370\n",
      "test accuracy on epoch 3590: 0.769\n",
      "train loss on epoch 3591 : 0.171\n",
      "train accuracy on epoch 3591: 0.944\n",
      "test loss on epoch 3591: 0.373\n",
      "test accuracy on epoch 3591: 0.769\n",
      "train loss on epoch 3592 : 0.162\n",
      "train accuracy on epoch 3592: 0.889\n",
      "test loss on epoch 3592: 0.364\n",
      "test accuracy on epoch 3592: 0.769\n",
      "train loss on epoch 3593 : 0.144\n",
      "train accuracy on epoch 3593: 0.889\n",
      "test loss on epoch 3593: 0.361\n",
      "test accuracy on epoch 3593: 0.769\n",
      "train loss on epoch 3594 : 0.261\n",
      "train accuracy on epoch 3594: 0.944\n",
      "test loss on epoch 3594: 0.355\n",
      "test accuracy on epoch 3594: 0.769\n",
      "train loss on epoch 3595 : 0.192\n",
      "train accuracy on epoch 3595: 0.944\n",
      "test loss on epoch 3595: 0.356\n",
      "test accuracy on epoch 3595: 0.769\n",
      "train loss on epoch 3596 : 0.143\n",
      "train accuracy on epoch 3596: 0.889\n",
      "test loss on epoch 3596: 0.357\n",
      "test accuracy on epoch 3596: 0.769\n",
      "train loss on epoch 3597 : 0.255\n",
      "train accuracy on epoch 3597: 0.944\n",
      "test loss on epoch 3597: 0.363\n",
      "test accuracy on epoch 3597: 0.769\n",
      "train loss on epoch 3598 : 0.282\n",
      "train accuracy on epoch 3598: 0.833\n",
      "test loss on epoch 3598: 0.370\n",
      "test accuracy on epoch 3598: 0.769\n",
      "train loss on epoch 3599 : 0.096\n",
      "train accuracy on epoch 3599: 1.000\n",
      "test loss on epoch 3599: 0.374\n",
      "test accuracy on epoch 3599: 0.769\n",
      "train loss on epoch 3600 : 0.242\n",
      "train accuracy on epoch 3600: 0.889\n",
      "test loss on epoch 3600: 0.374\n",
      "test accuracy on epoch 3600: 0.769\n",
      "train loss on epoch 3601 : 0.165\n",
      "train accuracy on epoch 3601: 0.889\n",
      "test loss on epoch 3601: 0.374\n",
      "test accuracy on epoch 3601: 0.769\n",
      "train loss on epoch 3602 : 0.154\n",
      "train accuracy on epoch 3602: 0.944\n",
      "test loss on epoch 3602: 0.369\n",
      "test accuracy on epoch 3602: 0.769\n",
      "train loss on epoch 3603 : 0.202\n",
      "train accuracy on epoch 3603: 0.889\n",
      "test loss on epoch 3603: 0.358\n",
      "test accuracy on epoch 3603: 0.769\n",
      "train loss on epoch 3604 : 0.156\n",
      "train accuracy on epoch 3604: 0.889\n",
      "test loss on epoch 3604: 0.344\n",
      "test accuracy on epoch 3604: 0.769\n",
      "train loss on epoch 3605 : 0.186\n",
      "train accuracy on epoch 3605: 0.889\n",
      "test loss on epoch 3605: 0.336\n",
      "test accuracy on epoch 3605: 0.769\n",
      "train loss on epoch 3606 : 0.342\n",
      "train accuracy on epoch 3606: 0.889\n",
      "test loss on epoch 3606: 0.335\n",
      "test accuracy on epoch 3606: 0.769\n",
      "train loss on epoch 3607 : 0.399\n",
      "train accuracy on epoch 3607: 0.778\n",
      "test loss on epoch 3607: 0.340\n",
      "test accuracy on epoch 3607: 0.769\n",
      "train loss on epoch 3608 : 0.243\n",
      "train accuracy on epoch 3608: 0.889\n",
      "test loss on epoch 3608: 0.333\n",
      "test accuracy on epoch 3608: 0.769\n",
      "train loss on epoch 3609 : 0.184\n",
      "train accuracy on epoch 3609: 0.889\n",
      "test loss on epoch 3609: 0.336\n",
      "test accuracy on epoch 3609: 0.692\n",
      "train loss on epoch 3610 : 0.207\n",
      "train accuracy on epoch 3610: 0.889\n",
      "test loss on epoch 3610: 0.337\n",
      "test accuracy on epoch 3610: 0.692\n",
      "train loss on epoch 3611 : 0.184\n",
      "train accuracy on epoch 3611: 0.889\n",
      "test loss on epoch 3611: 0.335\n",
      "test accuracy on epoch 3611: 0.769\n",
      "train loss on epoch 3612 : 0.067\n",
      "train accuracy on epoch 3612: 1.000\n",
      "test loss on epoch 3612: 0.331\n",
      "test accuracy on epoch 3612: 0.769\n",
      "train loss on epoch 3613 : 0.176\n",
      "train accuracy on epoch 3613: 0.889\n",
      "test loss on epoch 3613: 0.333\n",
      "test accuracy on epoch 3613: 0.769\n",
      "train loss on epoch 3614 : 0.101\n",
      "train accuracy on epoch 3614: 0.944\n",
      "test loss on epoch 3614: 0.333\n",
      "test accuracy on epoch 3614: 0.769\n",
      "train loss on epoch 3615 : 0.250\n",
      "train accuracy on epoch 3615: 0.833\n",
      "test loss on epoch 3615: 0.336\n",
      "test accuracy on epoch 3615: 0.769\n",
      "train loss on epoch 3616 : 0.149\n",
      "train accuracy on epoch 3616: 0.889\n",
      "test loss on epoch 3616: 0.332\n",
      "test accuracy on epoch 3616: 0.769\n",
      "train loss on epoch 3617 : 0.192\n",
      "train accuracy on epoch 3617: 0.944\n",
      "test loss on epoch 3617: 0.328\n",
      "test accuracy on epoch 3617: 0.769\n",
      "train loss on epoch 3618 : 0.135\n",
      "train accuracy on epoch 3618: 0.944\n",
      "test loss on epoch 3618: 0.331\n",
      "test accuracy on epoch 3618: 0.769\n",
      "train loss on epoch 3619 : 0.195\n",
      "train accuracy on epoch 3619: 0.944\n",
      "test loss on epoch 3619: 0.343\n",
      "test accuracy on epoch 3619: 0.769\n",
      "train loss on epoch 3620 : 0.209\n",
      "train accuracy on epoch 3620: 0.889\n",
      "test loss on epoch 3620: 0.343\n",
      "test accuracy on epoch 3620: 0.769\n",
      "train loss on epoch 3621 : 0.094\n",
      "train accuracy on epoch 3621: 0.944\n",
      "test loss on epoch 3621: 0.347\n",
      "test accuracy on epoch 3621: 0.769\n",
      "train loss on epoch 3622 : 0.403\n",
      "train accuracy on epoch 3622: 0.833\n",
      "test loss on epoch 3622: 0.337\n",
      "test accuracy on epoch 3622: 0.769\n",
      "train loss on epoch 3623 : 0.139\n",
      "train accuracy on epoch 3623: 0.944\n",
      "test loss on epoch 3623: 0.345\n",
      "test accuracy on epoch 3623: 0.769\n",
      "train loss on epoch 3624 : 0.131\n",
      "train accuracy on epoch 3624: 0.944\n",
      "test loss on epoch 3624: 0.343\n",
      "test accuracy on epoch 3624: 0.769\n",
      "train loss on epoch 3625 : 0.331\n",
      "train accuracy on epoch 3625: 0.889\n",
      "test loss on epoch 3625: 0.341\n",
      "test accuracy on epoch 3625: 0.769\n",
      "train loss on epoch 3626 : 0.060\n",
      "train accuracy on epoch 3626: 1.000\n",
      "test loss on epoch 3626: 0.341\n",
      "test accuracy on epoch 3626: 0.769\n",
      "train loss on epoch 3627 : 0.136\n",
      "train accuracy on epoch 3627: 1.000\n",
      "test loss on epoch 3627: 0.345\n",
      "test accuracy on epoch 3627: 0.769\n",
      "train loss on epoch 3628 : 0.088\n",
      "train accuracy on epoch 3628: 0.944\n",
      "test loss on epoch 3628: 0.346\n",
      "test accuracy on epoch 3628: 0.769\n",
      "train loss on epoch 3629 : 0.200\n",
      "train accuracy on epoch 3629: 0.944\n",
      "test loss on epoch 3629: 0.346\n",
      "test accuracy on epoch 3629: 0.769\n",
      "train loss on epoch 3630 : 0.278\n",
      "train accuracy on epoch 3630: 0.889\n",
      "test loss on epoch 3630: 0.346\n",
      "test accuracy on epoch 3630: 0.769\n",
      "train loss on epoch 3631 : 0.244\n",
      "train accuracy on epoch 3631: 0.889\n",
      "test loss on epoch 3631: 0.339\n",
      "test accuracy on epoch 3631: 0.769\n",
      "train loss on epoch 3632 : 0.102\n",
      "train accuracy on epoch 3632: 0.944\n",
      "test loss on epoch 3632: 0.340\n",
      "test accuracy on epoch 3632: 0.769\n",
      "train loss on epoch 3633 : 0.379\n",
      "train accuracy on epoch 3633: 0.833\n",
      "test loss on epoch 3633: 0.339\n",
      "test accuracy on epoch 3633: 0.769\n",
      "train loss on epoch 3634 : 0.140\n",
      "train accuracy on epoch 3634: 0.944\n",
      "test loss on epoch 3634: 0.345\n",
      "test accuracy on epoch 3634: 0.769\n",
      "train loss on epoch 3635 : 0.198\n",
      "train accuracy on epoch 3635: 0.944\n",
      "test loss on epoch 3635: 0.350\n",
      "test accuracy on epoch 3635: 0.769\n",
      "train loss on epoch 3636 : 0.433\n",
      "train accuracy on epoch 3636: 0.889\n",
      "test loss on epoch 3636: 0.356\n",
      "test accuracy on epoch 3636: 0.769\n",
      "train loss on epoch 3637 : 0.023\n",
      "train accuracy on epoch 3637: 1.000\n",
      "test loss on epoch 3637: 0.360\n",
      "test accuracy on epoch 3637: 0.769\n",
      "train loss on epoch 3638 : 0.134\n",
      "train accuracy on epoch 3638: 0.944\n",
      "test loss on epoch 3638: 0.369\n",
      "test accuracy on epoch 3638: 0.769\n",
      "train loss on epoch 3639 : 0.324\n",
      "train accuracy on epoch 3639: 0.889\n",
      "test loss on epoch 3639: 0.368\n",
      "test accuracy on epoch 3639: 0.769\n",
      "train loss on epoch 3640 : 0.202\n",
      "train accuracy on epoch 3640: 0.889\n",
      "test loss on epoch 3640: 0.360\n",
      "test accuracy on epoch 3640: 0.769\n",
      "train loss on epoch 3641 : 0.174\n",
      "train accuracy on epoch 3641: 0.944\n",
      "test loss on epoch 3641: 0.348\n",
      "test accuracy on epoch 3641: 0.769\n",
      "train loss on epoch 3642 : 0.063\n",
      "train accuracy on epoch 3642: 1.000\n",
      "test loss on epoch 3642: 0.347\n",
      "test accuracy on epoch 3642: 0.769\n",
      "train loss on epoch 3643 : 0.223\n",
      "train accuracy on epoch 3643: 0.833\n",
      "test loss on epoch 3643: 0.348\n",
      "test accuracy on epoch 3643: 0.769\n",
      "train loss on epoch 3644 : 0.170\n",
      "train accuracy on epoch 3644: 0.889\n",
      "test loss on epoch 3644: 0.343\n",
      "test accuracy on epoch 3644: 0.769\n",
      "train loss on epoch 3645 : 0.241\n",
      "train accuracy on epoch 3645: 0.944\n",
      "test loss on epoch 3645: 0.341\n",
      "test accuracy on epoch 3645: 0.769\n",
      "train loss on epoch 3646 : 0.101\n",
      "train accuracy on epoch 3646: 0.944\n",
      "test loss on epoch 3646: 0.333\n",
      "test accuracy on epoch 3646: 0.769\n",
      "train loss on epoch 3647 : 0.152\n",
      "train accuracy on epoch 3647: 0.944\n",
      "test loss on epoch 3647: 0.334\n",
      "test accuracy on epoch 3647: 0.769\n",
      "train loss on epoch 3648 : 0.122\n",
      "train accuracy on epoch 3648: 0.944\n",
      "test loss on epoch 3648: 0.331\n",
      "test accuracy on epoch 3648: 0.769\n",
      "train loss on epoch 3649 : 0.160\n",
      "train accuracy on epoch 3649: 0.889\n",
      "test loss on epoch 3649: 0.335\n",
      "test accuracy on epoch 3649: 0.769\n",
      "train loss on epoch 3650 : 0.059\n",
      "train accuracy on epoch 3650: 1.000\n",
      "test loss on epoch 3650: 0.335\n",
      "test accuracy on epoch 3650: 0.769\n",
      "train loss on epoch 3651 : 0.107\n",
      "train accuracy on epoch 3651: 0.944\n",
      "test loss on epoch 3651: 0.340\n",
      "test accuracy on epoch 3651: 0.769\n",
      "train loss on epoch 3652 : 0.208\n",
      "train accuracy on epoch 3652: 0.889\n",
      "test loss on epoch 3652: 0.337\n",
      "test accuracy on epoch 3652: 0.769\n",
      "train loss on epoch 3653 : 0.276\n",
      "train accuracy on epoch 3653: 0.889\n",
      "test loss on epoch 3653: 0.336\n",
      "test accuracy on epoch 3653: 0.769\n",
      "train loss on epoch 3654 : 0.186\n",
      "train accuracy on epoch 3654: 0.944\n",
      "test loss on epoch 3654: 0.333\n",
      "test accuracy on epoch 3654: 0.769\n",
      "train loss on epoch 3655 : 0.343\n",
      "train accuracy on epoch 3655: 0.889\n",
      "test loss on epoch 3655: 0.334\n",
      "test accuracy on epoch 3655: 0.769\n",
      "train loss on epoch 3656 : 0.184\n",
      "train accuracy on epoch 3656: 0.889\n",
      "test loss on epoch 3656: 0.342\n",
      "test accuracy on epoch 3656: 0.769\n",
      "train loss on epoch 3657 : 0.085\n",
      "train accuracy on epoch 3657: 0.944\n",
      "test loss on epoch 3657: 0.348\n",
      "test accuracy on epoch 3657: 0.769\n",
      "train loss on epoch 3658 : 0.206\n",
      "train accuracy on epoch 3658: 0.889\n",
      "test loss on epoch 3658: 0.359\n",
      "test accuracy on epoch 3658: 0.769\n",
      "train loss on epoch 3659 : 0.223\n",
      "train accuracy on epoch 3659: 0.944\n",
      "test loss on epoch 3659: 0.375\n",
      "test accuracy on epoch 3659: 0.769\n",
      "train loss on epoch 3660 : 0.174\n",
      "train accuracy on epoch 3660: 0.889\n",
      "test loss on epoch 3660: 0.373\n",
      "test accuracy on epoch 3660: 0.769\n",
      "train loss on epoch 3661 : 0.123\n",
      "train accuracy on epoch 3661: 0.944\n",
      "test loss on epoch 3661: 0.364\n",
      "test accuracy on epoch 3661: 0.769\n",
      "train loss on epoch 3662 : 0.187\n",
      "train accuracy on epoch 3662: 0.833\n",
      "test loss on epoch 3662: 0.351\n",
      "test accuracy on epoch 3662: 0.769\n",
      "train loss on epoch 3663 : 0.426\n",
      "train accuracy on epoch 3663: 0.889\n",
      "test loss on epoch 3663: 0.344\n",
      "test accuracy on epoch 3663: 0.769\n",
      "train loss on epoch 3664 : 0.082\n",
      "train accuracy on epoch 3664: 0.944\n",
      "test loss on epoch 3664: 0.353\n",
      "test accuracy on epoch 3664: 0.769\n",
      "train loss on epoch 3665 : 0.327\n",
      "train accuracy on epoch 3665: 0.889\n",
      "test loss on epoch 3665: 0.351\n",
      "test accuracy on epoch 3665: 0.769\n",
      "train loss on epoch 3666 : 0.376\n",
      "train accuracy on epoch 3666: 0.889\n",
      "test loss on epoch 3666: 0.343\n",
      "test accuracy on epoch 3666: 0.769\n",
      "train loss on epoch 3667 : 0.307\n",
      "train accuracy on epoch 3667: 0.833\n",
      "test loss on epoch 3667: 0.354\n",
      "test accuracy on epoch 3667: 0.769\n",
      "train loss on epoch 3668 : 0.148\n",
      "train accuracy on epoch 3668: 0.889\n",
      "test loss on epoch 3668: 0.351\n",
      "test accuracy on epoch 3668: 0.769\n",
      "train loss on epoch 3669 : 0.257\n",
      "train accuracy on epoch 3669: 0.944\n",
      "test loss on epoch 3669: 0.361\n",
      "test accuracy on epoch 3669: 0.769\n",
      "train loss on epoch 3670 : 0.152\n",
      "train accuracy on epoch 3670: 0.944\n",
      "test loss on epoch 3670: 0.359\n",
      "test accuracy on epoch 3670: 0.769\n",
      "train loss on epoch 3671 : 0.077\n",
      "train accuracy on epoch 3671: 1.000\n",
      "test loss on epoch 3671: 0.369\n",
      "test accuracy on epoch 3671: 0.769\n",
      "train loss on epoch 3672 : 0.208\n",
      "train accuracy on epoch 3672: 0.889\n",
      "test loss on epoch 3672: 0.366\n",
      "test accuracy on epoch 3672: 0.769\n",
      "train loss on epoch 3673 : 0.307\n",
      "train accuracy on epoch 3673: 0.833\n",
      "test loss on epoch 3673: 0.363\n",
      "test accuracy on epoch 3673: 0.769\n",
      "train loss on epoch 3674 : 0.083\n",
      "train accuracy on epoch 3674: 1.000\n",
      "test loss on epoch 3674: 0.353\n",
      "test accuracy on epoch 3674: 0.769\n",
      "train loss on epoch 3675 : 0.159\n",
      "train accuracy on epoch 3675: 0.889\n",
      "test loss on epoch 3675: 0.347\n",
      "test accuracy on epoch 3675: 0.769\n",
      "train loss on epoch 3676 : 0.236\n",
      "train accuracy on epoch 3676: 0.778\n",
      "test loss on epoch 3676: 0.343\n",
      "test accuracy on epoch 3676: 0.769\n",
      "train loss on epoch 3677 : 0.271\n",
      "train accuracy on epoch 3677: 0.889\n",
      "test loss on epoch 3677: 0.339\n",
      "test accuracy on epoch 3677: 0.769\n",
      "train loss on epoch 3678 : 0.073\n",
      "train accuracy on epoch 3678: 0.944\n",
      "test loss on epoch 3678: 0.340\n",
      "test accuracy on epoch 3678: 0.692\n",
      "train loss on epoch 3679 : 0.133\n",
      "train accuracy on epoch 3679: 0.944\n",
      "test loss on epoch 3679: 0.327\n",
      "test accuracy on epoch 3679: 0.846\n",
      "train loss on epoch 3680 : 0.253\n",
      "train accuracy on epoch 3680: 0.889\n",
      "test loss on epoch 3680: 0.335\n",
      "test accuracy on epoch 3680: 0.769\n",
      "train loss on epoch 3681 : 0.073\n",
      "train accuracy on epoch 3681: 1.000\n",
      "test loss on epoch 3681: 0.333\n",
      "test accuracy on epoch 3681: 0.769\n",
      "train loss on epoch 3682 : 0.137\n",
      "train accuracy on epoch 3682: 0.944\n",
      "test loss on epoch 3682: 0.339\n",
      "test accuracy on epoch 3682: 0.769\n",
      "train loss on epoch 3683 : 0.096\n",
      "train accuracy on epoch 3683: 1.000\n",
      "test loss on epoch 3683: 0.338\n",
      "test accuracy on epoch 3683: 0.769\n",
      "train loss on epoch 3684 : 0.278\n",
      "train accuracy on epoch 3684: 0.889\n",
      "test loss on epoch 3684: 0.338\n",
      "test accuracy on epoch 3684: 0.769\n",
      "train loss on epoch 3685 : 0.438\n",
      "train accuracy on epoch 3685: 0.833\n",
      "test loss on epoch 3685: 0.347\n",
      "test accuracy on epoch 3685: 0.769\n",
      "train loss on epoch 3686 : 0.165\n",
      "train accuracy on epoch 3686: 0.944\n",
      "test loss on epoch 3686: 0.352\n",
      "test accuracy on epoch 3686: 0.769\n",
      "train loss on epoch 3687 : 0.129\n",
      "train accuracy on epoch 3687: 0.889\n",
      "test loss on epoch 3687: 0.350\n",
      "test accuracy on epoch 3687: 0.769\n",
      "train loss on epoch 3688 : 0.253\n",
      "train accuracy on epoch 3688: 0.889\n",
      "test loss on epoch 3688: 0.341\n",
      "test accuracy on epoch 3688: 0.769\n",
      "train loss on epoch 3689 : 0.169\n",
      "train accuracy on epoch 3689: 0.944\n",
      "test loss on epoch 3689: 0.342\n",
      "test accuracy on epoch 3689: 0.769\n",
      "train loss on epoch 3690 : 0.120\n",
      "train accuracy on epoch 3690: 0.944\n",
      "test loss on epoch 3690: 0.348\n",
      "test accuracy on epoch 3690: 0.769\n",
      "train loss on epoch 3691 : 0.326\n",
      "train accuracy on epoch 3691: 0.833\n",
      "test loss on epoch 3691: 0.338\n",
      "test accuracy on epoch 3691: 0.769\n",
      "train loss on epoch 3692 : 0.049\n",
      "train accuracy on epoch 3692: 1.000\n",
      "test loss on epoch 3692: 0.344\n",
      "test accuracy on epoch 3692: 0.692\n",
      "train loss on epoch 3693 : 0.119\n",
      "train accuracy on epoch 3693: 0.889\n",
      "test loss on epoch 3693: 0.340\n",
      "test accuracy on epoch 3693: 0.692\n",
      "train loss on epoch 3694 : 0.178\n",
      "train accuracy on epoch 3694: 0.889\n",
      "test loss on epoch 3694: 0.337\n",
      "test accuracy on epoch 3694: 0.769\n",
      "train loss on epoch 3695 : 0.138\n",
      "train accuracy on epoch 3695: 0.944\n",
      "test loss on epoch 3695: 0.329\n",
      "test accuracy on epoch 3695: 0.846\n",
      "train loss on epoch 3696 : 0.185\n",
      "train accuracy on epoch 3696: 0.889\n",
      "test loss on epoch 3696: 0.330\n",
      "test accuracy on epoch 3696: 0.769\n",
      "train loss on epoch 3697 : 0.144\n",
      "train accuracy on epoch 3697: 0.944\n",
      "test loss on epoch 3697: 0.323\n",
      "test accuracy on epoch 3697: 0.846\n",
      "train loss on epoch 3698 : 0.132\n",
      "train accuracy on epoch 3698: 0.944\n",
      "test loss on epoch 3698: 0.328\n",
      "test accuracy on epoch 3698: 0.846\n",
      "train loss on epoch 3699 : 0.064\n",
      "train accuracy on epoch 3699: 1.000\n",
      "test loss on epoch 3699: 0.326\n",
      "test accuracy on epoch 3699: 0.846\n",
      "train loss on epoch 3700 : 0.341\n",
      "train accuracy on epoch 3700: 0.833\n",
      "test loss on epoch 3700: 0.337\n",
      "test accuracy on epoch 3700: 0.692\n",
      "train loss on epoch 3701 : 0.120\n",
      "train accuracy on epoch 3701: 1.000\n",
      "test loss on epoch 3701: 0.329\n",
      "test accuracy on epoch 3701: 0.769\n",
      "train loss on epoch 3702 : 0.081\n",
      "train accuracy on epoch 3702: 1.000\n",
      "test loss on epoch 3702: 0.330\n",
      "test accuracy on epoch 3702: 0.769\n",
      "train loss on epoch 3703 : 0.242\n",
      "train accuracy on epoch 3703: 0.833\n",
      "test loss on epoch 3703: 0.325\n",
      "test accuracy on epoch 3703: 0.769\n",
      "train loss on epoch 3704 : 0.262\n",
      "train accuracy on epoch 3704: 0.944\n",
      "test loss on epoch 3704: 0.336\n",
      "test accuracy on epoch 3704: 0.769\n",
      "train loss on epoch 3705 : 0.096\n",
      "train accuracy on epoch 3705: 0.944\n",
      "test loss on epoch 3705: 0.332\n",
      "test accuracy on epoch 3705: 0.769\n",
      "train loss on epoch 3706 : 0.036\n",
      "train accuracy on epoch 3706: 1.000\n",
      "test loss on epoch 3706: 0.331\n",
      "test accuracy on epoch 3706: 0.769\n",
      "train loss on epoch 3707 : 0.185\n",
      "train accuracy on epoch 3707: 0.944\n",
      "test loss on epoch 3707: 0.339\n",
      "test accuracy on epoch 3707: 0.769\n",
      "train loss on epoch 3708 : 0.051\n",
      "train accuracy on epoch 3708: 1.000\n",
      "test loss on epoch 3708: 0.340\n",
      "test accuracy on epoch 3708: 0.769\n",
      "train loss on epoch 3709 : 0.253\n",
      "train accuracy on epoch 3709: 0.889\n",
      "test loss on epoch 3709: 0.338\n",
      "test accuracy on epoch 3709: 0.769\n",
      "train loss on epoch 3710 : 0.140\n",
      "train accuracy on epoch 3710: 0.944\n",
      "test loss on epoch 3710: 0.330\n",
      "test accuracy on epoch 3710: 0.769\n",
      "train loss on epoch 3711 : 0.296\n",
      "train accuracy on epoch 3711: 0.889\n",
      "test loss on epoch 3711: 0.331\n",
      "test accuracy on epoch 3711: 0.769\n",
      "train loss on epoch 3712 : 0.231\n",
      "train accuracy on epoch 3712: 0.833\n",
      "test loss on epoch 3712: 0.339\n",
      "test accuracy on epoch 3712: 0.769\n",
      "train loss on epoch 3713 : 0.215\n",
      "train accuracy on epoch 3713: 0.889\n",
      "test loss on epoch 3713: 0.340\n",
      "test accuracy on epoch 3713: 0.769\n",
      "train loss on epoch 3714 : 0.183\n",
      "train accuracy on epoch 3714: 0.889\n",
      "test loss on epoch 3714: 0.335\n",
      "test accuracy on epoch 3714: 0.769\n",
      "train loss on epoch 3715 : 0.112\n",
      "train accuracy on epoch 3715: 0.944\n",
      "test loss on epoch 3715: 0.341\n",
      "test accuracy on epoch 3715: 0.769\n",
      "train loss on epoch 3716 : 0.190\n",
      "train accuracy on epoch 3716: 0.944\n",
      "test loss on epoch 3716: 0.335\n",
      "test accuracy on epoch 3716: 0.769\n",
      "train loss on epoch 3717 : 0.191\n",
      "train accuracy on epoch 3717: 0.889\n",
      "test loss on epoch 3717: 0.336\n",
      "test accuracy on epoch 3717: 0.769\n",
      "train loss on epoch 3718 : 0.089\n",
      "train accuracy on epoch 3718: 0.944\n",
      "test loss on epoch 3718: 0.339\n",
      "test accuracy on epoch 3718: 0.769\n",
      "train loss on epoch 3719 : 0.101\n",
      "train accuracy on epoch 3719: 1.000\n",
      "test loss on epoch 3719: 0.333\n",
      "test accuracy on epoch 3719: 0.769\n",
      "train loss on epoch 3720 : 0.296\n",
      "train accuracy on epoch 3720: 0.889\n",
      "test loss on epoch 3720: 0.327\n",
      "test accuracy on epoch 3720: 0.769\n",
      "train loss on epoch 3721 : 0.107\n",
      "train accuracy on epoch 3721: 0.944\n",
      "test loss on epoch 3721: 0.339\n",
      "test accuracy on epoch 3721: 0.769\n",
      "train loss on epoch 3722 : 0.256\n",
      "train accuracy on epoch 3722: 0.889\n",
      "test loss on epoch 3722: 0.326\n",
      "test accuracy on epoch 3722: 0.769\n",
      "train loss on epoch 3723 : 0.087\n",
      "train accuracy on epoch 3723: 1.000\n",
      "test loss on epoch 3723: 0.327\n",
      "test accuracy on epoch 3723: 0.846\n",
      "train loss on epoch 3724 : 0.039\n",
      "train accuracy on epoch 3724: 1.000\n",
      "test loss on epoch 3724: 0.331\n",
      "test accuracy on epoch 3724: 0.769\n",
      "train loss on epoch 3725 : 0.093\n",
      "train accuracy on epoch 3725: 0.944\n",
      "test loss on epoch 3725: 0.330\n",
      "test accuracy on epoch 3725: 0.846\n",
      "train loss on epoch 3726 : 0.194\n",
      "train accuracy on epoch 3726: 0.889\n",
      "test loss on epoch 3726: 0.327\n",
      "test accuracy on epoch 3726: 0.846\n",
      "train loss on epoch 3727 : 0.092\n",
      "train accuracy on epoch 3727: 0.944\n",
      "test loss on epoch 3727: 0.339\n",
      "test accuracy on epoch 3727: 0.769\n",
      "train loss on epoch 3728 : 0.127\n",
      "train accuracy on epoch 3728: 0.944\n",
      "test loss on epoch 3728: 0.338\n",
      "test accuracy on epoch 3728: 0.769\n",
      "train loss on epoch 3729 : 0.156\n",
      "train accuracy on epoch 3729: 0.889\n",
      "test loss on epoch 3729: 0.331\n",
      "test accuracy on epoch 3729: 0.769\n",
      "train loss on epoch 3730 : 0.077\n",
      "train accuracy on epoch 3730: 1.000\n",
      "test loss on epoch 3730: 0.341\n",
      "test accuracy on epoch 3730: 0.769\n",
      "train loss on epoch 3731 : 0.349\n",
      "train accuracy on epoch 3731: 0.833\n",
      "test loss on epoch 3731: 0.341\n",
      "test accuracy on epoch 3731: 0.769\n",
      "train loss on epoch 3732 : 0.200\n",
      "train accuracy on epoch 3732: 0.889\n",
      "test loss on epoch 3732: 0.343\n",
      "test accuracy on epoch 3732: 0.769\n",
      "train loss on epoch 3733 : 0.110\n",
      "train accuracy on epoch 3733: 0.944\n",
      "test loss on epoch 3733: 0.346\n",
      "test accuracy on epoch 3733: 0.769\n",
      "train loss on epoch 3734 : 0.538\n",
      "train accuracy on epoch 3734: 0.778\n",
      "test loss on epoch 3734: 0.356\n",
      "test accuracy on epoch 3734: 0.769\n",
      "train loss on epoch 3735 : 0.165\n",
      "train accuracy on epoch 3735: 0.944\n",
      "test loss on epoch 3735: 0.351\n",
      "test accuracy on epoch 3735: 0.769\n",
      "train loss on epoch 3736 : 0.058\n",
      "train accuracy on epoch 3736: 1.000\n",
      "test loss on epoch 3736: 0.343\n",
      "test accuracy on epoch 3736: 0.769\n",
      "train loss on epoch 3737 : 0.185\n",
      "train accuracy on epoch 3737: 0.944\n",
      "test loss on epoch 3737: 0.351\n",
      "test accuracy on epoch 3737: 0.769\n",
      "train loss on epoch 3738 : 0.215\n",
      "train accuracy on epoch 3738: 0.889\n",
      "test loss on epoch 3738: 0.338\n",
      "test accuracy on epoch 3738: 0.769\n",
      "train loss on epoch 3739 : 0.098\n",
      "train accuracy on epoch 3739: 1.000\n",
      "test loss on epoch 3739: 0.339\n",
      "test accuracy on epoch 3739: 0.769\n",
      "train loss on epoch 3740 : 0.050\n",
      "train accuracy on epoch 3740: 1.000\n",
      "test loss on epoch 3740: 0.337\n",
      "test accuracy on epoch 3740: 0.769\n",
      "train loss on epoch 3741 : 0.290\n",
      "train accuracy on epoch 3741: 0.944\n",
      "test loss on epoch 3741: 0.345\n",
      "test accuracy on epoch 3741: 0.769\n",
      "train loss on epoch 3742 : 0.063\n",
      "train accuracy on epoch 3742: 1.000\n",
      "test loss on epoch 3742: 0.341\n",
      "test accuracy on epoch 3742: 0.769\n",
      "train loss on epoch 3743 : 0.131\n",
      "train accuracy on epoch 3743: 0.944\n",
      "test loss on epoch 3743: 0.343\n",
      "test accuracy on epoch 3743: 0.769\n",
      "train loss on epoch 3744 : 0.169\n",
      "train accuracy on epoch 3744: 0.889\n",
      "test loss on epoch 3744: 0.346\n",
      "test accuracy on epoch 3744: 0.769\n",
      "train loss on epoch 3745 : 0.202\n",
      "train accuracy on epoch 3745: 0.944\n",
      "test loss on epoch 3745: 0.330\n",
      "test accuracy on epoch 3745: 0.846\n",
      "train loss on epoch 3746 : 0.078\n",
      "train accuracy on epoch 3746: 1.000\n",
      "test loss on epoch 3746: 0.335\n",
      "test accuracy on epoch 3746: 0.769\n",
      "train loss on epoch 3747 : 0.153\n",
      "train accuracy on epoch 3747: 0.889\n",
      "test loss on epoch 3747: 0.343\n",
      "test accuracy on epoch 3747: 0.692\n",
      "train loss on epoch 3748 : 0.127\n",
      "train accuracy on epoch 3748: 0.944\n",
      "test loss on epoch 3748: 0.337\n",
      "test accuracy on epoch 3748: 0.769\n",
      "train loss on epoch 3749 : 0.152\n",
      "train accuracy on epoch 3749: 0.889\n",
      "test loss on epoch 3749: 0.332\n",
      "test accuracy on epoch 3749: 0.769\n",
      "train loss on epoch 3750 : 0.203\n",
      "train accuracy on epoch 3750: 0.944\n",
      "test loss on epoch 3750: 0.329\n",
      "test accuracy on epoch 3750: 0.846\n",
      "train loss on epoch 3751 : 0.186\n",
      "train accuracy on epoch 3751: 0.944\n",
      "test loss on epoch 3751: 0.340\n",
      "test accuracy on epoch 3751: 0.769\n",
      "train loss on epoch 3752 : 0.206\n",
      "train accuracy on epoch 3752: 0.944\n",
      "test loss on epoch 3752: 0.337\n",
      "test accuracy on epoch 3752: 0.769\n",
      "train loss on epoch 3753 : 0.148\n",
      "train accuracy on epoch 3753: 0.889\n",
      "test loss on epoch 3753: 0.335\n",
      "test accuracy on epoch 3753: 0.769\n",
      "train loss on epoch 3754 : 0.050\n",
      "train accuracy on epoch 3754: 1.000\n",
      "test loss on epoch 3754: 0.338\n",
      "test accuracy on epoch 3754: 0.769\n",
      "train loss on epoch 3755 : 0.276\n",
      "train accuracy on epoch 3755: 0.889\n",
      "test loss on epoch 3755: 0.344\n",
      "test accuracy on epoch 3755: 0.769\n",
      "train loss on epoch 3756 : 0.363\n",
      "train accuracy on epoch 3756: 0.889\n",
      "test loss on epoch 3756: 0.334\n",
      "test accuracy on epoch 3756: 0.769\n",
      "train loss on epoch 3757 : 0.068\n",
      "train accuracy on epoch 3757: 1.000\n",
      "test loss on epoch 3757: 0.338\n",
      "test accuracy on epoch 3757: 0.769\n",
      "train loss on epoch 3758 : 0.132\n",
      "train accuracy on epoch 3758: 0.944\n",
      "test loss on epoch 3758: 0.327\n",
      "test accuracy on epoch 3758: 0.846\n",
      "train loss on epoch 3759 : 0.101\n",
      "train accuracy on epoch 3759: 0.944\n",
      "test loss on epoch 3759: 0.340\n",
      "test accuracy on epoch 3759: 0.692\n",
      "train loss on epoch 3760 : 0.096\n",
      "train accuracy on epoch 3760: 0.944\n",
      "test loss on epoch 3760: 0.331\n",
      "test accuracy on epoch 3760: 0.769\n",
      "train loss on epoch 3761 : 0.083\n",
      "train accuracy on epoch 3761: 1.000\n",
      "test loss on epoch 3761: 0.329\n",
      "test accuracy on epoch 3761: 0.846\n",
      "train loss on epoch 3762 : 0.088\n",
      "train accuracy on epoch 3762: 1.000\n",
      "test loss on epoch 3762: 0.335\n",
      "test accuracy on epoch 3762: 0.692\n",
      "train loss on epoch 3763 : 0.090\n",
      "train accuracy on epoch 3763: 0.944\n",
      "test loss on epoch 3763: 0.339\n",
      "test accuracy on epoch 3763: 0.692\n",
      "train loss on epoch 3764 : 0.046\n",
      "train accuracy on epoch 3764: 1.000\n",
      "test loss on epoch 3764: 0.332\n",
      "test accuracy on epoch 3764: 0.769\n",
      "train loss on epoch 3765 : 0.145\n",
      "train accuracy on epoch 3765: 0.944\n",
      "test loss on epoch 3765: 0.339\n",
      "test accuracy on epoch 3765: 0.769\n",
      "train loss on epoch 3766 : 0.117\n",
      "train accuracy on epoch 3766: 0.944\n",
      "test loss on epoch 3766: 0.344\n",
      "test accuracy on epoch 3766: 0.769\n",
      "train loss on epoch 3767 : 0.468\n",
      "train accuracy on epoch 3767: 0.833\n",
      "test loss on epoch 3767: 0.346\n",
      "test accuracy on epoch 3767: 0.769\n",
      "train loss on epoch 3768 : 0.235\n",
      "train accuracy on epoch 3768: 0.944\n",
      "test loss on epoch 3768: 0.342\n",
      "test accuracy on epoch 3768: 0.769\n",
      "train loss on epoch 3769 : 0.109\n",
      "train accuracy on epoch 3769: 0.944\n",
      "test loss on epoch 3769: 0.342\n",
      "test accuracy on epoch 3769: 0.769\n",
      "train loss on epoch 3770 : 0.096\n",
      "train accuracy on epoch 3770: 0.944\n",
      "test loss on epoch 3770: 0.343\n",
      "test accuracy on epoch 3770: 0.769\n",
      "train loss on epoch 3771 : 0.132\n",
      "train accuracy on epoch 3771: 0.944\n",
      "test loss on epoch 3771: 0.339\n",
      "test accuracy on epoch 3771: 0.769\n",
      "train loss on epoch 3772 : 0.222\n",
      "train accuracy on epoch 3772: 0.889\n",
      "test loss on epoch 3772: 0.340\n",
      "test accuracy on epoch 3772: 0.769\n",
      "train loss on epoch 3773 : 0.124\n",
      "train accuracy on epoch 3773: 0.944\n",
      "test loss on epoch 3773: 0.335\n",
      "test accuracy on epoch 3773: 0.769\n",
      "train loss on epoch 3774 : 0.079\n",
      "train accuracy on epoch 3774: 1.000\n",
      "test loss on epoch 3774: 0.333\n",
      "test accuracy on epoch 3774: 0.769\n",
      "train loss on epoch 3775 : 0.213\n",
      "train accuracy on epoch 3775: 0.889\n",
      "test loss on epoch 3775: 0.337\n",
      "test accuracy on epoch 3775: 0.769\n",
      "train loss on epoch 3776 : 0.041\n",
      "train accuracy on epoch 3776: 1.000\n",
      "test loss on epoch 3776: 0.334\n",
      "test accuracy on epoch 3776: 0.769\n",
      "train loss on epoch 3777 : 0.266\n",
      "train accuracy on epoch 3777: 0.889\n",
      "test loss on epoch 3777: 0.331\n",
      "test accuracy on epoch 3777: 0.769\n",
      "train loss on epoch 3778 : 0.088\n",
      "train accuracy on epoch 3778: 1.000\n",
      "test loss on epoch 3778: 0.335\n",
      "test accuracy on epoch 3778: 0.769\n",
      "train loss on epoch 3779 : 0.051\n",
      "train accuracy on epoch 3779: 1.000\n",
      "test loss on epoch 3779: 0.340\n",
      "test accuracy on epoch 3779: 0.769\n",
      "train loss on epoch 3780 : 0.171\n",
      "train accuracy on epoch 3780: 0.889\n",
      "test loss on epoch 3780: 0.347\n",
      "test accuracy on epoch 3780: 0.769\n",
      "train loss on epoch 3781 : 0.133\n",
      "train accuracy on epoch 3781: 0.944\n",
      "test loss on epoch 3781: 0.344\n",
      "test accuracy on epoch 3781: 0.769\n",
      "train loss on epoch 3782 : 0.185\n",
      "train accuracy on epoch 3782: 0.889\n",
      "test loss on epoch 3782: 0.345\n",
      "test accuracy on epoch 3782: 0.769\n",
      "train loss on epoch 3783 : 0.117\n",
      "train accuracy on epoch 3783: 0.944\n",
      "test loss on epoch 3783: 0.348\n",
      "test accuracy on epoch 3783: 0.769\n",
      "train loss on epoch 3784 : 0.117\n",
      "train accuracy on epoch 3784: 0.889\n",
      "test loss on epoch 3784: 0.339\n",
      "test accuracy on epoch 3784: 0.769\n",
      "train loss on epoch 3785 : 0.190\n",
      "train accuracy on epoch 3785: 0.889\n",
      "test loss on epoch 3785: 0.332\n",
      "test accuracy on epoch 3785: 0.769\n",
      "train loss on epoch 3786 : 0.198\n",
      "train accuracy on epoch 3786: 0.944\n",
      "test loss on epoch 3786: 0.339\n",
      "test accuracy on epoch 3786: 0.769\n",
      "train loss on epoch 3787 : 0.405\n",
      "train accuracy on epoch 3787: 0.944\n",
      "test loss on epoch 3787: 0.332\n",
      "test accuracy on epoch 3787: 0.769\n",
      "train loss on epoch 3788 : 0.219\n",
      "train accuracy on epoch 3788: 0.944\n",
      "test loss on epoch 3788: 0.338\n",
      "test accuracy on epoch 3788: 0.769\n",
      "train loss on epoch 3789 : 0.107\n",
      "train accuracy on epoch 3789: 0.944\n",
      "test loss on epoch 3789: 0.346\n",
      "test accuracy on epoch 3789: 0.769\n",
      "train loss on epoch 3790 : 0.294\n",
      "train accuracy on epoch 3790: 0.889\n",
      "test loss on epoch 3790: 0.347\n",
      "test accuracy on epoch 3790: 0.769\n",
      "train loss on epoch 3791 : 0.158\n",
      "train accuracy on epoch 3791: 0.889\n",
      "test loss on epoch 3791: 0.341\n",
      "test accuracy on epoch 3791: 0.769\n",
      "train loss on epoch 3792 : 0.081\n",
      "train accuracy on epoch 3792: 0.944\n",
      "test loss on epoch 3792: 0.343\n",
      "test accuracy on epoch 3792: 0.769\n",
      "train loss on epoch 3793 : 0.131\n",
      "train accuracy on epoch 3793: 0.944\n",
      "test loss on epoch 3793: 0.339\n",
      "test accuracy on epoch 3793: 0.769\n",
      "train loss on epoch 3794 : 0.093\n",
      "train accuracy on epoch 3794: 0.944\n",
      "test loss on epoch 3794: 0.337\n",
      "test accuracy on epoch 3794: 0.769\n",
      "train loss on epoch 3795 : 0.214\n",
      "train accuracy on epoch 3795: 0.944\n",
      "test loss on epoch 3795: 0.338\n",
      "test accuracy on epoch 3795: 0.769\n",
      "train loss on epoch 3796 : 0.346\n",
      "train accuracy on epoch 3796: 0.944\n",
      "test loss on epoch 3796: 0.328\n",
      "test accuracy on epoch 3796: 0.769\n",
      "train loss on epoch 3797 : 0.133\n",
      "train accuracy on epoch 3797: 0.944\n",
      "test loss on epoch 3797: 0.332\n",
      "test accuracy on epoch 3797: 0.692\n",
      "train loss on epoch 3798 : 0.158\n",
      "train accuracy on epoch 3798: 0.889\n",
      "test loss on epoch 3798: 0.326\n",
      "test accuracy on epoch 3798: 0.769\n",
      "train loss on epoch 3799 : 0.191\n",
      "train accuracy on epoch 3799: 0.889\n",
      "test loss on epoch 3799: 0.323\n",
      "test accuracy on epoch 3799: 0.846\n",
      "train loss on epoch 3800 : 0.106\n",
      "train accuracy on epoch 3800: 0.944\n",
      "test loss on epoch 3800: 0.331\n",
      "test accuracy on epoch 3800: 0.769\n",
      "train loss on epoch 3801 : 0.218\n",
      "train accuracy on epoch 3801: 0.889\n",
      "test loss on epoch 3801: 0.332\n",
      "test accuracy on epoch 3801: 0.769\n",
      "train loss on epoch 3802 : 0.107\n",
      "train accuracy on epoch 3802: 0.944\n",
      "test loss on epoch 3802: 0.327\n",
      "test accuracy on epoch 3802: 0.769\n",
      "train loss on epoch 3803 : 0.120\n",
      "train accuracy on epoch 3803: 0.944\n",
      "test loss on epoch 3803: 0.335\n",
      "test accuracy on epoch 3803: 0.769\n",
      "train loss on epoch 3804 : 0.075\n",
      "train accuracy on epoch 3804: 0.944\n",
      "test loss on epoch 3804: 0.339\n",
      "test accuracy on epoch 3804: 0.769\n",
      "train loss on epoch 3805 : 0.337\n",
      "train accuracy on epoch 3805: 0.889\n",
      "test loss on epoch 3805: 0.336\n",
      "test accuracy on epoch 3805: 0.769\n",
      "train loss on epoch 3806 : 0.152\n",
      "train accuracy on epoch 3806: 0.944\n",
      "test loss on epoch 3806: 0.337\n",
      "test accuracy on epoch 3806: 0.769\n",
      "train loss on epoch 3807 : 0.205\n",
      "train accuracy on epoch 3807: 0.944\n",
      "test loss on epoch 3807: 0.344\n",
      "test accuracy on epoch 3807: 0.769\n",
      "train loss on epoch 3808 : 0.342\n",
      "train accuracy on epoch 3808: 0.889\n",
      "test loss on epoch 3808: 0.343\n",
      "test accuracy on epoch 3808: 0.769\n",
      "train loss on epoch 3809 : 0.082\n",
      "train accuracy on epoch 3809: 0.944\n",
      "test loss on epoch 3809: 0.349\n",
      "test accuracy on epoch 3809: 0.769\n",
      "train loss on epoch 3810 : 0.071\n",
      "train accuracy on epoch 3810: 1.000\n",
      "test loss on epoch 3810: 0.355\n",
      "test accuracy on epoch 3810: 0.769\n",
      "train loss on epoch 3811 : 0.097\n",
      "train accuracy on epoch 3811: 0.944\n",
      "test loss on epoch 3811: 0.355\n",
      "test accuracy on epoch 3811: 0.769\n",
      "train loss on epoch 3812 : 0.170\n",
      "train accuracy on epoch 3812: 0.944\n",
      "test loss on epoch 3812: 0.349\n",
      "test accuracy on epoch 3812: 0.769\n",
      "train loss on epoch 3813 : 0.265\n",
      "train accuracy on epoch 3813: 0.944\n",
      "test loss on epoch 3813: 0.346\n",
      "test accuracy on epoch 3813: 0.769\n",
      "train loss on epoch 3814 : 0.105\n",
      "train accuracy on epoch 3814: 0.944\n",
      "test loss on epoch 3814: 0.346\n",
      "test accuracy on epoch 3814: 0.769\n",
      "train loss on epoch 3815 : 0.150\n",
      "train accuracy on epoch 3815: 0.944\n",
      "test loss on epoch 3815: 0.339\n",
      "test accuracy on epoch 3815: 0.769\n",
      "train loss on epoch 3816 : 0.154\n",
      "train accuracy on epoch 3816: 0.944\n",
      "test loss on epoch 3816: 0.340\n",
      "test accuracy on epoch 3816: 0.769\n",
      "train loss on epoch 3817 : 0.312\n",
      "train accuracy on epoch 3817: 0.889\n",
      "test loss on epoch 3817: 0.343\n",
      "test accuracy on epoch 3817: 0.769\n",
      "train loss on epoch 3818 : 0.111\n",
      "train accuracy on epoch 3818: 0.944\n",
      "test loss on epoch 3818: 0.350\n",
      "test accuracy on epoch 3818: 0.769\n",
      "train loss on epoch 3819 : 0.048\n",
      "train accuracy on epoch 3819: 1.000\n",
      "test loss on epoch 3819: 0.356\n",
      "test accuracy on epoch 3819: 0.769\n",
      "train loss on epoch 3820 : 0.262\n",
      "train accuracy on epoch 3820: 0.889\n",
      "test loss on epoch 3820: 0.348\n",
      "test accuracy on epoch 3820: 0.769\n",
      "train loss on epoch 3821 : 0.058\n",
      "train accuracy on epoch 3821: 1.000\n",
      "test loss on epoch 3821: 0.346\n",
      "test accuracy on epoch 3821: 0.769\n",
      "train loss on epoch 3822 : 0.168\n",
      "train accuracy on epoch 3822: 0.889\n",
      "test loss on epoch 3822: 0.339\n",
      "test accuracy on epoch 3822: 0.769\n",
      "train loss on epoch 3823 : 0.314\n",
      "train accuracy on epoch 3823: 0.833\n",
      "test loss on epoch 3823: 0.335\n",
      "test accuracy on epoch 3823: 0.769\n",
      "train loss on epoch 3824 : 0.208\n",
      "train accuracy on epoch 3824: 0.889\n",
      "test loss on epoch 3824: 0.333\n",
      "test accuracy on epoch 3824: 0.769\n",
      "train loss on epoch 3825 : 0.222\n",
      "train accuracy on epoch 3825: 0.944\n",
      "test loss on epoch 3825: 0.331\n",
      "test accuracy on epoch 3825: 0.769\n",
      "train loss on epoch 3826 : 0.288\n",
      "train accuracy on epoch 3826: 0.833\n",
      "test loss on epoch 3826: 0.330\n",
      "test accuracy on epoch 3826: 0.769\n",
      "train loss on epoch 3827 : 0.188\n",
      "train accuracy on epoch 3827: 0.944\n",
      "test loss on epoch 3827: 0.324\n",
      "test accuracy on epoch 3827: 0.769\n",
      "train loss on epoch 3828 : 0.099\n",
      "train accuracy on epoch 3828: 0.944\n",
      "test loss on epoch 3828: 0.339\n",
      "test accuracy on epoch 3828: 0.692\n",
      "train loss on epoch 3829 : 0.098\n",
      "train accuracy on epoch 3829: 1.000\n",
      "test loss on epoch 3829: 0.329\n",
      "test accuracy on epoch 3829: 0.769\n",
      "train loss on epoch 3830 : 0.342\n",
      "train accuracy on epoch 3830: 0.833\n",
      "test loss on epoch 3830: 0.330\n",
      "test accuracy on epoch 3830: 0.769\n",
      "train loss on epoch 3831 : 0.351\n",
      "train accuracy on epoch 3831: 0.889\n",
      "test loss on epoch 3831: 0.343\n",
      "test accuracy on epoch 3831: 0.769\n",
      "train loss on epoch 3832 : 0.160\n",
      "train accuracy on epoch 3832: 0.889\n",
      "test loss on epoch 3832: 0.328\n",
      "test accuracy on epoch 3832: 0.846\n",
      "train loss on epoch 3833 : 0.273\n",
      "train accuracy on epoch 3833: 0.889\n",
      "test loss on epoch 3833: 0.340\n",
      "test accuracy on epoch 3833: 0.692\n",
      "train loss on epoch 3834 : 0.229\n",
      "train accuracy on epoch 3834: 0.833\n",
      "test loss on epoch 3834: 0.340\n",
      "test accuracy on epoch 3834: 0.769\n",
      "train loss on epoch 3835 : 0.194\n",
      "train accuracy on epoch 3835: 0.944\n",
      "test loss on epoch 3835: 0.347\n",
      "test accuracy on epoch 3835: 0.769\n",
      "train loss on epoch 3836 : 0.320\n",
      "train accuracy on epoch 3836: 0.889\n",
      "test loss on epoch 3836: 0.349\n",
      "test accuracy on epoch 3836: 0.769\n",
      "train loss on epoch 3837 : 0.388\n",
      "train accuracy on epoch 3837: 0.833\n",
      "test loss on epoch 3837: 0.346\n",
      "test accuracy on epoch 3837: 0.769\n",
      "train loss on epoch 3838 : 0.149\n",
      "train accuracy on epoch 3838: 0.944\n",
      "test loss on epoch 3838: 0.343\n",
      "test accuracy on epoch 3838: 0.769\n",
      "train loss on epoch 3839 : 0.183\n",
      "train accuracy on epoch 3839: 0.944\n",
      "test loss on epoch 3839: 0.333\n",
      "test accuracy on epoch 3839: 0.769\n",
      "train loss on epoch 3840 : 0.156\n",
      "train accuracy on epoch 3840: 0.944\n",
      "test loss on epoch 3840: 0.337\n",
      "test accuracy on epoch 3840: 0.769\n",
      "train loss on epoch 3841 : 0.119\n",
      "train accuracy on epoch 3841: 0.944\n",
      "test loss on epoch 3841: 0.335\n",
      "test accuracy on epoch 3841: 0.846\n",
      "train loss on epoch 3842 : 0.211\n",
      "train accuracy on epoch 3842: 0.889\n",
      "test loss on epoch 3842: 0.340\n",
      "test accuracy on epoch 3842: 0.769\n",
      "train loss on epoch 3843 : 0.129\n",
      "train accuracy on epoch 3843: 1.000\n",
      "test loss on epoch 3843: 0.331\n",
      "test accuracy on epoch 3843: 0.846\n",
      "train loss on epoch 3844 : 0.182\n",
      "train accuracy on epoch 3844: 0.889\n",
      "test loss on epoch 3844: 0.343\n",
      "test accuracy on epoch 3844: 0.769\n",
      "train loss on epoch 3845 : 0.285\n",
      "train accuracy on epoch 3845: 0.833\n",
      "test loss on epoch 3845: 0.335\n",
      "test accuracy on epoch 3845: 0.846\n",
      "train loss on epoch 3846 : 0.266\n",
      "train accuracy on epoch 3846: 0.889\n",
      "test loss on epoch 3846: 0.341\n",
      "test accuracy on epoch 3846: 0.769\n",
      "train loss on epoch 3847 : 0.232\n",
      "train accuracy on epoch 3847: 0.889\n",
      "test loss on epoch 3847: 0.339\n",
      "test accuracy on epoch 3847: 0.846\n",
      "train loss on epoch 3848 : 0.187\n",
      "train accuracy on epoch 3848: 0.944\n",
      "test loss on epoch 3848: 0.352\n",
      "test accuracy on epoch 3848: 0.769\n",
      "train loss on epoch 3849 : 0.403\n",
      "train accuracy on epoch 3849: 0.833\n",
      "test loss on epoch 3849: 0.362\n",
      "test accuracy on epoch 3849: 0.769\n",
      "train loss on epoch 3850 : 0.355\n",
      "train accuracy on epoch 3850: 0.833\n",
      "test loss on epoch 3850: 0.355\n",
      "test accuracy on epoch 3850: 0.769\n",
      "train loss on epoch 3851 : 0.128\n",
      "train accuracy on epoch 3851: 0.944\n",
      "test loss on epoch 3851: 0.349\n",
      "test accuracy on epoch 3851: 0.769\n",
      "train loss on epoch 3852 : 0.282\n",
      "train accuracy on epoch 3852: 0.889\n",
      "test loss on epoch 3852: 0.354\n",
      "test accuracy on epoch 3852: 0.769\n",
      "train loss on epoch 3853 : 0.127\n",
      "train accuracy on epoch 3853: 0.889\n",
      "test loss on epoch 3853: 0.359\n",
      "test accuracy on epoch 3853: 0.769\n",
      "train loss on epoch 3854 : 0.061\n",
      "train accuracy on epoch 3854: 1.000\n",
      "test loss on epoch 3854: 0.345\n",
      "test accuracy on epoch 3854: 0.846\n",
      "train loss on epoch 3855 : 0.080\n",
      "train accuracy on epoch 3855: 1.000\n",
      "test loss on epoch 3855: 0.340\n",
      "test accuracy on epoch 3855: 0.846\n",
      "train loss on epoch 3856 : 0.222\n",
      "train accuracy on epoch 3856: 0.944\n",
      "test loss on epoch 3856: 0.359\n",
      "test accuracy on epoch 3856: 0.692\n",
      "train loss on epoch 3857 : 0.094\n",
      "train accuracy on epoch 3857: 0.944\n",
      "test loss on epoch 3857: 0.356\n",
      "test accuracy on epoch 3857: 0.769\n",
      "train loss on epoch 3858 : 0.249\n",
      "train accuracy on epoch 3858: 0.889\n",
      "test loss on epoch 3858: 0.358\n",
      "test accuracy on epoch 3858: 0.769\n",
      "train loss on epoch 3859 : 0.213\n",
      "train accuracy on epoch 3859: 0.889\n",
      "test loss on epoch 3859: 0.358\n",
      "test accuracy on epoch 3859: 0.769\n",
      "train loss on epoch 3860 : 0.258\n",
      "train accuracy on epoch 3860: 0.944\n",
      "test loss on epoch 3860: 0.350\n",
      "test accuracy on epoch 3860: 0.769\n",
      "train loss on epoch 3861 : 0.171\n",
      "train accuracy on epoch 3861: 0.944\n",
      "test loss on epoch 3861: 0.354\n",
      "test accuracy on epoch 3861: 0.769\n",
      "train loss on epoch 3862 : 0.157\n",
      "train accuracy on epoch 3862: 0.944\n",
      "test loss on epoch 3862: 0.353\n",
      "test accuracy on epoch 3862: 0.769\n",
      "train loss on epoch 3863 : 0.280\n",
      "train accuracy on epoch 3863: 0.889\n",
      "test loss on epoch 3863: 0.334\n",
      "test accuracy on epoch 3863: 0.769\n",
      "train loss on epoch 3864 : 0.269\n",
      "train accuracy on epoch 3864: 0.889\n",
      "test loss on epoch 3864: 0.351\n",
      "test accuracy on epoch 3864: 0.769\n",
      "train loss on epoch 3865 : 0.080\n",
      "train accuracy on epoch 3865: 1.000\n",
      "test loss on epoch 3865: 0.361\n",
      "test accuracy on epoch 3865: 0.769\n",
      "train loss on epoch 3866 : 0.138\n",
      "train accuracy on epoch 3866: 0.944\n",
      "test loss on epoch 3866: 0.356\n",
      "test accuracy on epoch 3866: 0.769\n",
      "train loss on epoch 3867 : 0.042\n",
      "train accuracy on epoch 3867: 1.000\n",
      "test loss on epoch 3867: 0.356\n",
      "test accuracy on epoch 3867: 0.769\n",
      "train loss on epoch 3868 : 0.134\n",
      "train accuracy on epoch 3868: 0.944\n",
      "test loss on epoch 3868: 0.363\n",
      "test accuracy on epoch 3868: 0.769\n",
      "train loss on epoch 3869 : 0.425\n",
      "train accuracy on epoch 3869: 0.833\n",
      "test loss on epoch 3869: 0.354\n",
      "test accuracy on epoch 3869: 0.769\n",
      "train loss on epoch 3870 : 0.269\n",
      "train accuracy on epoch 3870: 0.889\n",
      "test loss on epoch 3870: 0.362\n",
      "test accuracy on epoch 3870: 0.769\n",
      "train loss on epoch 3871 : 0.500\n",
      "train accuracy on epoch 3871: 0.833\n",
      "test loss on epoch 3871: 0.354\n",
      "test accuracy on epoch 3871: 0.769\n",
      "train loss on epoch 3872 : 0.253\n",
      "train accuracy on epoch 3872: 0.833\n",
      "test loss on epoch 3872: 0.356\n",
      "test accuracy on epoch 3872: 0.769\n",
      "train loss on epoch 3873 : 0.387\n",
      "train accuracy on epoch 3873: 0.889\n",
      "test loss on epoch 3873: 0.367\n",
      "test accuracy on epoch 3873: 0.769\n",
      "train loss on epoch 3874 : 0.181\n",
      "train accuracy on epoch 3874: 0.833\n",
      "test loss on epoch 3874: 0.354\n",
      "test accuracy on epoch 3874: 0.769\n",
      "train loss on epoch 3875 : 0.274\n",
      "train accuracy on epoch 3875: 0.889\n",
      "test loss on epoch 3875: 0.366\n",
      "test accuracy on epoch 3875: 0.769\n",
      "train loss on epoch 3876 : 0.166\n",
      "train accuracy on epoch 3876: 0.944\n",
      "test loss on epoch 3876: 0.362\n",
      "test accuracy on epoch 3876: 0.769\n",
      "train loss on epoch 3877 : 0.182\n",
      "train accuracy on epoch 3877: 0.889\n",
      "test loss on epoch 3877: 0.359\n",
      "test accuracy on epoch 3877: 0.769\n",
      "train loss on epoch 3878 : 0.218\n",
      "train accuracy on epoch 3878: 0.889\n",
      "test loss on epoch 3878: 0.351\n",
      "test accuracy on epoch 3878: 0.769\n",
      "train loss on epoch 3879 : 0.110\n",
      "train accuracy on epoch 3879: 1.000\n",
      "test loss on epoch 3879: 0.356\n",
      "test accuracy on epoch 3879: 0.769\n",
      "train loss on epoch 3880 : 0.069\n",
      "train accuracy on epoch 3880: 1.000\n",
      "test loss on epoch 3880: 0.358\n",
      "test accuracy on epoch 3880: 0.769\n",
      "train loss on epoch 3881 : 0.217\n",
      "train accuracy on epoch 3881: 0.944\n",
      "test loss on epoch 3881: 0.363\n",
      "test accuracy on epoch 3881: 0.769\n",
      "train loss on epoch 3882 : 0.200\n",
      "train accuracy on epoch 3882: 0.944\n",
      "test loss on epoch 3882: 0.352\n",
      "test accuracy on epoch 3882: 0.769\n",
      "train loss on epoch 3883 : 0.168\n",
      "train accuracy on epoch 3883: 0.889\n",
      "test loss on epoch 3883: 0.346\n",
      "test accuracy on epoch 3883: 0.769\n",
      "train loss on epoch 3884 : 0.148\n",
      "train accuracy on epoch 3884: 0.944\n",
      "test loss on epoch 3884: 0.347\n",
      "test accuracy on epoch 3884: 0.769\n",
      "train loss on epoch 3885 : 0.495\n",
      "train accuracy on epoch 3885: 0.833\n",
      "test loss on epoch 3885: 0.343\n",
      "test accuracy on epoch 3885: 0.769\n",
      "train loss on epoch 3886 : 0.161\n",
      "train accuracy on epoch 3886: 0.944\n",
      "test loss on epoch 3886: 0.351\n",
      "test accuracy on epoch 3886: 0.769\n",
      "train loss on epoch 3887 : 0.100\n",
      "train accuracy on epoch 3887: 1.000\n",
      "test loss on epoch 3887: 0.366\n",
      "test accuracy on epoch 3887: 0.769\n",
      "train loss on epoch 3888 : 0.237\n",
      "train accuracy on epoch 3888: 0.833\n",
      "test loss on epoch 3888: 0.360\n",
      "test accuracy on epoch 3888: 0.769\n",
      "train loss on epoch 3889 : 0.039\n",
      "train accuracy on epoch 3889: 1.000\n",
      "test loss on epoch 3889: 0.369\n",
      "test accuracy on epoch 3889: 0.769\n",
      "train loss on epoch 3890 : 0.254\n",
      "train accuracy on epoch 3890: 0.889\n",
      "test loss on epoch 3890: 0.363\n",
      "test accuracy on epoch 3890: 0.769\n",
      "train loss on epoch 3891 : 0.086\n",
      "train accuracy on epoch 3891: 0.944\n",
      "test loss on epoch 3891: 0.355\n",
      "test accuracy on epoch 3891: 0.769\n",
      "train loss on epoch 3892 : 0.213\n",
      "train accuracy on epoch 3892: 0.944\n",
      "test loss on epoch 3892: 0.357\n",
      "test accuracy on epoch 3892: 0.769\n",
      "train loss on epoch 3893 : 0.132\n",
      "train accuracy on epoch 3893: 0.944\n",
      "test loss on epoch 3893: 0.352\n",
      "test accuracy on epoch 3893: 0.769\n",
      "train loss on epoch 3894 : 0.086\n",
      "train accuracy on epoch 3894: 0.944\n",
      "test loss on epoch 3894: 0.358\n",
      "test accuracy on epoch 3894: 0.769\n",
      "train loss on epoch 3895 : 0.174\n",
      "train accuracy on epoch 3895: 0.944\n",
      "test loss on epoch 3895: 0.351\n",
      "test accuracy on epoch 3895: 0.769\n",
      "train loss on epoch 3896 : 0.044\n",
      "train accuracy on epoch 3896: 1.000\n",
      "test loss on epoch 3896: 0.349\n",
      "test accuracy on epoch 3896: 0.769\n",
      "train loss on epoch 3897 : 0.420\n",
      "train accuracy on epoch 3897: 0.889\n",
      "test loss on epoch 3897: 0.352\n",
      "test accuracy on epoch 3897: 0.769\n",
      "train loss on epoch 3898 : 0.334\n",
      "train accuracy on epoch 3898: 0.889\n",
      "test loss on epoch 3898: 0.352\n",
      "test accuracy on epoch 3898: 0.769\n",
      "train loss on epoch 3899 : 0.350\n",
      "train accuracy on epoch 3899: 0.889\n",
      "test loss on epoch 3899: 0.361\n",
      "test accuracy on epoch 3899: 0.769\n",
      "train loss on epoch 3900 : 0.406\n",
      "train accuracy on epoch 3900: 0.889\n",
      "test loss on epoch 3900: 0.363\n",
      "test accuracy on epoch 3900: 0.769\n",
      "train loss on epoch 3901 : 0.184\n",
      "train accuracy on epoch 3901: 0.944\n",
      "test loss on epoch 3901: 0.358\n",
      "test accuracy on epoch 3901: 0.769\n",
      "train loss on epoch 3902 : 0.207\n",
      "train accuracy on epoch 3902: 0.889\n",
      "test loss on epoch 3902: 0.359\n",
      "test accuracy on epoch 3902: 0.769\n",
      "train loss on epoch 3903 : 0.090\n",
      "train accuracy on epoch 3903: 0.944\n",
      "test loss on epoch 3903: 0.356\n",
      "test accuracy on epoch 3903: 0.769\n",
      "train loss on epoch 3904 : 0.177\n",
      "train accuracy on epoch 3904: 0.944\n",
      "test loss on epoch 3904: 0.359\n",
      "test accuracy on epoch 3904: 0.769\n",
      "train loss on epoch 3905 : 0.140\n",
      "train accuracy on epoch 3905: 0.944\n",
      "test loss on epoch 3905: 0.364\n",
      "test accuracy on epoch 3905: 0.769\n",
      "train loss on epoch 3906 : 0.067\n",
      "train accuracy on epoch 3906: 1.000\n",
      "test loss on epoch 3906: 0.360\n",
      "test accuracy on epoch 3906: 0.769\n",
      "train loss on epoch 3907 : 0.068\n",
      "train accuracy on epoch 3907: 1.000\n",
      "test loss on epoch 3907: 0.359\n",
      "test accuracy on epoch 3907: 0.769\n",
      "train loss on epoch 3908 : 0.185\n",
      "train accuracy on epoch 3908: 0.889\n",
      "test loss on epoch 3908: 0.357\n",
      "test accuracy on epoch 3908: 0.769\n",
      "train loss on epoch 3909 : 0.178\n",
      "train accuracy on epoch 3909: 0.889\n",
      "test loss on epoch 3909: 0.354\n",
      "test accuracy on epoch 3909: 0.769\n",
      "train loss on epoch 3910 : 0.245\n",
      "train accuracy on epoch 3910: 0.889\n",
      "test loss on epoch 3910: 0.360\n",
      "test accuracy on epoch 3910: 0.769\n",
      "train loss on epoch 3911 : 0.183\n",
      "train accuracy on epoch 3911: 0.944\n",
      "test loss on epoch 3911: 0.359\n",
      "test accuracy on epoch 3911: 0.769\n",
      "train loss on epoch 3912 : 0.107\n",
      "train accuracy on epoch 3912: 0.944\n",
      "test loss on epoch 3912: 0.358\n",
      "test accuracy on epoch 3912: 0.692\n",
      "train loss on epoch 3913 : 0.116\n",
      "train accuracy on epoch 3913: 0.944\n",
      "test loss on epoch 3913: 0.356\n",
      "test accuracy on epoch 3913: 0.692\n",
      "train loss on epoch 3914 : 0.157\n",
      "train accuracy on epoch 3914: 0.944\n",
      "test loss on epoch 3914: 0.356\n",
      "test accuracy on epoch 3914: 0.769\n",
      "train loss on epoch 3915 : 0.208\n",
      "train accuracy on epoch 3915: 0.944\n",
      "test loss on epoch 3915: 0.356\n",
      "test accuracy on epoch 3915: 0.769\n",
      "train loss on epoch 3916 : 0.326\n",
      "train accuracy on epoch 3916: 0.889\n",
      "test loss on epoch 3916: 0.341\n",
      "test accuracy on epoch 3916: 0.769\n",
      "train loss on epoch 3917 : 0.082\n",
      "train accuracy on epoch 3917: 0.944\n",
      "test loss on epoch 3917: 0.340\n",
      "test accuracy on epoch 3917: 0.769\n",
      "train loss on epoch 3918 : 0.080\n",
      "train accuracy on epoch 3918: 0.944\n",
      "test loss on epoch 3918: 0.359\n",
      "test accuracy on epoch 3918: 0.769\n",
      "train loss on epoch 3919 : 0.281\n",
      "train accuracy on epoch 3919: 0.833\n",
      "test loss on epoch 3919: 0.358\n",
      "test accuracy on epoch 3919: 0.769\n",
      "train loss on epoch 3920 : 0.045\n",
      "train accuracy on epoch 3920: 1.000\n",
      "test loss on epoch 3920: 0.354\n",
      "test accuracy on epoch 3920: 0.769\n",
      "train loss on epoch 3921 : 0.236\n",
      "train accuracy on epoch 3921: 0.889\n",
      "test loss on epoch 3921: 0.350\n",
      "test accuracy on epoch 3921: 0.769\n",
      "train loss on epoch 3922 : 0.232\n",
      "train accuracy on epoch 3922: 0.889\n",
      "test loss on epoch 3922: 0.333\n",
      "test accuracy on epoch 3922: 0.769\n",
      "train loss on epoch 3923 : 0.120\n",
      "train accuracy on epoch 3923: 0.944\n",
      "test loss on epoch 3923: 0.348\n",
      "test accuracy on epoch 3923: 0.769\n",
      "train loss on epoch 3924 : 0.037\n",
      "train accuracy on epoch 3924: 1.000\n",
      "test loss on epoch 3924: 0.344\n",
      "test accuracy on epoch 3924: 0.769\n",
      "train loss on epoch 3925 : 0.162\n",
      "train accuracy on epoch 3925: 0.889\n",
      "test loss on epoch 3925: 0.338\n",
      "test accuracy on epoch 3925: 0.846\n",
      "train loss on epoch 3926 : 0.279\n",
      "train accuracy on epoch 3926: 0.944\n",
      "test loss on epoch 3926: 0.360\n",
      "test accuracy on epoch 3926: 0.692\n",
      "train loss on epoch 3927 : 0.129\n",
      "train accuracy on epoch 3927: 0.944\n",
      "test loss on epoch 3927: 0.352\n",
      "test accuracy on epoch 3927: 0.769\n",
      "train loss on epoch 3928 : 0.184\n",
      "train accuracy on epoch 3928: 0.944\n",
      "test loss on epoch 3928: 0.350\n",
      "test accuracy on epoch 3928: 0.769\n",
      "train loss on epoch 3929 : 0.126\n",
      "train accuracy on epoch 3929: 0.944\n",
      "test loss on epoch 3929: 0.359\n",
      "test accuracy on epoch 3929: 0.692\n",
      "train loss on epoch 3930 : 0.188\n",
      "train accuracy on epoch 3930: 0.889\n",
      "test loss on epoch 3930: 0.356\n",
      "test accuracy on epoch 3930: 0.769\n",
      "train loss on epoch 3931 : 0.093\n",
      "train accuracy on epoch 3931: 0.944\n",
      "test loss on epoch 3931: 0.337\n",
      "test accuracy on epoch 3931: 0.769\n",
      "train loss on epoch 3932 : 0.270\n",
      "train accuracy on epoch 3932: 0.944\n",
      "test loss on epoch 3932: 0.351\n",
      "test accuracy on epoch 3932: 0.769\n",
      "train loss on epoch 3933 : 0.146\n",
      "train accuracy on epoch 3933: 0.944\n",
      "test loss on epoch 3933: 0.345\n",
      "test accuracy on epoch 3933: 0.769\n",
      "train loss on epoch 3934 : 0.103\n",
      "train accuracy on epoch 3934: 0.944\n",
      "test loss on epoch 3934: 0.359\n",
      "test accuracy on epoch 3934: 0.769\n",
      "train loss on epoch 3935 : 0.126\n",
      "train accuracy on epoch 3935: 0.944\n",
      "test loss on epoch 3935: 0.364\n",
      "test accuracy on epoch 3935: 0.769\n",
      "train loss on epoch 3936 : 0.101\n",
      "train accuracy on epoch 3936: 0.944\n",
      "test loss on epoch 3936: 0.340\n",
      "test accuracy on epoch 3936: 0.769\n",
      "train loss on epoch 3937 : 0.036\n",
      "train accuracy on epoch 3937: 1.000\n",
      "test loss on epoch 3937: 0.337\n",
      "test accuracy on epoch 3937: 0.769\n",
      "train loss on epoch 3938 : 0.564\n",
      "train accuracy on epoch 3938: 0.833\n",
      "test loss on epoch 3938: 0.355\n",
      "test accuracy on epoch 3938: 0.769\n",
      "train loss on epoch 3939 : 0.269\n",
      "train accuracy on epoch 3939: 0.833\n",
      "test loss on epoch 3939: 0.354\n",
      "test accuracy on epoch 3939: 0.769\n",
      "train loss on epoch 3940 : 0.026\n",
      "train accuracy on epoch 3940: 1.000\n",
      "test loss on epoch 3940: 0.354\n",
      "test accuracy on epoch 3940: 0.769\n",
      "train loss on epoch 3941 : 0.539\n",
      "train accuracy on epoch 3941: 0.889\n",
      "test loss on epoch 3941: 0.352\n",
      "test accuracy on epoch 3941: 0.769\n",
      "train loss on epoch 3942 : 0.253\n",
      "train accuracy on epoch 3942: 0.889\n",
      "test loss on epoch 3942: 0.339\n",
      "test accuracy on epoch 3942: 0.769\n",
      "train loss on epoch 3943 : 0.290\n",
      "train accuracy on epoch 3943: 0.833\n",
      "test loss on epoch 3943: 0.340\n",
      "test accuracy on epoch 3943: 0.846\n",
      "train loss on epoch 3944 : 0.114\n",
      "train accuracy on epoch 3944: 0.944\n",
      "test loss on epoch 3944: 0.351\n",
      "test accuracy on epoch 3944: 0.769\n",
      "train loss on epoch 3945 : 0.127\n",
      "train accuracy on epoch 3945: 0.944\n",
      "test loss on epoch 3945: 0.360\n",
      "test accuracy on epoch 3945: 0.769\n",
      "train loss on epoch 3946 : 0.069\n",
      "train accuracy on epoch 3946: 1.000\n",
      "test loss on epoch 3946: 0.345\n",
      "test accuracy on epoch 3946: 0.769\n",
      "train loss on epoch 3947 : 0.365\n",
      "train accuracy on epoch 3947: 0.833\n",
      "test loss on epoch 3947: 0.340\n",
      "test accuracy on epoch 3947: 0.769\n",
      "train loss on epoch 3948 : 0.096\n",
      "train accuracy on epoch 3948: 0.944\n",
      "test loss on epoch 3948: 0.356\n",
      "test accuracy on epoch 3948: 0.769\n",
      "train loss on epoch 3949 : 0.277\n",
      "train accuracy on epoch 3949: 0.889\n",
      "test loss on epoch 3949: 0.343\n",
      "test accuracy on epoch 3949: 0.769\n",
      "train loss on epoch 3950 : 0.064\n",
      "train accuracy on epoch 3950: 1.000\n",
      "test loss on epoch 3950: 0.357\n",
      "test accuracy on epoch 3950: 0.769\n",
      "train loss on epoch 3951 : 0.280\n",
      "train accuracy on epoch 3951: 0.889\n",
      "test loss on epoch 3951: 0.355\n",
      "test accuracy on epoch 3951: 0.769\n",
      "train loss on epoch 3952 : 0.078\n",
      "train accuracy on epoch 3952: 1.000\n",
      "test loss on epoch 3952: 0.354\n",
      "test accuracy on epoch 3952: 0.769\n",
      "train loss on epoch 3953 : 0.180\n",
      "train accuracy on epoch 3953: 0.889\n",
      "test loss on epoch 3953: 0.353\n",
      "test accuracy on epoch 3953: 0.769\n",
      "train loss on epoch 3954 : 0.047\n",
      "train accuracy on epoch 3954: 1.000\n",
      "test loss on epoch 3954: 0.357\n",
      "test accuracy on epoch 3954: 0.769\n",
      "train loss on epoch 3955 : 0.303\n",
      "train accuracy on epoch 3955: 0.833\n",
      "test loss on epoch 3955: 0.332\n",
      "test accuracy on epoch 3955: 0.769\n",
      "train loss on epoch 3956 : 0.214\n",
      "train accuracy on epoch 3956: 0.889\n",
      "test loss on epoch 3956: 0.353\n",
      "test accuracy on epoch 3956: 0.769\n",
      "train loss on epoch 3957 : 0.154\n",
      "train accuracy on epoch 3957: 0.889\n",
      "test loss on epoch 3957: 0.341\n",
      "test accuracy on epoch 3957: 0.769\n",
      "train loss on epoch 3958 : 0.111\n",
      "train accuracy on epoch 3958: 0.944\n",
      "test loss on epoch 3958: 0.355\n",
      "test accuracy on epoch 3958: 0.769\n",
      "train loss on epoch 3959 : 0.168\n",
      "train accuracy on epoch 3959: 0.944\n",
      "test loss on epoch 3959: 0.346\n",
      "test accuracy on epoch 3959: 0.769\n",
      "train loss on epoch 3960 : 0.081\n",
      "train accuracy on epoch 3960: 0.944\n",
      "test loss on epoch 3960: 0.334\n",
      "test accuracy on epoch 3960: 0.769\n",
      "train loss on epoch 3961 : 0.112\n",
      "train accuracy on epoch 3961: 0.889\n",
      "test loss on epoch 3961: 0.339\n",
      "test accuracy on epoch 3961: 0.769\n",
      "train loss on epoch 3962 : 0.345\n",
      "train accuracy on epoch 3962: 0.944\n",
      "test loss on epoch 3962: 0.352\n",
      "test accuracy on epoch 3962: 0.769\n",
      "train loss on epoch 3963 : 0.118\n",
      "train accuracy on epoch 3963: 0.944\n",
      "test loss on epoch 3963: 0.356\n",
      "test accuracy on epoch 3963: 0.692\n",
      "train loss on epoch 3964 : 0.141\n",
      "train accuracy on epoch 3964: 0.889\n",
      "test loss on epoch 3964: 0.351\n",
      "test accuracy on epoch 3964: 0.769\n",
      "train loss on epoch 3965 : 0.114\n",
      "train accuracy on epoch 3965: 0.889\n",
      "test loss on epoch 3965: 0.349\n",
      "test accuracy on epoch 3965: 0.769\n",
      "train loss on epoch 3966 : 0.164\n",
      "train accuracy on epoch 3966: 0.889\n",
      "test loss on epoch 3966: 0.362\n",
      "test accuracy on epoch 3966: 0.769\n",
      "train loss on epoch 3967 : 0.187\n",
      "train accuracy on epoch 3967: 0.889\n",
      "test loss on epoch 3967: 0.356\n",
      "test accuracy on epoch 3967: 0.769\n",
      "train loss on epoch 3968 : 0.149\n",
      "train accuracy on epoch 3968: 0.944\n",
      "test loss on epoch 3968: 0.348\n",
      "test accuracy on epoch 3968: 0.769\n",
      "train loss on epoch 3969 : 0.042\n",
      "train accuracy on epoch 3969: 1.000\n",
      "test loss on epoch 3969: 0.351\n",
      "test accuracy on epoch 3969: 0.769\n",
      "train loss on epoch 3970 : 0.118\n",
      "train accuracy on epoch 3970: 0.944\n",
      "test loss on epoch 3970: 0.353\n",
      "test accuracy on epoch 3970: 0.769\n",
      "train loss on epoch 3971 : 0.043\n",
      "train accuracy on epoch 3971: 1.000\n",
      "test loss on epoch 3971: 0.358\n",
      "test accuracy on epoch 3971: 0.769\n",
      "train loss on epoch 3972 : 0.094\n",
      "train accuracy on epoch 3972: 1.000\n",
      "test loss on epoch 3972: 0.361\n",
      "test accuracy on epoch 3972: 0.769\n",
      "train loss on epoch 3973 : 0.050\n",
      "train accuracy on epoch 3973: 1.000\n",
      "test loss on epoch 3973: 0.359\n",
      "test accuracy on epoch 3973: 0.769\n",
      "train loss on epoch 3974 : 0.252\n",
      "train accuracy on epoch 3974: 0.889\n",
      "test loss on epoch 3974: 0.356\n",
      "test accuracy on epoch 3974: 0.769\n",
      "train loss on epoch 3975 : 0.253\n",
      "train accuracy on epoch 3975: 0.944\n",
      "test loss on epoch 3975: 0.350\n",
      "test accuracy on epoch 3975: 0.769\n",
      "train loss on epoch 3976 : 0.169\n",
      "train accuracy on epoch 3976: 0.944\n",
      "test loss on epoch 3976: 0.338\n",
      "test accuracy on epoch 3976: 0.769\n",
      "train loss on epoch 3977 : 0.050\n",
      "train accuracy on epoch 3977: 1.000\n",
      "test loss on epoch 3977: 0.345\n",
      "test accuracy on epoch 3977: 0.769\n",
      "train loss on epoch 3978 : 0.065\n",
      "train accuracy on epoch 3978: 1.000\n",
      "test loss on epoch 3978: 0.339\n",
      "test accuracy on epoch 3978: 0.769\n",
      "train loss on epoch 3979 : 0.260\n",
      "train accuracy on epoch 3979: 0.833\n",
      "test loss on epoch 3979: 0.353\n",
      "test accuracy on epoch 3979: 0.769\n",
      "train loss on epoch 3980 : 0.162\n",
      "train accuracy on epoch 3980: 0.889\n",
      "test loss on epoch 3980: 0.357\n",
      "test accuracy on epoch 3980: 0.769\n",
      "train loss on epoch 3981 : 0.208\n",
      "train accuracy on epoch 3981: 0.889\n",
      "test loss on epoch 3981: 0.362\n",
      "test accuracy on epoch 3981: 0.769\n",
      "train loss on epoch 3982 : 0.091\n",
      "train accuracy on epoch 3982: 1.000\n",
      "test loss on epoch 3982: 0.353\n",
      "test accuracy on epoch 3982: 0.769\n",
      "train loss on epoch 3983 : 0.090\n",
      "train accuracy on epoch 3983: 1.000\n",
      "test loss on epoch 3983: 0.354\n",
      "test accuracy on epoch 3983: 0.769\n",
      "train loss on epoch 3984 : 0.168\n",
      "train accuracy on epoch 3984: 0.944\n",
      "test loss on epoch 3984: 0.362\n",
      "test accuracy on epoch 3984: 0.769\n",
      "train loss on epoch 3985 : 0.097\n",
      "train accuracy on epoch 3985: 0.944\n",
      "test loss on epoch 3985: 0.349\n",
      "test accuracy on epoch 3985: 0.769\n",
      "train loss on epoch 3986 : 0.060\n",
      "train accuracy on epoch 3986: 1.000\n",
      "test loss on epoch 3986: 0.354\n",
      "test accuracy on epoch 3986: 0.769\n",
      "train loss on epoch 3987 : 0.108\n",
      "train accuracy on epoch 3987: 0.944\n",
      "test loss on epoch 3987: 0.337\n",
      "test accuracy on epoch 3987: 0.846\n",
      "train loss on epoch 3988 : 0.223\n",
      "train accuracy on epoch 3988: 0.889\n",
      "test loss on epoch 3988: 0.338\n",
      "test accuracy on epoch 3988: 0.846\n",
      "train loss on epoch 3989 : 0.143\n",
      "train accuracy on epoch 3989: 0.944\n",
      "test loss on epoch 3989: 0.337\n",
      "test accuracy on epoch 3989: 0.769\n",
      "train loss on epoch 3990 : 0.072\n",
      "train accuracy on epoch 3990: 1.000\n",
      "test loss on epoch 3990: 0.344\n",
      "test accuracy on epoch 3990: 0.769\n",
      "train loss on epoch 3991 : 0.093\n",
      "train accuracy on epoch 3991: 1.000\n",
      "test loss on epoch 3991: 0.340\n",
      "test accuracy on epoch 3991: 0.769\n",
      "train loss on epoch 3992 : 0.210\n",
      "train accuracy on epoch 3992: 0.889\n",
      "test loss on epoch 3992: 0.352\n",
      "test accuracy on epoch 3992: 0.769\n",
      "train loss on epoch 3993 : 0.403\n",
      "train accuracy on epoch 3993: 0.889\n",
      "test loss on epoch 3993: 0.337\n",
      "test accuracy on epoch 3993: 0.846\n",
      "train loss on epoch 3994 : 0.201\n",
      "train accuracy on epoch 3994: 0.889\n",
      "test loss on epoch 3994: 0.353\n",
      "test accuracy on epoch 3994: 0.692\n",
      "train loss on epoch 3995 : 0.149\n",
      "train accuracy on epoch 3995: 0.944\n",
      "test loss on epoch 3995: 0.354\n",
      "test accuracy on epoch 3995: 0.769\n",
      "train loss on epoch 3996 : 0.075\n",
      "train accuracy on epoch 3996: 1.000\n",
      "test loss on epoch 3996: 0.344\n",
      "test accuracy on epoch 3996: 0.769\n",
      "train loss on epoch 3997 : 0.191\n",
      "train accuracy on epoch 3997: 0.944\n",
      "test loss on epoch 3997: 0.352\n",
      "test accuracy on epoch 3997: 0.769\n",
      "train loss on epoch 3998 : 0.210\n",
      "train accuracy on epoch 3998: 0.944\n",
      "test loss on epoch 3998: 0.356\n",
      "test accuracy on epoch 3998: 0.769\n",
      "train loss on epoch 3999 : 0.124\n",
      "train accuracy on epoch 3999: 0.944\n",
      "test loss on epoch 3999: 0.348\n",
      "test accuracy on epoch 3999: 0.769\n",
      "train loss on epoch 4000 : 0.217\n",
      "train accuracy on epoch 4000: 0.889\n",
      "test loss on epoch 4000: 0.363\n",
      "test accuracy on epoch 4000: 0.769\n",
      "train loss on epoch 4001 : 0.240\n",
      "train accuracy on epoch 4001: 0.889\n",
      "test loss on epoch 4001: 0.361\n",
      "test accuracy on epoch 4001: 0.769\n",
      "train loss on epoch 4002 : 0.211\n",
      "train accuracy on epoch 4002: 0.944\n",
      "test loss on epoch 4002: 0.360\n",
      "test accuracy on epoch 4002: 0.769\n",
      "train loss on epoch 4003 : 0.366\n",
      "train accuracy on epoch 4003: 0.944\n",
      "test loss on epoch 4003: 0.344\n",
      "test accuracy on epoch 4003: 0.769\n",
      "train loss on epoch 4004 : 0.167\n",
      "train accuracy on epoch 4004: 0.944\n",
      "test loss on epoch 4004: 0.349\n",
      "test accuracy on epoch 4004: 0.769\n",
      "train loss on epoch 4005 : 0.249\n",
      "train accuracy on epoch 4005: 0.889\n",
      "test loss on epoch 4005: 0.351\n",
      "test accuracy on epoch 4005: 0.769\n",
      "train loss on epoch 4006 : 0.087\n",
      "train accuracy on epoch 4006: 0.944\n",
      "test loss on epoch 4006: 0.352\n",
      "test accuracy on epoch 4006: 0.769\n",
      "train loss on epoch 4007 : 0.162\n",
      "train accuracy on epoch 4007: 0.944\n",
      "test loss on epoch 4007: 0.337\n",
      "test accuracy on epoch 4007: 0.769\n",
      "train loss on epoch 4008 : 0.280\n",
      "train accuracy on epoch 4008: 0.889\n",
      "test loss on epoch 4008: 0.352\n",
      "test accuracy on epoch 4008: 0.769\n",
      "train loss on epoch 4009 : 0.058\n",
      "train accuracy on epoch 4009: 1.000\n",
      "test loss on epoch 4009: 0.343\n",
      "test accuracy on epoch 4009: 0.769\n",
      "train loss on epoch 4010 : 0.108\n",
      "train accuracy on epoch 4010: 0.944\n",
      "test loss on epoch 4010: 0.352\n",
      "test accuracy on epoch 4010: 0.769\n",
      "train loss on epoch 4011 : 0.247\n",
      "train accuracy on epoch 4011: 0.889\n",
      "test loss on epoch 4011: 0.354\n",
      "test accuracy on epoch 4011: 0.769\n",
      "train loss on epoch 4012 : 0.098\n",
      "train accuracy on epoch 4012: 0.944\n",
      "test loss on epoch 4012: 0.360\n",
      "test accuracy on epoch 4012: 0.769\n",
      "train loss on epoch 4013 : 0.319\n",
      "train accuracy on epoch 4013: 0.889\n",
      "test loss on epoch 4013: 0.346\n",
      "test accuracy on epoch 4013: 0.769\n",
      "train loss on epoch 4014 : 0.138\n",
      "train accuracy on epoch 4014: 0.944\n",
      "test loss on epoch 4014: 0.362\n",
      "test accuracy on epoch 4014: 0.769\n",
      "train loss on epoch 4015 : 0.200\n",
      "train accuracy on epoch 4015: 0.944\n",
      "test loss on epoch 4015: 0.342\n",
      "test accuracy on epoch 4015: 0.769\n",
      "train loss on epoch 4016 : 0.047\n",
      "train accuracy on epoch 4016: 1.000\n",
      "test loss on epoch 4016: 0.345\n",
      "test accuracy on epoch 4016: 0.769\n",
      "train loss on epoch 4017 : 0.167\n",
      "train accuracy on epoch 4017: 0.944\n",
      "test loss on epoch 4017: 0.351\n",
      "test accuracy on epoch 4017: 0.769\n",
      "train loss on epoch 4018 : 0.275\n",
      "train accuracy on epoch 4018: 0.944\n",
      "test loss on epoch 4018: 0.361\n",
      "test accuracy on epoch 4018: 0.692\n",
      "train loss on epoch 4019 : 0.066\n",
      "train accuracy on epoch 4019: 1.000\n",
      "test loss on epoch 4019: 0.359\n",
      "test accuracy on epoch 4019: 0.769\n",
      "train loss on epoch 4020 : 0.246\n",
      "train accuracy on epoch 4020: 0.944\n",
      "test loss on epoch 4020: 0.350\n",
      "test accuracy on epoch 4020: 0.846\n",
      "train loss on epoch 4021 : 0.141\n",
      "train accuracy on epoch 4021: 0.944\n",
      "test loss on epoch 4021: 0.359\n",
      "test accuracy on epoch 4021: 0.692\n",
      "train loss on epoch 4022 : 0.158\n",
      "train accuracy on epoch 4022: 0.944\n",
      "test loss on epoch 4022: 0.348\n",
      "test accuracy on epoch 4022: 0.769\n",
      "train loss on epoch 4023 : 0.194\n",
      "train accuracy on epoch 4023: 0.944\n",
      "test loss on epoch 4023: 0.361\n",
      "test accuracy on epoch 4023: 0.769\n",
      "train loss on epoch 4024 : 0.139\n",
      "train accuracy on epoch 4024: 0.944\n",
      "test loss on epoch 4024: 0.359\n",
      "test accuracy on epoch 4024: 0.769\n",
      "train loss on epoch 4025 : 0.233\n",
      "train accuracy on epoch 4025: 0.889\n",
      "test loss on epoch 4025: 0.351\n",
      "test accuracy on epoch 4025: 0.769\n",
      "train loss on epoch 4026 : 0.103\n",
      "train accuracy on epoch 4026: 1.000\n",
      "test loss on epoch 4026: 0.352\n",
      "test accuracy on epoch 4026: 0.846\n",
      "train loss on epoch 4027 : 0.189\n",
      "train accuracy on epoch 4027: 0.889\n",
      "test loss on epoch 4027: 0.348\n",
      "test accuracy on epoch 4027: 0.846\n",
      "train loss on epoch 4028 : 0.040\n",
      "train accuracy on epoch 4028: 1.000\n",
      "test loss on epoch 4028: 0.358\n",
      "test accuracy on epoch 4028: 0.769\n",
      "train loss on epoch 4029 : 0.103\n",
      "train accuracy on epoch 4029: 0.944\n",
      "test loss on epoch 4029: 0.358\n",
      "test accuracy on epoch 4029: 0.769\n",
      "train loss on epoch 4030 : 0.257\n",
      "train accuracy on epoch 4030: 0.889\n",
      "test loss on epoch 4030: 0.363\n",
      "test accuracy on epoch 4030: 0.769\n",
      "train loss on epoch 4031 : 0.270\n",
      "train accuracy on epoch 4031: 0.944\n",
      "test loss on epoch 4031: 0.357\n",
      "test accuracy on epoch 4031: 0.769\n",
      "train loss on epoch 4032 : 0.152\n",
      "train accuracy on epoch 4032: 0.889\n",
      "test loss on epoch 4032: 0.363\n",
      "test accuracy on epoch 4032: 0.769\n",
      "train loss on epoch 4033 : 0.075\n",
      "train accuracy on epoch 4033: 1.000\n",
      "test loss on epoch 4033: 0.364\n",
      "test accuracy on epoch 4033: 0.769\n",
      "train loss on epoch 4034 : 0.315\n",
      "train accuracy on epoch 4034: 0.889\n",
      "test loss on epoch 4034: 0.373\n",
      "test accuracy on epoch 4034: 0.769\n",
      "train loss on epoch 4035 : 0.091\n",
      "train accuracy on epoch 4035: 1.000\n",
      "test loss on epoch 4035: 0.373\n",
      "test accuracy on epoch 4035: 0.769\n",
      "train loss on epoch 4036 : 0.113\n",
      "train accuracy on epoch 4036: 0.944\n",
      "test loss on epoch 4036: 0.381\n",
      "test accuracy on epoch 4036: 0.769\n",
      "train loss on epoch 4037 : 0.069\n",
      "train accuracy on epoch 4037: 1.000\n",
      "test loss on epoch 4037: 0.382\n",
      "test accuracy on epoch 4037: 0.769\n",
      "train loss on epoch 4038 : 0.215\n",
      "train accuracy on epoch 4038: 0.944\n",
      "test loss on epoch 4038: 0.378\n",
      "test accuracy on epoch 4038: 0.769\n",
      "train loss on epoch 4039 : 0.256\n",
      "train accuracy on epoch 4039: 0.889\n",
      "test loss on epoch 4039: 0.374\n",
      "test accuracy on epoch 4039: 0.769\n",
      "train loss on epoch 4040 : 0.138\n",
      "train accuracy on epoch 4040: 0.889\n",
      "test loss on epoch 4040: 0.375\n",
      "test accuracy on epoch 4040: 0.769\n",
      "train loss on epoch 4041 : 0.139\n",
      "train accuracy on epoch 4041: 0.889\n",
      "test loss on epoch 4041: 0.380\n",
      "test accuracy on epoch 4041: 0.769\n",
      "train loss on epoch 4042 : 0.287\n",
      "train accuracy on epoch 4042: 0.944\n",
      "test loss on epoch 4042: 0.381\n",
      "test accuracy on epoch 4042: 0.769\n",
      "train loss on epoch 4043 : 0.155\n",
      "train accuracy on epoch 4043: 0.889\n",
      "test loss on epoch 4043: 0.375\n",
      "test accuracy on epoch 4043: 0.769\n",
      "train loss on epoch 4044 : 0.231\n",
      "train accuracy on epoch 4044: 0.944\n",
      "test loss on epoch 4044: 0.363\n",
      "test accuracy on epoch 4044: 0.769\n",
      "train loss on epoch 4045 : 0.251\n",
      "train accuracy on epoch 4045: 0.889\n",
      "test loss on epoch 4045: 0.357\n",
      "test accuracy on epoch 4045: 0.769\n",
      "train loss on epoch 4046 : 0.128\n",
      "train accuracy on epoch 4046: 1.000\n",
      "test loss on epoch 4046: 0.365\n",
      "test accuracy on epoch 4046: 0.692\n",
      "train loss on epoch 4047 : 0.521\n",
      "train accuracy on epoch 4047: 0.889\n",
      "test loss on epoch 4047: 0.361\n",
      "test accuracy on epoch 4047: 0.692\n",
      "train loss on epoch 4048 : 0.124\n",
      "train accuracy on epoch 4048: 0.944\n",
      "test loss on epoch 4048: 0.361\n",
      "test accuracy on epoch 4048: 0.692\n",
      "train loss on epoch 4049 : 0.264\n",
      "train accuracy on epoch 4049: 0.889\n",
      "test loss on epoch 4049: 0.354\n",
      "test accuracy on epoch 4049: 0.769\n",
      "train loss on epoch 4050 : 0.155\n",
      "train accuracy on epoch 4050: 0.944\n",
      "test loss on epoch 4050: 0.357\n",
      "test accuracy on epoch 4050: 0.769\n",
      "train loss on epoch 4051 : 0.198\n",
      "train accuracy on epoch 4051: 0.944\n",
      "test loss on epoch 4051: 0.358\n",
      "test accuracy on epoch 4051: 0.769\n",
      "train loss on epoch 4052 : 0.179\n",
      "train accuracy on epoch 4052: 0.889\n",
      "test loss on epoch 4052: 0.353\n",
      "test accuracy on epoch 4052: 0.769\n",
      "train loss on epoch 4053 : 0.300\n",
      "train accuracy on epoch 4053: 0.833\n",
      "test loss on epoch 4053: 0.359\n",
      "test accuracy on epoch 4053: 0.769\n",
      "train loss on epoch 4054 : 0.139\n",
      "train accuracy on epoch 4054: 0.889\n",
      "test loss on epoch 4054: 0.362\n",
      "test accuracy on epoch 4054: 0.769\n",
      "train loss on epoch 4055 : 0.158\n",
      "train accuracy on epoch 4055: 0.944\n",
      "test loss on epoch 4055: 0.363\n",
      "test accuracy on epoch 4055: 0.769\n",
      "train loss on epoch 4056 : 0.061\n",
      "train accuracy on epoch 4056: 1.000\n",
      "test loss on epoch 4056: 0.366\n",
      "test accuracy on epoch 4056: 0.769\n",
      "train loss on epoch 4057 : 0.066\n",
      "train accuracy on epoch 4057: 0.944\n",
      "test loss on epoch 4057: 0.369\n",
      "test accuracy on epoch 4057: 0.769\n",
      "train loss on epoch 4058 : 0.261\n",
      "train accuracy on epoch 4058: 0.944\n",
      "test loss on epoch 4058: 0.366\n",
      "test accuracy on epoch 4058: 0.769\n",
      "train loss on epoch 4059 : 0.298\n",
      "train accuracy on epoch 4059: 0.889\n",
      "test loss on epoch 4059: 0.372\n",
      "test accuracy on epoch 4059: 0.769\n",
      "train loss on epoch 4060 : 0.238\n",
      "train accuracy on epoch 4060: 0.833\n",
      "test loss on epoch 4060: 0.383\n",
      "test accuracy on epoch 4060: 0.769\n",
      "train loss on epoch 4061 : 0.480\n",
      "train accuracy on epoch 4061: 0.833\n",
      "test loss on epoch 4061: 0.390\n",
      "test accuracy on epoch 4061: 0.769\n",
      "train loss on epoch 4062 : 0.230\n",
      "train accuracy on epoch 4062: 0.944\n",
      "test loss on epoch 4062: 0.386\n",
      "test accuracy on epoch 4062: 0.769\n",
      "train loss on epoch 4063 : 0.080\n",
      "train accuracy on epoch 4063: 1.000\n",
      "test loss on epoch 4063: 0.378\n",
      "test accuracy on epoch 4063: 0.769\n",
      "train loss on epoch 4064 : 0.226\n",
      "train accuracy on epoch 4064: 0.944\n",
      "test loss on epoch 4064: 0.368\n",
      "test accuracy on epoch 4064: 0.769\n",
      "train loss on epoch 4065 : 0.178\n",
      "train accuracy on epoch 4065: 0.833\n",
      "test loss on epoch 4065: 0.360\n",
      "test accuracy on epoch 4065: 0.769\n",
      "train loss on epoch 4066 : 0.155\n",
      "train accuracy on epoch 4066: 0.944\n",
      "test loss on epoch 4066: 0.358\n",
      "test accuracy on epoch 4066: 0.769\n",
      "train loss on epoch 4067 : 0.031\n",
      "train accuracy on epoch 4067: 1.000\n",
      "test loss on epoch 4067: 0.358\n",
      "test accuracy on epoch 4067: 0.769\n",
      "train loss on epoch 4068 : 0.063\n",
      "train accuracy on epoch 4068: 1.000\n",
      "test loss on epoch 4068: 0.354\n",
      "test accuracy on epoch 4068: 0.769\n",
      "train loss on epoch 4069 : 0.084\n",
      "train accuracy on epoch 4069: 1.000\n",
      "test loss on epoch 4069: 0.351\n",
      "test accuracy on epoch 4069: 0.769\n",
      "train loss on epoch 4070 : 0.068\n",
      "train accuracy on epoch 4070: 1.000\n",
      "test loss on epoch 4070: 0.355\n",
      "test accuracy on epoch 4070: 0.769\n",
      "train loss on epoch 4071 : 0.201\n",
      "train accuracy on epoch 4071: 0.944\n",
      "test loss on epoch 4071: 0.347\n",
      "test accuracy on epoch 4071: 0.769\n",
      "train loss on epoch 4072 : 0.276\n",
      "train accuracy on epoch 4072: 0.944\n",
      "test loss on epoch 4072: 0.341\n",
      "test accuracy on epoch 4072: 0.769\n",
      "train loss on epoch 4073 : 0.153\n",
      "train accuracy on epoch 4073: 0.944\n",
      "test loss on epoch 4073: 0.340\n",
      "test accuracy on epoch 4073: 0.769\n",
      "train loss on epoch 4074 : 0.079\n",
      "train accuracy on epoch 4074: 0.944\n",
      "test loss on epoch 4074: 0.345\n",
      "test accuracy on epoch 4074: 0.769\n",
      "train loss on epoch 4075 : 0.187\n",
      "train accuracy on epoch 4075: 0.889\n",
      "test loss on epoch 4075: 0.346\n",
      "test accuracy on epoch 4075: 0.769\n",
      "train loss on epoch 4076 : 0.148\n",
      "train accuracy on epoch 4076: 0.944\n",
      "test loss on epoch 4076: 0.347\n",
      "test accuracy on epoch 4076: 0.769\n",
      "train loss on epoch 4077 : 0.165\n",
      "train accuracy on epoch 4077: 0.889\n",
      "test loss on epoch 4077: 0.352\n",
      "test accuracy on epoch 4077: 0.769\n",
      "train loss on epoch 4078 : 0.260\n",
      "train accuracy on epoch 4078: 0.889\n",
      "test loss on epoch 4078: 0.357\n",
      "test accuracy on epoch 4078: 0.769\n",
      "train loss on epoch 4079 : 0.101\n",
      "train accuracy on epoch 4079: 0.944\n",
      "test loss on epoch 4079: 0.348\n",
      "test accuracy on epoch 4079: 0.769\n",
      "train loss on epoch 4080 : 0.506\n",
      "train accuracy on epoch 4080: 0.833\n",
      "test loss on epoch 4080: 0.343\n",
      "test accuracy on epoch 4080: 0.769\n",
      "train loss on epoch 4081 : 0.303\n",
      "train accuracy on epoch 4081: 0.889\n",
      "test loss on epoch 4081: 0.350\n",
      "test accuracy on epoch 4081: 0.769\n",
      "train loss on epoch 4082 : 0.264\n",
      "train accuracy on epoch 4082: 0.833\n",
      "test loss on epoch 4082: 0.358\n",
      "test accuracy on epoch 4082: 0.769\n",
      "train loss on epoch 4083 : 0.091\n",
      "train accuracy on epoch 4083: 0.944\n",
      "test loss on epoch 4083: 0.358\n",
      "test accuracy on epoch 4083: 0.769\n",
      "train loss on epoch 4084 : 0.172\n",
      "train accuracy on epoch 4084: 0.944\n",
      "test loss on epoch 4084: 0.353\n",
      "test accuracy on epoch 4084: 0.769\n",
      "train loss on epoch 4085 : 0.037\n",
      "train accuracy on epoch 4085: 1.000\n",
      "test loss on epoch 4085: 0.354\n",
      "test accuracy on epoch 4085: 0.769\n",
      "train loss on epoch 4086 : 0.148\n",
      "train accuracy on epoch 4086: 0.944\n",
      "test loss on epoch 4086: 0.350\n",
      "test accuracy on epoch 4086: 0.769\n",
      "train loss on epoch 4087 : 0.192\n",
      "train accuracy on epoch 4087: 0.889\n",
      "test loss on epoch 4087: 0.347\n",
      "test accuracy on epoch 4087: 0.769\n",
      "train loss on epoch 4088 : 0.102\n",
      "train accuracy on epoch 4088: 1.000\n",
      "test loss on epoch 4088: 0.340\n",
      "test accuracy on epoch 4088: 0.846\n",
      "train loss on epoch 4089 : 0.051\n",
      "train accuracy on epoch 4089: 1.000\n",
      "test loss on epoch 4089: 0.347\n",
      "test accuracy on epoch 4089: 0.769\n",
      "train loss on epoch 4090 : 0.282\n",
      "train accuracy on epoch 4090: 0.889\n",
      "test loss on epoch 4090: 0.351\n",
      "test accuracy on epoch 4090: 0.769\n",
      "train loss on epoch 4091 : 0.265\n",
      "train accuracy on epoch 4091: 0.889\n",
      "test loss on epoch 4091: 0.342\n",
      "test accuracy on epoch 4091: 0.846\n",
      "train loss on epoch 4092 : 0.294\n",
      "train accuracy on epoch 4092: 0.889\n",
      "test loss on epoch 4092: 0.341\n",
      "test accuracy on epoch 4092: 0.846\n",
      "train loss on epoch 4093 : 0.095\n",
      "train accuracy on epoch 4093: 0.944\n",
      "test loss on epoch 4093: 0.356\n",
      "test accuracy on epoch 4093: 0.769\n",
      "train loss on epoch 4094 : 0.244\n",
      "train accuracy on epoch 4094: 0.944\n",
      "test loss on epoch 4094: 0.365\n",
      "test accuracy on epoch 4094: 0.769\n",
      "train loss on epoch 4095 : 0.126\n",
      "train accuracy on epoch 4095: 0.889\n",
      "test loss on epoch 4095: 0.354\n",
      "test accuracy on epoch 4095: 0.769\n",
      "train loss on epoch 4096 : 0.074\n",
      "train accuracy on epoch 4096: 1.000\n",
      "test loss on epoch 4096: 0.362\n",
      "test accuracy on epoch 4096: 0.769\n",
      "train loss on epoch 4097 : 0.089\n",
      "train accuracy on epoch 4097: 1.000\n",
      "test loss on epoch 4097: 0.356\n",
      "test accuracy on epoch 4097: 0.769\n",
      "train loss on epoch 4098 : 0.127\n",
      "train accuracy on epoch 4098: 0.889\n",
      "test loss on epoch 4098: 0.363\n",
      "test accuracy on epoch 4098: 0.769\n",
      "train loss on epoch 4099 : 0.113\n",
      "train accuracy on epoch 4099: 0.944\n",
      "test loss on epoch 4099: 0.352\n",
      "test accuracy on epoch 4099: 0.769\n",
      "train loss on epoch 4100 : 0.044\n",
      "train accuracy on epoch 4100: 1.000\n",
      "test loss on epoch 4100: 0.350\n",
      "test accuracy on epoch 4100: 0.769\n",
      "train loss on epoch 4101 : 0.046\n",
      "train accuracy on epoch 4101: 1.000\n",
      "test loss on epoch 4101: 0.362\n",
      "test accuracy on epoch 4101: 0.769\n",
      "train loss on epoch 4102 : 0.391\n",
      "train accuracy on epoch 4102: 0.778\n",
      "test loss on epoch 4102: 0.363\n",
      "test accuracy on epoch 4102: 0.769\n",
      "train loss on epoch 4103 : 0.190\n",
      "train accuracy on epoch 4103: 0.889\n",
      "test loss on epoch 4103: 0.361\n",
      "test accuracy on epoch 4103: 0.769\n",
      "train loss on epoch 4104 : 0.272\n",
      "train accuracy on epoch 4104: 0.889\n",
      "test loss on epoch 4104: 0.363\n",
      "test accuracy on epoch 4104: 0.769\n",
      "train loss on epoch 4105 : 0.304\n",
      "train accuracy on epoch 4105: 0.889\n",
      "test loss on epoch 4105: 0.360\n",
      "test accuracy on epoch 4105: 0.769\n",
      "train loss on epoch 4106 : 0.062\n",
      "train accuracy on epoch 4106: 1.000\n",
      "test loss on epoch 4106: 0.367\n",
      "test accuracy on epoch 4106: 0.769\n",
      "train loss on epoch 4107 : 0.275\n",
      "train accuracy on epoch 4107: 0.889\n",
      "test loss on epoch 4107: 0.366\n",
      "test accuracy on epoch 4107: 0.769\n",
      "train loss on epoch 4108 : 0.188\n",
      "train accuracy on epoch 4108: 0.944\n",
      "test loss on epoch 4108: 0.361\n",
      "test accuracy on epoch 4108: 0.769\n",
      "train loss on epoch 4109 : 0.168\n",
      "train accuracy on epoch 4109: 0.944\n",
      "test loss on epoch 4109: 0.360\n",
      "test accuracy on epoch 4109: 0.769\n",
      "train loss on epoch 4110 : 0.178\n",
      "train accuracy on epoch 4110: 0.889\n",
      "test loss on epoch 4110: 0.365\n",
      "test accuracy on epoch 4110: 0.769\n",
      "train loss on epoch 4111 : 0.045\n",
      "train accuracy on epoch 4111: 1.000\n",
      "test loss on epoch 4111: 0.353\n",
      "test accuracy on epoch 4111: 0.769\n",
      "train loss on epoch 4112 : 0.211\n",
      "train accuracy on epoch 4112: 0.889\n",
      "test loss on epoch 4112: 0.357\n",
      "test accuracy on epoch 4112: 0.769\n",
      "train loss on epoch 4113 : 0.151\n",
      "train accuracy on epoch 4113: 0.944\n",
      "test loss on epoch 4113: 0.362\n",
      "test accuracy on epoch 4113: 0.692\n",
      "train loss on epoch 4114 : 0.139\n",
      "train accuracy on epoch 4114: 0.944\n",
      "test loss on epoch 4114: 0.353\n",
      "test accuracy on epoch 4114: 0.769\n",
      "train loss on epoch 4115 : 0.215\n",
      "train accuracy on epoch 4115: 0.889\n",
      "test loss on epoch 4115: 0.356\n",
      "test accuracy on epoch 4115: 0.769\n",
      "train loss on epoch 4116 : 0.295\n",
      "train accuracy on epoch 4116: 0.833\n",
      "test loss on epoch 4116: 0.363\n",
      "test accuracy on epoch 4116: 0.769\n",
      "train loss on epoch 4117 : 0.080\n",
      "train accuracy on epoch 4117: 1.000\n",
      "test loss on epoch 4117: 0.364\n",
      "test accuracy on epoch 4117: 0.769\n",
      "train loss on epoch 4118 : 0.145\n",
      "train accuracy on epoch 4118: 0.889\n",
      "test loss on epoch 4118: 0.355\n",
      "test accuracy on epoch 4118: 0.769\n",
      "train loss on epoch 4119 : 0.112\n",
      "train accuracy on epoch 4119: 0.944\n",
      "test loss on epoch 4119: 0.343\n",
      "test accuracy on epoch 4119: 0.769\n",
      "train loss on epoch 4120 : 0.146\n",
      "train accuracy on epoch 4120: 0.944\n",
      "test loss on epoch 4120: 0.351\n",
      "test accuracy on epoch 4120: 0.769\n",
      "train loss on epoch 4121 : 0.203\n",
      "train accuracy on epoch 4121: 0.944\n",
      "test loss on epoch 4121: 0.350\n",
      "test accuracy on epoch 4121: 0.769\n",
      "train loss on epoch 4122 : 0.147\n",
      "train accuracy on epoch 4122: 0.944\n",
      "test loss on epoch 4122: 0.352\n",
      "test accuracy on epoch 4122: 0.769\n",
      "train loss on epoch 4123 : 0.092\n",
      "train accuracy on epoch 4123: 0.944\n",
      "test loss on epoch 4123: 0.351\n",
      "test accuracy on epoch 4123: 0.769\n",
      "train loss on epoch 4124 : 0.172\n",
      "train accuracy on epoch 4124: 0.944\n",
      "test loss on epoch 4124: 0.363\n",
      "test accuracy on epoch 4124: 0.692\n",
      "train loss on epoch 4125 : 0.475\n",
      "train accuracy on epoch 4125: 0.833\n",
      "test loss on epoch 4125: 0.357\n",
      "test accuracy on epoch 4125: 0.769\n",
      "train loss on epoch 4126 : 0.075\n",
      "train accuracy on epoch 4126: 1.000\n",
      "test loss on epoch 4126: 0.366\n",
      "test accuracy on epoch 4126: 0.692\n",
      "train loss on epoch 4127 : 0.139\n",
      "train accuracy on epoch 4127: 0.889\n",
      "test loss on epoch 4127: 0.355\n",
      "test accuracy on epoch 4127: 0.769\n",
      "train loss on epoch 4128 : 0.058\n",
      "train accuracy on epoch 4128: 1.000\n",
      "test loss on epoch 4128: 0.348\n",
      "test accuracy on epoch 4128: 0.769\n",
      "train loss on epoch 4129 : 0.059\n",
      "train accuracy on epoch 4129: 1.000\n",
      "test loss on epoch 4129: 0.356\n",
      "test accuracy on epoch 4129: 0.769\n",
      "train loss on epoch 4130 : 0.145\n",
      "train accuracy on epoch 4130: 0.889\n",
      "test loss on epoch 4130: 0.368\n",
      "test accuracy on epoch 4130: 0.769\n",
      "train loss on epoch 4131 : 0.321\n",
      "train accuracy on epoch 4131: 0.889\n",
      "test loss on epoch 4131: 0.357\n",
      "test accuracy on epoch 4131: 0.769\n",
      "train loss on epoch 4132 : 0.116\n",
      "train accuracy on epoch 4132: 0.944\n",
      "test loss on epoch 4132: 0.368\n",
      "test accuracy on epoch 4132: 0.769\n",
      "train loss on epoch 4133 : 0.175\n",
      "train accuracy on epoch 4133: 0.944\n",
      "test loss on epoch 4133: 0.376\n",
      "test accuracy on epoch 4133: 0.769\n",
      "train loss on epoch 4134 : 0.121\n",
      "train accuracy on epoch 4134: 0.944\n",
      "test loss on epoch 4134: 0.384\n",
      "test accuracy on epoch 4134: 0.769\n",
      "train loss on epoch 4135 : 0.108\n",
      "train accuracy on epoch 4135: 0.944\n",
      "test loss on epoch 4135: 0.383\n",
      "test accuracy on epoch 4135: 0.769\n",
      "train loss on epoch 4136 : 0.231\n",
      "train accuracy on epoch 4136: 0.889\n",
      "test loss on epoch 4136: 0.369\n",
      "test accuracy on epoch 4136: 0.769\n",
      "train loss on epoch 4137 : 0.099\n",
      "train accuracy on epoch 4137: 0.944\n",
      "test loss on epoch 4137: 0.362\n",
      "test accuracy on epoch 4137: 0.769\n",
      "train loss on epoch 4138 : 0.118\n",
      "train accuracy on epoch 4138: 0.944\n",
      "test loss on epoch 4138: 0.358\n",
      "test accuracy on epoch 4138: 0.769\n",
      "train loss on epoch 4139 : 0.277\n",
      "train accuracy on epoch 4139: 0.889\n",
      "test loss on epoch 4139: 0.349\n",
      "test accuracy on epoch 4139: 0.769\n",
      "train loss on epoch 4140 : 0.355\n",
      "train accuracy on epoch 4140: 0.889\n",
      "test loss on epoch 4140: 0.345\n",
      "test accuracy on epoch 4140: 0.846\n",
      "train loss on epoch 4141 : 0.048\n",
      "train accuracy on epoch 4141: 1.000\n",
      "test loss on epoch 4141: 0.355\n",
      "test accuracy on epoch 4141: 0.769\n",
      "train loss on epoch 4142 : 0.214\n",
      "train accuracy on epoch 4142: 0.889\n",
      "test loss on epoch 4142: 0.359\n",
      "test accuracy on epoch 4142: 0.692\n",
      "train loss on epoch 4143 : 0.147\n",
      "train accuracy on epoch 4143: 0.889\n",
      "test loss on epoch 4143: 0.358\n",
      "test accuracy on epoch 4143: 0.692\n",
      "train loss on epoch 4144 : 0.104\n",
      "train accuracy on epoch 4144: 0.944\n",
      "test loss on epoch 4144: 0.359\n",
      "test accuracy on epoch 4144: 0.769\n",
      "train loss on epoch 4145 : 0.314\n",
      "train accuracy on epoch 4145: 0.889\n",
      "test loss on epoch 4145: 0.340\n",
      "test accuracy on epoch 4145: 0.769\n",
      "train loss on epoch 4146 : 0.262\n",
      "train accuracy on epoch 4146: 0.833\n",
      "test loss on epoch 4146: 0.355\n",
      "test accuracy on epoch 4146: 0.769\n",
      "train loss on epoch 4147 : 0.129\n",
      "train accuracy on epoch 4147: 0.944\n",
      "test loss on epoch 4147: 0.345\n",
      "test accuracy on epoch 4147: 0.846\n",
      "train loss on epoch 4148 : 0.072\n",
      "train accuracy on epoch 4148: 1.000\n",
      "test loss on epoch 4148: 0.356\n",
      "test accuracy on epoch 4148: 0.769\n",
      "train loss on epoch 4149 : 0.137\n",
      "train accuracy on epoch 4149: 0.889\n",
      "test loss on epoch 4149: 0.347\n",
      "test accuracy on epoch 4149: 0.769\n",
      "train loss on epoch 4150 : 0.196\n",
      "train accuracy on epoch 4150: 0.944\n",
      "test loss on epoch 4150: 0.356\n",
      "test accuracy on epoch 4150: 0.769\n",
      "train loss on epoch 4151 : 0.182\n",
      "train accuracy on epoch 4151: 0.889\n",
      "test loss on epoch 4151: 0.355\n",
      "test accuracy on epoch 4151: 0.769\n",
      "train loss on epoch 4152 : 0.187\n",
      "train accuracy on epoch 4152: 0.944\n",
      "test loss on epoch 4152: 0.349\n",
      "test accuracy on epoch 4152: 0.769\n",
      "train loss on epoch 4153 : 0.056\n",
      "train accuracy on epoch 4153: 1.000\n",
      "test loss on epoch 4153: 0.347\n",
      "test accuracy on epoch 4153: 0.769\n",
      "train loss on epoch 4154 : 0.250\n",
      "train accuracy on epoch 4154: 0.944\n",
      "test loss on epoch 4154: 0.348\n",
      "test accuracy on epoch 4154: 0.769\n",
      "train loss on epoch 4155 : 0.159\n",
      "train accuracy on epoch 4155: 0.944\n",
      "test loss on epoch 4155: 0.353\n",
      "test accuracy on epoch 4155: 0.769\n",
      "train loss on epoch 4156 : 0.125\n",
      "train accuracy on epoch 4156: 0.889\n",
      "test loss on epoch 4156: 0.368\n",
      "test accuracy on epoch 4156: 0.769\n",
      "train loss on epoch 4157 : 0.233\n",
      "train accuracy on epoch 4157: 0.944\n",
      "test loss on epoch 4157: 0.373\n",
      "test accuracy on epoch 4157: 0.769\n",
      "train loss on epoch 4158 : 0.100\n",
      "train accuracy on epoch 4158: 0.944\n",
      "test loss on epoch 4158: 0.364\n",
      "test accuracy on epoch 4158: 0.769\n",
      "train loss on epoch 4159 : 0.122\n",
      "train accuracy on epoch 4159: 0.889\n",
      "test loss on epoch 4159: 0.371\n",
      "test accuracy on epoch 4159: 0.769\n",
      "train loss on epoch 4160 : 0.201\n",
      "train accuracy on epoch 4160: 0.944\n",
      "test loss on epoch 4160: 0.378\n",
      "test accuracy on epoch 4160: 0.769\n",
      "train loss on epoch 4161 : 0.161\n",
      "train accuracy on epoch 4161: 0.944\n",
      "test loss on epoch 4161: 0.373\n",
      "test accuracy on epoch 4161: 0.769\n",
      "train loss on epoch 4162 : 0.086\n",
      "train accuracy on epoch 4162: 1.000\n",
      "test loss on epoch 4162: 0.377\n",
      "test accuracy on epoch 4162: 0.769\n",
      "train loss on epoch 4163 : 0.377\n",
      "train accuracy on epoch 4163: 0.889\n",
      "test loss on epoch 4163: 0.354\n",
      "test accuracy on epoch 4163: 0.769\n",
      "train loss on epoch 4164 : 0.538\n",
      "train accuracy on epoch 4164: 0.833\n",
      "test loss on epoch 4164: 0.354\n",
      "test accuracy on epoch 4164: 0.769\n",
      "train loss on epoch 4165 : 0.151\n",
      "train accuracy on epoch 4165: 0.944\n",
      "test loss on epoch 4165: 0.338\n",
      "test accuracy on epoch 4165: 0.846\n",
      "train loss on epoch 4166 : 0.298\n",
      "train accuracy on epoch 4166: 0.889\n",
      "test loss on epoch 4166: 0.338\n",
      "test accuracy on epoch 4166: 0.769\n",
      "train loss on epoch 4167 : 0.221\n",
      "train accuracy on epoch 4167: 0.889\n",
      "test loss on epoch 4167: 0.340\n",
      "test accuracy on epoch 4167: 0.769\n",
      "train loss on epoch 4168 : 0.030\n",
      "train accuracy on epoch 4168: 1.000\n",
      "test loss on epoch 4168: 0.342\n",
      "test accuracy on epoch 4168: 0.846\n",
      "train loss on epoch 4169 : 0.153\n",
      "train accuracy on epoch 4169: 0.944\n",
      "test loss on epoch 4169: 0.361\n",
      "test accuracy on epoch 4169: 0.692\n",
      "train loss on epoch 4170 : 0.213\n",
      "train accuracy on epoch 4170: 0.944\n",
      "test loss on epoch 4170: 0.350\n",
      "test accuracy on epoch 4170: 0.769\n",
      "train loss on epoch 4171 : 0.093\n",
      "train accuracy on epoch 4171: 1.000\n",
      "test loss on epoch 4171: 0.345\n",
      "test accuracy on epoch 4171: 0.846\n",
      "train loss on epoch 4172 : 0.048\n",
      "train accuracy on epoch 4172: 1.000\n",
      "test loss on epoch 4172: 0.345\n",
      "test accuracy on epoch 4172: 0.846\n",
      "train loss on epoch 4173 : 0.225\n",
      "train accuracy on epoch 4173: 0.944\n",
      "test loss on epoch 4173: 0.353\n",
      "test accuracy on epoch 4173: 0.769\n",
      "train loss on epoch 4174 : 0.204\n",
      "train accuracy on epoch 4174: 0.944\n",
      "test loss on epoch 4174: 0.353\n",
      "test accuracy on epoch 4174: 0.769\n",
      "train loss on epoch 4175 : 0.051\n",
      "train accuracy on epoch 4175: 1.000\n",
      "test loss on epoch 4175: 0.356\n",
      "test accuracy on epoch 4175: 0.769\n",
      "train loss on epoch 4176 : 0.181\n",
      "train accuracy on epoch 4176: 0.889\n",
      "test loss on epoch 4176: 0.353\n",
      "test accuracy on epoch 4176: 0.769\n",
      "train loss on epoch 4177 : 0.347\n",
      "train accuracy on epoch 4177: 0.944\n",
      "test loss on epoch 4177: 0.351\n",
      "test accuracy on epoch 4177: 0.769\n",
      "train loss on epoch 4178 : 0.212\n",
      "train accuracy on epoch 4178: 0.889\n",
      "test loss on epoch 4178: 0.350\n",
      "test accuracy on epoch 4178: 0.769\n",
      "train loss on epoch 4179 : 0.189\n",
      "train accuracy on epoch 4179: 0.944\n",
      "test loss on epoch 4179: 0.354\n",
      "test accuracy on epoch 4179: 0.769\n",
      "train loss on epoch 4180 : 0.165\n",
      "train accuracy on epoch 4180: 0.944\n",
      "test loss on epoch 4180: 0.363\n",
      "test accuracy on epoch 4180: 0.692\n",
      "train loss on epoch 4181 : 0.101\n",
      "train accuracy on epoch 4181: 0.944\n",
      "test loss on epoch 4181: 0.355\n",
      "test accuracy on epoch 4181: 0.769\n",
      "train loss on epoch 4182 : 0.230\n",
      "train accuracy on epoch 4182: 0.889\n",
      "test loss on epoch 4182: 0.344\n",
      "test accuracy on epoch 4182: 0.846\n",
      "train loss on epoch 4183 : 0.096\n",
      "train accuracy on epoch 4183: 0.944\n",
      "test loss on epoch 4183: 0.345\n",
      "test accuracy on epoch 4183: 0.846\n",
      "train loss on epoch 4184 : 0.157\n",
      "train accuracy on epoch 4184: 0.889\n",
      "test loss on epoch 4184: 0.361\n",
      "test accuracy on epoch 4184: 0.692\n",
      "train loss on epoch 4185 : 0.399\n",
      "train accuracy on epoch 4185: 0.889\n",
      "test loss on epoch 4185: 0.352\n",
      "test accuracy on epoch 4185: 0.769\n",
      "train loss on epoch 4186 : 0.042\n",
      "train accuracy on epoch 4186: 1.000\n",
      "test loss on epoch 4186: 0.364\n",
      "test accuracy on epoch 4186: 0.692\n",
      "train loss on epoch 4187 : 0.127\n",
      "train accuracy on epoch 4187: 0.944\n",
      "test loss on epoch 4187: 0.355\n",
      "test accuracy on epoch 4187: 0.769\n",
      "train loss on epoch 4188 : 0.126\n",
      "train accuracy on epoch 4188: 0.944\n",
      "test loss on epoch 4188: 0.360\n",
      "test accuracy on epoch 4188: 0.769\n",
      "train loss on epoch 4189 : 0.179\n",
      "train accuracy on epoch 4189: 0.944\n",
      "test loss on epoch 4189: 0.356\n",
      "test accuracy on epoch 4189: 0.769\n",
      "train loss on epoch 4190 : 0.123\n",
      "train accuracy on epoch 4190: 0.944\n",
      "test loss on epoch 4190: 0.364\n",
      "test accuracy on epoch 4190: 0.769\n",
      "train loss on epoch 4191 : 0.347\n",
      "train accuracy on epoch 4191: 0.833\n",
      "test loss on epoch 4191: 0.368\n",
      "test accuracy on epoch 4191: 0.769\n",
      "train loss on epoch 4192 : 0.049\n",
      "train accuracy on epoch 4192: 1.000\n",
      "test loss on epoch 4192: 0.381\n",
      "test accuracy on epoch 4192: 0.769\n",
      "train loss on epoch 4193 : 0.337\n",
      "train accuracy on epoch 4193: 0.889\n",
      "test loss on epoch 4193: 0.380\n",
      "test accuracy on epoch 4193: 0.769\n",
      "train loss on epoch 4194 : 0.102\n",
      "train accuracy on epoch 4194: 0.944\n",
      "test loss on epoch 4194: 0.370\n",
      "test accuracy on epoch 4194: 0.769\n",
      "train loss on epoch 4195 : 0.314\n",
      "train accuracy on epoch 4195: 0.833\n",
      "test loss on epoch 4195: 0.365\n",
      "test accuracy on epoch 4195: 0.769\n",
      "train loss on epoch 4196 : 0.049\n",
      "train accuracy on epoch 4196: 1.000\n",
      "test loss on epoch 4196: 0.352\n",
      "test accuracy on epoch 4196: 0.769\n",
      "train loss on epoch 4197 : 0.133\n",
      "train accuracy on epoch 4197: 0.944\n",
      "test loss on epoch 4197: 0.361\n",
      "test accuracy on epoch 4197: 0.769\n",
      "train loss on epoch 4198 : 0.384\n",
      "train accuracy on epoch 4198: 0.889\n",
      "test loss on epoch 4198: 0.352\n",
      "test accuracy on epoch 4198: 0.769\n",
      "train loss on epoch 4199 : 0.215\n",
      "train accuracy on epoch 4199: 0.889\n",
      "test loss on epoch 4199: 0.352\n",
      "test accuracy on epoch 4199: 0.769\n",
      "train loss on epoch 4200 : 0.283\n",
      "train accuracy on epoch 4200: 0.889\n",
      "test loss on epoch 4200: 0.353\n",
      "test accuracy on epoch 4200: 0.692\n",
      "train loss on epoch 4201 : 0.070\n",
      "train accuracy on epoch 4201: 1.000\n",
      "test loss on epoch 4201: 0.350\n",
      "test accuracy on epoch 4201: 0.769\n",
      "train loss on epoch 4202 : 0.171\n",
      "train accuracy on epoch 4202: 0.944\n",
      "test loss on epoch 4202: 0.340\n",
      "test accuracy on epoch 4202: 0.769\n",
      "train loss on epoch 4203 : 0.288\n",
      "train accuracy on epoch 4203: 0.833\n",
      "test loss on epoch 4203: 0.353\n",
      "test accuracy on epoch 4203: 0.769\n",
      "train loss on epoch 4204 : 0.347\n",
      "train accuracy on epoch 4204: 0.778\n",
      "test loss on epoch 4204: 0.342\n",
      "test accuracy on epoch 4204: 0.769\n",
      "train loss on epoch 4205 : 0.453\n",
      "train accuracy on epoch 4205: 0.833\n",
      "test loss on epoch 4205: 0.348\n",
      "test accuracy on epoch 4205: 0.769\n",
      "train loss on epoch 4206 : 0.225\n",
      "train accuracy on epoch 4206: 0.889\n",
      "test loss on epoch 4206: 0.343\n",
      "test accuracy on epoch 4206: 0.846\n",
      "train loss on epoch 4207 : 0.301\n",
      "train accuracy on epoch 4207: 0.833\n",
      "test loss on epoch 4207: 0.358\n",
      "test accuracy on epoch 4207: 0.692\n",
      "train loss on epoch 4208 : 0.105\n",
      "train accuracy on epoch 4208: 1.000\n",
      "test loss on epoch 4208: 0.357\n",
      "test accuracy on epoch 4208: 0.692\n",
      "train loss on epoch 4209 : 0.194\n",
      "train accuracy on epoch 4209: 0.889\n",
      "test loss on epoch 4209: 0.346\n",
      "test accuracy on epoch 4209: 0.846\n",
      "train loss on epoch 4210 : 0.091\n",
      "train accuracy on epoch 4210: 0.944\n",
      "test loss on epoch 4210: 0.359\n",
      "test accuracy on epoch 4210: 0.692\n",
      "train loss on epoch 4211 : 0.110\n",
      "train accuracy on epoch 4211: 0.889\n",
      "test loss on epoch 4211: 0.363\n",
      "test accuracy on epoch 4211: 0.692\n",
      "train loss on epoch 4212 : 0.196\n",
      "train accuracy on epoch 4212: 0.833\n",
      "test loss on epoch 4212: 0.349\n",
      "test accuracy on epoch 4212: 0.846\n",
      "train loss on epoch 4213 : 0.179\n",
      "train accuracy on epoch 4213: 0.944\n",
      "test loss on epoch 4213: 0.364\n",
      "test accuracy on epoch 4213: 0.692\n",
      "train loss on epoch 4214 : 0.339\n",
      "train accuracy on epoch 4214: 0.889\n",
      "test loss on epoch 4214: 0.367\n",
      "test accuracy on epoch 4214: 0.769\n",
      "train loss on epoch 4215 : 0.171\n",
      "train accuracy on epoch 4215: 0.944\n",
      "test loss on epoch 4215: 0.360\n",
      "test accuracy on epoch 4215: 0.769\n",
      "train loss on epoch 4216 : 0.082\n",
      "train accuracy on epoch 4216: 1.000\n",
      "test loss on epoch 4216: 0.362\n",
      "test accuracy on epoch 4216: 0.769\n",
      "train loss on epoch 4217 : 0.182\n",
      "train accuracy on epoch 4217: 0.889\n",
      "test loss on epoch 4217: 0.364\n",
      "test accuracy on epoch 4217: 0.769\n",
      "train loss on epoch 4218 : 0.226\n",
      "train accuracy on epoch 4218: 0.889\n",
      "test loss on epoch 4218: 0.361\n",
      "test accuracy on epoch 4218: 0.769\n",
      "train loss on epoch 4219 : 0.058\n",
      "train accuracy on epoch 4219: 1.000\n",
      "test loss on epoch 4219: 0.368\n",
      "test accuracy on epoch 4219: 0.769\n",
      "train loss on epoch 4220 : 0.167\n",
      "train accuracy on epoch 4220: 0.889\n",
      "test loss on epoch 4220: 0.368\n",
      "test accuracy on epoch 4220: 0.769\n",
      "train loss on epoch 4221 : 0.121\n",
      "train accuracy on epoch 4221: 0.944\n",
      "test loss on epoch 4221: 0.365\n",
      "test accuracy on epoch 4221: 0.769\n",
      "train loss on epoch 4222 : 0.159\n",
      "train accuracy on epoch 4222: 0.889\n",
      "test loss on epoch 4222: 0.367\n",
      "test accuracy on epoch 4222: 0.769\n",
      "train loss on epoch 4223 : 0.111\n",
      "train accuracy on epoch 4223: 0.944\n",
      "test loss on epoch 4223: 0.357\n",
      "test accuracy on epoch 4223: 0.769\n",
      "train loss on epoch 4224 : 0.314\n",
      "train accuracy on epoch 4224: 0.833\n",
      "test loss on epoch 4224: 0.360\n",
      "test accuracy on epoch 4224: 0.769\n",
      "train loss on epoch 4225 : 0.075\n",
      "train accuracy on epoch 4225: 1.000\n",
      "test loss on epoch 4225: 0.350\n",
      "test accuracy on epoch 4225: 0.769\n",
      "train loss on epoch 4226 : 0.093\n",
      "train accuracy on epoch 4226: 1.000\n",
      "test loss on epoch 4226: 0.357\n",
      "test accuracy on epoch 4226: 0.769\n",
      "train loss on epoch 4227 : 0.198\n",
      "train accuracy on epoch 4227: 0.889\n",
      "test loss on epoch 4227: 0.362\n",
      "test accuracy on epoch 4227: 0.769\n",
      "train loss on epoch 4228 : 0.159\n",
      "train accuracy on epoch 4228: 0.889\n",
      "test loss on epoch 4228: 0.352\n",
      "test accuracy on epoch 4228: 0.769\n",
      "train loss on epoch 4229 : 0.199\n",
      "train accuracy on epoch 4229: 0.944\n",
      "test loss on epoch 4229: 0.351\n",
      "test accuracy on epoch 4229: 0.769\n",
      "train loss on epoch 4230 : 0.120\n",
      "train accuracy on epoch 4230: 1.000\n",
      "test loss on epoch 4230: 0.353\n",
      "test accuracy on epoch 4230: 0.769\n",
      "train loss on epoch 4231 : 0.172\n",
      "train accuracy on epoch 4231: 0.944\n",
      "test loss on epoch 4231: 0.366\n",
      "test accuracy on epoch 4231: 0.769\n",
      "train loss on epoch 4232 : 0.113\n",
      "train accuracy on epoch 4232: 1.000\n",
      "test loss on epoch 4232: 0.373\n",
      "test accuracy on epoch 4232: 0.769\n",
      "train loss on epoch 4233 : 0.186\n",
      "train accuracy on epoch 4233: 0.889\n",
      "test loss on epoch 4233: 0.356\n",
      "test accuracy on epoch 4233: 0.769\n",
      "train loss on epoch 4234 : 0.131\n",
      "train accuracy on epoch 4234: 0.889\n",
      "test loss on epoch 4234: 0.360\n",
      "test accuracy on epoch 4234: 0.769\n",
      "train loss on epoch 4235 : 0.166\n",
      "train accuracy on epoch 4235: 0.889\n",
      "test loss on epoch 4235: 0.369\n",
      "test accuracy on epoch 4235: 0.769\n",
      "train loss on epoch 4236 : 0.246\n",
      "train accuracy on epoch 4236: 0.944\n",
      "test loss on epoch 4236: 0.360\n",
      "test accuracy on epoch 4236: 0.846\n",
      "train loss on epoch 4237 : 0.412\n",
      "train accuracy on epoch 4237: 0.889\n",
      "test loss on epoch 4237: 0.371\n",
      "test accuracy on epoch 4237: 0.769\n",
      "train loss on epoch 4238 : 0.351\n",
      "train accuracy on epoch 4238: 0.889\n",
      "test loss on epoch 4238: 0.379\n",
      "test accuracy on epoch 4238: 0.692\n",
      "train loss on epoch 4239 : 0.116\n",
      "train accuracy on epoch 4239: 0.944\n",
      "test loss on epoch 4239: 0.379\n",
      "test accuracy on epoch 4239: 0.769\n",
      "train loss on epoch 4240 : 0.080\n",
      "train accuracy on epoch 4240: 1.000\n",
      "test loss on epoch 4240: 0.368\n",
      "test accuracy on epoch 4240: 0.769\n",
      "train loss on epoch 4241 : 0.141\n",
      "train accuracy on epoch 4241: 0.889\n",
      "test loss on epoch 4241: 0.370\n",
      "test accuracy on epoch 4241: 0.769\n",
      "train loss on epoch 4242 : 0.116\n",
      "train accuracy on epoch 4242: 0.944\n",
      "test loss on epoch 4242: 0.377\n",
      "test accuracy on epoch 4242: 0.769\n",
      "train loss on epoch 4243 : 0.260\n",
      "train accuracy on epoch 4243: 0.889\n",
      "test loss on epoch 4243: 0.382\n",
      "test accuracy on epoch 4243: 0.769\n",
      "train loss on epoch 4244 : 0.039\n",
      "train accuracy on epoch 4244: 1.000\n",
      "test loss on epoch 4244: 0.377\n",
      "test accuracy on epoch 4244: 0.769\n",
      "train loss on epoch 4245 : 0.502\n",
      "train accuracy on epoch 4245: 0.889\n",
      "test loss on epoch 4245: 0.381\n",
      "test accuracy on epoch 4245: 0.769\n",
      "train loss on epoch 4246 : 0.262\n",
      "train accuracy on epoch 4246: 0.889\n",
      "test loss on epoch 4246: 0.371\n",
      "test accuracy on epoch 4246: 0.846\n",
      "train loss on epoch 4247 : 0.339\n",
      "train accuracy on epoch 4247: 0.889\n",
      "test loss on epoch 4247: 0.376\n",
      "test accuracy on epoch 4247: 0.769\n",
      "train loss on epoch 4248 : 0.193\n",
      "train accuracy on epoch 4248: 0.889\n",
      "test loss on epoch 4248: 0.372\n",
      "test accuracy on epoch 4248: 0.769\n",
      "train loss on epoch 4249 : 0.104\n",
      "train accuracy on epoch 4249: 0.944\n",
      "test loss on epoch 4249: 0.378\n",
      "test accuracy on epoch 4249: 0.769\n",
      "train loss on epoch 4250 : 0.222\n",
      "train accuracy on epoch 4250: 0.944\n",
      "test loss on epoch 4250: 0.371\n",
      "test accuracy on epoch 4250: 0.846\n",
      "train loss on epoch 4251 : 0.150\n",
      "train accuracy on epoch 4251: 0.944\n",
      "test loss on epoch 4251: 0.379\n",
      "test accuracy on epoch 4251: 0.769\n",
      "train loss on epoch 4252 : 0.071\n",
      "train accuracy on epoch 4252: 1.000\n",
      "test loss on epoch 4252: 0.380\n",
      "test accuracy on epoch 4252: 0.769\n",
      "train loss on epoch 4253 : 0.132\n",
      "train accuracy on epoch 4253: 0.944\n",
      "test loss on epoch 4253: 0.374\n",
      "test accuracy on epoch 4253: 0.769\n",
      "train loss on epoch 4254 : 0.049\n",
      "train accuracy on epoch 4254: 1.000\n",
      "test loss on epoch 4254: 0.379\n",
      "test accuracy on epoch 4254: 0.769\n",
      "train loss on epoch 4255 : 0.197\n",
      "train accuracy on epoch 4255: 0.944\n",
      "test loss on epoch 4255: 0.381\n",
      "test accuracy on epoch 4255: 0.769\n",
      "train loss on epoch 4256 : 0.143\n",
      "train accuracy on epoch 4256: 0.889\n",
      "test loss on epoch 4256: 0.381\n",
      "test accuracy on epoch 4256: 0.769\n",
      "train loss on epoch 4257 : 0.172\n",
      "train accuracy on epoch 4257: 0.944\n",
      "test loss on epoch 4257: 0.389\n",
      "test accuracy on epoch 4257: 0.769\n",
      "train loss on epoch 4258 : 0.205\n",
      "train accuracy on epoch 4258: 0.944\n",
      "test loss on epoch 4258: 0.393\n",
      "test accuracy on epoch 4258: 0.769\n",
      "train loss on epoch 4259 : 0.195\n",
      "train accuracy on epoch 4259: 0.944\n",
      "test loss on epoch 4259: 0.384\n",
      "test accuracy on epoch 4259: 0.769\n",
      "train loss on epoch 4260 : 0.060\n",
      "train accuracy on epoch 4260: 1.000\n",
      "test loss on epoch 4260: 0.387\n",
      "test accuracy on epoch 4260: 0.769\n",
      "train loss on epoch 4261 : 0.167\n",
      "train accuracy on epoch 4261: 0.944\n",
      "test loss on epoch 4261: 0.377\n",
      "test accuracy on epoch 4261: 0.769\n",
      "train loss on epoch 4262 : 0.122\n",
      "train accuracy on epoch 4262: 0.944\n",
      "test loss on epoch 4262: 0.380\n",
      "test accuracy on epoch 4262: 0.769\n",
      "train loss on epoch 4263 : 0.078\n",
      "train accuracy on epoch 4263: 0.944\n",
      "test loss on epoch 4263: 0.368\n",
      "test accuracy on epoch 4263: 0.769\n",
      "train loss on epoch 4264 : 0.473\n",
      "train accuracy on epoch 4264: 0.833\n",
      "test loss on epoch 4264: 0.363\n",
      "test accuracy on epoch 4264: 0.769\n",
      "train loss on epoch 4265 : 0.130\n",
      "train accuracy on epoch 4265: 0.944\n",
      "test loss on epoch 4265: 0.365\n",
      "test accuracy on epoch 4265: 0.769\n",
      "train loss on epoch 4266 : 0.216\n",
      "train accuracy on epoch 4266: 0.944\n",
      "test loss on epoch 4266: 0.364\n",
      "test accuracy on epoch 4266: 0.769\n",
      "train loss on epoch 4267 : 0.083\n",
      "train accuracy on epoch 4267: 1.000\n",
      "test loss on epoch 4267: 0.360\n",
      "test accuracy on epoch 4267: 0.769\n",
      "train loss on epoch 4268 : 0.185\n",
      "train accuracy on epoch 4268: 0.944\n",
      "test loss on epoch 4268: 0.377\n",
      "test accuracy on epoch 4268: 0.769\n",
      "train loss on epoch 4269 : 0.254\n",
      "train accuracy on epoch 4269: 0.889\n",
      "test loss on epoch 4269: 0.367\n",
      "test accuracy on epoch 4269: 0.769\n",
      "train loss on epoch 4270 : 0.120\n",
      "train accuracy on epoch 4270: 0.944\n",
      "test loss on epoch 4270: 0.360\n",
      "test accuracy on epoch 4270: 0.769\n",
      "train loss on epoch 4271 : 0.294\n",
      "train accuracy on epoch 4271: 0.889\n",
      "test loss on epoch 4271: 0.360\n",
      "test accuracy on epoch 4271: 0.769\n",
      "train loss on epoch 4272 : 0.188\n",
      "train accuracy on epoch 4272: 0.944\n",
      "test loss on epoch 4272: 0.361\n",
      "test accuracy on epoch 4272: 0.769\n",
      "train loss on epoch 4273 : 0.069\n",
      "train accuracy on epoch 4273: 1.000\n",
      "test loss on epoch 4273: 0.370\n",
      "test accuracy on epoch 4273: 0.769\n",
      "train loss on epoch 4274 : 0.404\n",
      "train accuracy on epoch 4274: 0.889\n",
      "test loss on epoch 4274: 0.373\n",
      "test accuracy on epoch 4274: 0.769\n",
      "train loss on epoch 4275 : 0.207\n",
      "train accuracy on epoch 4275: 0.889\n",
      "test loss on epoch 4275: 0.377\n",
      "test accuracy on epoch 4275: 0.769\n",
      "train loss on epoch 4276 : 0.134\n",
      "train accuracy on epoch 4276: 0.889\n",
      "test loss on epoch 4276: 0.377\n",
      "test accuracy on epoch 4276: 0.769\n",
      "train loss on epoch 4277 : 0.199\n",
      "train accuracy on epoch 4277: 0.944\n",
      "test loss on epoch 4277: 0.374\n",
      "test accuracy on epoch 4277: 0.769\n",
      "train loss on epoch 4278 : 0.257\n",
      "train accuracy on epoch 4278: 0.889\n",
      "test loss on epoch 4278: 0.374\n",
      "test accuracy on epoch 4278: 0.769\n",
      "train loss on epoch 4279 : 0.062\n",
      "train accuracy on epoch 4279: 1.000\n",
      "test loss on epoch 4279: 0.382\n",
      "test accuracy on epoch 4279: 0.769\n",
      "train loss on epoch 4280 : 0.291\n",
      "train accuracy on epoch 4280: 0.889\n",
      "test loss on epoch 4280: 0.387\n",
      "test accuracy on epoch 4280: 0.769\n",
      "train loss on epoch 4281 : 0.260\n",
      "train accuracy on epoch 4281: 0.944\n",
      "test loss on epoch 4281: 0.387\n",
      "test accuracy on epoch 4281: 0.769\n",
      "train loss on epoch 4282 : 0.071\n",
      "train accuracy on epoch 4282: 1.000\n",
      "test loss on epoch 4282: 0.388\n",
      "test accuracy on epoch 4282: 0.769\n",
      "train loss on epoch 4283 : 0.041\n",
      "train accuracy on epoch 4283: 1.000\n",
      "test loss on epoch 4283: 0.392\n",
      "test accuracy on epoch 4283: 0.769\n",
      "train loss on epoch 4284 : 0.119\n",
      "train accuracy on epoch 4284: 0.944\n",
      "test loss on epoch 4284: 0.388\n",
      "test accuracy on epoch 4284: 0.769\n",
      "train loss on epoch 4285 : 0.100\n",
      "train accuracy on epoch 4285: 1.000\n",
      "test loss on epoch 4285: 0.385\n",
      "test accuracy on epoch 4285: 0.769\n",
      "train loss on epoch 4286 : 0.077\n",
      "train accuracy on epoch 4286: 1.000\n",
      "test loss on epoch 4286: 0.388\n",
      "test accuracy on epoch 4286: 0.769\n",
      "train loss on epoch 4287 : 0.083\n",
      "train accuracy on epoch 4287: 0.944\n",
      "test loss on epoch 4287: 0.381\n",
      "test accuracy on epoch 4287: 0.769\n",
      "train loss on epoch 4288 : 0.185\n",
      "train accuracy on epoch 4288: 0.889\n",
      "test loss on epoch 4288: 0.383\n",
      "test accuracy on epoch 4288: 0.769\n",
      "train loss on epoch 4289 : 0.104\n",
      "train accuracy on epoch 4289: 1.000\n",
      "test loss on epoch 4289: 0.380\n",
      "test accuracy on epoch 4289: 0.769\n",
      "train loss on epoch 4290 : 0.101\n",
      "train accuracy on epoch 4290: 1.000\n",
      "test loss on epoch 4290: 0.379\n",
      "test accuracy on epoch 4290: 0.769\n",
      "train loss on epoch 4291 : 0.068\n",
      "train accuracy on epoch 4291: 1.000\n",
      "test loss on epoch 4291: 0.366\n",
      "test accuracy on epoch 4291: 0.769\n",
      "train loss on epoch 4292 : 0.062\n",
      "train accuracy on epoch 4292: 0.944\n",
      "test loss on epoch 4292: 0.371\n",
      "test accuracy on epoch 4292: 0.769\n",
      "train loss on epoch 4293 : 0.191\n",
      "train accuracy on epoch 4293: 0.889\n",
      "test loss on epoch 4293: 0.369\n",
      "test accuracy on epoch 4293: 0.769\n",
      "train loss on epoch 4294 : 0.159\n",
      "train accuracy on epoch 4294: 0.944\n",
      "test loss on epoch 4294: 0.368\n",
      "test accuracy on epoch 4294: 0.769\n",
      "train loss on epoch 4295 : 0.102\n",
      "train accuracy on epoch 4295: 0.944\n",
      "test loss on epoch 4295: 0.376\n",
      "test accuracy on epoch 4295: 0.769\n",
      "train loss on epoch 4296 : 0.226\n",
      "train accuracy on epoch 4296: 0.889\n",
      "test loss on epoch 4296: 0.376\n",
      "test accuracy on epoch 4296: 0.769\n",
      "train loss on epoch 4297 : 0.148\n",
      "train accuracy on epoch 4297: 0.944\n",
      "test loss on epoch 4297: 0.375\n",
      "test accuracy on epoch 4297: 0.769\n",
      "train loss on epoch 4298 : 0.173\n",
      "train accuracy on epoch 4298: 0.944\n",
      "test loss on epoch 4298: 0.377\n",
      "test accuracy on epoch 4298: 0.769\n",
      "train loss on epoch 4299 : 0.102\n",
      "train accuracy on epoch 4299: 0.944\n",
      "test loss on epoch 4299: 0.369\n",
      "test accuracy on epoch 4299: 0.769\n",
      "train loss on epoch 4300 : 0.190\n",
      "train accuracy on epoch 4300: 0.944\n",
      "test loss on epoch 4300: 0.363\n",
      "test accuracy on epoch 4300: 0.769\n",
      "train loss on epoch 4301 : 0.061\n",
      "train accuracy on epoch 4301: 1.000\n",
      "test loss on epoch 4301: 0.376\n",
      "test accuracy on epoch 4301: 0.769\n",
      "train loss on epoch 4302 : 0.215\n",
      "train accuracy on epoch 4302: 0.944\n",
      "test loss on epoch 4302: 0.374\n",
      "test accuracy on epoch 4302: 0.692\n",
      "train loss on epoch 4303 : 0.118\n",
      "train accuracy on epoch 4303: 1.000\n",
      "test loss on epoch 4303: 0.360\n",
      "test accuracy on epoch 4303: 0.769\n",
      "train loss on epoch 4304 : 0.081\n",
      "train accuracy on epoch 4304: 1.000\n",
      "test loss on epoch 4304: 0.364\n",
      "test accuracy on epoch 4304: 0.769\n",
      "train loss on epoch 4305 : 0.206\n",
      "train accuracy on epoch 4305: 0.889\n",
      "test loss on epoch 4305: 0.355\n",
      "test accuracy on epoch 4305: 0.769\n",
      "train loss on epoch 4306 : 0.067\n",
      "train accuracy on epoch 4306: 1.000\n",
      "test loss on epoch 4306: 0.367\n",
      "test accuracy on epoch 4306: 0.769\n",
      "train loss on epoch 4307 : 0.123\n",
      "train accuracy on epoch 4307: 1.000\n",
      "test loss on epoch 4307: 0.356\n",
      "test accuracy on epoch 4307: 0.846\n",
      "train loss on epoch 4308 : 0.282\n",
      "train accuracy on epoch 4308: 0.944\n",
      "test loss on epoch 4308: 0.372\n",
      "test accuracy on epoch 4308: 0.692\n",
      "train loss on epoch 4309 : 0.267\n",
      "train accuracy on epoch 4309: 0.889\n",
      "test loss on epoch 4309: 0.363\n",
      "test accuracy on epoch 4309: 0.769\n",
      "train loss on epoch 4310 : 0.080\n",
      "train accuracy on epoch 4310: 1.000\n",
      "test loss on epoch 4310: 0.357\n",
      "test accuracy on epoch 4310: 0.769\n",
      "train loss on epoch 4311 : 0.284\n",
      "train accuracy on epoch 4311: 0.944\n",
      "test loss on epoch 4311: 0.361\n",
      "test accuracy on epoch 4311: 0.769\n",
      "train loss on epoch 4312 : 0.312\n",
      "train accuracy on epoch 4312: 0.889\n",
      "test loss on epoch 4312: 0.357\n",
      "test accuracy on epoch 4312: 0.769\n",
      "train loss on epoch 4313 : 0.216\n",
      "train accuracy on epoch 4313: 0.889\n",
      "test loss on epoch 4313: 0.357\n",
      "test accuracy on epoch 4313: 0.769\n",
      "train loss on epoch 4314 : 0.287\n",
      "train accuracy on epoch 4314: 0.944\n",
      "test loss on epoch 4314: 0.379\n",
      "test accuracy on epoch 4314: 0.769\n",
      "train loss on epoch 4315 : 0.265\n",
      "train accuracy on epoch 4315: 0.944\n",
      "test loss on epoch 4315: 0.378\n",
      "test accuracy on epoch 4315: 0.769\n",
      "train loss on epoch 4316 : 0.250\n",
      "train accuracy on epoch 4316: 0.944\n",
      "test loss on epoch 4316: 0.368\n",
      "test accuracy on epoch 4316: 0.769\n",
      "train loss on epoch 4317 : 0.186\n",
      "train accuracy on epoch 4317: 0.944\n",
      "test loss on epoch 4317: 0.377\n",
      "test accuracy on epoch 4317: 0.692\n",
      "train loss on epoch 4318 : 0.140\n",
      "train accuracy on epoch 4318: 0.944\n",
      "test loss on epoch 4318: 0.363\n",
      "test accuracy on epoch 4318: 0.769\n",
      "train loss on epoch 4319 : 0.140\n",
      "train accuracy on epoch 4319: 1.000\n",
      "test loss on epoch 4319: 0.371\n",
      "test accuracy on epoch 4319: 0.769\n",
      "train loss on epoch 4320 : 0.326\n",
      "train accuracy on epoch 4320: 0.889\n",
      "test loss on epoch 4320: 0.386\n",
      "test accuracy on epoch 4320: 0.769\n",
      "train loss on epoch 4321 : 0.060\n",
      "train accuracy on epoch 4321: 1.000\n",
      "test loss on epoch 4321: 0.388\n",
      "test accuracy on epoch 4321: 0.769\n",
      "train loss on epoch 4322 : 0.033\n",
      "train accuracy on epoch 4322: 1.000\n",
      "test loss on epoch 4322: 0.388\n",
      "test accuracy on epoch 4322: 0.769\n",
      "train loss on epoch 4323 : 0.203\n",
      "train accuracy on epoch 4323: 0.889\n",
      "test loss on epoch 4323: 0.390\n",
      "test accuracy on epoch 4323: 0.769\n",
      "train loss on epoch 4324 : 0.280\n",
      "train accuracy on epoch 4324: 0.889\n",
      "test loss on epoch 4324: 0.390\n",
      "test accuracy on epoch 4324: 0.769\n",
      "train loss on epoch 4325 : 0.096\n",
      "train accuracy on epoch 4325: 0.944\n",
      "test loss on epoch 4325: 0.387\n",
      "test accuracy on epoch 4325: 0.769\n",
      "train loss on epoch 4326 : 0.117\n",
      "train accuracy on epoch 4326: 0.944\n",
      "test loss on epoch 4326: 0.399\n",
      "test accuracy on epoch 4326: 0.769\n",
      "train loss on epoch 4327 : 0.267\n",
      "train accuracy on epoch 4327: 0.944\n",
      "test loss on epoch 4327: 0.396\n",
      "test accuracy on epoch 4327: 0.769\n",
      "train loss on epoch 4328 : 0.254\n",
      "train accuracy on epoch 4328: 0.833\n",
      "test loss on epoch 4328: 0.384\n",
      "test accuracy on epoch 4328: 0.769\n",
      "train loss on epoch 4329 : 0.238\n",
      "train accuracy on epoch 4329: 0.944\n",
      "test loss on epoch 4329: 0.369\n",
      "test accuracy on epoch 4329: 0.769\n",
      "train loss on epoch 4330 : 0.372\n",
      "train accuracy on epoch 4330: 0.944\n",
      "test loss on epoch 4330: 0.371\n",
      "test accuracy on epoch 4330: 0.769\n",
      "train loss on epoch 4331 : 0.109\n",
      "train accuracy on epoch 4331: 0.944\n",
      "test loss on epoch 4331: 0.362\n",
      "test accuracy on epoch 4331: 0.769\n",
      "train loss on epoch 4332 : 0.134\n",
      "train accuracy on epoch 4332: 0.889\n",
      "test loss on epoch 4332: 0.362\n",
      "test accuracy on epoch 4332: 0.769\n",
      "train loss on epoch 4333 : 0.104\n",
      "train accuracy on epoch 4333: 0.944\n",
      "test loss on epoch 4333: 0.371\n",
      "test accuracy on epoch 4333: 0.769\n",
      "train loss on epoch 4334 : 0.156\n",
      "train accuracy on epoch 4334: 0.944\n",
      "test loss on epoch 4334: 0.366\n",
      "test accuracy on epoch 4334: 0.769\n",
      "train loss on epoch 4335 : 0.153\n",
      "train accuracy on epoch 4335: 0.889\n",
      "test loss on epoch 4335: 0.379\n",
      "test accuracy on epoch 4335: 0.769\n",
      "train loss on epoch 4336 : 0.122\n",
      "train accuracy on epoch 4336: 0.944\n",
      "test loss on epoch 4336: 0.376\n",
      "test accuracy on epoch 4336: 0.769\n",
      "train loss on epoch 4337 : 0.385\n",
      "train accuracy on epoch 4337: 0.889\n",
      "test loss on epoch 4337: 0.376\n",
      "test accuracy on epoch 4337: 0.769\n",
      "train loss on epoch 4338 : 0.057\n",
      "train accuracy on epoch 4338: 1.000\n",
      "test loss on epoch 4338: 0.372\n",
      "test accuracy on epoch 4338: 0.769\n",
      "train loss on epoch 4339 : 0.109\n",
      "train accuracy on epoch 4339: 0.944\n",
      "test loss on epoch 4339: 0.374\n",
      "test accuracy on epoch 4339: 0.769\n",
      "train loss on epoch 4340 : 0.286\n",
      "train accuracy on epoch 4340: 0.833\n",
      "test loss on epoch 4340: 0.374\n",
      "test accuracy on epoch 4340: 0.769\n",
      "train loss on epoch 4341 : 0.291\n",
      "train accuracy on epoch 4341: 0.778\n",
      "test loss on epoch 4341: 0.376\n",
      "test accuracy on epoch 4341: 0.769\n",
      "train loss on epoch 4342 : 0.218\n",
      "train accuracy on epoch 4342: 0.889\n",
      "test loss on epoch 4342: 0.371\n",
      "test accuracy on epoch 4342: 0.769\n",
      "train loss on epoch 4343 : 0.090\n",
      "train accuracy on epoch 4343: 1.000\n",
      "test loss on epoch 4343: 0.377\n",
      "test accuracy on epoch 4343: 0.769\n",
      "train loss on epoch 4344 : 0.058\n",
      "train accuracy on epoch 4344: 1.000\n",
      "test loss on epoch 4344: 0.383\n",
      "test accuracy on epoch 4344: 0.769\n",
      "train loss on epoch 4345 : 0.164\n",
      "train accuracy on epoch 4345: 0.889\n",
      "test loss on epoch 4345: 0.384\n",
      "test accuracy on epoch 4345: 0.769\n",
      "train loss on epoch 4346 : 0.150\n",
      "train accuracy on epoch 4346: 0.944\n",
      "test loss on epoch 4346: 0.390\n",
      "test accuracy on epoch 4346: 0.769\n",
      "train loss on epoch 4347 : 0.144\n",
      "train accuracy on epoch 4347: 0.889\n",
      "test loss on epoch 4347: 0.400\n",
      "test accuracy on epoch 4347: 0.769\n",
      "train loss on epoch 4348 : 0.219\n",
      "train accuracy on epoch 4348: 0.944\n",
      "test loss on epoch 4348: 0.400\n",
      "test accuracy on epoch 4348: 0.769\n",
      "train loss on epoch 4349 : 0.203\n",
      "train accuracy on epoch 4349: 0.944\n",
      "test loss on epoch 4349: 0.407\n",
      "test accuracy on epoch 4349: 0.769\n",
      "train loss on epoch 4350 : 0.140\n",
      "train accuracy on epoch 4350: 0.944\n",
      "test loss on epoch 4350: 0.406\n",
      "test accuracy on epoch 4350: 0.769\n",
      "train loss on epoch 4351 : 0.212\n",
      "train accuracy on epoch 4351: 0.889\n",
      "test loss on epoch 4351: 0.413\n",
      "test accuracy on epoch 4351: 0.769\n",
      "train loss on epoch 4352 : 0.196\n",
      "train accuracy on epoch 4352: 0.944\n",
      "test loss on epoch 4352: 0.401\n",
      "test accuracy on epoch 4352: 0.769\n",
      "train loss on epoch 4353 : 0.112\n",
      "train accuracy on epoch 4353: 0.944\n",
      "test loss on epoch 4353: 0.397\n",
      "test accuracy on epoch 4353: 0.769\n",
      "train loss on epoch 4354 : 0.040\n",
      "train accuracy on epoch 4354: 1.000\n",
      "test loss on epoch 4354: 0.390\n",
      "test accuracy on epoch 4354: 0.769\n",
      "train loss on epoch 4355 : 0.278\n",
      "train accuracy on epoch 4355: 0.889\n",
      "test loss on epoch 4355: 0.386\n",
      "test accuracy on epoch 4355: 0.769\n",
      "train loss on epoch 4356 : 0.180\n",
      "train accuracy on epoch 4356: 0.944\n",
      "test loss on epoch 4356: 0.385\n",
      "test accuracy on epoch 4356: 0.769\n",
      "train loss on epoch 4357 : 0.232\n",
      "train accuracy on epoch 4357: 0.889\n",
      "test loss on epoch 4357: 0.381\n",
      "test accuracy on epoch 4357: 0.769\n",
      "train loss on epoch 4358 : 0.144\n",
      "train accuracy on epoch 4358: 0.944\n",
      "test loss on epoch 4358: 0.379\n",
      "test accuracy on epoch 4358: 0.769\n",
      "train loss on epoch 4359 : 0.088\n",
      "train accuracy on epoch 4359: 0.944\n",
      "test loss on epoch 4359: 0.385\n",
      "test accuracy on epoch 4359: 0.769\n",
      "train loss on epoch 4360 : 0.143\n",
      "train accuracy on epoch 4360: 0.889\n",
      "test loss on epoch 4360: 0.385\n",
      "test accuracy on epoch 4360: 0.769\n",
      "train loss on epoch 4361 : 0.062\n",
      "train accuracy on epoch 4361: 1.000\n",
      "test loss on epoch 4361: 0.389\n",
      "test accuracy on epoch 4361: 0.769\n",
      "train loss on epoch 4362 : 0.276\n",
      "train accuracy on epoch 4362: 0.944\n",
      "test loss on epoch 4362: 0.394\n",
      "test accuracy on epoch 4362: 0.769\n",
      "train loss on epoch 4363 : 0.344\n",
      "train accuracy on epoch 4363: 0.833\n",
      "test loss on epoch 4363: 0.397\n",
      "test accuracy on epoch 4363: 0.769\n",
      "train loss on epoch 4364 : 0.344\n",
      "train accuracy on epoch 4364: 0.944\n",
      "test loss on epoch 4364: 0.397\n",
      "test accuracy on epoch 4364: 0.769\n",
      "train loss on epoch 4365 : 0.068\n",
      "train accuracy on epoch 4365: 0.944\n",
      "test loss on epoch 4365: 0.402\n",
      "test accuracy on epoch 4365: 0.769\n",
      "train loss on epoch 4366 : 0.070\n",
      "train accuracy on epoch 4366: 0.944\n",
      "test loss on epoch 4366: 0.399\n",
      "test accuracy on epoch 4366: 0.769\n",
      "train loss on epoch 4367 : 0.123\n",
      "train accuracy on epoch 4367: 0.944\n",
      "test loss on epoch 4367: 0.395\n",
      "test accuracy on epoch 4367: 0.769\n",
      "train loss on epoch 4368 : 0.217\n",
      "train accuracy on epoch 4368: 0.889\n",
      "test loss on epoch 4368: 0.393\n",
      "test accuracy on epoch 4368: 0.769\n",
      "train loss on epoch 4369 : 0.168\n",
      "train accuracy on epoch 4369: 0.944\n",
      "test loss on epoch 4369: 0.384\n",
      "test accuracy on epoch 4369: 0.769\n",
      "train loss on epoch 4370 : 0.183\n",
      "train accuracy on epoch 4370: 0.889\n",
      "test loss on epoch 4370: 0.377\n",
      "test accuracy on epoch 4370: 0.769\n",
      "train loss on epoch 4371 : 0.114\n",
      "train accuracy on epoch 4371: 0.944\n",
      "test loss on epoch 4371: 0.373\n",
      "test accuracy on epoch 4371: 0.769\n",
      "train loss on epoch 4372 : 0.036\n",
      "train accuracy on epoch 4372: 1.000\n",
      "test loss on epoch 4372: 0.371\n",
      "test accuracy on epoch 4372: 0.769\n",
      "train loss on epoch 4373 : 0.124\n",
      "train accuracy on epoch 4373: 0.944\n",
      "test loss on epoch 4373: 0.370\n",
      "test accuracy on epoch 4373: 0.769\n",
      "train loss on epoch 4374 : 0.121\n",
      "train accuracy on epoch 4374: 0.944\n",
      "test loss on epoch 4374: 0.374\n",
      "test accuracy on epoch 4374: 0.769\n",
      "train loss on epoch 4375 : 0.094\n",
      "train accuracy on epoch 4375: 0.944\n",
      "test loss on epoch 4375: 0.372\n",
      "test accuracy on epoch 4375: 0.769\n",
      "train loss on epoch 4376 : 0.157\n",
      "train accuracy on epoch 4376: 0.944\n",
      "test loss on epoch 4376: 0.376\n",
      "test accuracy on epoch 4376: 0.769\n",
      "train loss on epoch 4377 : 0.142\n",
      "train accuracy on epoch 4377: 0.944\n",
      "test loss on epoch 4377: 0.376\n",
      "test accuracy on epoch 4377: 0.769\n",
      "train loss on epoch 4378 : 0.161\n",
      "train accuracy on epoch 4378: 0.944\n",
      "test loss on epoch 4378: 0.373\n",
      "test accuracy on epoch 4378: 0.769\n",
      "train loss on epoch 4379 : 0.168\n",
      "train accuracy on epoch 4379: 0.944\n",
      "test loss on epoch 4379: 0.379\n",
      "test accuracy on epoch 4379: 0.769\n",
      "train loss on epoch 4380 : 0.295\n",
      "train accuracy on epoch 4380: 0.833\n",
      "test loss on epoch 4380: 0.383\n",
      "test accuracy on epoch 4380: 0.769\n",
      "train loss on epoch 4381 : 0.121\n",
      "train accuracy on epoch 4381: 0.889\n",
      "test loss on epoch 4381: 0.385\n",
      "test accuracy on epoch 4381: 0.769\n",
      "train loss on epoch 4382 : 0.165\n",
      "train accuracy on epoch 4382: 0.944\n",
      "test loss on epoch 4382: 0.389\n",
      "test accuracy on epoch 4382: 0.769\n",
      "train loss on epoch 4383 : 0.074\n",
      "train accuracy on epoch 4383: 1.000\n",
      "test loss on epoch 4383: 0.391\n",
      "test accuracy on epoch 4383: 0.769\n",
      "train loss on epoch 4384 : 0.057\n",
      "train accuracy on epoch 4384: 0.944\n",
      "test loss on epoch 4384: 0.391\n",
      "test accuracy on epoch 4384: 0.769\n",
      "train loss on epoch 4385 : 0.307\n",
      "train accuracy on epoch 4385: 0.889\n",
      "test loss on epoch 4385: 0.385\n",
      "test accuracy on epoch 4385: 0.769\n",
      "train loss on epoch 4386 : 0.126\n",
      "train accuracy on epoch 4386: 0.944\n",
      "test loss on epoch 4386: 0.381\n",
      "test accuracy on epoch 4386: 0.769\n",
      "train loss on epoch 4387 : 0.345\n",
      "train accuracy on epoch 4387: 0.889\n",
      "test loss on epoch 4387: 0.381\n",
      "test accuracy on epoch 4387: 0.769\n",
      "train loss on epoch 4388 : 0.240\n",
      "train accuracy on epoch 4388: 0.889\n",
      "test loss on epoch 4388: 0.384\n",
      "test accuracy on epoch 4388: 0.769\n",
      "train loss on epoch 4389 : 0.134\n",
      "train accuracy on epoch 4389: 0.944\n",
      "test loss on epoch 4389: 0.388\n",
      "test accuracy on epoch 4389: 0.769\n",
      "train loss on epoch 4390 : 0.327\n",
      "train accuracy on epoch 4390: 0.944\n",
      "test loss on epoch 4390: 0.383\n",
      "test accuracy on epoch 4390: 0.769\n",
      "train loss on epoch 4391 : 0.221\n",
      "train accuracy on epoch 4391: 0.944\n",
      "test loss on epoch 4391: 0.376\n",
      "test accuracy on epoch 4391: 0.769\n",
      "train loss on epoch 4392 : 0.188\n",
      "train accuracy on epoch 4392: 0.889\n",
      "test loss on epoch 4392: 0.386\n",
      "test accuracy on epoch 4392: 0.769\n",
      "train loss on epoch 4393 : 0.233\n",
      "train accuracy on epoch 4393: 0.889\n",
      "test loss on epoch 4393: 0.377\n",
      "test accuracy on epoch 4393: 0.769\n",
      "train loss on epoch 4394 : 0.320\n",
      "train accuracy on epoch 4394: 0.889\n",
      "test loss on epoch 4394: 0.377\n",
      "test accuracy on epoch 4394: 0.769\n",
      "train loss on epoch 4395 : 0.078\n",
      "train accuracy on epoch 4395: 1.000\n",
      "test loss on epoch 4395: 0.375\n",
      "test accuracy on epoch 4395: 0.769\n",
      "train loss on epoch 4396 : 0.337\n",
      "train accuracy on epoch 4396: 0.944\n",
      "test loss on epoch 4396: 0.377\n",
      "test accuracy on epoch 4396: 0.769\n",
      "train loss on epoch 4397 : 0.103\n",
      "train accuracy on epoch 4397: 0.944\n",
      "test loss on epoch 4397: 0.379\n",
      "test accuracy on epoch 4397: 0.769\n",
      "train loss on epoch 4398 : 0.049\n",
      "train accuracy on epoch 4398: 1.000\n",
      "test loss on epoch 4398: 0.379\n",
      "test accuracy on epoch 4398: 0.769\n",
      "train loss on epoch 4399 : 0.154\n",
      "train accuracy on epoch 4399: 0.944\n",
      "test loss on epoch 4399: 0.377\n",
      "test accuracy on epoch 4399: 0.769\n",
      "train loss on epoch 4400 : 0.150\n",
      "train accuracy on epoch 4400: 0.944\n",
      "test loss on epoch 4400: 0.375\n",
      "test accuracy on epoch 4400: 0.769\n",
      "train loss on epoch 4401 : 0.178\n",
      "train accuracy on epoch 4401: 0.889\n",
      "test loss on epoch 4401: 0.379\n",
      "test accuracy on epoch 4401: 0.769\n",
      "train loss on epoch 4402 : 0.090\n",
      "train accuracy on epoch 4402: 0.944\n",
      "test loss on epoch 4402: 0.382\n",
      "test accuracy on epoch 4402: 0.769\n",
      "train loss on epoch 4403 : 0.150\n",
      "train accuracy on epoch 4403: 0.889\n",
      "test loss on epoch 4403: 0.373\n",
      "test accuracy on epoch 4403: 0.769\n",
      "train loss on epoch 4404 : 0.228\n",
      "train accuracy on epoch 4404: 0.944\n",
      "test loss on epoch 4404: 0.381\n",
      "test accuracy on epoch 4404: 0.769\n",
      "train loss on epoch 4405 : 0.213\n",
      "train accuracy on epoch 4405: 0.944\n",
      "test loss on epoch 4405: 0.377\n",
      "test accuracy on epoch 4405: 0.769\n",
      "train loss on epoch 4406 : 0.221\n",
      "train accuracy on epoch 4406: 0.889\n",
      "test loss on epoch 4406: 0.380\n",
      "test accuracy on epoch 4406: 0.769\n",
      "train loss on epoch 4407 : 0.106\n",
      "train accuracy on epoch 4407: 0.944\n",
      "test loss on epoch 4407: 0.384\n",
      "test accuracy on epoch 4407: 0.769\n",
      "train loss on epoch 4408 : 0.200\n",
      "train accuracy on epoch 4408: 0.889\n",
      "test loss on epoch 4408: 0.385\n",
      "test accuracy on epoch 4408: 0.769\n",
      "train loss on epoch 4409 : 0.333\n",
      "train accuracy on epoch 4409: 0.889\n",
      "test loss on epoch 4409: 0.384\n",
      "test accuracy on epoch 4409: 0.769\n",
      "train loss on epoch 4410 : 0.238\n",
      "train accuracy on epoch 4410: 0.944\n",
      "test loss on epoch 4410: 0.387\n",
      "test accuracy on epoch 4410: 0.769\n",
      "train loss on epoch 4411 : 0.325\n",
      "train accuracy on epoch 4411: 0.889\n",
      "test loss on epoch 4411: 0.384\n",
      "test accuracy on epoch 4411: 0.769\n",
      "train loss on epoch 4412 : 0.172\n",
      "train accuracy on epoch 4412: 0.889\n",
      "test loss on epoch 4412: 0.391\n",
      "test accuracy on epoch 4412: 0.769\n",
      "train loss on epoch 4413 : 0.205\n",
      "train accuracy on epoch 4413: 0.889\n",
      "test loss on epoch 4413: 0.386\n",
      "test accuracy on epoch 4413: 0.769\n",
      "train loss on epoch 4414 : 0.147\n",
      "train accuracy on epoch 4414: 0.944\n",
      "test loss on epoch 4414: 0.383\n",
      "test accuracy on epoch 4414: 0.769\n",
      "train loss on epoch 4415 : 0.040\n",
      "train accuracy on epoch 4415: 1.000\n",
      "test loss on epoch 4415: 0.375\n",
      "test accuracy on epoch 4415: 0.769\n",
      "train loss on epoch 4416 : 0.144\n",
      "train accuracy on epoch 4416: 0.889\n",
      "test loss on epoch 4416: 0.388\n",
      "test accuracy on epoch 4416: 0.769\n",
      "train loss on epoch 4417 : 0.157\n",
      "train accuracy on epoch 4417: 0.944\n",
      "test loss on epoch 4417: 0.383\n",
      "test accuracy on epoch 4417: 0.769\n",
      "train loss on epoch 4418 : 0.105\n",
      "train accuracy on epoch 4418: 1.000\n",
      "test loss on epoch 4418: 0.366\n",
      "test accuracy on epoch 4418: 0.769\n",
      "train loss on epoch 4419 : 0.113\n",
      "train accuracy on epoch 4419: 0.944\n",
      "test loss on epoch 4419: 0.373\n",
      "test accuracy on epoch 4419: 0.769\n",
      "train loss on epoch 4420 : 0.354\n",
      "train accuracy on epoch 4420: 0.889\n",
      "test loss on epoch 4420: 0.364\n",
      "test accuracy on epoch 4420: 0.846\n",
      "train loss on epoch 4421 : 0.167\n",
      "train accuracy on epoch 4421: 0.944\n",
      "test loss on epoch 4421: 0.376\n",
      "test accuracy on epoch 4421: 0.769\n",
      "train loss on epoch 4422 : 0.157\n",
      "train accuracy on epoch 4422: 0.944\n",
      "test loss on epoch 4422: 0.385\n",
      "test accuracy on epoch 4422: 0.769\n",
      "train loss on epoch 4423 : 0.222\n",
      "train accuracy on epoch 4423: 0.944\n",
      "test loss on epoch 4423: 0.389\n",
      "test accuracy on epoch 4423: 0.769\n",
      "train loss on epoch 4424 : 0.215\n",
      "train accuracy on epoch 4424: 0.889\n",
      "test loss on epoch 4424: 0.383\n",
      "test accuracy on epoch 4424: 0.769\n",
      "train loss on epoch 4425 : 0.194\n",
      "train accuracy on epoch 4425: 0.889\n",
      "test loss on epoch 4425: 0.384\n",
      "test accuracy on epoch 4425: 0.769\n",
      "train loss on epoch 4426 : 0.110\n",
      "train accuracy on epoch 4426: 1.000\n",
      "test loss on epoch 4426: 0.376\n",
      "test accuracy on epoch 4426: 0.769\n",
      "train loss on epoch 4427 : 0.136\n",
      "train accuracy on epoch 4427: 0.944\n",
      "test loss on epoch 4427: 0.370\n",
      "test accuracy on epoch 4427: 0.769\n",
      "train loss on epoch 4428 : 0.056\n",
      "train accuracy on epoch 4428: 1.000\n",
      "test loss on epoch 4428: 0.359\n",
      "test accuracy on epoch 4428: 0.769\n",
      "train loss on epoch 4429 : 0.243\n",
      "train accuracy on epoch 4429: 0.889\n",
      "test loss on epoch 4429: 0.378\n",
      "test accuracy on epoch 4429: 0.692\n",
      "train loss on epoch 4430 : 0.230\n",
      "train accuracy on epoch 4430: 0.889\n",
      "test loss on epoch 4430: 0.372\n",
      "test accuracy on epoch 4430: 0.769\n",
      "train loss on epoch 4431 : 0.186\n",
      "train accuracy on epoch 4431: 0.944\n",
      "test loss on epoch 4431: 0.364\n",
      "test accuracy on epoch 4431: 0.846\n",
      "train loss on epoch 4432 : 0.026\n",
      "train accuracy on epoch 4432: 1.000\n",
      "test loss on epoch 4432: 0.373\n",
      "test accuracy on epoch 4432: 0.769\n",
      "train loss on epoch 4433 : 0.156\n",
      "train accuracy on epoch 4433: 0.944\n",
      "test loss on epoch 4433: 0.365\n",
      "test accuracy on epoch 4433: 0.846\n",
      "train loss on epoch 4434 : 0.220\n",
      "train accuracy on epoch 4434: 0.944\n",
      "test loss on epoch 4434: 0.383\n",
      "test accuracy on epoch 4434: 0.692\n",
      "train loss on epoch 4435 : 0.128\n",
      "train accuracy on epoch 4435: 0.944\n",
      "test loss on epoch 4435: 0.382\n",
      "test accuracy on epoch 4435: 0.692\n",
      "train loss on epoch 4436 : 0.093\n",
      "train accuracy on epoch 4436: 0.889\n",
      "test loss on epoch 4436: 0.374\n",
      "test accuracy on epoch 4436: 0.769\n",
      "train loss on epoch 4437 : 0.066\n",
      "train accuracy on epoch 4437: 1.000\n",
      "test loss on epoch 4437: 0.364\n",
      "test accuracy on epoch 4437: 0.846\n",
      "train loss on epoch 4438 : 0.153\n",
      "train accuracy on epoch 4438: 0.889\n",
      "test loss on epoch 4438: 0.382\n",
      "test accuracy on epoch 4438: 0.692\n",
      "train loss on epoch 4439 : 0.103\n",
      "train accuracy on epoch 4439: 0.944\n",
      "test loss on epoch 4439: 0.368\n",
      "test accuracy on epoch 4439: 0.769\n",
      "train loss on epoch 4440 : 0.170\n",
      "train accuracy on epoch 4440: 0.889\n",
      "test loss on epoch 4440: 0.381\n",
      "test accuracy on epoch 4440: 0.692\n",
      "train loss on epoch 4441 : 0.184\n",
      "train accuracy on epoch 4441: 0.833\n",
      "test loss on epoch 4441: 0.373\n",
      "test accuracy on epoch 4441: 0.769\n",
      "train loss on epoch 4442 : 0.149\n",
      "train accuracy on epoch 4442: 0.889\n",
      "test loss on epoch 4442: 0.382\n",
      "test accuracy on epoch 4442: 0.769\n",
      "train loss on epoch 4443 : 0.066\n",
      "train accuracy on epoch 4443: 0.944\n",
      "test loss on epoch 4443: 0.392\n",
      "test accuracy on epoch 4443: 0.769\n",
      "train loss on epoch 4444 : 0.109\n",
      "train accuracy on epoch 4444: 0.944\n",
      "test loss on epoch 4444: 0.380\n",
      "test accuracy on epoch 4444: 0.769\n",
      "train loss on epoch 4445 : 0.473\n",
      "train accuracy on epoch 4445: 0.833\n",
      "test loss on epoch 4445: 0.389\n",
      "test accuracy on epoch 4445: 0.769\n",
      "train loss on epoch 4446 : 0.072\n",
      "train accuracy on epoch 4446: 1.000\n",
      "test loss on epoch 4446: 0.391\n",
      "test accuracy on epoch 4446: 0.769\n",
      "train loss on epoch 4447 : 0.129\n",
      "train accuracy on epoch 4447: 0.944\n",
      "test loss on epoch 4447: 0.386\n",
      "test accuracy on epoch 4447: 0.769\n",
      "train loss on epoch 4448 : 0.104\n",
      "train accuracy on epoch 4448: 1.000\n",
      "test loss on epoch 4448: 0.390\n",
      "test accuracy on epoch 4448: 0.769\n",
      "train loss on epoch 4449 : 0.170\n",
      "train accuracy on epoch 4449: 0.944\n",
      "test loss on epoch 4449: 0.393\n",
      "test accuracy on epoch 4449: 0.769\n",
      "train loss on epoch 4450 : 0.131\n",
      "train accuracy on epoch 4450: 0.944\n",
      "test loss on epoch 4450: 0.384\n",
      "test accuracy on epoch 4450: 0.769\n",
      "train loss on epoch 4451 : 0.072\n",
      "train accuracy on epoch 4451: 1.000\n",
      "test loss on epoch 4451: 0.375\n",
      "test accuracy on epoch 4451: 0.769\n",
      "train loss on epoch 4452 : 0.177\n",
      "train accuracy on epoch 4452: 0.944\n",
      "test loss on epoch 4452: 0.379\n",
      "test accuracy on epoch 4452: 0.769\n",
      "train loss on epoch 4453 : 0.165\n",
      "train accuracy on epoch 4453: 0.944\n",
      "test loss on epoch 4453: 0.378\n",
      "test accuracy on epoch 4453: 0.769\n",
      "train loss on epoch 4454 : 0.217\n",
      "train accuracy on epoch 4454: 0.944\n",
      "test loss on epoch 4454: 0.373\n",
      "test accuracy on epoch 4454: 0.769\n",
      "train loss on epoch 4455 : 0.198\n",
      "train accuracy on epoch 4455: 0.944\n",
      "test loss on epoch 4455: 0.368\n",
      "test accuracy on epoch 4455: 0.769\n",
      "train loss on epoch 4456 : 0.142\n",
      "train accuracy on epoch 4456: 0.944\n",
      "test loss on epoch 4456: 0.376\n",
      "test accuracy on epoch 4456: 0.769\n",
      "train loss on epoch 4457 : 0.194\n",
      "train accuracy on epoch 4457: 0.944\n",
      "test loss on epoch 4457: 0.388\n",
      "test accuracy on epoch 4457: 0.692\n",
      "train loss on epoch 4458 : 0.163\n",
      "train accuracy on epoch 4458: 0.944\n",
      "test loss on epoch 4458: 0.392\n",
      "test accuracy on epoch 4458: 0.769\n",
      "train loss on epoch 4459 : 0.186\n",
      "train accuracy on epoch 4459: 0.944\n",
      "test loss on epoch 4459: 0.387\n",
      "test accuracy on epoch 4459: 0.769\n",
      "train loss on epoch 4460 : 0.135\n",
      "train accuracy on epoch 4460: 0.944\n",
      "test loss on epoch 4460: 0.388\n",
      "test accuracy on epoch 4460: 0.769\n",
      "train loss on epoch 4461 : 0.034\n",
      "train accuracy on epoch 4461: 1.000\n",
      "test loss on epoch 4461: 0.387\n",
      "test accuracy on epoch 4461: 0.769\n",
      "train loss on epoch 4462 : 0.070\n",
      "train accuracy on epoch 4462: 1.000\n",
      "test loss on epoch 4462: 0.393\n",
      "test accuracy on epoch 4462: 0.769\n",
      "train loss on epoch 4463 : 0.101\n",
      "train accuracy on epoch 4463: 0.944\n",
      "test loss on epoch 4463: 0.382\n",
      "test accuracy on epoch 4463: 0.846\n",
      "train loss on epoch 4464 : 0.203\n",
      "train accuracy on epoch 4464: 0.944\n",
      "test loss on epoch 4464: 0.389\n",
      "test accuracy on epoch 4464: 0.769\n",
      "train loss on epoch 4465 : 0.227\n",
      "train accuracy on epoch 4465: 0.944\n",
      "test loss on epoch 4465: 0.391\n",
      "test accuracy on epoch 4465: 0.769\n",
      "train loss on epoch 4466 : 0.157\n",
      "train accuracy on epoch 4466: 0.889\n",
      "test loss on epoch 4466: 0.385\n",
      "test accuracy on epoch 4466: 0.769\n",
      "train loss on epoch 4467 : 0.176\n",
      "train accuracy on epoch 4467: 0.944\n",
      "test loss on epoch 4467: 0.378\n",
      "test accuracy on epoch 4467: 0.769\n",
      "train loss on epoch 4468 : 0.170\n",
      "train accuracy on epoch 4468: 0.944\n",
      "test loss on epoch 4468: 0.399\n",
      "test accuracy on epoch 4468: 0.769\n",
      "train loss on epoch 4469 : 0.314\n",
      "train accuracy on epoch 4469: 0.778\n",
      "test loss on epoch 4469: 0.375\n",
      "test accuracy on epoch 4469: 0.769\n",
      "train loss on epoch 4470 : 0.167\n",
      "train accuracy on epoch 4470: 0.944\n",
      "test loss on epoch 4470: 0.379\n",
      "test accuracy on epoch 4470: 0.769\n",
      "train loss on epoch 4471 : 0.046\n",
      "train accuracy on epoch 4471: 1.000\n",
      "test loss on epoch 4471: 0.379\n",
      "test accuracy on epoch 4471: 0.846\n",
      "train loss on epoch 4472 : 0.146\n",
      "train accuracy on epoch 4472: 0.889\n",
      "test loss on epoch 4472: 0.393\n",
      "test accuracy on epoch 4472: 0.769\n",
      "train loss on epoch 4473 : 0.105\n",
      "train accuracy on epoch 4473: 0.944\n",
      "test loss on epoch 4473: 0.408\n",
      "test accuracy on epoch 4473: 0.769\n",
      "train loss on epoch 4474 : 0.137\n",
      "train accuracy on epoch 4474: 0.944\n",
      "test loss on epoch 4474: 0.404\n",
      "test accuracy on epoch 4474: 0.769\n",
      "train loss on epoch 4475 : 0.153\n",
      "train accuracy on epoch 4475: 0.889\n",
      "test loss on epoch 4475: 0.415\n",
      "test accuracy on epoch 4475: 0.769\n",
      "train loss on epoch 4476 : 0.165\n",
      "train accuracy on epoch 4476: 0.889\n",
      "test loss on epoch 4476: 0.420\n",
      "test accuracy on epoch 4476: 0.769\n",
      "train loss on epoch 4477 : 0.038\n",
      "train accuracy on epoch 4477: 1.000\n",
      "test loss on epoch 4477: 0.420\n",
      "test accuracy on epoch 4477: 0.769\n",
      "train loss on epoch 4478 : 0.106\n",
      "train accuracy on epoch 4478: 0.944\n",
      "test loss on epoch 4478: 0.415\n",
      "test accuracy on epoch 4478: 0.769\n",
      "train loss on epoch 4479 : 0.491\n",
      "train accuracy on epoch 4479: 0.889\n",
      "test loss on epoch 4479: 0.421\n",
      "test accuracy on epoch 4479: 0.769\n",
      "train loss on epoch 4480 : 0.145\n",
      "train accuracy on epoch 4480: 0.944\n",
      "test loss on epoch 4480: 0.419\n",
      "test accuracy on epoch 4480: 0.769\n",
      "train loss on epoch 4481 : 0.157\n",
      "train accuracy on epoch 4481: 0.889\n",
      "test loss on epoch 4481: 0.403\n",
      "test accuracy on epoch 4481: 0.769\n",
      "train loss on epoch 4482 : 0.043\n",
      "train accuracy on epoch 4482: 1.000\n",
      "test loss on epoch 4482: 0.406\n",
      "test accuracy on epoch 4482: 0.769\n",
      "train loss on epoch 4483 : 0.063\n",
      "train accuracy on epoch 4483: 1.000\n",
      "test loss on epoch 4483: 0.400\n",
      "test accuracy on epoch 4483: 0.769\n",
      "train loss on epoch 4484 : 0.179\n",
      "train accuracy on epoch 4484: 0.944\n",
      "test loss on epoch 4484: 0.410\n",
      "test accuracy on epoch 4484: 0.769\n",
      "train loss on epoch 4485 : 0.177\n",
      "train accuracy on epoch 4485: 0.889\n",
      "test loss on epoch 4485: 0.403\n",
      "test accuracy on epoch 4485: 0.769\n",
      "train loss on epoch 4486 : 0.037\n",
      "train accuracy on epoch 4486: 1.000\n",
      "test loss on epoch 4486: 0.407\n",
      "test accuracy on epoch 4486: 0.769\n",
      "train loss on epoch 4487 : 0.072\n",
      "train accuracy on epoch 4487: 1.000\n",
      "test loss on epoch 4487: 0.392\n",
      "test accuracy on epoch 4487: 0.769\n",
      "train loss on epoch 4488 : 0.490\n",
      "train accuracy on epoch 4488: 0.833\n",
      "test loss on epoch 4488: 0.403\n",
      "test accuracy on epoch 4488: 0.769\n",
      "train loss on epoch 4489 : 0.110\n",
      "train accuracy on epoch 4489: 0.944\n",
      "test loss on epoch 4489: 0.402\n",
      "test accuracy on epoch 4489: 0.692\n",
      "train loss on epoch 4490 : 0.232\n",
      "train accuracy on epoch 4490: 0.889\n",
      "test loss on epoch 4490: 0.406\n",
      "test accuracy on epoch 4490: 0.692\n",
      "train loss on epoch 4491 : 0.112\n",
      "train accuracy on epoch 4491: 0.944\n",
      "test loss on epoch 4491: 0.407\n",
      "test accuracy on epoch 4491: 0.769\n",
      "train loss on epoch 4492 : 0.060\n",
      "train accuracy on epoch 4492: 1.000\n",
      "test loss on epoch 4492: 0.408\n",
      "test accuracy on epoch 4492: 0.769\n",
      "train loss on epoch 4493 : 0.229\n",
      "train accuracy on epoch 4493: 0.889\n",
      "test loss on epoch 4493: 0.413\n",
      "test accuracy on epoch 4493: 0.769\n",
      "train loss on epoch 4494 : 0.241\n",
      "train accuracy on epoch 4494: 0.944\n",
      "test loss on epoch 4494: 0.411\n",
      "test accuracy on epoch 4494: 0.769\n",
      "train loss on epoch 4495 : 0.388\n",
      "train accuracy on epoch 4495: 0.944\n",
      "test loss on epoch 4495: 0.417\n",
      "test accuracy on epoch 4495: 0.769\n",
      "train loss on epoch 4496 : 0.455\n",
      "train accuracy on epoch 4496: 0.889\n",
      "test loss on epoch 4496: 0.419\n",
      "test accuracy on epoch 4496: 0.769\n",
      "train loss on epoch 4497 : 0.196\n",
      "train accuracy on epoch 4497: 0.944\n",
      "test loss on epoch 4497: 0.413\n",
      "test accuracy on epoch 4497: 0.769\n",
      "train loss on epoch 4498 : 0.094\n",
      "train accuracy on epoch 4498: 0.944\n",
      "test loss on epoch 4498: 0.410\n",
      "test accuracy on epoch 4498: 0.692\n",
      "train loss on epoch 4499 : 0.180\n",
      "train accuracy on epoch 4499: 0.944\n",
      "test loss on epoch 4499: 0.409\n",
      "test accuracy on epoch 4499: 0.692\n",
      "train loss on epoch 4500 : 0.290\n",
      "train accuracy on epoch 4500: 0.944\n",
      "test loss on epoch 4500: 0.395\n",
      "test accuracy on epoch 4500: 0.769\n",
      "train loss on epoch 4501 : 0.070\n",
      "train accuracy on epoch 4501: 1.000\n",
      "test loss on epoch 4501: 0.384\n",
      "test accuracy on epoch 4501: 0.769\n",
      "train loss on epoch 4502 : 0.282\n",
      "train accuracy on epoch 4502: 0.889\n",
      "test loss on epoch 4502: 0.408\n",
      "test accuracy on epoch 4502: 0.769\n",
      "train loss on epoch 4503 : 0.173\n",
      "train accuracy on epoch 4503: 0.889\n",
      "test loss on epoch 4503: 0.384\n",
      "test accuracy on epoch 4503: 0.846\n",
      "train loss on epoch 4504 : 0.200\n",
      "train accuracy on epoch 4504: 0.889\n",
      "test loss on epoch 4504: 0.402\n",
      "test accuracy on epoch 4504: 0.692\n",
      "train loss on epoch 4505 : 0.215\n",
      "train accuracy on epoch 4505: 0.889\n",
      "test loss on epoch 4505: 0.402\n",
      "test accuracy on epoch 4505: 0.692\n",
      "train loss on epoch 4506 : 0.104\n",
      "train accuracy on epoch 4506: 0.944\n",
      "test loss on epoch 4506: 0.384\n",
      "test accuracy on epoch 4506: 0.846\n",
      "train loss on epoch 4507 : 0.075\n",
      "train accuracy on epoch 4507: 0.944\n",
      "test loss on epoch 4507: 0.392\n",
      "test accuracy on epoch 4507: 0.769\n",
      "train loss on epoch 4508 : 0.211\n",
      "train accuracy on epoch 4508: 0.944\n",
      "test loss on epoch 4508: 0.382\n",
      "test accuracy on epoch 4508: 0.846\n",
      "train loss on epoch 4509 : 0.355\n",
      "train accuracy on epoch 4509: 0.778\n",
      "test loss on epoch 4509: 0.385\n",
      "test accuracy on epoch 4509: 0.846\n",
      "train loss on epoch 4510 : 0.144\n",
      "train accuracy on epoch 4510: 0.889\n",
      "test loss on epoch 4510: 0.384\n",
      "test accuracy on epoch 4510: 0.846\n",
      "train loss on epoch 4511 : 0.086\n",
      "train accuracy on epoch 4511: 0.944\n",
      "test loss on epoch 4511: 0.392\n",
      "test accuracy on epoch 4511: 0.769\n",
      "train loss on epoch 4512 : 0.183\n",
      "train accuracy on epoch 4512: 0.944\n",
      "test loss on epoch 4512: 0.388\n",
      "test accuracy on epoch 4512: 0.769\n",
      "train loss on epoch 4513 : 0.139\n",
      "train accuracy on epoch 4513: 0.889\n",
      "test loss on epoch 4513: 0.387\n",
      "test accuracy on epoch 4513: 0.769\n",
      "train loss on epoch 4514 : 0.466\n",
      "train accuracy on epoch 4514: 0.889\n",
      "test loss on epoch 4514: 0.385\n",
      "test accuracy on epoch 4514: 0.769\n",
      "train loss on epoch 4515 : 0.169\n",
      "train accuracy on epoch 4515: 0.944\n",
      "test loss on epoch 4515: 0.390\n",
      "test accuracy on epoch 4515: 0.769\n",
      "train loss on epoch 4516 : 0.213\n",
      "train accuracy on epoch 4516: 0.944\n",
      "test loss on epoch 4516: 0.387\n",
      "test accuracy on epoch 4516: 0.769\n",
      "train loss on epoch 4517 : 0.091\n",
      "train accuracy on epoch 4517: 1.000\n",
      "test loss on epoch 4517: 0.395\n",
      "test accuracy on epoch 4517: 0.769\n",
      "train loss on epoch 4518 : 0.344\n",
      "train accuracy on epoch 4518: 0.944\n",
      "test loss on epoch 4518: 0.393\n",
      "test accuracy on epoch 4518: 0.769\n",
      "train loss on epoch 4519 : 0.274\n",
      "train accuracy on epoch 4519: 0.889\n",
      "test loss on epoch 4519: 0.384\n",
      "test accuracy on epoch 4519: 0.769\n",
      "train loss on epoch 4520 : 0.331\n",
      "train accuracy on epoch 4520: 0.889\n",
      "test loss on epoch 4520: 0.395\n",
      "test accuracy on epoch 4520: 0.769\n",
      "train loss on epoch 4521 : 0.100\n",
      "train accuracy on epoch 4521: 0.944\n",
      "test loss on epoch 4521: 0.388\n",
      "test accuracy on epoch 4521: 0.769\n",
      "train loss on epoch 4522 : 0.491\n",
      "train accuracy on epoch 4522: 0.889\n",
      "test loss on epoch 4522: 0.391\n",
      "test accuracy on epoch 4522: 0.769\n",
      "train loss on epoch 4523 : 0.175\n",
      "train accuracy on epoch 4523: 0.944\n",
      "test loss on epoch 4523: 0.394\n",
      "test accuracy on epoch 4523: 0.769\n",
      "train loss on epoch 4524 : 0.102\n",
      "train accuracy on epoch 4524: 1.000\n",
      "test loss on epoch 4524: 0.381\n",
      "test accuracy on epoch 4524: 0.769\n",
      "train loss on epoch 4525 : 0.068\n",
      "train accuracy on epoch 4525: 1.000\n",
      "test loss on epoch 4525: 0.385\n",
      "test accuracy on epoch 4525: 0.769\n",
      "train loss on epoch 4526 : 0.468\n",
      "train accuracy on epoch 4526: 0.833\n",
      "test loss on epoch 4526: 0.382\n",
      "test accuracy on epoch 4526: 0.769\n",
      "train loss on epoch 4527 : 0.316\n",
      "train accuracy on epoch 4527: 0.944\n",
      "test loss on epoch 4527: 0.386\n",
      "test accuracy on epoch 4527: 0.692\n",
      "train loss on epoch 4528 : 0.184\n",
      "train accuracy on epoch 4528: 0.944\n",
      "test loss on epoch 4528: 0.371\n",
      "test accuracy on epoch 4528: 0.846\n",
      "train loss on epoch 4529 : 0.219\n",
      "train accuracy on epoch 4529: 0.944\n",
      "test loss on epoch 4529: 0.376\n",
      "test accuracy on epoch 4529: 0.769\n",
      "train loss on epoch 4530 : 0.192\n",
      "train accuracy on epoch 4530: 0.944\n",
      "test loss on epoch 4530: 0.384\n",
      "test accuracy on epoch 4530: 0.692\n",
      "train loss on epoch 4531 : 0.093\n",
      "train accuracy on epoch 4531: 0.944\n",
      "test loss on epoch 4531: 0.387\n",
      "test accuracy on epoch 4531: 0.769\n",
      "train loss on epoch 4532 : 0.158\n",
      "train accuracy on epoch 4532: 0.889\n",
      "test loss on epoch 4532: 0.389\n",
      "test accuracy on epoch 4532: 0.769\n",
      "train loss on epoch 4533 : 0.158\n",
      "train accuracy on epoch 4533: 0.889\n",
      "test loss on epoch 4533: 0.395\n",
      "test accuracy on epoch 4533: 0.769\n",
      "train loss on epoch 4534 : 0.127\n",
      "train accuracy on epoch 4534: 0.944\n",
      "test loss on epoch 4534: 0.407\n",
      "test accuracy on epoch 4534: 0.769\n",
      "train loss on epoch 4535 : 0.221\n",
      "train accuracy on epoch 4535: 0.889\n",
      "test loss on epoch 4535: 0.413\n",
      "test accuracy on epoch 4535: 0.769\n",
      "train loss on epoch 4536 : 0.133\n",
      "train accuracy on epoch 4536: 0.944\n",
      "test loss on epoch 4536: 0.409\n",
      "test accuracy on epoch 4536: 0.769\n",
      "train loss on epoch 4537 : 0.338\n",
      "train accuracy on epoch 4537: 0.833\n",
      "test loss on epoch 4537: 0.406\n",
      "test accuracy on epoch 4537: 0.769\n",
      "train loss on epoch 4538 : 0.134\n",
      "train accuracy on epoch 4538: 0.944\n",
      "test loss on epoch 4538: 0.399\n",
      "test accuracy on epoch 4538: 0.769\n",
      "train loss on epoch 4539 : 0.043\n",
      "train accuracy on epoch 4539: 1.000\n",
      "test loss on epoch 4539: 0.401\n",
      "test accuracy on epoch 4539: 0.769\n",
      "train loss on epoch 4540 : 0.206\n",
      "train accuracy on epoch 4540: 0.944\n",
      "test loss on epoch 4540: 0.398\n",
      "test accuracy on epoch 4540: 0.769\n",
      "train loss on epoch 4541 : 0.199\n",
      "train accuracy on epoch 4541: 0.944\n",
      "test loss on epoch 4541: 0.404\n",
      "test accuracy on epoch 4541: 0.769\n",
      "train loss on epoch 4542 : 0.220\n",
      "train accuracy on epoch 4542: 0.889\n",
      "test loss on epoch 4542: 0.400\n",
      "test accuracy on epoch 4542: 0.769\n",
      "train loss on epoch 4543 : 0.095\n",
      "train accuracy on epoch 4543: 1.000\n",
      "test loss on epoch 4543: 0.401\n",
      "test accuracy on epoch 4543: 0.769\n",
      "train loss on epoch 4544 : 0.162\n",
      "train accuracy on epoch 4544: 0.889\n",
      "test loss on epoch 4544: 0.399\n",
      "test accuracy on epoch 4544: 0.769\n",
      "train loss on epoch 4545 : 0.198\n",
      "train accuracy on epoch 4545: 0.944\n",
      "test loss on epoch 4545: 0.383\n",
      "test accuracy on epoch 4545: 0.769\n",
      "train loss on epoch 4546 : 0.287\n",
      "train accuracy on epoch 4546: 0.833\n",
      "test loss on epoch 4546: 0.388\n",
      "test accuracy on epoch 4546: 0.769\n",
      "train loss on epoch 4547 : 0.178\n",
      "train accuracy on epoch 4547: 0.833\n",
      "test loss on epoch 4547: 0.377\n",
      "test accuracy on epoch 4547: 0.769\n",
      "train loss on epoch 4548 : 0.339\n",
      "train accuracy on epoch 4548: 0.833\n",
      "test loss on epoch 4548: 0.377\n",
      "test accuracy on epoch 4548: 0.769\n",
      "train loss on epoch 4549 : 0.030\n",
      "train accuracy on epoch 4549: 1.000\n",
      "test loss on epoch 4549: 0.375\n",
      "test accuracy on epoch 4549: 0.769\n",
      "train loss on epoch 4550 : 0.341\n",
      "train accuracy on epoch 4550: 0.889\n",
      "test loss on epoch 4550: 0.365\n",
      "test accuracy on epoch 4550: 0.769\n",
      "train loss on epoch 4551 : 0.153\n",
      "train accuracy on epoch 4551: 0.889\n",
      "test loss on epoch 4551: 0.375\n",
      "test accuracy on epoch 4551: 0.769\n",
      "train loss on epoch 4552 : 0.401\n",
      "train accuracy on epoch 4552: 0.889\n",
      "test loss on epoch 4552: 0.362\n",
      "test accuracy on epoch 4552: 0.769\n",
      "train loss on epoch 4553 : 0.463\n",
      "train accuracy on epoch 4553: 0.889\n",
      "test loss on epoch 4553: 0.366\n",
      "test accuracy on epoch 4553: 0.769\n",
      "train loss on epoch 4554 : 0.132\n",
      "train accuracy on epoch 4554: 0.944\n",
      "test loss on epoch 4554: 0.365\n",
      "test accuracy on epoch 4554: 0.769\n",
      "train loss on epoch 4555 : 0.140\n",
      "train accuracy on epoch 4555: 0.944\n",
      "test loss on epoch 4555: 0.369\n",
      "test accuracy on epoch 4555: 0.769\n",
      "train loss on epoch 4556 : 0.306\n",
      "train accuracy on epoch 4556: 0.944\n",
      "test loss on epoch 4556: 0.367\n",
      "test accuracy on epoch 4556: 0.769\n",
      "train loss on epoch 4557 : 0.073\n",
      "train accuracy on epoch 4557: 1.000\n",
      "test loss on epoch 4557: 0.357\n",
      "test accuracy on epoch 4557: 0.846\n",
      "train loss on epoch 4558 : 0.377\n",
      "train accuracy on epoch 4558: 0.778\n",
      "test loss on epoch 4558: 0.367\n",
      "test accuracy on epoch 4558: 0.769\n",
      "train loss on epoch 4559 : 0.202\n",
      "train accuracy on epoch 4559: 0.889\n",
      "test loss on epoch 4559: 0.364\n",
      "test accuracy on epoch 4559: 0.769\n",
      "train loss on epoch 4560 : 0.233\n",
      "train accuracy on epoch 4560: 0.889\n",
      "test loss on epoch 4560: 0.363\n",
      "test accuracy on epoch 4560: 0.769\n",
      "train loss on epoch 4561 : 0.287\n",
      "train accuracy on epoch 4561: 0.944\n",
      "test loss on epoch 4561: 0.361\n",
      "test accuracy on epoch 4561: 0.846\n",
      "train loss on epoch 4562 : 0.104\n",
      "train accuracy on epoch 4562: 1.000\n",
      "test loss on epoch 4562: 0.363\n",
      "test accuracy on epoch 4562: 0.769\n",
      "train loss on epoch 4563 : 0.197\n",
      "train accuracy on epoch 4563: 0.944\n",
      "test loss on epoch 4563: 0.371\n",
      "test accuracy on epoch 4563: 0.769\n",
      "train loss on epoch 4564 : 0.381\n",
      "train accuracy on epoch 4564: 0.833\n",
      "test loss on epoch 4564: 0.376\n",
      "test accuracy on epoch 4564: 0.769\n",
      "train loss on epoch 4565 : 0.135\n",
      "train accuracy on epoch 4565: 0.944\n",
      "test loss on epoch 4565: 0.363\n",
      "test accuracy on epoch 4565: 0.769\n",
      "train loss on epoch 4566 : 0.326\n",
      "train accuracy on epoch 4566: 0.889\n",
      "test loss on epoch 4566: 0.374\n",
      "test accuracy on epoch 4566: 0.769\n",
      "train loss on epoch 4567 : 0.087\n",
      "train accuracy on epoch 4567: 0.944\n",
      "test loss on epoch 4567: 0.373\n",
      "test accuracy on epoch 4567: 0.769\n",
      "train loss on epoch 4568 : 0.081\n",
      "train accuracy on epoch 4568: 0.944\n",
      "test loss on epoch 4568: 0.377\n",
      "test accuracy on epoch 4568: 0.769\n",
      "train loss on epoch 4569 : 0.125\n",
      "train accuracy on epoch 4569: 0.889\n",
      "test loss on epoch 4569: 0.383\n",
      "test accuracy on epoch 4569: 0.769\n",
      "train loss on epoch 4570 : 0.159\n",
      "train accuracy on epoch 4570: 0.944\n",
      "test loss on epoch 4570: 0.382\n",
      "test accuracy on epoch 4570: 0.769\n",
      "train loss on epoch 4571 : 0.270\n",
      "train accuracy on epoch 4571: 0.944\n",
      "test loss on epoch 4571: 0.384\n",
      "test accuracy on epoch 4571: 0.769\n",
      "train loss on epoch 4572 : 0.245\n",
      "train accuracy on epoch 4572: 0.944\n",
      "test loss on epoch 4572: 0.387\n",
      "test accuracy on epoch 4572: 0.769\n",
      "train loss on epoch 4573 : 0.230\n",
      "train accuracy on epoch 4573: 0.889\n",
      "test loss on epoch 4573: 0.383\n",
      "test accuracy on epoch 4573: 0.769\n",
      "train loss on epoch 4574 : 0.180\n",
      "train accuracy on epoch 4574: 0.944\n",
      "test loss on epoch 4574: 0.375\n",
      "test accuracy on epoch 4574: 0.769\n",
      "train loss on epoch 4575 : 0.235\n",
      "train accuracy on epoch 4575: 0.833\n",
      "test loss on epoch 4575: 0.367\n",
      "test accuracy on epoch 4575: 0.769\n",
      "train loss on epoch 4576 : 0.191\n",
      "train accuracy on epoch 4576: 0.944\n",
      "test loss on epoch 4576: 0.363\n",
      "test accuracy on epoch 4576: 0.769\n",
      "train loss on epoch 4577 : 0.290\n",
      "train accuracy on epoch 4577: 0.889\n",
      "test loss on epoch 4577: 0.363\n",
      "test accuracy on epoch 4577: 0.769\n",
      "train loss on epoch 4578 : 0.095\n",
      "train accuracy on epoch 4578: 0.944\n",
      "test loss on epoch 4578: 0.362\n",
      "test accuracy on epoch 4578: 0.769\n",
      "train loss on epoch 4579 : 0.131\n",
      "train accuracy on epoch 4579: 0.889\n",
      "test loss on epoch 4579: 0.367\n",
      "test accuracy on epoch 4579: 0.769\n",
      "train loss on epoch 4580 : 0.484\n",
      "train accuracy on epoch 4580: 0.889\n",
      "test loss on epoch 4580: 0.355\n",
      "test accuracy on epoch 4580: 0.769\n",
      "train loss on epoch 4581 : 0.244\n",
      "train accuracy on epoch 4581: 0.944\n",
      "test loss on epoch 4581: 0.365\n",
      "test accuracy on epoch 4581: 0.769\n",
      "train loss on epoch 4582 : 0.127\n",
      "train accuracy on epoch 4582: 0.944\n",
      "test loss on epoch 4582: 0.363\n",
      "test accuracy on epoch 4582: 0.769\n",
      "train loss on epoch 4583 : 0.253\n",
      "train accuracy on epoch 4583: 0.833\n",
      "test loss on epoch 4583: 0.372\n",
      "test accuracy on epoch 4583: 0.769\n",
      "train loss on epoch 4584 : 0.254\n",
      "train accuracy on epoch 4584: 0.944\n",
      "test loss on epoch 4584: 0.374\n",
      "test accuracy on epoch 4584: 0.769\n",
      "train loss on epoch 4585 : 0.498\n",
      "train accuracy on epoch 4585: 0.833\n",
      "test loss on epoch 4585: 0.374\n",
      "test accuracy on epoch 4585: 0.769\n",
      "train loss on epoch 4586 : 0.073\n",
      "train accuracy on epoch 4586: 1.000\n",
      "test loss on epoch 4586: 0.369\n",
      "test accuracy on epoch 4586: 0.769\n",
      "train loss on epoch 4587 : 0.162\n",
      "train accuracy on epoch 4587: 0.889\n",
      "test loss on epoch 4587: 0.373\n",
      "test accuracy on epoch 4587: 0.769\n",
      "train loss on epoch 4588 : 0.205\n",
      "train accuracy on epoch 4588: 0.889\n",
      "test loss on epoch 4588: 0.378\n",
      "test accuracy on epoch 4588: 0.769\n",
      "train loss on epoch 4589 : 0.136\n",
      "train accuracy on epoch 4589: 0.889\n",
      "test loss on epoch 4589: 0.382\n",
      "test accuracy on epoch 4589: 0.769\n",
      "train loss on epoch 4590 : 0.099\n",
      "train accuracy on epoch 4590: 1.000\n",
      "test loss on epoch 4590: 0.380\n",
      "test accuracy on epoch 4590: 0.769\n",
      "train loss on epoch 4591 : 0.281\n",
      "train accuracy on epoch 4591: 0.889\n",
      "test loss on epoch 4591: 0.375\n",
      "test accuracy on epoch 4591: 0.769\n",
      "train loss on epoch 4592 : 0.063\n",
      "train accuracy on epoch 4592: 1.000\n",
      "test loss on epoch 4592: 0.370\n",
      "test accuracy on epoch 4592: 0.769\n",
      "train loss on epoch 4593 : 0.240\n",
      "train accuracy on epoch 4593: 0.889\n",
      "test loss on epoch 4593: 0.354\n",
      "test accuracy on epoch 4593: 0.769\n",
      "train loss on epoch 4594 : 0.211\n",
      "train accuracy on epoch 4594: 0.889\n",
      "test loss on epoch 4594: 0.364\n",
      "test accuracy on epoch 4594: 0.692\n",
      "train loss on epoch 4595 : 0.320\n",
      "train accuracy on epoch 4595: 0.889\n",
      "test loss on epoch 4595: 0.356\n",
      "test accuracy on epoch 4595: 0.769\n",
      "train loss on epoch 4596 : 0.171\n",
      "train accuracy on epoch 4596: 0.944\n",
      "test loss on epoch 4596: 0.363\n",
      "test accuracy on epoch 4596: 0.769\n",
      "train loss on epoch 4597 : 0.035\n",
      "train accuracy on epoch 4597: 1.000\n",
      "test loss on epoch 4597: 0.365\n",
      "test accuracy on epoch 4597: 0.769\n",
      "train loss on epoch 4598 : 0.160\n",
      "train accuracy on epoch 4598: 0.889\n",
      "test loss on epoch 4598: 0.355\n",
      "test accuracy on epoch 4598: 0.769\n",
      "train loss on epoch 4599 : 0.285\n",
      "train accuracy on epoch 4599: 0.889\n",
      "test loss on epoch 4599: 0.360\n",
      "test accuracy on epoch 4599: 0.769\n",
      "train loss on epoch 4600 : 0.110\n",
      "train accuracy on epoch 4600: 0.944\n",
      "test loss on epoch 4600: 0.356\n",
      "test accuracy on epoch 4600: 0.769\n",
      "train loss on epoch 4601 : 0.032\n",
      "train accuracy on epoch 4601: 1.000\n",
      "test loss on epoch 4601: 0.358\n",
      "test accuracy on epoch 4601: 0.769\n",
      "train loss on epoch 4602 : 0.053\n",
      "train accuracy on epoch 4602: 1.000\n",
      "test loss on epoch 4602: 0.359\n",
      "test accuracy on epoch 4602: 0.769\n",
      "train loss on epoch 4603 : 0.096\n",
      "train accuracy on epoch 4603: 1.000\n",
      "test loss on epoch 4603: 0.358\n",
      "test accuracy on epoch 4603: 0.769\n",
      "train loss on epoch 4604 : 0.039\n",
      "train accuracy on epoch 4604: 1.000\n",
      "test loss on epoch 4604: 0.354\n",
      "test accuracy on epoch 4604: 0.769\n",
      "train loss on epoch 4605 : 0.385\n",
      "train accuracy on epoch 4605: 0.833\n",
      "test loss on epoch 4605: 0.357\n",
      "test accuracy on epoch 4605: 0.769\n",
      "train loss on epoch 4606 : 0.119\n",
      "train accuracy on epoch 4606: 0.944\n",
      "test loss on epoch 4606: 0.358\n",
      "test accuracy on epoch 4606: 0.769\n",
      "train loss on epoch 4607 : 0.102\n",
      "train accuracy on epoch 4607: 0.944\n",
      "test loss on epoch 4607: 0.361\n",
      "test accuracy on epoch 4607: 0.769\n",
      "train loss on epoch 4608 : 0.194\n",
      "train accuracy on epoch 4608: 0.944\n",
      "test loss on epoch 4608: 0.362\n",
      "test accuracy on epoch 4608: 0.769\n",
      "train loss on epoch 4609 : 0.131\n",
      "train accuracy on epoch 4609: 0.944\n",
      "test loss on epoch 4609: 0.361\n",
      "test accuracy on epoch 4609: 0.769\n",
      "train loss on epoch 4610 : 0.270\n",
      "train accuracy on epoch 4610: 0.833\n",
      "test loss on epoch 4610: 0.356\n",
      "test accuracy on epoch 4610: 0.769\n",
      "train loss on epoch 4611 : 0.144\n",
      "train accuracy on epoch 4611: 1.000\n",
      "test loss on epoch 4611: 0.361\n",
      "test accuracy on epoch 4611: 0.769\n",
      "train loss on epoch 4612 : 0.042\n",
      "train accuracy on epoch 4612: 1.000\n",
      "test loss on epoch 4612: 0.361\n",
      "test accuracy on epoch 4612: 0.769\n",
      "train loss on epoch 4613 : 0.218\n",
      "train accuracy on epoch 4613: 0.889\n",
      "test loss on epoch 4613: 0.360\n",
      "test accuracy on epoch 4613: 0.769\n",
      "train loss on epoch 4614 : 0.402\n",
      "train accuracy on epoch 4614: 0.833\n",
      "test loss on epoch 4614: 0.356\n",
      "test accuracy on epoch 4614: 0.769\n",
      "train loss on epoch 4615 : 0.248\n",
      "train accuracy on epoch 4615: 0.889\n",
      "test loss on epoch 4615: 0.360\n",
      "test accuracy on epoch 4615: 0.769\n",
      "train loss on epoch 4616 : 0.153\n",
      "train accuracy on epoch 4616: 0.944\n",
      "test loss on epoch 4616: 0.357\n",
      "test accuracy on epoch 4616: 0.769\n",
      "train loss on epoch 4617 : 0.158\n",
      "train accuracy on epoch 4617: 0.944\n",
      "test loss on epoch 4617: 0.357\n",
      "test accuracy on epoch 4617: 0.769\n",
      "train loss on epoch 4618 : 0.211\n",
      "train accuracy on epoch 4618: 0.944\n",
      "test loss on epoch 4618: 0.359\n",
      "test accuracy on epoch 4618: 0.769\n",
      "train loss on epoch 4619 : 0.107\n",
      "train accuracy on epoch 4619: 0.944\n",
      "test loss on epoch 4619: 0.362\n",
      "test accuracy on epoch 4619: 0.769\n",
      "train loss on epoch 4620 : 0.043\n",
      "train accuracy on epoch 4620: 1.000\n",
      "test loss on epoch 4620: 0.361\n",
      "test accuracy on epoch 4620: 0.769\n",
      "train loss on epoch 4621 : 0.173\n",
      "train accuracy on epoch 4621: 0.889\n",
      "test loss on epoch 4621: 0.364\n",
      "test accuracy on epoch 4621: 0.769\n",
      "train loss on epoch 4622 : 0.238\n",
      "train accuracy on epoch 4622: 0.944\n",
      "test loss on epoch 4622: 0.367\n",
      "test accuracy on epoch 4622: 0.769\n",
      "train loss on epoch 4623 : 0.047\n",
      "train accuracy on epoch 4623: 1.000\n",
      "test loss on epoch 4623: 0.367\n",
      "test accuracy on epoch 4623: 0.769\n",
      "train loss on epoch 4624 : 0.126\n",
      "train accuracy on epoch 4624: 0.944\n",
      "test loss on epoch 4624: 0.366\n",
      "test accuracy on epoch 4624: 0.769\n",
      "train loss on epoch 4625 : 0.105\n",
      "train accuracy on epoch 4625: 0.944\n",
      "test loss on epoch 4625: 0.368\n",
      "test accuracy on epoch 4625: 0.769\n",
      "train loss on epoch 4626 : 0.097\n",
      "train accuracy on epoch 4626: 0.944\n",
      "test loss on epoch 4626: 0.363\n",
      "test accuracy on epoch 4626: 0.769\n",
      "train loss on epoch 4627 : 0.086\n",
      "train accuracy on epoch 4627: 0.944\n",
      "test loss on epoch 4627: 0.362\n",
      "test accuracy on epoch 4627: 0.769\n",
      "train loss on epoch 4628 : 0.101\n",
      "train accuracy on epoch 4628: 0.944\n",
      "test loss on epoch 4628: 0.360\n",
      "test accuracy on epoch 4628: 0.769\n",
      "train loss on epoch 4629 : 0.253\n",
      "train accuracy on epoch 4629: 0.889\n",
      "test loss on epoch 4629: 0.364\n",
      "test accuracy on epoch 4629: 0.769\n",
      "train loss on epoch 4630 : 0.232\n",
      "train accuracy on epoch 4630: 0.889\n",
      "test loss on epoch 4630: 0.360\n",
      "test accuracy on epoch 4630: 0.769\n",
      "train loss on epoch 4631 : 0.087\n",
      "train accuracy on epoch 4631: 1.000\n",
      "test loss on epoch 4631: 0.361\n",
      "test accuracy on epoch 4631: 0.769\n",
      "train loss on epoch 4632 : 0.167\n",
      "train accuracy on epoch 4632: 0.889\n",
      "test loss on epoch 4632: 0.371\n",
      "test accuracy on epoch 4632: 0.769\n",
      "train loss on epoch 4633 : 0.059\n",
      "train accuracy on epoch 4633: 1.000\n",
      "test loss on epoch 4633: 0.377\n",
      "test accuracy on epoch 4633: 0.769\n",
      "train loss on epoch 4634 : 0.128\n",
      "train accuracy on epoch 4634: 0.944\n",
      "test loss on epoch 4634: 0.370\n",
      "test accuracy on epoch 4634: 0.769\n",
      "train loss on epoch 4635 : 0.222\n",
      "train accuracy on epoch 4635: 0.833\n",
      "test loss on epoch 4635: 0.371\n",
      "test accuracy on epoch 4635: 0.769\n",
      "train loss on epoch 4636 : 0.288\n",
      "train accuracy on epoch 4636: 0.833\n",
      "test loss on epoch 4636: 0.376\n",
      "test accuracy on epoch 4636: 0.769\n",
      "train loss on epoch 4637 : 0.092\n",
      "train accuracy on epoch 4637: 0.944\n",
      "test loss on epoch 4637: 0.379\n",
      "test accuracy on epoch 4637: 0.769\n",
      "train loss on epoch 4638 : 0.158\n",
      "train accuracy on epoch 4638: 0.944\n",
      "test loss on epoch 4638: 0.374\n",
      "test accuracy on epoch 4638: 0.769\n",
      "train loss on epoch 4639 : 0.221\n",
      "train accuracy on epoch 4639: 0.889\n",
      "test loss on epoch 4639: 0.369\n",
      "test accuracy on epoch 4639: 0.769\n",
      "train loss on epoch 4640 : 0.241\n",
      "train accuracy on epoch 4640: 0.944\n",
      "test loss on epoch 4640: 0.371\n",
      "test accuracy on epoch 4640: 0.769\n",
      "train loss on epoch 4641 : 0.065\n",
      "train accuracy on epoch 4641: 1.000\n",
      "test loss on epoch 4641: 0.373\n",
      "test accuracy on epoch 4641: 0.769\n",
      "train loss on epoch 4642 : 0.113\n",
      "train accuracy on epoch 4642: 1.000\n",
      "test loss on epoch 4642: 0.367\n",
      "test accuracy on epoch 4642: 0.769\n",
      "train loss on epoch 4643 : 0.358\n",
      "train accuracy on epoch 4643: 0.833\n",
      "test loss on epoch 4643: 0.364\n",
      "test accuracy on epoch 4643: 0.769\n",
      "train loss on epoch 4644 : 0.112\n",
      "train accuracy on epoch 4644: 0.944\n",
      "test loss on epoch 4644: 0.356\n",
      "test accuracy on epoch 4644: 0.769\n",
      "train loss on epoch 4645 : 0.193\n",
      "train accuracy on epoch 4645: 0.944\n",
      "test loss on epoch 4645: 0.357\n",
      "test accuracy on epoch 4645: 0.769\n",
      "train loss on epoch 4646 : 0.331\n",
      "train accuracy on epoch 4646: 0.889\n",
      "test loss on epoch 4646: 0.356\n",
      "test accuracy on epoch 4646: 0.769\n",
      "train loss on epoch 4647 : 0.130\n",
      "train accuracy on epoch 4647: 0.944\n",
      "test loss on epoch 4647: 0.358\n",
      "test accuracy on epoch 4647: 0.769\n",
      "train loss on epoch 4648 : 0.437\n",
      "train accuracy on epoch 4648: 0.833\n",
      "test loss on epoch 4648: 0.359\n",
      "test accuracy on epoch 4648: 0.769\n",
      "train loss on epoch 4649 : 0.287\n",
      "train accuracy on epoch 4649: 0.833\n",
      "test loss on epoch 4649: 0.363\n",
      "test accuracy on epoch 4649: 0.769\n",
      "train loss on epoch 4650 : 0.134\n",
      "train accuracy on epoch 4650: 0.944\n",
      "test loss on epoch 4650: 0.370\n",
      "test accuracy on epoch 4650: 0.769\n",
      "train loss on epoch 4651 : 0.122\n",
      "train accuracy on epoch 4651: 0.944\n",
      "test loss on epoch 4651: 0.366\n",
      "test accuracy on epoch 4651: 0.769\n",
      "train loss on epoch 4652 : 0.138\n",
      "train accuracy on epoch 4652: 0.889\n",
      "test loss on epoch 4652: 0.367\n",
      "test accuracy on epoch 4652: 0.769\n",
      "train loss on epoch 4653 : 0.254\n",
      "train accuracy on epoch 4653: 0.889\n",
      "test loss on epoch 4653: 0.378\n",
      "test accuracy on epoch 4653: 0.769\n",
      "train loss on epoch 4654 : 0.101\n",
      "train accuracy on epoch 4654: 1.000\n",
      "test loss on epoch 4654: 0.387\n",
      "test accuracy on epoch 4654: 0.769\n",
      "train loss on epoch 4655 : 0.255\n",
      "train accuracy on epoch 4655: 0.889\n",
      "test loss on epoch 4655: 0.391\n",
      "test accuracy on epoch 4655: 0.769\n",
      "train loss on epoch 4656 : 0.302\n",
      "train accuracy on epoch 4656: 0.944\n",
      "test loss on epoch 4656: 0.392\n",
      "test accuracy on epoch 4656: 0.769\n",
      "train loss on epoch 4657 : 0.250\n",
      "train accuracy on epoch 4657: 0.889\n",
      "test loss on epoch 4657: 0.393\n",
      "test accuracy on epoch 4657: 0.769\n",
      "train loss on epoch 4658 : 0.328\n",
      "train accuracy on epoch 4658: 0.889\n",
      "test loss on epoch 4658: 0.376\n",
      "test accuracy on epoch 4658: 0.769\n",
      "train loss on epoch 4659 : 0.364\n",
      "train accuracy on epoch 4659: 0.889\n",
      "test loss on epoch 4659: 0.365\n",
      "test accuracy on epoch 4659: 0.769\n",
      "train loss on epoch 4660 : 0.256\n",
      "train accuracy on epoch 4660: 0.944\n",
      "test loss on epoch 4660: 0.356\n",
      "test accuracy on epoch 4660: 0.769\n",
      "train loss on epoch 4661 : 0.128\n",
      "train accuracy on epoch 4661: 1.000\n",
      "test loss on epoch 4661: 0.354\n",
      "test accuracy on epoch 4661: 0.769\n",
      "train loss on epoch 4662 : 0.136\n",
      "train accuracy on epoch 4662: 0.944\n",
      "test loss on epoch 4662: 0.352\n",
      "test accuracy on epoch 4662: 0.769\n",
      "train loss on epoch 4663 : 0.160\n",
      "train accuracy on epoch 4663: 0.889\n",
      "test loss on epoch 4663: 0.356\n",
      "test accuracy on epoch 4663: 0.769\n",
      "train loss on epoch 4664 : 0.430\n",
      "train accuracy on epoch 4664: 0.778\n",
      "test loss on epoch 4664: 0.349\n",
      "test accuracy on epoch 4664: 0.769\n",
      "train loss on epoch 4665 : 0.104\n",
      "train accuracy on epoch 4665: 0.944\n",
      "test loss on epoch 4665: 0.353\n",
      "test accuracy on epoch 4665: 0.769\n",
      "train loss on epoch 4666 : 0.197\n",
      "train accuracy on epoch 4666: 0.889\n",
      "test loss on epoch 4666: 0.355\n",
      "test accuracy on epoch 4666: 0.769\n",
      "train loss on epoch 4667 : 0.337\n",
      "train accuracy on epoch 4667: 0.889\n",
      "test loss on epoch 4667: 0.358\n",
      "test accuracy on epoch 4667: 0.769\n",
      "train loss on epoch 4668 : 0.355\n",
      "train accuracy on epoch 4668: 0.889\n",
      "test loss on epoch 4668: 0.362\n",
      "test accuracy on epoch 4668: 0.769\n",
      "train loss on epoch 4669 : 0.064\n",
      "train accuracy on epoch 4669: 1.000\n",
      "test loss on epoch 4669: 0.366\n",
      "test accuracy on epoch 4669: 0.769\n",
      "train loss on epoch 4670 : 0.441\n",
      "train accuracy on epoch 4670: 0.889\n",
      "test loss on epoch 4670: 0.369\n",
      "test accuracy on epoch 4670: 0.769\n",
      "train loss on epoch 4671 : 0.075\n",
      "train accuracy on epoch 4671: 1.000\n",
      "test loss on epoch 4671: 0.372\n",
      "test accuracy on epoch 4671: 0.769\n",
      "train loss on epoch 4672 : 0.348\n",
      "train accuracy on epoch 4672: 0.889\n",
      "test loss on epoch 4672: 0.376\n",
      "test accuracy on epoch 4672: 0.769\n",
      "train loss on epoch 4673 : 0.147\n",
      "train accuracy on epoch 4673: 0.944\n",
      "test loss on epoch 4673: 0.376\n",
      "test accuracy on epoch 4673: 0.769\n",
      "train loss on epoch 4674 : 0.318\n",
      "train accuracy on epoch 4674: 0.889\n",
      "test loss on epoch 4674: 0.370\n",
      "test accuracy on epoch 4674: 0.769\n",
      "train loss on epoch 4675 : 0.222\n",
      "train accuracy on epoch 4675: 0.889\n",
      "test loss on epoch 4675: 0.372\n",
      "test accuracy on epoch 4675: 0.769\n",
      "train loss on epoch 4676 : 0.261\n",
      "train accuracy on epoch 4676: 0.889\n",
      "test loss on epoch 4676: 0.373\n",
      "test accuracy on epoch 4676: 0.769\n",
      "train loss on epoch 4677 : 0.076\n",
      "train accuracy on epoch 4677: 1.000\n",
      "test loss on epoch 4677: 0.375\n",
      "test accuracy on epoch 4677: 0.769\n",
      "train loss on epoch 4678 : 0.194\n",
      "train accuracy on epoch 4678: 0.944\n",
      "test loss on epoch 4678: 0.368\n",
      "test accuracy on epoch 4678: 0.769\n",
      "train loss on epoch 4679 : 0.299\n",
      "train accuracy on epoch 4679: 0.833\n",
      "test loss on epoch 4679: 0.362\n",
      "test accuracy on epoch 4679: 0.769\n",
      "train loss on epoch 4680 : 0.184\n",
      "train accuracy on epoch 4680: 0.944\n",
      "test loss on epoch 4680: 0.359\n",
      "test accuracy on epoch 4680: 0.769\n",
      "train loss on epoch 4681 : 0.354\n",
      "train accuracy on epoch 4681: 0.778\n",
      "test loss on epoch 4681: 0.354\n",
      "test accuracy on epoch 4681: 0.769\n",
      "train loss on epoch 4682 : 0.044\n",
      "train accuracy on epoch 4682: 1.000\n",
      "test loss on epoch 4682: 0.344\n",
      "test accuracy on epoch 4682: 0.769\n",
      "train loss on epoch 4683 : 0.098\n",
      "train accuracy on epoch 4683: 0.944\n",
      "test loss on epoch 4683: 0.343\n",
      "test accuracy on epoch 4683: 0.769\n",
      "train loss on epoch 4684 : 0.188\n",
      "train accuracy on epoch 4684: 0.944\n",
      "test loss on epoch 4684: 0.354\n",
      "test accuracy on epoch 4684: 0.692\n",
      "train loss on epoch 4685 : 0.149\n",
      "train accuracy on epoch 4685: 0.944\n",
      "test loss on epoch 4685: 0.345\n",
      "test accuracy on epoch 4685: 0.769\n",
      "train loss on epoch 4686 : 0.144\n",
      "train accuracy on epoch 4686: 0.944\n",
      "test loss on epoch 4686: 0.353\n",
      "test accuracy on epoch 4686: 0.769\n",
      "train loss on epoch 4687 : 0.196\n",
      "train accuracy on epoch 4687: 0.889\n",
      "test loss on epoch 4687: 0.348\n",
      "test accuracy on epoch 4687: 0.769\n",
      "train loss on epoch 4688 : 0.167\n",
      "train accuracy on epoch 4688: 0.944\n",
      "test loss on epoch 4688: 0.346\n",
      "test accuracy on epoch 4688: 0.846\n",
      "train loss on epoch 4689 : 0.183\n",
      "train accuracy on epoch 4689: 0.833\n",
      "test loss on epoch 4689: 0.348\n",
      "test accuracy on epoch 4689: 0.846\n",
      "train loss on epoch 4690 : 0.226\n",
      "train accuracy on epoch 4690: 0.889\n",
      "test loss on epoch 4690: 0.349\n",
      "test accuracy on epoch 4690: 0.846\n",
      "train loss on epoch 4691 : 0.111\n",
      "train accuracy on epoch 4691: 0.944\n",
      "test loss on epoch 4691: 0.353\n",
      "test accuracy on epoch 4691: 0.769\n",
      "train loss on epoch 4692 : 0.126\n",
      "train accuracy on epoch 4692: 0.944\n",
      "test loss on epoch 4692: 0.348\n",
      "test accuracy on epoch 4692: 0.769\n",
      "train loss on epoch 4693 : 0.044\n",
      "train accuracy on epoch 4693: 1.000\n",
      "test loss on epoch 4693: 0.348\n",
      "test accuracy on epoch 4693: 0.769\n",
      "train loss on epoch 4694 : 0.203\n",
      "train accuracy on epoch 4694: 0.889\n",
      "test loss on epoch 4694: 0.356\n",
      "test accuracy on epoch 4694: 0.769\n",
      "train loss on epoch 4695 : 0.109\n",
      "train accuracy on epoch 4695: 0.944\n",
      "test loss on epoch 4695: 0.359\n",
      "test accuracy on epoch 4695: 0.769\n",
      "train loss on epoch 4696 : 0.162\n",
      "train accuracy on epoch 4696: 0.944\n",
      "test loss on epoch 4696: 0.359\n",
      "test accuracy on epoch 4696: 0.769\n",
      "train loss on epoch 4697 : 0.187\n",
      "train accuracy on epoch 4697: 0.944\n",
      "test loss on epoch 4697: 0.360\n",
      "test accuracy on epoch 4697: 0.769\n",
      "train loss on epoch 4698 : 0.130\n",
      "train accuracy on epoch 4698: 0.889\n",
      "test loss on epoch 4698: 0.361\n",
      "test accuracy on epoch 4698: 0.769\n",
      "train loss on epoch 4699 : 0.178\n",
      "train accuracy on epoch 4699: 0.944\n",
      "test loss on epoch 4699: 0.358\n",
      "test accuracy on epoch 4699: 0.769\n",
      "train loss on epoch 4700 : 0.268\n",
      "train accuracy on epoch 4700: 0.889\n",
      "test loss on epoch 4700: 0.351\n",
      "test accuracy on epoch 4700: 0.769\n",
      "train loss on epoch 4701 : 0.163\n",
      "train accuracy on epoch 4701: 0.944\n",
      "test loss on epoch 4701: 0.347\n",
      "test accuracy on epoch 4701: 0.769\n",
      "train loss on epoch 4702 : 0.200\n",
      "train accuracy on epoch 4702: 0.944\n",
      "test loss on epoch 4702: 0.342\n",
      "test accuracy on epoch 4702: 0.769\n",
      "train loss on epoch 4703 : 0.070\n",
      "train accuracy on epoch 4703: 1.000\n",
      "test loss on epoch 4703: 0.346\n",
      "test accuracy on epoch 4703: 0.769\n",
      "train loss on epoch 4704 : 0.038\n",
      "train accuracy on epoch 4704: 1.000\n",
      "test loss on epoch 4704: 0.347\n",
      "test accuracy on epoch 4704: 0.769\n",
      "train loss on epoch 4705 : 0.101\n",
      "train accuracy on epoch 4705: 0.944\n",
      "test loss on epoch 4705: 0.348\n",
      "test accuracy on epoch 4705: 0.769\n",
      "train loss on epoch 4706 : 0.241\n",
      "train accuracy on epoch 4706: 0.889\n",
      "test loss on epoch 4706: 0.348\n",
      "test accuracy on epoch 4706: 0.769\n",
      "train loss on epoch 4707 : 0.121\n",
      "train accuracy on epoch 4707: 0.944\n",
      "test loss on epoch 4707: 0.351\n",
      "test accuracy on epoch 4707: 0.769\n",
      "train loss on epoch 4708 : 0.173\n",
      "train accuracy on epoch 4708: 0.944\n",
      "test loss on epoch 4708: 0.353\n",
      "test accuracy on epoch 4708: 0.769\n",
      "train loss on epoch 4709 : 0.197\n",
      "train accuracy on epoch 4709: 0.889\n",
      "test loss on epoch 4709: 0.354\n",
      "test accuracy on epoch 4709: 0.769\n",
      "train loss on epoch 4710 : 0.221\n",
      "train accuracy on epoch 4710: 0.889\n",
      "test loss on epoch 4710: 0.354\n",
      "test accuracy on epoch 4710: 0.769\n",
      "train loss on epoch 4711 : 0.319\n",
      "train accuracy on epoch 4711: 0.889\n",
      "test loss on epoch 4711: 0.353\n",
      "test accuracy on epoch 4711: 0.769\n",
      "train loss on epoch 4712 : 0.148\n",
      "train accuracy on epoch 4712: 0.889\n",
      "test loss on epoch 4712: 0.355\n",
      "test accuracy on epoch 4712: 0.769\n",
      "train loss on epoch 4713 : 0.083\n",
      "train accuracy on epoch 4713: 1.000\n",
      "test loss on epoch 4713: 0.361\n",
      "test accuracy on epoch 4713: 0.769\n",
      "train loss on epoch 4714 : 0.214\n",
      "train accuracy on epoch 4714: 0.833\n",
      "test loss on epoch 4714: 0.371\n",
      "test accuracy on epoch 4714: 0.769\n",
      "train loss on epoch 4715 : 0.087\n",
      "train accuracy on epoch 4715: 1.000\n",
      "test loss on epoch 4715: 0.381\n",
      "test accuracy on epoch 4715: 0.769\n",
      "train loss on epoch 4716 : 0.046\n",
      "train accuracy on epoch 4716: 1.000\n",
      "test loss on epoch 4716: 0.390\n",
      "test accuracy on epoch 4716: 0.769\n",
      "train loss on epoch 4717 : 0.190\n",
      "train accuracy on epoch 4717: 0.944\n",
      "test loss on epoch 4717: 0.403\n",
      "test accuracy on epoch 4717: 0.769\n",
      "train loss on epoch 4718 : 0.275\n",
      "train accuracy on epoch 4718: 0.944\n",
      "test loss on epoch 4718: 0.402\n",
      "test accuracy on epoch 4718: 0.769\n",
      "train loss on epoch 4719 : 0.160\n",
      "train accuracy on epoch 4719: 0.889\n",
      "test loss on epoch 4719: 0.398\n",
      "test accuracy on epoch 4719: 0.769\n",
      "train loss on epoch 4720 : 0.351\n",
      "train accuracy on epoch 4720: 0.944\n",
      "test loss on epoch 4720: 0.400\n",
      "test accuracy on epoch 4720: 0.769\n",
      "train loss on epoch 4721 : 0.074\n",
      "train accuracy on epoch 4721: 0.944\n",
      "test loss on epoch 4721: 0.383\n",
      "test accuracy on epoch 4721: 0.769\n",
      "train loss on epoch 4722 : 0.067\n",
      "train accuracy on epoch 4722: 1.000\n",
      "test loss on epoch 4722: 0.378\n",
      "test accuracy on epoch 4722: 0.769\n",
      "train loss on epoch 4723 : 0.307\n",
      "train accuracy on epoch 4723: 0.889\n",
      "test loss on epoch 4723: 0.383\n",
      "test accuracy on epoch 4723: 0.769\n",
      "train loss on epoch 4724 : 0.239\n",
      "train accuracy on epoch 4724: 0.889\n",
      "test loss on epoch 4724: 0.386\n",
      "test accuracy on epoch 4724: 0.769\n",
      "train loss on epoch 4725 : 0.063\n",
      "train accuracy on epoch 4725: 1.000\n",
      "test loss on epoch 4725: 0.393\n",
      "test accuracy on epoch 4725: 0.769\n",
      "train loss on epoch 4726 : 0.290\n",
      "train accuracy on epoch 4726: 0.889\n",
      "test loss on epoch 4726: 0.395\n",
      "test accuracy on epoch 4726: 0.769\n",
      "train loss on epoch 4727 : 0.216\n",
      "train accuracy on epoch 4727: 0.889\n",
      "test loss on epoch 4727: 0.388\n",
      "test accuracy on epoch 4727: 0.769\n",
      "train loss on epoch 4728 : 0.288\n",
      "train accuracy on epoch 4728: 0.889\n",
      "test loss on epoch 4728: 0.382\n",
      "test accuracy on epoch 4728: 0.769\n",
      "train loss on epoch 4729 : 0.250\n",
      "train accuracy on epoch 4729: 0.944\n",
      "test loss on epoch 4729: 0.375\n",
      "test accuracy on epoch 4729: 0.769\n",
      "train loss on epoch 4730 : 0.061\n",
      "train accuracy on epoch 4730: 1.000\n",
      "test loss on epoch 4730: 0.369\n",
      "test accuracy on epoch 4730: 0.769\n",
      "train loss on epoch 4731 : 0.203\n",
      "train accuracy on epoch 4731: 0.889\n",
      "test loss on epoch 4731: 0.369\n",
      "test accuracy on epoch 4731: 0.769\n",
      "train loss on epoch 4732 : 0.170\n",
      "train accuracy on epoch 4732: 0.944\n",
      "test loss on epoch 4732: 0.367\n",
      "test accuracy on epoch 4732: 0.769\n",
      "train loss on epoch 4733 : 0.166\n",
      "train accuracy on epoch 4733: 0.944\n",
      "test loss on epoch 4733: 0.361\n",
      "test accuracy on epoch 4733: 0.769\n",
      "train loss on epoch 4734 : 0.136\n",
      "train accuracy on epoch 4734: 0.944\n",
      "test loss on epoch 4734: 0.356\n",
      "test accuracy on epoch 4734: 0.769\n",
      "train loss on epoch 4735 : 0.072\n",
      "train accuracy on epoch 4735: 1.000\n",
      "test loss on epoch 4735: 0.357\n",
      "test accuracy on epoch 4735: 0.769\n",
      "train loss on epoch 4736 : 0.176\n",
      "train accuracy on epoch 4736: 0.944\n",
      "test loss on epoch 4736: 0.364\n",
      "test accuracy on epoch 4736: 0.769\n",
      "train loss on epoch 4737 : 0.158\n",
      "train accuracy on epoch 4737: 0.944\n",
      "test loss on epoch 4737: 0.364\n",
      "test accuracy on epoch 4737: 0.769\n",
      "train loss on epoch 4738 : 0.361\n",
      "train accuracy on epoch 4738: 0.833\n",
      "test loss on epoch 4738: 0.358\n",
      "test accuracy on epoch 4738: 0.769\n",
      "train loss on epoch 4739 : 0.063\n",
      "train accuracy on epoch 4739: 1.000\n",
      "test loss on epoch 4739: 0.353\n",
      "test accuracy on epoch 4739: 0.769\n",
      "train loss on epoch 4740 : 0.195\n",
      "train accuracy on epoch 4740: 0.944\n",
      "test loss on epoch 4740: 0.363\n",
      "test accuracy on epoch 4740: 0.769\n",
      "train loss on epoch 4741 : 0.137\n",
      "train accuracy on epoch 4741: 0.944\n",
      "test loss on epoch 4741: 0.352\n",
      "test accuracy on epoch 4741: 0.769\n",
      "train loss on epoch 4742 : 0.356\n",
      "train accuracy on epoch 4742: 0.889\n",
      "test loss on epoch 4742: 0.352\n",
      "test accuracy on epoch 4742: 0.769\n",
      "train loss on epoch 4743 : 0.126\n",
      "train accuracy on epoch 4743: 1.000\n",
      "test loss on epoch 4743: 0.344\n",
      "test accuracy on epoch 4743: 0.769\n",
      "train loss on epoch 4744 : 0.227\n",
      "train accuracy on epoch 4744: 0.889\n",
      "test loss on epoch 4744: 0.337\n",
      "test accuracy on epoch 4744: 0.769\n",
      "train loss on epoch 4745 : 0.131\n",
      "train accuracy on epoch 4745: 0.889\n",
      "test loss on epoch 4745: 0.343\n",
      "test accuracy on epoch 4745: 0.769\n",
      "train loss on epoch 4746 : 0.060\n",
      "train accuracy on epoch 4746: 1.000\n",
      "test loss on epoch 4746: 0.333\n",
      "test accuracy on epoch 4746: 0.769\n",
      "train loss on epoch 4747 : 0.190\n",
      "train accuracy on epoch 4747: 0.889\n",
      "test loss on epoch 4747: 0.352\n",
      "test accuracy on epoch 4747: 0.769\n",
      "train loss on epoch 4748 : 0.358\n",
      "train accuracy on epoch 4748: 0.889\n",
      "test loss on epoch 4748: 0.354\n",
      "test accuracy on epoch 4748: 0.769\n",
      "train loss on epoch 4749 : 0.167\n",
      "train accuracy on epoch 4749: 0.944\n",
      "test loss on epoch 4749: 0.364\n",
      "test accuracy on epoch 4749: 0.769\n",
      "train loss on epoch 4750 : 0.054\n",
      "train accuracy on epoch 4750: 1.000\n",
      "test loss on epoch 4750: 0.372\n",
      "test accuracy on epoch 4750: 0.769\n",
      "train loss on epoch 4751 : 0.111\n",
      "train accuracy on epoch 4751: 0.944\n",
      "test loss on epoch 4751: 0.369\n",
      "test accuracy on epoch 4751: 0.769\n",
      "train loss on epoch 4752 : 0.291\n",
      "train accuracy on epoch 4752: 0.944\n",
      "test loss on epoch 4752: 0.364\n",
      "test accuracy on epoch 4752: 0.769\n",
      "train loss on epoch 4753 : 0.179\n",
      "train accuracy on epoch 4753: 0.944\n",
      "test loss on epoch 4753: 0.354\n",
      "test accuracy on epoch 4753: 0.769\n",
      "train loss on epoch 4754 : 0.213\n",
      "train accuracy on epoch 4754: 0.889\n",
      "test loss on epoch 4754: 0.348\n",
      "test accuracy on epoch 4754: 0.769\n",
      "train loss on epoch 4755 : 0.244\n",
      "train accuracy on epoch 4755: 0.889\n",
      "test loss on epoch 4755: 0.358\n",
      "test accuracy on epoch 4755: 0.769\n",
      "train loss on epoch 4756 : 0.058\n",
      "train accuracy on epoch 4756: 1.000\n",
      "test loss on epoch 4756: 0.356\n",
      "test accuracy on epoch 4756: 0.769\n",
      "train loss on epoch 4757 : 0.034\n",
      "train accuracy on epoch 4757: 1.000\n",
      "test loss on epoch 4757: 0.349\n",
      "test accuracy on epoch 4757: 0.769\n",
      "train loss on epoch 4758 : 0.243\n",
      "train accuracy on epoch 4758: 0.944\n",
      "test loss on epoch 4758: 0.344\n",
      "test accuracy on epoch 4758: 0.769\n",
      "train loss on epoch 4759 : 0.068\n",
      "train accuracy on epoch 4759: 0.944\n",
      "test loss on epoch 4759: 0.344\n",
      "test accuracy on epoch 4759: 0.769\n",
      "train loss on epoch 4760 : 0.276\n",
      "train accuracy on epoch 4760: 0.889\n",
      "test loss on epoch 4760: 0.346\n",
      "test accuracy on epoch 4760: 0.769\n",
      "train loss on epoch 4761 : 0.208\n",
      "train accuracy on epoch 4761: 0.889\n",
      "test loss on epoch 4761: 0.347\n",
      "test accuracy on epoch 4761: 0.769\n",
      "train loss on epoch 4762 : 0.100\n",
      "train accuracy on epoch 4762: 0.944\n",
      "test loss on epoch 4762: 0.361\n",
      "test accuracy on epoch 4762: 0.769\n",
      "train loss on epoch 4763 : 0.258\n",
      "train accuracy on epoch 4763: 0.889\n",
      "test loss on epoch 4763: 0.367\n",
      "test accuracy on epoch 4763: 0.769\n",
      "train loss on epoch 4764 : 0.141\n",
      "train accuracy on epoch 4764: 0.944\n",
      "test loss on epoch 4764: 0.371\n",
      "test accuracy on epoch 4764: 0.769\n",
      "train loss on epoch 4765 : 0.232\n",
      "train accuracy on epoch 4765: 0.944\n",
      "test loss on epoch 4765: 0.367\n",
      "test accuracy on epoch 4765: 0.769\n",
      "train loss on epoch 4766 : 0.198\n",
      "train accuracy on epoch 4766: 0.944\n",
      "test loss on epoch 4766: 0.366\n",
      "test accuracy on epoch 4766: 0.769\n",
      "train loss on epoch 4767 : 0.137\n",
      "train accuracy on epoch 4767: 0.944\n",
      "test loss on epoch 4767: 0.368\n",
      "test accuracy on epoch 4767: 0.769\n",
      "train loss on epoch 4768 : 0.422\n",
      "train accuracy on epoch 4768: 0.889\n",
      "test loss on epoch 4768: 0.370\n",
      "test accuracy on epoch 4768: 0.769\n",
      "train loss on epoch 4769 : 0.207\n",
      "train accuracy on epoch 4769: 0.889\n",
      "test loss on epoch 4769: 0.369\n",
      "test accuracy on epoch 4769: 0.769\n",
      "train loss on epoch 4770 : 0.049\n",
      "train accuracy on epoch 4770: 1.000\n",
      "test loss on epoch 4770: 0.368\n",
      "test accuracy on epoch 4770: 0.769\n",
      "train loss on epoch 4771 : 0.250\n",
      "train accuracy on epoch 4771: 0.889\n",
      "test loss on epoch 4771: 0.362\n",
      "test accuracy on epoch 4771: 0.769\n",
      "train loss on epoch 4772 : 0.209\n",
      "train accuracy on epoch 4772: 0.944\n",
      "test loss on epoch 4772: 0.362\n",
      "test accuracy on epoch 4772: 0.769\n",
      "train loss on epoch 4773 : 0.076\n",
      "train accuracy on epoch 4773: 0.944\n",
      "test loss on epoch 4773: 0.364\n",
      "test accuracy on epoch 4773: 0.769\n",
      "train loss on epoch 4774 : 0.065\n",
      "train accuracy on epoch 4774: 1.000\n",
      "test loss on epoch 4774: 0.363\n",
      "test accuracy on epoch 4774: 0.769\n",
      "train loss on epoch 4775 : 0.187\n",
      "train accuracy on epoch 4775: 0.889\n",
      "test loss on epoch 4775: 0.361\n",
      "test accuracy on epoch 4775: 0.769\n",
      "train loss on epoch 4776 : 0.239\n",
      "train accuracy on epoch 4776: 0.889\n",
      "test loss on epoch 4776: 0.357\n",
      "test accuracy on epoch 4776: 0.769\n",
      "train loss on epoch 4777 : 0.333\n",
      "train accuracy on epoch 4777: 0.944\n",
      "test loss on epoch 4777: 0.353\n",
      "test accuracy on epoch 4777: 0.769\n",
      "train loss on epoch 4778 : 0.143\n",
      "train accuracy on epoch 4778: 0.944\n",
      "test loss on epoch 4778: 0.355\n",
      "test accuracy on epoch 4778: 0.769\n",
      "train loss on epoch 4779 : 0.054\n",
      "train accuracy on epoch 4779: 1.000\n",
      "test loss on epoch 4779: 0.346\n",
      "test accuracy on epoch 4779: 0.769\n",
      "train loss on epoch 4780 : 0.156\n",
      "train accuracy on epoch 4780: 0.889\n",
      "test loss on epoch 4780: 0.353\n",
      "test accuracy on epoch 4780: 0.769\n",
      "train loss on epoch 4781 : 0.239\n",
      "train accuracy on epoch 4781: 0.889\n",
      "test loss on epoch 4781: 0.351\n",
      "test accuracy on epoch 4781: 0.769\n",
      "train loss on epoch 4782 : 0.066\n",
      "train accuracy on epoch 4782: 1.000\n",
      "test loss on epoch 4782: 0.362\n",
      "test accuracy on epoch 4782: 0.769\n",
      "train loss on epoch 4783 : 0.194\n",
      "train accuracy on epoch 4783: 0.889\n",
      "test loss on epoch 4783: 0.363\n",
      "test accuracy on epoch 4783: 0.769\n",
      "train loss on epoch 4784 : 0.305\n",
      "train accuracy on epoch 4784: 0.889\n",
      "test loss on epoch 4784: 0.368\n",
      "test accuracy on epoch 4784: 0.769\n",
      "train loss on epoch 4785 : 0.191\n",
      "train accuracy on epoch 4785: 0.944\n",
      "test loss on epoch 4785: 0.367\n",
      "test accuracy on epoch 4785: 0.769\n",
      "train loss on epoch 4786 : 0.310\n",
      "train accuracy on epoch 4786: 0.889\n",
      "test loss on epoch 4786: 0.369\n",
      "test accuracy on epoch 4786: 0.769\n",
      "train loss on epoch 4787 : 0.080\n",
      "train accuracy on epoch 4787: 0.944\n",
      "test loss on epoch 4787: 0.361\n",
      "test accuracy on epoch 4787: 0.769\n",
      "train loss on epoch 4788 : 0.128\n",
      "train accuracy on epoch 4788: 0.944\n",
      "test loss on epoch 4788: 0.353\n",
      "test accuracy on epoch 4788: 0.769\n",
      "train loss on epoch 4789 : 0.240\n",
      "train accuracy on epoch 4789: 0.944\n",
      "test loss on epoch 4789: 0.355\n",
      "test accuracy on epoch 4789: 0.769\n",
      "train loss on epoch 4790 : 0.117\n",
      "train accuracy on epoch 4790: 0.944\n",
      "test loss on epoch 4790: 0.351\n",
      "test accuracy on epoch 4790: 0.769\n",
      "train loss on epoch 4791 : 0.303\n",
      "train accuracy on epoch 4791: 0.889\n",
      "test loss on epoch 4791: 0.355\n",
      "test accuracy on epoch 4791: 0.769\n",
      "train loss on epoch 4792 : 0.160\n",
      "train accuracy on epoch 4792: 0.944\n",
      "test loss on epoch 4792: 0.357\n",
      "test accuracy on epoch 4792: 0.769\n",
      "train loss on epoch 4793 : 0.288\n",
      "train accuracy on epoch 4793: 0.889\n",
      "test loss on epoch 4793: 0.351\n",
      "test accuracy on epoch 4793: 0.769\n",
      "train loss on epoch 4794 : 0.044\n",
      "train accuracy on epoch 4794: 1.000\n",
      "test loss on epoch 4794: 0.357\n",
      "test accuracy on epoch 4794: 0.769\n",
      "train loss on epoch 4795 : 0.243\n",
      "train accuracy on epoch 4795: 0.889\n",
      "test loss on epoch 4795: 0.356\n",
      "test accuracy on epoch 4795: 0.769\n",
      "train loss on epoch 4796 : 0.234\n",
      "train accuracy on epoch 4796: 0.944\n",
      "test loss on epoch 4796: 0.363\n",
      "test accuracy on epoch 4796: 0.769\n",
      "train loss on epoch 4797 : 0.137\n",
      "train accuracy on epoch 4797: 0.944\n",
      "test loss on epoch 4797: 0.365\n",
      "test accuracy on epoch 4797: 0.769\n",
      "train loss on epoch 4798 : 0.143\n",
      "train accuracy on epoch 4798: 0.889\n",
      "test loss on epoch 4798: 0.373\n",
      "test accuracy on epoch 4798: 0.769\n",
      "train loss on epoch 4799 : 0.062\n",
      "train accuracy on epoch 4799: 1.000\n",
      "test loss on epoch 4799: 0.377\n",
      "test accuracy on epoch 4799: 0.769\n",
      "train loss on epoch 4800 : 0.090\n",
      "train accuracy on epoch 4800: 1.000\n",
      "test loss on epoch 4800: 0.378\n",
      "test accuracy on epoch 4800: 0.769\n",
      "train loss on epoch 4801 : 0.252\n",
      "train accuracy on epoch 4801: 0.889\n",
      "test loss on epoch 4801: 0.383\n",
      "test accuracy on epoch 4801: 0.769\n",
      "train loss on epoch 4802 : 0.242\n",
      "train accuracy on epoch 4802: 0.889\n",
      "test loss on epoch 4802: 0.379\n",
      "test accuracy on epoch 4802: 0.769\n",
      "train loss on epoch 4803 : 0.188\n",
      "train accuracy on epoch 4803: 0.944\n",
      "test loss on epoch 4803: 0.382\n",
      "test accuracy on epoch 4803: 0.769\n",
      "train loss on epoch 4804 : 0.188\n",
      "train accuracy on epoch 4804: 0.944\n",
      "test loss on epoch 4804: 0.380\n",
      "test accuracy on epoch 4804: 0.769\n",
      "train loss on epoch 4805 : 0.089\n",
      "train accuracy on epoch 4805: 0.944\n",
      "test loss on epoch 4805: 0.383\n",
      "test accuracy on epoch 4805: 0.769\n",
      "train loss on epoch 4806 : 0.100\n",
      "train accuracy on epoch 4806: 0.944\n",
      "test loss on epoch 4806: 0.383\n",
      "test accuracy on epoch 4806: 0.769\n",
      "train loss on epoch 4807 : 0.079\n",
      "train accuracy on epoch 4807: 1.000\n",
      "test loss on epoch 4807: 0.383\n",
      "test accuracy on epoch 4807: 0.769\n",
      "train loss on epoch 4808 : 0.080\n",
      "train accuracy on epoch 4808: 1.000\n",
      "test loss on epoch 4808: 0.377\n",
      "test accuracy on epoch 4808: 0.769\n",
      "train loss on epoch 4809 : 0.104\n",
      "train accuracy on epoch 4809: 0.944\n",
      "test loss on epoch 4809: 0.375\n",
      "test accuracy on epoch 4809: 0.769\n",
      "train loss on epoch 4810 : 0.113\n",
      "train accuracy on epoch 4810: 0.944\n",
      "test loss on epoch 4810: 0.369\n",
      "test accuracy on epoch 4810: 0.769\n",
      "train loss on epoch 4811 : 0.235\n",
      "train accuracy on epoch 4811: 0.833\n",
      "test loss on epoch 4811: 0.365\n",
      "test accuracy on epoch 4811: 0.769\n",
      "train loss on epoch 4812 : 0.390\n",
      "train accuracy on epoch 4812: 0.889\n",
      "test loss on epoch 4812: 0.357\n",
      "test accuracy on epoch 4812: 0.769\n",
      "train loss on epoch 4813 : 0.095\n",
      "train accuracy on epoch 4813: 1.000\n",
      "test loss on epoch 4813: 0.350\n",
      "test accuracy on epoch 4813: 0.769\n",
      "train loss on epoch 4814 : 0.210\n",
      "train accuracy on epoch 4814: 0.944\n",
      "test loss on epoch 4814: 0.360\n",
      "test accuracy on epoch 4814: 0.769\n",
      "train loss on epoch 4815 : 0.077\n",
      "train accuracy on epoch 4815: 1.000\n",
      "test loss on epoch 4815: 0.358\n",
      "test accuracy on epoch 4815: 0.769\n",
      "train loss on epoch 4816 : 0.157\n",
      "train accuracy on epoch 4816: 0.889\n",
      "test loss on epoch 4816: 0.359\n",
      "test accuracy on epoch 4816: 0.769\n",
      "train loss on epoch 4817 : 0.269\n",
      "train accuracy on epoch 4817: 0.889\n",
      "test loss on epoch 4817: 0.360\n",
      "test accuracy on epoch 4817: 0.769\n",
      "train loss on epoch 4818 : 0.146\n",
      "train accuracy on epoch 4818: 0.944\n",
      "test loss on epoch 4818: 0.356\n",
      "test accuracy on epoch 4818: 0.769\n",
      "train loss on epoch 4819 : 0.146\n",
      "train accuracy on epoch 4819: 0.944\n",
      "test loss on epoch 4819: 0.354\n",
      "test accuracy on epoch 4819: 0.769\n",
      "train loss on epoch 4820 : 0.320\n",
      "train accuracy on epoch 4820: 0.889\n",
      "test loss on epoch 4820: 0.347\n",
      "test accuracy on epoch 4820: 0.769\n",
      "train loss on epoch 4821 : 0.199\n",
      "train accuracy on epoch 4821: 0.944\n",
      "test loss on epoch 4821: 0.340\n",
      "test accuracy on epoch 4821: 0.769\n",
      "train loss on epoch 4822 : 0.177\n",
      "train accuracy on epoch 4822: 0.944\n",
      "test loss on epoch 4822: 0.339\n",
      "test accuracy on epoch 4822: 0.769\n",
      "train loss on epoch 4823 : 0.251\n",
      "train accuracy on epoch 4823: 0.889\n",
      "test loss on epoch 4823: 0.326\n",
      "test accuracy on epoch 4823: 0.769\n",
      "train loss on epoch 4824 : 0.225\n",
      "train accuracy on epoch 4824: 0.889\n",
      "test loss on epoch 4824: 0.340\n",
      "test accuracy on epoch 4824: 0.769\n",
      "train loss on epoch 4825 : 0.121\n",
      "train accuracy on epoch 4825: 0.944\n",
      "test loss on epoch 4825: 0.328\n",
      "test accuracy on epoch 4825: 0.769\n",
      "train loss on epoch 4826 : 0.277\n",
      "train accuracy on epoch 4826: 0.944\n",
      "test loss on epoch 4826: 0.340\n",
      "test accuracy on epoch 4826: 0.692\n",
      "train loss on epoch 4827 : 0.189\n",
      "train accuracy on epoch 4827: 0.889\n",
      "test loss on epoch 4827: 0.342\n",
      "test accuracy on epoch 4827: 0.692\n",
      "train loss on epoch 4828 : 0.218\n",
      "train accuracy on epoch 4828: 0.889\n",
      "test loss on epoch 4828: 0.333\n",
      "test accuracy on epoch 4828: 0.769\n",
      "train loss on epoch 4829 : 0.068\n",
      "train accuracy on epoch 4829: 1.000\n",
      "test loss on epoch 4829: 0.341\n",
      "test accuracy on epoch 4829: 0.769\n",
      "train loss on epoch 4830 : 0.310\n",
      "train accuracy on epoch 4830: 0.889\n",
      "test loss on epoch 4830: 0.338\n",
      "test accuracy on epoch 4830: 0.769\n",
      "train loss on epoch 4831 : 0.356\n",
      "train accuracy on epoch 4831: 0.889\n",
      "test loss on epoch 4831: 0.345\n",
      "test accuracy on epoch 4831: 0.769\n",
      "train loss on epoch 4832 : 0.154\n",
      "train accuracy on epoch 4832: 0.944\n",
      "test loss on epoch 4832: 0.329\n",
      "test accuracy on epoch 4832: 0.769\n",
      "train loss on epoch 4833 : 0.072\n",
      "train accuracy on epoch 4833: 0.944\n",
      "test loss on epoch 4833: 0.322\n",
      "test accuracy on epoch 4833: 0.769\n",
      "train loss on epoch 4834 : 0.141\n",
      "train accuracy on epoch 4834: 0.944\n",
      "test loss on epoch 4834: 0.344\n",
      "test accuracy on epoch 4834: 0.769\n",
      "train loss on epoch 4835 : 0.128\n",
      "train accuracy on epoch 4835: 0.944\n",
      "test loss on epoch 4835: 0.351\n",
      "test accuracy on epoch 4835: 0.769\n",
      "train loss on epoch 4836 : 0.074\n",
      "train accuracy on epoch 4836: 0.944\n",
      "test loss on epoch 4836: 0.352\n",
      "test accuracy on epoch 4836: 0.769\n",
      "train loss on epoch 4837 : 0.226\n",
      "train accuracy on epoch 4837: 0.944\n",
      "test loss on epoch 4837: 0.330\n",
      "test accuracy on epoch 4837: 0.769\n",
      "train loss on epoch 4838 : 0.275\n",
      "train accuracy on epoch 4838: 0.889\n",
      "test loss on epoch 4838: 0.329\n",
      "test accuracy on epoch 4838: 0.769\n",
      "train loss on epoch 4839 : 0.254\n",
      "train accuracy on epoch 4839: 0.889\n",
      "test loss on epoch 4839: 0.332\n",
      "test accuracy on epoch 4839: 0.769\n",
      "train loss on epoch 4840 : 0.193\n",
      "train accuracy on epoch 4840: 0.833\n",
      "test loss on epoch 4840: 0.350\n",
      "test accuracy on epoch 4840: 0.692\n",
      "train loss on epoch 4841 : 0.300\n",
      "train accuracy on epoch 4841: 0.889\n",
      "test loss on epoch 4841: 0.353\n",
      "test accuracy on epoch 4841: 0.692\n",
      "train loss on epoch 4842 : 0.168\n",
      "train accuracy on epoch 4842: 0.944\n",
      "test loss on epoch 4842: 0.337\n",
      "test accuracy on epoch 4842: 0.769\n",
      "train loss on epoch 4843 : 0.288\n",
      "train accuracy on epoch 4843: 0.944\n",
      "test loss on epoch 4843: 0.327\n",
      "test accuracy on epoch 4843: 0.769\n",
      "train loss on epoch 4844 : 0.103\n",
      "train accuracy on epoch 4844: 0.944\n",
      "test loss on epoch 4844: 0.353\n",
      "test accuracy on epoch 4844: 0.769\n",
      "train loss on epoch 4845 : 0.110\n",
      "train accuracy on epoch 4845: 1.000\n",
      "test loss on epoch 4845: 0.335\n",
      "test accuracy on epoch 4845: 0.769\n",
      "train loss on epoch 4846 : 0.244\n",
      "train accuracy on epoch 4846: 0.889\n",
      "test loss on epoch 4846: 0.326\n",
      "test accuracy on epoch 4846: 0.769\n",
      "train loss on epoch 4847 : 0.254\n",
      "train accuracy on epoch 4847: 0.944\n",
      "test loss on epoch 4847: 0.354\n",
      "test accuracy on epoch 4847: 0.769\n",
      "train loss on epoch 4848 : 0.084\n",
      "train accuracy on epoch 4848: 1.000\n",
      "test loss on epoch 4848: 0.351\n",
      "test accuracy on epoch 4848: 0.769\n",
      "train loss on epoch 4849 : 0.090\n",
      "train accuracy on epoch 4849: 0.944\n",
      "test loss on epoch 4849: 0.352\n",
      "test accuracy on epoch 4849: 0.769\n",
      "train loss on epoch 4850 : 0.309\n",
      "train accuracy on epoch 4850: 0.944\n",
      "test loss on epoch 4850: 0.342\n",
      "test accuracy on epoch 4850: 0.769\n",
      "train loss on epoch 4851 : 0.204\n",
      "train accuracy on epoch 4851: 0.889\n",
      "test loss on epoch 4851: 0.327\n",
      "test accuracy on epoch 4851: 0.769\n",
      "train loss on epoch 4852 : 0.074\n",
      "train accuracy on epoch 4852: 1.000\n",
      "test loss on epoch 4852: 0.348\n",
      "test accuracy on epoch 4852: 0.769\n",
      "train loss on epoch 4853 : 0.131\n",
      "train accuracy on epoch 4853: 0.944\n",
      "test loss on epoch 4853: 0.346\n",
      "test accuracy on epoch 4853: 0.692\n",
      "train loss on epoch 4854 : 0.262\n",
      "train accuracy on epoch 4854: 0.833\n",
      "test loss on epoch 4854: 0.327\n",
      "test accuracy on epoch 4854: 0.846\n",
      "train loss on epoch 4855 : 0.161\n",
      "train accuracy on epoch 4855: 0.889\n",
      "test loss on epoch 4855: 0.334\n",
      "test accuracy on epoch 4855: 0.769\n",
      "train loss on epoch 4856 : 0.076\n",
      "train accuracy on epoch 4856: 1.000\n",
      "test loss on epoch 4856: 0.336\n",
      "test accuracy on epoch 4856: 0.769\n",
      "train loss on epoch 4857 : 0.210\n",
      "train accuracy on epoch 4857: 0.944\n",
      "test loss on epoch 4857: 0.339\n",
      "test accuracy on epoch 4857: 0.769\n",
      "train loss on epoch 4858 : 0.090\n",
      "train accuracy on epoch 4858: 0.944\n",
      "test loss on epoch 4858: 0.340\n",
      "test accuracy on epoch 4858: 0.769\n",
      "train loss on epoch 4859 : 0.170\n",
      "train accuracy on epoch 4859: 0.889\n",
      "test loss on epoch 4859: 0.339\n",
      "test accuracy on epoch 4859: 0.769\n",
      "train loss on epoch 4860 : 0.080\n",
      "train accuracy on epoch 4860: 1.000\n",
      "test loss on epoch 4860: 0.348\n",
      "test accuracy on epoch 4860: 0.769\n",
      "train loss on epoch 4861 : 0.203\n",
      "train accuracy on epoch 4861: 0.889\n",
      "test loss on epoch 4861: 0.338\n",
      "test accuracy on epoch 4861: 0.769\n",
      "train loss on epoch 4862 : 0.042\n",
      "train accuracy on epoch 4862: 1.000\n",
      "test loss on epoch 4862: 0.343\n",
      "test accuracy on epoch 4862: 0.769\n",
      "train loss on epoch 4863 : 0.124\n",
      "train accuracy on epoch 4863: 0.944\n",
      "test loss on epoch 4863: 0.346\n",
      "test accuracy on epoch 4863: 0.769\n",
      "train loss on epoch 4864 : 0.180\n",
      "train accuracy on epoch 4864: 0.889\n",
      "test loss on epoch 4864: 0.330\n",
      "test accuracy on epoch 4864: 0.769\n",
      "train loss on epoch 4865 : 0.226\n",
      "train accuracy on epoch 4865: 0.889\n",
      "test loss on epoch 4865: 0.342\n",
      "test accuracy on epoch 4865: 0.769\n",
      "train loss on epoch 4866 : 0.132\n",
      "train accuracy on epoch 4866: 0.944\n",
      "test loss on epoch 4866: 0.328\n",
      "test accuracy on epoch 4866: 0.769\n",
      "train loss on epoch 4867 : 0.297\n",
      "train accuracy on epoch 4867: 0.889\n",
      "test loss on epoch 4867: 0.341\n",
      "test accuracy on epoch 4867: 0.692\n",
      "train loss on epoch 4868 : 0.098\n",
      "train accuracy on epoch 4868: 0.944\n",
      "test loss on epoch 4868: 0.322\n",
      "test accuracy on epoch 4868: 0.846\n",
      "train loss on epoch 4869 : 0.380\n",
      "train accuracy on epoch 4869: 0.833\n",
      "test loss on epoch 4869: 0.338\n",
      "test accuracy on epoch 4869: 0.692\n",
      "train loss on epoch 4870 : 0.196\n",
      "train accuracy on epoch 4870: 0.889\n",
      "test loss on epoch 4870: 0.331\n",
      "test accuracy on epoch 4870: 0.769\n",
      "train loss on epoch 4871 : 0.163\n",
      "train accuracy on epoch 4871: 0.889\n",
      "test loss on epoch 4871: 0.320\n",
      "test accuracy on epoch 4871: 0.769\n",
      "train loss on epoch 4872 : 0.114\n",
      "train accuracy on epoch 4872: 1.000\n",
      "test loss on epoch 4872: 0.332\n",
      "test accuracy on epoch 4872: 0.769\n",
      "train loss on epoch 4873 : 0.086\n",
      "train accuracy on epoch 4873: 1.000\n",
      "test loss on epoch 4873: 0.340\n",
      "test accuracy on epoch 4873: 0.769\n",
      "train loss on epoch 4874 : 0.214\n",
      "train accuracy on epoch 4874: 0.889\n",
      "test loss on epoch 4874: 0.320\n",
      "test accuracy on epoch 4874: 0.769\n",
      "train loss on epoch 4875 : 0.313\n",
      "train accuracy on epoch 4875: 0.833\n",
      "test loss on epoch 4875: 0.320\n",
      "test accuracy on epoch 4875: 0.846\n",
      "train loss on epoch 4876 : 0.241\n",
      "train accuracy on epoch 4876: 0.889\n",
      "test loss on epoch 4876: 0.332\n",
      "test accuracy on epoch 4876: 0.769\n",
      "train loss on epoch 4877 : 0.080\n",
      "train accuracy on epoch 4877: 1.000\n",
      "test loss on epoch 4877: 0.333\n",
      "test accuracy on epoch 4877: 0.769\n",
      "train loss on epoch 4878 : 0.278\n",
      "train accuracy on epoch 4878: 0.833\n",
      "test loss on epoch 4878: 0.345\n",
      "test accuracy on epoch 4878: 0.769\n",
      "train loss on epoch 4879 : 0.303\n",
      "train accuracy on epoch 4879: 0.889\n",
      "test loss on epoch 4879: 0.338\n",
      "test accuracy on epoch 4879: 0.769\n",
      "train loss on epoch 4880 : 0.104\n",
      "train accuracy on epoch 4880: 0.944\n",
      "test loss on epoch 4880: 0.341\n",
      "test accuracy on epoch 4880: 0.769\n",
      "train loss on epoch 4881 : 0.066\n",
      "train accuracy on epoch 4881: 1.000\n",
      "test loss on epoch 4881: 0.341\n",
      "test accuracy on epoch 4881: 0.769\n",
      "train loss on epoch 4882 : 0.213\n",
      "train accuracy on epoch 4882: 0.944\n",
      "test loss on epoch 4882: 0.332\n",
      "test accuracy on epoch 4882: 0.769\n",
      "train loss on epoch 4883 : 0.264\n",
      "train accuracy on epoch 4883: 0.889\n",
      "test loss on epoch 4883: 0.338\n",
      "test accuracy on epoch 4883: 0.769\n",
      "train loss on epoch 4884 : 0.071\n",
      "train accuracy on epoch 4884: 1.000\n",
      "test loss on epoch 4884: 0.344\n",
      "test accuracy on epoch 4884: 0.769\n",
      "train loss on epoch 4885 : 0.158\n",
      "train accuracy on epoch 4885: 0.944\n",
      "test loss on epoch 4885: 0.342\n",
      "test accuracy on epoch 4885: 0.769\n",
      "train loss on epoch 4886 : 0.146\n",
      "train accuracy on epoch 4886: 0.944\n",
      "test loss on epoch 4886: 0.341\n",
      "test accuracy on epoch 4886: 0.769\n",
      "train loss on epoch 4887 : 0.087\n",
      "train accuracy on epoch 4887: 1.000\n",
      "test loss on epoch 4887: 0.351\n",
      "test accuracy on epoch 4887: 0.769\n",
      "train loss on epoch 4888 : 0.237\n",
      "train accuracy on epoch 4888: 0.944\n",
      "test loss on epoch 4888: 0.339\n",
      "test accuracy on epoch 4888: 0.769\n",
      "train loss on epoch 4889 : 0.136\n",
      "train accuracy on epoch 4889: 0.944\n",
      "test loss on epoch 4889: 0.333\n",
      "test accuracy on epoch 4889: 0.769\n",
      "train loss on epoch 4890 : 0.198\n",
      "train accuracy on epoch 4890: 0.833\n",
      "test loss on epoch 4890: 0.331\n",
      "test accuracy on epoch 4890: 0.769\n",
      "train loss on epoch 4891 : 0.085\n",
      "train accuracy on epoch 4891: 0.944\n",
      "test loss on epoch 4891: 0.317\n",
      "test accuracy on epoch 4891: 0.846\n",
      "train loss on epoch 4892 : 0.047\n",
      "train accuracy on epoch 4892: 1.000\n",
      "test loss on epoch 4892: 0.313\n",
      "test accuracy on epoch 4892: 0.769\n",
      "train loss on epoch 4893 : 0.487\n",
      "train accuracy on epoch 4893: 0.833\n",
      "test loss on epoch 4893: 0.330\n",
      "test accuracy on epoch 4893: 0.769\n",
      "train loss on epoch 4894 : 0.319\n",
      "train accuracy on epoch 4894: 0.833\n",
      "test loss on epoch 4894: 0.315\n",
      "test accuracy on epoch 4894: 0.769\n",
      "train loss on epoch 4895 : 0.099\n",
      "train accuracy on epoch 4895: 0.944\n",
      "test loss on epoch 4895: 0.323\n",
      "test accuracy on epoch 4895: 0.769\n",
      "train loss on epoch 4896 : 0.512\n",
      "train accuracy on epoch 4896: 0.778\n",
      "test loss on epoch 4896: 0.329\n",
      "test accuracy on epoch 4896: 0.769\n",
      "train loss on epoch 4897 : 0.095\n",
      "train accuracy on epoch 4897: 0.944\n",
      "test loss on epoch 4897: 0.316\n",
      "test accuracy on epoch 4897: 0.846\n",
      "train loss on epoch 4898 : 0.228\n",
      "train accuracy on epoch 4898: 0.889\n",
      "test loss on epoch 4898: 0.332\n",
      "test accuracy on epoch 4898: 0.692\n",
      "train loss on epoch 4899 : 0.103\n",
      "train accuracy on epoch 4899: 1.000\n",
      "test loss on epoch 4899: 0.316\n",
      "test accuracy on epoch 4899: 0.846\n",
      "train loss on epoch 4900 : 0.144\n",
      "train accuracy on epoch 4900: 0.944\n",
      "test loss on epoch 4900: 0.325\n",
      "test accuracy on epoch 4900: 0.769\n",
      "train loss on epoch 4901 : 0.156\n",
      "train accuracy on epoch 4901: 0.944\n",
      "test loss on epoch 4901: 0.321\n",
      "test accuracy on epoch 4901: 0.769\n",
      "train loss on epoch 4902 : 0.156\n",
      "train accuracy on epoch 4902: 0.944\n",
      "test loss on epoch 4902: 0.323\n",
      "test accuracy on epoch 4902: 0.769\n",
      "train loss on epoch 4903 : 0.213\n",
      "train accuracy on epoch 4903: 0.889\n",
      "test loss on epoch 4903: 0.329\n",
      "test accuracy on epoch 4903: 0.692\n",
      "train loss on epoch 4904 : 0.170\n",
      "train accuracy on epoch 4904: 0.889\n",
      "test loss on epoch 4904: 0.335\n",
      "test accuracy on epoch 4904: 0.769\n",
      "train loss on epoch 4905 : 0.184\n",
      "train accuracy on epoch 4905: 0.889\n",
      "test loss on epoch 4905: 0.329\n",
      "test accuracy on epoch 4905: 0.769\n",
      "train loss on epoch 4906 : 0.087\n",
      "train accuracy on epoch 4906: 0.944\n",
      "test loss on epoch 4906: 0.337\n",
      "test accuracy on epoch 4906: 0.769\n",
      "train loss on epoch 4907 : 0.511\n",
      "train accuracy on epoch 4907: 0.833\n",
      "test loss on epoch 4907: 0.356\n",
      "test accuracy on epoch 4907: 0.769\n",
      "train loss on epoch 4908 : 0.328\n",
      "train accuracy on epoch 4908: 0.889\n",
      "test loss on epoch 4908: 0.349\n",
      "test accuracy on epoch 4908: 0.769\n",
      "train loss on epoch 4909 : 0.108\n",
      "train accuracy on epoch 4909: 0.944\n",
      "test loss on epoch 4909: 0.342\n",
      "test accuracy on epoch 4909: 0.769\n",
      "train loss on epoch 4910 : 0.268\n",
      "train accuracy on epoch 4910: 0.944\n",
      "test loss on epoch 4910: 0.324\n",
      "test accuracy on epoch 4910: 0.769\n",
      "train loss on epoch 4911 : 0.235\n",
      "train accuracy on epoch 4911: 0.889\n",
      "test loss on epoch 4911: 0.325\n",
      "test accuracy on epoch 4911: 0.769\n",
      "train loss on epoch 4912 : 0.081\n",
      "train accuracy on epoch 4912: 1.000\n",
      "test loss on epoch 4912: 0.318\n",
      "test accuracy on epoch 4912: 0.846\n",
      "train loss on epoch 4913 : 0.067\n",
      "train accuracy on epoch 4913: 1.000\n",
      "test loss on epoch 4913: 0.329\n",
      "test accuracy on epoch 4913: 0.769\n",
      "train loss on epoch 4914 : 0.400\n",
      "train accuracy on epoch 4914: 0.833\n",
      "test loss on epoch 4914: 0.329\n",
      "test accuracy on epoch 4914: 0.769\n",
      "train loss on epoch 4915 : 0.098\n",
      "train accuracy on epoch 4915: 0.944\n",
      "test loss on epoch 4915: 0.329\n",
      "test accuracy on epoch 4915: 0.769\n",
      "train loss on epoch 4916 : 0.112\n",
      "train accuracy on epoch 4916: 0.944\n",
      "test loss on epoch 4916: 0.318\n",
      "test accuracy on epoch 4916: 0.769\n",
      "train loss on epoch 4917 : 0.138\n",
      "train accuracy on epoch 4917: 1.000\n",
      "test loss on epoch 4917: 0.324\n",
      "test accuracy on epoch 4917: 0.769\n",
      "train loss on epoch 4918 : 0.407\n",
      "train accuracy on epoch 4918: 0.889\n",
      "test loss on epoch 4918: 0.329\n",
      "test accuracy on epoch 4918: 0.769\n",
      "train loss on epoch 4919 : 0.211\n",
      "train accuracy on epoch 4919: 0.889\n",
      "test loss on epoch 4919: 0.330\n",
      "test accuracy on epoch 4919: 0.769\n",
      "train loss on epoch 4920 : 0.055\n",
      "train accuracy on epoch 4920: 1.000\n",
      "test loss on epoch 4920: 0.328\n",
      "test accuracy on epoch 4920: 0.769\n",
      "train loss on epoch 4921 : 0.228\n",
      "train accuracy on epoch 4921: 0.944\n",
      "test loss on epoch 4921: 0.334\n",
      "test accuracy on epoch 4921: 0.769\n",
      "train loss on epoch 4922 : 0.219\n",
      "train accuracy on epoch 4922: 0.889\n",
      "test loss on epoch 4922: 0.329\n",
      "test accuracy on epoch 4922: 0.769\n",
      "train loss on epoch 4923 : 0.069\n",
      "train accuracy on epoch 4923: 1.000\n",
      "test loss on epoch 4923: 0.339\n",
      "test accuracy on epoch 4923: 0.769\n",
      "train loss on epoch 4924 : 0.084\n",
      "train accuracy on epoch 4924: 1.000\n",
      "test loss on epoch 4924: 0.336\n",
      "test accuracy on epoch 4924: 0.769\n",
      "train loss on epoch 4925 : 0.114\n",
      "train accuracy on epoch 4925: 0.944\n",
      "test loss on epoch 4925: 0.338\n",
      "test accuracy on epoch 4925: 0.769\n",
      "train loss on epoch 4926 : 0.226\n",
      "train accuracy on epoch 4926: 0.944\n",
      "test loss on epoch 4926: 0.330\n",
      "test accuracy on epoch 4926: 0.769\n",
      "train loss on epoch 4927 : 0.125\n",
      "train accuracy on epoch 4927: 0.944\n",
      "test loss on epoch 4927: 0.324\n",
      "test accuracy on epoch 4927: 0.769\n",
      "train loss on epoch 4928 : 0.373\n",
      "train accuracy on epoch 4928: 0.833\n",
      "test loss on epoch 4928: 0.329\n",
      "test accuracy on epoch 4928: 0.846\n",
      "train loss on epoch 4929 : 0.035\n",
      "train accuracy on epoch 4929: 1.000\n",
      "test loss on epoch 4929: 0.329\n",
      "test accuracy on epoch 4929: 0.769\n",
      "train loss on epoch 4930 : 0.204\n",
      "train accuracy on epoch 4930: 0.833\n",
      "test loss on epoch 4930: 0.330\n",
      "test accuracy on epoch 4930: 0.769\n",
      "train loss on epoch 4931 : 0.194\n",
      "train accuracy on epoch 4931: 0.889\n",
      "test loss on epoch 4931: 0.330\n",
      "test accuracy on epoch 4931: 0.846\n",
      "train loss on epoch 4932 : 0.124\n",
      "train accuracy on epoch 4932: 1.000\n",
      "test loss on epoch 4932: 0.337\n",
      "test accuracy on epoch 4932: 0.769\n",
      "train loss on epoch 4933 : 0.237\n",
      "train accuracy on epoch 4933: 0.889\n",
      "test loss on epoch 4933: 0.335\n",
      "test accuracy on epoch 4933: 0.769\n",
      "train loss on epoch 4934 : 0.306\n",
      "train accuracy on epoch 4934: 0.944\n",
      "test loss on epoch 4934: 0.318\n",
      "test accuracy on epoch 4934: 0.769\n",
      "train loss on epoch 4935 : 0.068\n",
      "train accuracy on epoch 4935: 1.000\n",
      "test loss on epoch 4935: 0.323\n",
      "test accuracy on epoch 4935: 0.769\n",
      "train loss on epoch 4936 : 0.166\n",
      "train accuracy on epoch 4936: 0.889\n",
      "test loss on epoch 4936: 0.325\n",
      "test accuracy on epoch 4936: 0.769\n",
      "train loss on epoch 4937 : 0.194\n",
      "train accuracy on epoch 4937: 0.944\n",
      "test loss on epoch 4937: 0.337\n",
      "test accuracy on epoch 4937: 0.769\n",
      "train loss on epoch 4938 : 0.117\n",
      "train accuracy on epoch 4938: 0.944\n",
      "test loss on epoch 4938: 0.318\n",
      "test accuracy on epoch 4938: 0.769\n",
      "train loss on epoch 4939 : 0.166\n",
      "train accuracy on epoch 4939: 0.944\n",
      "test loss on epoch 4939: 0.324\n",
      "test accuracy on epoch 4939: 0.769\n",
      "train loss on epoch 4940 : 0.199\n",
      "train accuracy on epoch 4940: 0.944\n",
      "test loss on epoch 4940: 0.333\n",
      "test accuracy on epoch 4940: 0.769\n",
      "train loss on epoch 4941 : 0.450\n",
      "train accuracy on epoch 4941: 0.833\n",
      "test loss on epoch 4941: 0.321\n",
      "test accuracy on epoch 4941: 0.769\n",
      "train loss on epoch 4942 : 0.198\n",
      "train accuracy on epoch 4942: 0.944\n",
      "test loss on epoch 4942: 0.329\n",
      "test accuracy on epoch 4942: 0.769\n",
      "train loss on epoch 4943 : 0.158\n",
      "train accuracy on epoch 4943: 0.944\n",
      "test loss on epoch 4943: 0.321\n",
      "test accuracy on epoch 4943: 0.846\n",
      "train loss on epoch 4944 : 0.261\n",
      "train accuracy on epoch 4944: 0.889\n",
      "test loss on epoch 4944: 0.336\n",
      "test accuracy on epoch 4944: 0.692\n",
      "train loss on epoch 4945 : 0.174\n",
      "train accuracy on epoch 4945: 0.889\n",
      "test loss on epoch 4945: 0.329\n",
      "test accuracy on epoch 4945: 0.769\n",
      "train loss on epoch 4946 : 0.122\n",
      "train accuracy on epoch 4946: 0.944\n",
      "test loss on epoch 4946: 0.341\n",
      "test accuracy on epoch 4946: 0.769\n",
      "train loss on epoch 4947 : 0.304\n",
      "train accuracy on epoch 4947: 0.889\n",
      "test loss on epoch 4947: 0.338\n",
      "test accuracy on epoch 4947: 0.769\n",
      "train loss on epoch 4948 : 0.180\n",
      "train accuracy on epoch 4948: 0.944\n",
      "test loss on epoch 4948: 0.343\n",
      "test accuracy on epoch 4948: 0.769\n",
      "train loss on epoch 4949 : 0.138\n",
      "train accuracy on epoch 4949: 0.944\n",
      "test loss on epoch 4949: 0.351\n",
      "test accuracy on epoch 4949: 0.769\n",
      "train loss on epoch 4950 : 0.155\n",
      "train accuracy on epoch 4950: 0.889\n",
      "test loss on epoch 4950: 0.362\n",
      "test accuracy on epoch 4950: 0.769\n",
      "train loss on epoch 4951 : 0.113\n",
      "train accuracy on epoch 4951: 0.944\n",
      "test loss on epoch 4951: 0.367\n",
      "test accuracy on epoch 4951: 0.769\n",
      "train loss on epoch 4952 : 0.214\n",
      "train accuracy on epoch 4952: 0.889\n",
      "test loss on epoch 4952: 0.368\n",
      "test accuracy on epoch 4952: 0.769\n",
      "train loss on epoch 4953 : 0.067\n",
      "train accuracy on epoch 4953: 1.000\n",
      "test loss on epoch 4953: 0.365\n",
      "test accuracy on epoch 4953: 0.769\n",
      "train loss on epoch 4954 : 0.205\n",
      "train accuracy on epoch 4954: 0.944\n",
      "test loss on epoch 4954: 0.358\n",
      "test accuracy on epoch 4954: 0.769\n",
      "train loss on epoch 4955 : 0.157\n",
      "train accuracy on epoch 4955: 0.944\n",
      "test loss on epoch 4955: 0.350\n",
      "test accuracy on epoch 4955: 0.769\n",
      "train loss on epoch 4956 : 0.061\n",
      "train accuracy on epoch 4956: 0.944\n",
      "test loss on epoch 4956: 0.345\n",
      "test accuracy on epoch 4956: 0.769\n",
      "train loss on epoch 4957 : 0.254\n",
      "train accuracy on epoch 4957: 0.889\n",
      "test loss on epoch 4957: 0.344\n",
      "test accuracy on epoch 4957: 0.769\n",
      "train loss on epoch 4958 : 0.197\n",
      "train accuracy on epoch 4958: 0.889\n",
      "test loss on epoch 4958: 0.348\n",
      "test accuracy on epoch 4958: 0.769\n",
      "train loss on epoch 4959 : 0.120\n",
      "train accuracy on epoch 4959: 0.944\n",
      "test loss on epoch 4959: 0.348\n",
      "test accuracy on epoch 4959: 0.769\n",
      "train loss on epoch 4960 : 0.166\n",
      "train accuracy on epoch 4960: 0.944\n",
      "test loss on epoch 4960: 0.345\n",
      "test accuracy on epoch 4960: 0.769\n",
      "train loss on epoch 4961 : 0.046\n",
      "train accuracy on epoch 4961: 1.000\n",
      "test loss on epoch 4961: 0.336\n",
      "test accuracy on epoch 4961: 0.769\n",
      "train loss on epoch 4962 : 0.233\n",
      "train accuracy on epoch 4962: 0.944\n",
      "test loss on epoch 4962: 0.330\n",
      "test accuracy on epoch 4962: 0.769\n",
      "train loss on epoch 4963 : 0.117\n",
      "train accuracy on epoch 4963: 0.944\n",
      "test loss on epoch 4963: 0.337\n",
      "test accuracy on epoch 4963: 0.692\n",
      "train loss on epoch 4964 : 0.064\n",
      "train accuracy on epoch 4964: 0.944\n",
      "test loss on epoch 4964: 0.329\n",
      "test accuracy on epoch 4964: 0.769\n",
      "train loss on epoch 4965 : 0.064\n",
      "train accuracy on epoch 4965: 1.000\n",
      "test loss on epoch 4965: 0.336\n",
      "test accuracy on epoch 4965: 0.692\n",
      "train loss on epoch 4966 : 0.218\n",
      "train accuracy on epoch 4966: 0.944\n",
      "test loss on epoch 4966: 0.324\n",
      "test accuracy on epoch 4966: 0.846\n",
      "train loss on epoch 4967 : 0.083\n",
      "train accuracy on epoch 4967: 1.000\n",
      "test loss on epoch 4967: 0.324\n",
      "test accuracy on epoch 4967: 0.846\n",
      "train loss on epoch 4968 : 0.159\n",
      "train accuracy on epoch 4968: 0.944\n",
      "test loss on epoch 4968: 0.322\n",
      "test accuracy on epoch 4968: 0.769\n",
      "train loss on epoch 4969 : 0.212\n",
      "train accuracy on epoch 4969: 0.944\n",
      "test loss on epoch 4969: 0.322\n",
      "test accuracy on epoch 4969: 0.846\n",
      "train loss on epoch 4970 : 0.120\n",
      "train accuracy on epoch 4970: 0.944\n",
      "test loss on epoch 4970: 0.316\n",
      "test accuracy on epoch 4970: 0.769\n",
      "train loss on epoch 4971 : 0.301\n",
      "train accuracy on epoch 4971: 0.889\n",
      "test loss on epoch 4971: 0.327\n",
      "test accuracy on epoch 4971: 0.769\n",
      "train loss on epoch 4972 : 0.180\n",
      "train accuracy on epoch 4972: 0.944\n",
      "test loss on epoch 4972: 0.324\n",
      "test accuracy on epoch 4972: 0.769\n",
      "train loss on epoch 4973 : 0.038\n",
      "train accuracy on epoch 4973: 1.000\n",
      "test loss on epoch 4973: 0.331\n",
      "test accuracy on epoch 4973: 0.692\n",
      "train loss on epoch 4974 : 0.181\n",
      "train accuracy on epoch 4974: 0.889\n",
      "test loss on epoch 4974: 0.332\n",
      "test accuracy on epoch 4974: 0.692\n",
      "train loss on epoch 4975 : 0.099\n",
      "train accuracy on epoch 4975: 0.944\n",
      "test loss on epoch 4975: 0.330\n",
      "test accuracy on epoch 4975: 0.769\n",
      "train loss on epoch 4976 : 0.203\n",
      "train accuracy on epoch 4976: 0.889\n",
      "test loss on epoch 4976: 0.335\n",
      "test accuracy on epoch 4976: 0.769\n",
      "train loss on epoch 4977 : 0.194\n",
      "train accuracy on epoch 4977: 0.833\n",
      "test loss on epoch 4977: 0.323\n",
      "test accuracy on epoch 4977: 0.769\n",
      "train loss on epoch 4978 : 0.072\n",
      "train accuracy on epoch 4978: 1.000\n",
      "test loss on epoch 4978: 0.330\n",
      "test accuracy on epoch 4978: 0.769\n",
      "train loss on epoch 4979 : 0.123\n",
      "train accuracy on epoch 4979: 0.944\n",
      "test loss on epoch 4979: 0.335\n",
      "test accuracy on epoch 4979: 0.769\n",
      "train loss on epoch 4980 : 0.089\n",
      "train accuracy on epoch 4980: 0.944\n",
      "test loss on epoch 4980: 0.331\n",
      "test accuracy on epoch 4980: 0.769\n",
      "train loss on epoch 4981 : 0.209\n",
      "train accuracy on epoch 4981: 0.944\n",
      "test loss on epoch 4981: 0.340\n",
      "test accuracy on epoch 4981: 0.769\n",
      "train loss on epoch 4982 : 0.028\n",
      "train accuracy on epoch 4982: 1.000\n",
      "test loss on epoch 4982: 0.332\n",
      "test accuracy on epoch 4982: 0.769\n",
      "train loss on epoch 4983 : 0.109\n",
      "train accuracy on epoch 4983: 0.944\n",
      "test loss on epoch 4983: 0.320\n",
      "test accuracy on epoch 4983: 0.769\n",
      "train loss on epoch 4984 : 0.189\n",
      "train accuracy on epoch 4984: 0.944\n",
      "test loss on epoch 4984: 0.322\n",
      "test accuracy on epoch 4984: 0.769\n",
      "train loss on epoch 4985 : 0.107\n",
      "train accuracy on epoch 4985: 0.944\n",
      "test loss on epoch 4985: 0.319\n",
      "test accuracy on epoch 4985: 0.769\n",
      "train loss on epoch 4986 : 0.040\n",
      "train accuracy on epoch 4986: 1.000\n",
      "test loss on epoch 4986: 0.333\n",
      "test accuracy on epoch 4986: 0.769\n",
      "train loss on epoch 4987 : 0.185\n",
      "train accuracy on epoch 4987: 0.889\n",
      "test loss on epoch 4987: 0.326\n",
      "test accuracy on epoch 4987: 0.769\n",
      "train loss on epoch 4988 : 0.175\n",
      "train accuracy on epoch 4988: 0.889\n",
      "test loss on epoch 4988: 0.327\n",
      "test accuracy on epoch 4988: 0.769\n",
      "train loss on epoch 4989 : 0.220\n",
      "train accuracy on epoch 4989: 0.889\n",
      "test loss on epoch 4989: 0.333\n",
      "test accuracy on epoch 4989: 0.769\n",
      "train loss on epoch 4990 : 0.144\n",
      "train accuracy on epoch 4990: 0.944\n",
      "test loss on epoch 4990: 0.321\n",
      "test accuracy on epoch 4990: 0.769\n",
      "train loss on epoch 4991 : 0.190\n",
      "train accuracy on epoch 4991: 0.944\n",
      "test loss on epoch 4991: 0.314\n",
      "test accuracy on epoch 4991: 0.769\n",
      "train loss on epoch 4992 : 0.060\n",
      "train accuracy on epoch 4992: 0.944\n",
      "test loss on epoch 4992: 0.337\n",
      "test accuracy on epoch 4992: 0.769\n",
      "train loss on epoch 4993 : 0.170\n",
      "train accuracy on epoch 4993: 0.944\n",
      "test loss on epoch 4993: 0.333\n",
      "test accuracy on epoch 4993: 0.769\n",
      "train loss on epoch 4994 : 0.163\n",
      "train accuracy on epoch 4994: 0.889\n",
      "test loss on epoch 4994: 0.331\n",
      "test accuracy on epoch 4994: 0.769\n",
      "train loss on epoch 4995 : 0.410\n",
      "train accuracy on epoch 4995: 0.833\n",
      "test loss on epoch 4995: 0.314\n",
      "test accuracy on epoch 4995: 0.769\n",
      "train loss on epoch 4996 : 0.191\n",
      "train accuracy on epoch 4996: 0.889\n",
      "test loss on epoch 4996: 0.329\n",
      "test accuracy on epoch 4996: 0.769\n",
      "train loss on epoch 4997 : 0.078\n",
      "train accuracy on epoch 4997: 1.000\n",
      "test loss on epoch 4997: 0.339\n",
      "test accuracy on epoch 4997: 0.769\n",
      "train loss on epoch 4998 : 0.057\n",
      "train accuracy on epoch 4998: 1.000\n",
      "test loss on epoch 4998: 0.315\n",
      "test accuracy on epoch 4998: 0.769\n",
      "train loss on epoch 4999 : 0.260\n",
      "train accuracy on epoch 4999: 0.944\n",
      "test loss on epoch 4999: 0.313\n",
      "test accuracy on epoch 4999: 0.769\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "net = MyModel(pad_idx + 1, 50, pad_idx, dropout=0.5)\n",
    "weights=torch.concat((torch.tensor(embeds.vectors), torch.zeros((1,embeds.vectors.shape[-1]))),0)\n",
    "net.embedding = nn.Embedding.from_pretrained(weights)\n",
    "net.embedding.weight.requires_grad = False\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(net.parameters(), lr=0.01, weight_decay=0.0)\n",
    "epochs = 5000\n",
    "train_test(train_loader, test_loader, net, loss_function, optimizer, epochs, clip=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a6345cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the bike drives on the road {'logits': tensor([[-4.8995,  4.6348]]), 'prediction': tensor([1.])} truth= 1\n",
      "a lion and a cat in a tree {'logits': tensor([[ 2.2625, -2.7670]]), 'prediction': tensor([0.])} truth= 0\n",
      "two cars crashed {'logits': tensor([[-6.4715,  6.2089]]), 'prediction': tensor([1.])} truth= 1\n",
      "i always go to work by bike {'logits': tensor([[-1.4148,  1.2771]]), 'prediction': tensor([1.])} truth= 1\n",
      "i have no animal at home {'logits': tensor([[ 1.0626, -1.2931]]), 'prediction': tensor([0.])} truth= 0\n",
      "dogs like cheese {'logits': tensor([[ 3.8166, -4.3029]]), 'prediction': tensor([0.])} truth= 0\n",
      "a pink flamingo {'logits': tensor([[ 3.5121, -3.8444]]), 'prediction': tensor([0.])} truth= 0\n",
      "trucks {'logits': tensor([[-7.5192,  7.4204]]), 'prediction': tensor([1.])} truth= 1\n",
      "truckks {'logits': tensor([[ 0.2306, -0.3512]]), 'prediction': tensor([0.])} truth= 1\n",
      "truckmegatruck {'logits': tensor([[ 0.2306, -0.3512]]), 'prediction': tensor([0.])} truth= 1\n",
      "a text about trucks, not animals {'logits': tensor([[-0.6927,  0.0769]]), 'prediction': tensor([1.])} truth= 1\n",
      "a text about animals, not trucks {'logits': tensor([[-0.6927,  0.0769]]), 'prediction': tensor([1.])} truth= 0\n",
      "doggs {'logits': tensor([[ 0.2306, -0.3512]]), 'prediction': tensor([0.])} truth= 0\n",
      "tensor([0.7692])\n"
     ]
    }
   ],
   "source": [
    "print(predict_from_pandas(vdf,net))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978edd51",
   "metadata": {},
   "source": [
    "Ok c'est beaucoup mieux, mais on a toujours des limites importantes: \n",
    "- Pas de prise en compte de l'ordre des mots : on voit que \"a text about trucks, not animals\" et \"a text about animals, not trucks\" retournent exactement les mêmes scores de prediction\n",
    "- Pas de gestion des mots hors vocabulaire. Exemple truckmegatruck n'est pas géré, et retourne exactement les mêmes predictions que les deux textes avec fautes de frappe truckks et doggs \n",
    "\n",
    "###  Tokenizers \n",
    "\n",
    "Pour aller plus loin, on propose maintenant d'utiliser des tokens issus d'un tokenizer plus évolué, du type Byte Pair Encoding, pour voir si cela pourrait améliorer les performances. \n",
    "\n",
    "On commence par récupérer un tokenizer pré-entraîné sur un corpus : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b9a167e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'leaping': 21216, 'dose': 13004, 'association': 2523, '##：': 30519, 'flavors': 26389, 'reject': 15454, '##hearted': 27693, 'neutron': 20393, 'herald': 9536, 'anniversary': 5315, 'km²': 3186, 'newsweek': 26948, '[unused574]': 579, 'princess': 4615, 'transmitter': 11659, 'utter': 14395, 'pointedly': 28713, 'rai': 15547, 'jersey': 3933, 'checked': 7039, 'entertain': 20432, 'himalayan': 28333, 'forward': 2830, 'romantic': 6298, 'decker': 20946, 'blu': 14154, 'starts': 4627, '[unused207]': 212, '[unused969]': 974, 'continue': 3613, '##bba': 22414, 'glasgow': 6785, 'bing': 17620, 'preparations': 12929, '##lag': 17802, 'routledge': 21180, 'billionaire': 22301, 'forceful': 28552, '##chal': 18598, '##oed': 29099, 'champions': 3966, '##grove': 21525, 'seafood': 23621, '##光': 30296, 'remarkably': 17431, '##ɣ': 29683, 'piloted': 27220, 'flower': 6546, '[unused696]': 701, 'suspense': 23873, 'smells': 14747, 'bromwich': 27888, 'd': 1040, 'sounded': 5015, 'customs': 8205, 'combo': 25025, '##oft': 15794, 'richard': 2957, 'newspaper': 3780, 'danger': 5473, '##grad': 16307, 'appreciation': 12284, 'civilization': 10585, 'instant': 7107, 'reacting': 24868, 'convex': 18309, 'strained': 12250, '[unused900]': 905, 'inscription': 9315, '##bas': 22083, 'americana': 25988, 'korea': 4420, 'royale': 24483, '##zen': 10431, '´': 1084, 'pure': 5760, 'retire': 11036, 'thwarted': 28409, '(': 1006, 'worthy': 11007, '##uddin': 17375, 'intersection': 6840, 'depictions': 20818, '311': 23532, '##pants': 27578, '133': 14506, 'huddled': 20258, 'scots': 12196, 'streets': 4534, 'guitars': 7334, 'dorsal': 12759, 'rescuing': 23659, 'augusto': 29085, 'truss': 24224, 'brit': 28101, '[unused739]': 744, 'intervene': 18793, 'scuba': 28651, '清': 1903, 'profound': 13769, 'contacts': 10402, 'regulation': 7816, '[unused153]': 158, '[unused431]': 436, '[unused932]': 937, 'causing': 4786, 'nova': 6846, 'sussex': 9503, 'generalized': 18960, '##♯': 30154, 'wayne': 6159, 'stair': 23788, 'classified': 6219, 'guilty': 5905, 'basic': 3937, 'breeders': 20823, '##hoff': 17896, 'antennas': 26315, 'bleak': 21657, '##ulator': 20350, '##≥': 30136, '##rting': 24097, '藤': 1944, 'breath': 3052, 'coming': 2746, 'random': 6721, 'therefore': 3568, '##graphy': 12565, 'emil': 16243, 'newer': 10947, 'miniatures': 28615, 'regarding': 4953, 'necessity': 13185, 'blues': 5132, 'totaled': 23596, '##দ': 29900, '？': 1994, '##geny': 17487, 'prophet': 12168, '##script': 22483, '##bed': 8270, 'ravaged': 25537, 'motifs': 17366, 'patients': 5022, 'recruitment': 15680, 'turkish': 5037, 'decomposition': 22511, 'infamous': 14429, 'graffiti': 17990, 'choir': 6596, '[unused24]': 25, 'proto': 15053, '##elt': 20042, 'contractors': 16728, '146': 16333, '[unused72]': 73, 'canning': 24549, 'cheese': 8808, '142': 16087, '##lau': 17298, '##糹': 30460, 'broadcast': 3743, 'troy': 9553, 'yahoo': 20643, 'guess': 3984, 'peripheral': 15965, 'booked': 17414, '##tti': 6916, 'squash': 18794, 'assume': 7868, '00': 4002, 'forestry': 13116, 'jack': 2990, 'chop': 24494, '[unused186]': 191, '##dak': 23597, 'dawn': 6440, 'elects': 27161, 'submissions': 27842, 'paces': 24785, '《': 1639, '##scope': 26127, 'roadway': 18799, 'provence': 19923, '1920s': 6641, 'leading': 2877, 'thistle': 24328, '2010': 2230, 'supplying': 17731, 'ac': 9353, 'advertisement': 15147, 'hank': 9180, 'decoration': 11446, 'hunched': 20498, 'dog': 3899, 'interim': 9455, 'cbn': 21824, '356': 27509, 'hare': 14263, '##ffie': 29055, 'presidents': 11274, 'tanzania': 11959, 'branch': 3589, 'bergen': 12674, 'premise': 18458, '##mies': 28397, 'alloys': 28655, 'disgrace': 29591, '##語': 30476, 'free': 2489, 'tucked': 9332, 'pratt': 13208, 'clutching': 14197, 'squads': 20191, 'ז': 1247, 'admiral': 5902, 'lowland': 15234, 'westbound': 24717, '##ʎ': 29701, 'abstract': 10061, '##yang': 12198, 'graphic': 8425, 'mercury': 8714, '1782': 16890, '昭': 1868, 'sporting': 7419, 'screenings': 25437, 'demi': 27668, 'understand': 3305, 'dry': 4318, 'depend': 12530, 'pendleton': 24349, '##¾': 29665, '[unused518]': 523, '1086': 28196, 'okay': 3100, 'casino': 9270, 'gene': 4962, 'zeke': 18270, 'pereira': 23857, 'merton': 28169, 'mohammed': 12619, 'asturias': 26798, 'customers': 6304, 'feared': 8615, 'jericho': 17309, 'obsession': 17418, 'constable': 12294, 'tandem': 18231, 'dishes': 10447, '##pressive': 27484, '##ppel': 27877, 'clothing': 5929, 'judging': 13325, 'flap': 20916, 'accomplish': 14570, 'drumming': 22980, 'fritz': 12880, 'matches': 3503, 'tko': 26537, 'fledged': 26712, 'austrians': 28439, '##atics': 17592, '##出': 30300, 'caress': 21753, 'posthumous': 19086, 'casually': 13217, 'julia': 6423, 'endowed': 19038, 'excitedly': 23885, '##orestation': 25794, 'scientific': 4045, 'hottest': 20930, 'flicking': 26259, 'followers': 8771, 'flung': 14016, 'cm': 4642, '##ras': 8180, 'ذ': 1279, 'foam': 17952, '##brush': 18623, 'surveillance': 9867, 'hangs': 17991, 'pete': 6969, 'cents': 16653, 'quo': 22035, '[unused592]': 597, 'ar': 12098, 'mankind': 14938, 'sandwich': 11642, '[unused493]': 498, 'convened': 19596, '##ə': 29681, 'jesus': 4441, 'injection': 13341, 'miranda': 9392, '##al': 2389, '##ད': 29964, '##eded': 19082, 'victor': 5125, 'strangers': 12358, '##right': 15950, 'progressed': 12506, 'dashboard': 24923, '##meric': 25531, 'usa': 3915, 'improve': 5335, 'guiding': 14669, '##sit': 28032, 'double': 3313, 'mint': 12927, 'solitary': 14348, 'churchyard': 19812, 'ceylon': 16447, 'clade': 21697, 'compilation': 6268, 'dared': 15048, '##ong': 5063, 'ᵍ': 1500, 'torpedo': 9862, 'analyze': 17908, '237': 23297, 'obedience': 22645, 'creation': 4325, '##gos': 12333, '##missive': 27876, 'downloadable': 26720, 'flea': 26735, 'nationally': 9582, 'consul': 11801, 'preserving': 15224, 'submerged': 13563, 'enactment': 26465, '##博': 30312, 'gage': 10012, '##46': 21472, '4000': 20143, 'experiencing': 13417, 'tracy': 10555, '##uding': 24539, 'eastman': 24252, 'vaudeville': 19698, 'solvent': 23735, '##ne': 2638, '##lift': 18412, 'bertha': 25079, 'coasts': 20266, 'henley': 20768, 'tugged': 10621, '##ame': 14074, 'ducks': 14875, 'consultative': 28581, 'respiratory': 16464, 'tensed': 15225, 'leg': 4190, 'mix': 4666, 'secure': 5851, 'viruses': 18191, 'spokesperson': 15974, 'alarmed': 19260, 'corpse': 11547, 'unavailable': 20165, '##ulation': 9513, '##rh': 25032, 'irs': 25760, '##hs': 7898, 'scholarly': 12683, 'inaugurated': 11070, 'moat': 25266, 'eyeing': 19285, 'nominate': 23388, '秀': 1928, 'ufc': 11966, 'lindsey': 17518, 'ab': 11113, '##bilis': 27965, '##urized': 28405, 'carlos': 5828, '##iate': 13143, '##ard': 4232, '##ging': 4726, 'gesturing': 22317, 'latino': 7402, 'doubling': 19383, '##mounted': 27632, 'educational': 4547, 'ivory': 11554, 'decisions': 6567, 'enroll': 25612, 'hate': 5223, 'psychiatric': 13691, 'ノ': 1717, '##rank': 26763, '##nism': 28113, '40': 2871, 'european': 2647, '##aceae': 18339, '##burn': 8022, '##asse': 27241, 'phase': 4403, 'maximize': 25845, '1830': 9500, 'freeman': 11462, 'occasions': 6642, '##jic': 25008, '##ma': 2863, 'associated': 3378, 'butch': 17520, 'rebranded': 18233, '##ɫ': 29686, 'fueled': 17999, '\"': 1000, 'athenian': 26956, 'interspersed': 25338, 'parenting': 28586, 'without': 2302, '##mond': 11442, 'emphasized': 13155, 'making': 2437, 'forcing': 6932, 'compartment': 15273, 'russia': 3607, 'denver': 7573, 'stung': 19280, '1918': 4271, 'hubbard': 16580, 'monasteries': 15947, 'presley': 17229, '##hri': 26378, 'thirteenth': 14725, 'hart': 7530, 'clusters': 12906, 'assortment': 26285, 'antoinette': 28429, 'strengthening': 16003, 'andrew': 4080, 'venture': 6957, '##rama': 14672, 'conceived': 10141, '##ckman': 24080, '##aur': 21159, '2005': 2384, '##lor': 10626, 'merchants': 10310, 'duty': 4611, 'reused': 26513, 'sui': 24086, '##logical': 9966, 'frogs': 17582, 'ட': 1384, 'concept': 4145, 'seasons': 3692, '##vy': 10736, 'fourier': 26899, 'spectral': 17435, '1877': 7557, 'chopin': 25479, 'armando': 26716, 'bulge': 23708, 'savage': 9576, 'hearth': 22885, 'slices': 25609, 'calculus': 19276, 'stunning': 14726, 'wig': 24405, 'oleg': 25841, '221': 19594, 'renovated': 10601, 'evaluated': 16330, 'byte': 24880, 'oblast': 10379, 'appleton': 26050, 'skimmed': 28862, 'detainees': 26485, 'proving': 13946, 'bones': 5944, '##ado': 9365, '##tu': 8525, 'newark': 12948, 'lizard': 15450, 'incarnation': 15715, 'heartbreak': 27724, 'spices': 21729, 'amazingly': 29350, 'chooses': 15867, 'participate': 5589, '☉': 1622, 'independent': 2981, 'noises': 14950, 'sleeve': 10353, '州': 1836, 'directive': 16449, 'vincent': 6320, '##lide': 24198, '##ieri': 21939, '##night': 15864, 'prosecuted': 21651, 'bedfordshire': 28430, 'then': 2059, 'excited': 7568, 'naturally': 8100, 'thousand': 4595, 'expressing': 14026, '##oping': 17686, '##dern': 25888, '##orescence': 20030, 'journalist': 4988, 'useless': 11809, 'sons': 4124, 'pounding': 9836, 'worship': 7425, 'extracts': 27059, 'rally': 8320, 'hearts': 8072, 'indicator': 17245, 'tatiana': 22725, 'tool': 6994, '##ying': 14147, '##tify': 27351, 'investments': 10518, 'bags': 8641, 'dominion': 13738, '##app': 29098, 'リ': 1732, '##ℓ': 30105, 'grande': 9026, 'eroded': 23300, 'norte': 19931, '##hit': 16584, 'contract': 3206, 'fishery': 22880, 'phil': 6316, 'volta': 26089, 'careful': 6176, '[unused697]': 702, 'andreas': 12460, 'traders': 13066, '##ctor': 16761, 'beat': 3786, '##bush': 22427, '##rao': 25667, 'harmonies': 28594, '宣': 1823, 'mma': 21021, '比': 1890, '##lto': 23223, '##lus': 7393, '43': 4724, 'song': 2299, 'harder': 6211, '##flies': 24019, 'inflammatory': 20187, 'downloads': 22956, 'hard': 2524, 'copy': 6100, '[unused435]': 440, 'ik': 20912, 'tonnes': 11000, '[unused127]': 132, '##to': 3406, 'beside': 3875, '##て': 30191, 'fierce': 9205, 'fixture': 15083, 'martyr': 17216, 'tyrant': 26508, 'continuous': 7142, '##idon': 28201, 'decorations': 14529, 'sighs': 19906, 'schedule': 6134, 'osborn': 26999, 'guessed': 11445, 'virtually': 8990, 'limitation': 22718, 'entries': 10445, '[unused449]': 454, 'convince': 8054, 'gains': 12154, '##riam': 25557, 'mirrored': 22243, 'argentine': 8511, 'colourful': 26102, 'expense': 10961, 'amy': 6864, 'debuts': 26740, '##avi': 18891, '1751': 24440, 'basin': 6403, 'practically': 8134, 'nest': 9089, 'precise': 10480, '##cas': 15671, 'hms': 7220, 'sizes': 10826, '##tip': 25101, 'academy': 2914, 'defending': 6984, 'tube': 7270, '##ale': 9453, 'radcliffe': 22603, '##asian': 24145, 'quarterly': 12174, 'suv': 15620, 'rises': 9466, 'reclaimed': 23119, 'kong': 4290, '[unused700]': 705, 'monograph': 24039, 'melancholy': 22247, '##zaki': 18637, '##tson': 25656, 'filmfare': 25648, 'sucked': 8631, 'fulfilling': 21570, 'pistol': 8779, 'audio': 5746, 'knee': 6181, '##ﬂ': 30511, '##izes': 10057, 'elaborated': 25187, 'vicinity': 9884, 'undo': 25672, '##date': 13701, 'sar': 18906, 'repetitive': 23563, 'mosques': 21922, 'grey': 4462, 'evaluation': 9312, 'egan': 27889, '##dor': 7983, '[unused763]': 768, 'robots': 13507, 'meanwhile': 5564, 'osaka': 13000, 'vernon': 11447, '##uin': 20023, 'receipts': 28258, 'katy': 17870, 'skaters': 24789, '##sport': 20205, 'blocks': 5991, 'salary': 10300, 'sell': 5271, 'identical': 7235, '##eri': 11124, 'runs': 3216, 'omar': 13192, '[unused810]': 815, 'indications': 24936, 'maddie': 17805, 'ghetto': 17276, '##gles': 17125, 'casualty': 19844, '##ducted': 29510, 'ninth': 6619, '1852': 8784, 'rejects': 19164, 'coping': 27520, '##oto': 11439, '##oris': 21239, '##haya': 26115, '##akh': 27573, 'vidal': 29259, 'aden': 16298, '##ণ': 29897, 'อ': 1421, 'sworn': 10741, 'darren': 12270, 'advises': 25453, 'shrubs': 18812, '##ical': 7476, 'haggard': 27912, 'exiled': 14146, 'undertaking': 18457, 'tempo': 13657, '##tila': 26065, 'posted': 6866, 'saetan': 23515, '##umatic': 25360, 'yeast': 21957, '##tail': 14162, 'woo': 15854, '[unused109]': 114, 'selangor': 28904, '[unused11]': 12, 'spanish': 3009, '##bots': 27014, '##zog': 28505, '##view': 8584, 'culinary': 20560, 'ritchie': 20404, '##hort': 27794, 'თ': 1444, 'really': 2428, 'gothic': 7788, 'fly': 4875, 'honors': 7836, '##堂': 30331, 'comprises': 8681, 'sections': 5433, 'basque': 13915, 'classrooms': 12463, 'regulator': 21618, 'simultaneous': 17424, '[unused462]': 467, 'wales': 3575, '305': 20405, 'gulp': 26546, 'burying': 20491, 'boilers': 20412, 'estadio': 14143, '[unused355]': 360, 'pray': 11839, 'protector': 16167, '##rked': 19849, 'boards': 7923, 'nuts': 12264, '[unused887]': 892, '[unused816]': 821, 'russell': 5735, 'vic': 10967, 'working': 2551, 'norwegian': 5046, 'mckenzie': 18506, 'james': 2508, 'apologize': 12134, 'mcnamara': 28340, 'pressures': 15399, 'churning': 26765, 'char': 25869, 'kentucky': 5612, 'shades': 13178, 'sergei': 14543, 'shrub': 15751, 'deco': 21933, 'amar': 23204, 'weddings': 20429, 'stephane': 26624, 'lex': 17244, '[unused871]': 876, 'cbc': 13581, 'rees': 22131, 'picking': 8130, 'prelude': 19508, 'ل': 1294, 'sperm': 18047, 'statistics': 6747, '[unused506]': 511, 'submitted': 7864, '##空': 30456, 'kids': 4268, '!': 999, 'stipulated': 25729, 'atp': 12649, 'pulmonary': 21908, 'trial': 3979, 'airborne': 10519, 'battista': 28422, 'janet': 9965, '##ade': 9648, 'frank': 3581, 'ג': 1243, 'pentagon': 20864, 'eighth': 5964, 'hm': 20287, '宿': 1826, 'turbo': 15386, '1965': 3551, 'addison': 18403, 'stadion': 20756, 'aboriginal': 9757, 'legally': 10142, '‰': 1530, 'zones': 10019, '##mates': 15416, 'caught': 3236, '##sso': 24137, '##rvis': 29074, 'shia': 20474, '##lain': 15987, 'ᶜ': 1511, '[unused304]': 309, '280': 13427, 'stigma': 26453, '[unused346]': 351, '##hol': 14854, 'boar': 24187, 'reginald': 14435, '##τ': 29734, 'hd': 10751, 'rushing': 8375, 'syriac': 24109, '##志': 30377, 'segment': 6903, 'paige': 17031, 'verlag': 14552, 'outlines': 22106, 'launches': 18989, 'vertebrae': 25517, 'beings': 9552, 'tate': 9902, '[unused90]': 91, 'growl': 13349, 'away': 2185, 'license': 6105, 'chesterfield': 22699, '1991': 2889, 'imprisoned': 8580, 'winston': 10180, 'rim': 11418, '##cade': 21869, 'person': 2711, 'minister': 2704, 'matters': 5609, 'brentford': 26550, '##ception': 24422, 'clause': 11075, 'demographics': 28321, 'appeals': 9023, '##suke': 16867, '##sse': 11393, '##oku': 21940, '[unused830]': 835, 'swayed': 20122, 'locked': 5299, '##haw': 14238, 'protagonist': 10191, 'supernatural': 11189, 'weaker': 15863, 'quadrant': 29371, 'forcibly': 20951, 'jurassic': 19996, 'plucked': 20780, 'palette': 27396, 'magnetic': 8060, 'beam': 7504, 'azerbaijan': 8365, 'stabilization': 28715, '[unused525]': 530, '##ve': 3726, '##river': 24352, '##uno': 27819, 'salamanca': 29608, 'collaborators': 21315, 'reflex': 22259, 'garth': 21523, 'cards': 5329, 'spotlight': 17763, '##due': 20041, '##ium': 5007, '##家': 30351, 'electrically': 29103, '##ag': 8490, 'database': 7809, '省': 1920, 'selector': 27000, 'couldn': 2481, 'deciding': 10561, 'tipped': 11182, 'readily': 12192, 'analogy': 23323, 'chalmers': 29069, 'oliveira': 24263, '##hl': 7317, '##gs': 5620, 'kim': 5035, '##ocation': 23909, 'registering': 25719, 'sing': 6170, 'chaos': 8488, 'killed': 2730, 'phi': 13569, 'asia': 4021, '41st': 24233, 'sequential': 25582, '##ckle': 19250, 'hua': 23064, '##os': 2891, 'xu': 15990, 'harmful': 17631, 'ingram': 23231, 'billing': 25640, '##ג': 29790, 'flexible': 12379, '##heads': 13038, 'needles': 17044, '##wley': 20609, '##39': 23499, 'tobin': 28096, 'trey': 14826, 'neatly': 15981, '##sp': 13102, 'ledge': 18283, 'collaboration': 5792, 'hiroshi': 26882, 'ɹ': 1125, 'cage': 7980, 'vicky': 22845, 'kurt': 9679, 'hymn': 14825, '##bola': 24290, 'rye': 20926, '##gb': 18259, '##itaire': 29422, 'manual': 6410, 'niagara': 15473, 'nuclei': 23767, 'cologne': 10918, 'ferrari': 13632, 'engines': 5209, 'lips': 2970, '[unused231]': 236, '##rne': 12119, 'cellar': 15423, 'asher': 17243, 'buddha': 11903, 'nearer': 20388, 'hunt': 5690, 'murray': 6264, 'loneliness': 20334, 'argus': 25294, 'vidhan': 28590, 'pediatric': 23614, 'manuscript': 8356, 'waterfront': 16317, 'parade': 7700, '##北': 30307, '[unused80]': 81, 'pedestrian': 14662, 'dallas': 5759, '##bius': 17028, 'connector': 19400, '##kim': 21138, 'defendants': 16362, 'mermaid': 22322, 'centralized': 22493, '##ys': 7274, 'invaders': 17347, 'chickens': 21868, 'scarred': 21985, 'abstracts': 29474, 'patted': 11930, '[unused40]': 41, 'brushed': 7765, 'accommodation': 11366, 'printed': 6267, 'candle': 13541, 'coastal': 5780, 'result': 2765, 'sadie': 21330, 'mischief': 25166, 'sellers': 19041, 'emission': 15760, 'mess': 6752, 'miami': 5631, '219': 20636, 'derby': 7350, 'lowest': 7290, 'glen': 8904, 'volley': 28073, 'at': 2012, 'shriek': 24795, 'standing': 3061, 'hawaiian': 12188, 'deputy': 4112, 'devastating': 14886, '##bala': 25060, 'duran': 22959, '##dner': 28001, 'mouthful': 28462, 'glamorgan': 23861, 'gun': 3282, 'professor': 2934, 'households': 3911, '##icing': 23553, 'fetch': 18584, 'data': 2951, 'reasoning': 13384, 'wren': 16255, 'freud': 19338, 'already': 2525, 'chronology': 17873, 'reigns': 23481, 'slid': 4934, 'wanna': 10587, 'lining': 14834, 'niccolo': 27033, 'swimming': 5742, 'badge': 10780, 'cavalier': 28778, '1679': 27924, 'proctor': 28770, 'competed': 3879, 'charting': 17918, 'mortar': 14335, '[unused553]': 558, 'rayon': 26810, 'dream': 3959, '[unused481]': 486, 'glow': 8652, '##eves': 23047, 'dazzling': 28190, 'caucus': 13965, '[unused725]': 730, 'swamps': 22722, '##hair': 26227, '##pc': 15042, '##ingen': 15542, '##ulin': 18639, 'elevated': 8319, '##haling': 23896, 'insignificant': 27018, 'december': 2285, 'canceled': 13261, '##chet': 20318, '47': 4700, 'hollywood': 5365, 'steelers': 15280, 'chelsea': 9295, '##vert': 16874, 'flemish': 15427, '##yna': 18279, '220': 10545, 'ม': 1415, '訁': 1949, 'poverty': 5635, '73': 6421, 'marathi': 18388, 'espionage': 21003, '##rington': 17281, '##oux': 28700, 'semifinals': 8565, 'weakened': 11855, 'zee': 23727, '##tyn': 25680, '##erre': 28849, 'archive': 8756, 'ড': 1360, '[unused740]': 745, 'seem': 4025, 'з': 1187, 'ན': 1429, 'lombard': 23441, 'deepest': 17578, '[unused688]': 693, 'wu': 8814, 'opium': 21075, 'sonny': 13584, '##ginal': 24965, '##oria': 11069, 'greco': 18180, '##esses': 26636, 'stimulating': 27295, 'lucifer': 24184, 'tomas': 12675, 'sec': 10819, 'fries': 22201, 'sorted': 19616, 'gabon': 23129, '##lved': 26832, 'cradled': 23774, '##imate': 21499, 'knew': 2354, 'medicare': 27615, '[unused787]': 792, 'armstrong': 9143, 'daytime': 12217, 'ᴰ': 1492, 'hectares': 9076, '##merie': 28677, '[unused212]': 217, 'kilograms': 18857, 'quarrel': 26260, 'constructive': 26157, 'currently': 2747, 'swansea': 16085, 'allied': 6035, 'freshmen': 26612, '1814': 10977, 'athena': 21880, 'leader': 3003, 'provider': 10802, '##hab': 25459, 'curriculum': 8882, 'whorls': 25637, 'hoffman': 15107, 'equation': 8522, 'hog': 27589, 'polgara': 27041, '##御': 30373, 'innovation': 8144, 'royals': 15426, '##ital': 18400, 'hut': 12570, 'predators': 12630, 'upgrades': 18739, 'ultimate': 7209, 'bill': 3021, 'contrary': 10043, '##chin': 17231, '##ᄒ': 30005, 'diego': 5277, '##utation': 26117, '402': 28048, '##rrie': 22155, '##え': 30175, 'ultrasound': 27312, 'reflective': 21346, '[unused246]': 251, 'fruits': 10962, 'railroad': 4296, '##ara': 5400, 'measure': 5468, 'messenger': 11981, 'molten': 23548, 'jackson': 4027, 'charming': 11951, '##ケ': 30229, 'resented': 27188, '##sier': 20236, 'francesco': 11400, '##lver': 26229, 'knight': 5000, 'flare': 17748, 'prominence': 12144, 'exceeds': 23651, 'fang': 15197, '##თ': 29980, 'translators': 28396, 'culminating': 16979, 'removal': 8208, '6000': 25961, 'weapon': 5195, 'industries': 6088, '##rkin': 26891, '##acy': 15719, 'autism': 19465, 'crazy': 4689, 'wwii': 25755, 'ravens': 17272, 'skulls': 21542, 'britney': 29168, 'din': 11586, 'spectrum': 8674, 'mutation': 16221, '##sho': 22231, 'dahl': 27934, '56th': 29087, 'short': 2460, 'bath': 7198, 'raging': 17559, '##led': 3709, 'misunderstanding': 24216, '##tension': 29048, 'sight': 4356, '##posed': 19155, '##as': 3022, 'shy': 11004, 'marshall': 5832, '##rod': 14127, 'narratives': 22143, 'provision': 9347, 'dora': 21008, 'taxes': 7773, '##⁹': 30078, 'inferior': 14092, '##bles': 13510, 'rearview': 28726, 'gerry': 14926, '##weig': 27204, 'lunar': 11926, 'appellate': 23240, 'rican': 13641, 'rumors': 11256, 'bits': 9017, '##eg': 13910, 'smoked': 20482, 'dialect': 9329, 'striker': 11854, 'huron': 21899, '##zek': 24506, 'shortstop': 25359, '##iferous': 23930, 'modify': 19933, 'ћ': 1217, 'fiscal': 10807, 'disused': 26958, '##lving': 25780, 'coefficients': 21374, 'bundesliga': 14250, 'mascara': 27700, '##oe': 8913, 'unhappy': 12511, 'ancient': 3418, '定': 1822, 'fulfill': 13883, 'disastrous': 16775, 'insulting': 23979, 'organic': 7554, 'ernest': 8471, '##maker': 8571, 'avatar': 22128, 'dense': 9742, 'rejecting': 21936, 'serviced': 22858, '331': 27533, 'sticky': 15875, 'monkey': 10608, '[unused398]': 403, '1734': 27367, '£100': 27708, 'summer': 2621, 'nominations': 9930, 'functioning': 12285, 'dubious': 22917, 'honneur': 28197, 'fade': 12985, '195': 17317, 'maneuvers': 23802, 'blazing': 17162, 'banged': 22843, 'deputies': 11964, 'terrific': 27547, '##上': 30268, 'internal': 4722, 'hearing': 4994, 'priesthood': 17911, '##ut': 4904, '##meyer': 24344, 'exquisite': 19401, 'transvaal': 28336, 'meter': 8316, 'accordance': 10388, '##osis': 12650, 'rashid': 22080, 'pinch': 18392, 'doses': 21656, 'overwhelmingly': 24783, '##mark': 10665, 'through': 2083, 'tidal': 15065, '##rman': 14515, 'spd': 23772, 'musical': 3315, '[unused132]': 137, 'า': 1422, '##ate': 3686, '##puram': 17809, 'blowing': 11221, 'seminary': 8705, 'asshole': 22052, 'pleasant': 8242, 'novo': 24576, 'mentally': 10597, 'dragged': 7944, 'franz': 8965, 'holly': 9079, 'pitchfork': 22355, '205': 16327, 'unbeaten': 20458, '##vara': 24516, 'dorothy': 9984, 'politely': 16954, 'bel': 19337, 'laughter': 7239, '##oga': 18170, 'luzon': 21622, '1961': 3777, 'psychologist': 15034, 'scrub': 18157, '##stream': 21422, 'strategies': 9942, 'twinned': 25901, 'dramatically': 12099, 'provide': 3073, 'delivers': 18058, 'refueling': 23026, '##of': 11253, '##nor': 12131, 'lahore': 15036, 'vice': 3580, 'reacher': 22970, 'encounters': 11340, '##´s': 21932, '##同': 30320, 'strategy': 5656, '##genase': 28835, 'classmates': 19846, 'greenville': 20967, 'securely': 28999, 'instrument': 6602, '##ril': 15928, 'macdonald': 10406, 'territories': 6500, 'mysteriously': 29239, 'announcing': 13856, 'endeavor': 23855, '##earing': 27242, '##lica': 19341, 'sterile': 25403, 'widow': 7794, 'オ': 1699, 'darts': 17493, 'manifested': 24906, 'ina': 27118, 'prepare': 7374, '[unused655]': 660, 'juveniles': 25406, 'subsequent': 4745, '##rent': 22787, '##guchi': 16918, 'devout': 26092, 'cease': 13236, 'chuckles': 26088, 'northwestern': 7855, '##版': 30433, 'team': 2136, 'kirby': 15129, 'serb': 20180, 'roundabout': 22831, 'icao': 18055, 'ו': 1246, 'demeanor': 21745, '##emi': 23238, 'administrative': 3831, 'vanished': 9955, 'ecclesiastical': 12301, 'gallantry': 22111, '##boy': 11097, 'treats': 18452, 'coughing': 21454, 'interesting': 5875, 'sidney': 11430, 'pinto': 25066, 'rudolph': 18466, 'explaining': 9990, 'titular': 12960, '##¹': 27904, '##lom': 21297, '##ம': 29925, 'freeing': 22198, 'sings': 10955, 'picturesque': 23273, 'departure': 6712, '##him': 14341, 'fugitive': 21329, '##cens': 19023, 'bringing': 5026, '##oo': 9541, 'throwing': 6886, 'tibetan': 11953, 'arithmetic': 20204, '1662': 25909, 'clashes': 17783, 'thorn': 16337, '##ʒ': 29704, '##ᄇ': 29996, 'tournament': 2977, '##義': 30462, 'erich': 17513, '1724': 26423, 'libel': 26201, 'reinforced': 11013, '[unused48]': 49, 'folly': 26272, '##lma': 19145, 'aaa': 13360, 'hoover': 17443, 'snacks': 27962, 'triumphant': 25251, 'warden': 13745, 'kara': 13173, '820': 26667, 'knotted': 27039, '##sible': 19307, 'convictions': 20488, 'winners': 4791, 'chaotic': 19633, 'flutter': 23638, '[unused180]': 185, 'assumes': 15980, 'improper': 24156, '##ija': 14713, 'successively': 24288, 'tombs': 16623, 'blackout': 26717, 'waterloo': 13784, 'prized': 25695, '1634': 28502, 'dust': 6497, 'eireann': 29157, 'showcase': 13398, 'nucleus': 13502, 'yelling': 13175, '##oped': 24174, '##uttered': 23128, 'familiarity': 24666, 'brest': 22451, 'iain': 24486, '##islav': 19834, '##秀': 30454, 'mrna': 28848, '1300': 19527, 'initials': 20381, '##aco': 22684, 'blog': 9927, 'integrity': 11109, 'wallace': 7825, 'fergus': 21023, 'inquired': 24849, '234': 22018, 'carly': 18431, '##श': 29872, '[unused454]': 459, 'professionally': 12145, 'critical': 4187, 'zoning': 27462, '##nger': 11392, 'ʉ': 1131, '[unused807]': 812, 'partly': 6576, 'prohibition': 13574, 'dealing': 7149, 'seneca': 21224, 'seeker': 29444, 'sydney': 3994, '##hardt': 13778, 'red': 2417, 'ninja': 14104, '##enna': 24397, '[unused561]': 566, '##ime': 14428, 'spiked': 25362, 'iata': 9833, '##ische': 13719, '##inian': 15830, '88': 6070, 'brightly': 14224, 'stanford': 8422, 'confronted': 12892, '##vera': 26061, 'screwed': 14180, 'miraculous': 26106, '[unused354]': 359, 'shells': 10986, 'parades': 26635, '##ᄋ': 29999, 'leadership': 4105, 'grabbing': 9775, 'mansions': 26842, '##ろ': 30215, 'mclean': 17602, '##cased': 28969, '##dm': 22117, '##ever': 22507, 'christchurch': 15880, 'controversial': 6801, 'xvi': 16855, 'psychotic': 27756, 'subdivision': 12572, 'kelley': 19543, 'ensuring': 12725, '##λ': 29727, 'subtle': 11259, 'beginning': 2927, 'participates': 17257, '##fk': 24316, 'topography': 21535, '##entation': 19304, '##pose': 20688, '##js': 22578, 'murder': 4028, 'cyclists': 21912, 'exhibiting': 22922, 'greyish': 26916, 'carnival': 11485, 'lessons': 8220, '1786': 17436, '##aking': 15495, '##lier': 14355, '60s': 20341, '##cer': 17119, 'church': 2277, 'tribune': 10969, 'neon': 16231, 'w': 1059, 'seattle': 5862, '##aw': 10376, '##esa': 22447, '##nes': 5267, 'wryly': 28325, 'straighten': 28568, 'gigantic': 20193, 'exits': 16639, 'patriarch': 12626, 'wished': 6257, '##uc': 14194, '##11': 14526, 'ethel': 19180, '[unused0]': 1, '[unused534]': 539, 'plenty': 7564, 'omega': 14827, 'bilingual': 17636, 'bursts': 19239, 'mabel': 19486, 'pomerania': 18631, 'end': 2203, '⇄': 1589, 'small': 2235, 'flagship': 10565, 'hinduism': 19632, 'surgical': 11707, 'dilapidated': 29283, '325': 19652, 'roxy': 23682, 'joao': 16304, 'dungeons': 20264, 'lady': 3203, 'kade': 26159, 'immediately': 3202, 'enforcement': 7285, 'fifty': 5595, 'թ': 1224, 'spoken': 5287, 'minority': 7162, 'rotten': 11083, 'superintendent': 9133, 'arrow': 8612, 'hee': 18235, 'meteorological': 20557, '##pio': 22071, '323': 25392, 'racetrack': 28435, 'san': 2624, 'vacancy': 15619, 'oasis': 18128, 'etc': 4385, 'ems': 29031, 'prices': 7597, 'fide': 26000, 'indians': 6505, 'padres': 21577, 'holiness': 27692, '##☉': 30149, '##gly': 25643, 'triggers': 27099, 'infant': 10527, '生': 1910, '義': 1936, 'floyd': 12305, 'remake': 12661, 'hancock': 13849, '##enburg': 12059, 'markedly': 29295, '##ᅪ': 30012, '##vyn': 23487, 'mumbai': 8955, 'pupil': 11136, 'recommends': 26021, 'clay': 5726, '##oid': 9314, '##ness': 2791, 'dynamics': 10949, 'sleepy': 17056, 'cat': 4937, 'grasping': 20854, 'creatures': 7329, '2016': 2355, 'solos': 24339, 'trend': 9874, 'thanked': 15583, 'hey': 4931, '##hawks': 16043, '##ene': 8625, '##sboro': 25623, 'cannabis': 17985, 'reelected': 20847, 'yankees': 11081, 'hue': 20639, 'octave': 21817, '##riety': 27840, '##iri': 15735, 'documentaries': 15693, '##cz': 27966, 'venetian': 15510, 'muse': 18437, '[unused867]': 872, '##म': 29867, 'lucien': 11732, 'bench': 6847, '##ivism': 22486, '##lberg': 22927, 'etymology': 26803, 'removed': 3718, '##rai': 14995, 'ferdinand': 9684, 'chris': 3782, 'redhead': 26705, 'pile': 8632, 'priscilla': 25193, 'hired': 5086, 'circle': 4418, 'projecting': 18398, 'sulfur': 17864, 'dei': 14866, 'wr': 23277, 'interpreted': 10009, 'aching': 14750, 'memoir': 12558, 'faintly': 18587, 'propaganda': 10398, 'yeah': 3398, '401': 22649, 'kira': 15163, 'concentrations': 14061, 'straining': 21366, '##⊕': 30139, '##1': 2487, 'compensate': 19079, 'provoked': 19157, 'stay': 2994, '##ड': 29857, 'fisheries': 13424, 'paolo': 14174, 'grady': 19011, '411': 27517, 'newtown': 26382, 'montagu': 26241, '##talk': 28014, 'prevented': 8729, 'acc': 16222, '##cd': 19797, 'dolly': 19958, '##、': 30161, 'spectacular': 12656, '##ezer': 25733, 'inverse': 19262, '##ivating': 17441, '1972': 3285, '##iful': 18424, '[unused985]': 990, 'fm': 4718, 'gathers': 29438, 'fungal': 28079, 'ェ': 1697, 'na': 6583, 'combination': 5257, 'ᄀ': 1455, 'obtained': 4663, 'ticket': 7281, 'permitted': 7936, 'noel': 10716, 'darwin': 11534, 'stays': 12237, 'bumped': 19030, 'treat': 7438, 'bradley': 8981, 'philipp': 20765, 'pensions': 22024, 'conservatoire': 29233, 'download': 8816, 'daylight': 11695, 'detailed': 6851, 'weave': 25308, 'concession': 16427, 'maynard': 22130, 'conversely': 18868, 'mannheim': 25116, 'portico': 23104, '##urne': 21737, '##glers': 25555, 'business': 2449, '原': 1787, '##planes': 26634, '248': 24568, 'cape': 4880, 'cute': 10140, 'mls': 16287, 'reconciled': 28348, 'catherine': 6615, 'locus': 25206, '##press': 20110, '##elle': 14774, '##ctum': 27272, 'enjoyed': 5632, 'uncredited': 8104, 'rectangular': 10806, 'aggravated': 25817, 'milton': 9660, '##jn': 22895, '##my': 8029, 'translator': 11403, 'promised': 5763, 'gill': 12267, 'dante': 9649, 'garland': 17017, 'versailles': 18346, 'paste': 19351, 'client': 7396, 'promotions': 15365, '##iel': 9257, '[unused937]': 942, 'dazed': 19720, 'pac': 14397, 'alf': 24493, 'graders': 23256, 'alberta': 7649, 'calculated': 10174, 'tasmania': 12343, '##trix': 29184, 'barefoot': 22985, 'collects': 17427, '##ungen': 23239, '##bla': 28522, 'marry': 5914, 'consider': 5136, 'biggest': 5221, 'arun': 28217, 'yoko': 28758, '[unused199]': 204, 'lesson': 10800, '##ast': 14083, 'giggled': 15889, '870': 28864, 'helsinki': 12331, 'pages': 5530, 'improvised': 19641, '1998': 2687, '1897': 6347, 'crimes': 6997, 'assessments': 20794, 'society': 2554, 'perished': 23181, '##ulus': 11627, 'dispersed': 15484, '##₆': 30082, 'retrieve': 12850, 'valentin': 24632, 'pun': 26136, 'marital': 23143, 'chow': 20209, 'barangay': 20937, '[unused123]': 128, 'amongst': 5921, 'dh': 28144, 'skirt': 9764, 'circling': 18519, 'prem': 26563, 'constance': 15713, 'elsewhere': 6974, '284': 26871, 'championships': 3219, 'omnibus': 27284, 'trials': 7012, '179': 20311, 'coptic': 27672, 'conquered': 11438, 'woken': 22795, 'pasadena': 18880, 'confinement': 21551, 'kiss': 3610, 'whatever': 3649, 'column': 5930, '##written': 15773, 'arguing': 9177, 'even': 2130, '##tore': 19277, 'donation': 13445, 'flame': 8457, 'brook': 9566, 'pounder': 20091, '##go': 3995, '##rated': 9250, '##ɾ': 29692, '##hesive': 21579, '273': 25371, '327': 28469, '##hedron': 26440, '##mise': 28732, '##rit': 14778, 'afb': 16909, 'rouge': 12801, 'varies': 9783, 'runoff': 19550, 'gabby': 27183, '##道': 30483, 'cunningham': 13652, 'arriving': 7194, 'halifax': 10475, '三': 1741, 'jerking': 22387, 'dig': 10667, 'trumpets': 26506, 'boyle': 16694, '##quet': 12647, 'oskar': 28626, 'ط': 1286, 'gag': 18201, 'thorough': 16030, 'hazard': 15559, '##world': 11108, '##氷': 30421, 'notably': 5546, 'axis': 8123, 'snap': 10245, 'massacre': 9288, 'sailors': 11279, 'denmark': 5842, 'scrutiny': 17423, 'mouse': 8000, '1811': 13086, 'chairman': 3472, 'rory': 14285, 'kato': 27496, 'ease': 7496, 'nowhere': 7880, '##va': 3567, 'equestrian': 19191, 'wolves': 8588, 'hove': 25215, '##橋': 30411, 'bases': 7888, 'serves': 4240, 'robber': 27307, 'warner': 6654, '##rted': 17724, 'oak': 6116, 'annex': 17827, 'nudged': 18666, '##uche': 19140, 'granada': 16553, 'versa': 18601, 'vote': 3789, 'ح': 1276, 'help': 2393, 'ད': 1428, 'corresponding': 7978, 'conan': 16608, 'fraud': 9861, 'arroyo': 23882, 'koreans': 24651, '##men': 3549, '##ping': 4691, 'examining': 12843, 'exhausted': 9069, 'organized': 4114, 'supply': 4425, 'parking': 5581, 'monks': 9978, '##urance': 25863, '##།': 29961, '‐': 1513, 'declines': 26451, '1825': 11384, 'enlightenment': 16724, 'contrasted': 22085, 'classifications': 26739, '##cript': 23235, '##lah': 14431, '##nagar': 14346, 'trans': 9099, 'egyptian': 6811, '[unused479]': 484, '##cheng': 21043, 'royalist': 20796, '##age': 4270, '##ں': 29843, 'occurrence': 14404, 'prolonged': 15330, '##gues': 22967, 'finer': 26954, '##raz': 20409, 'ami': 26445, 'reservation': 11079, 'serial': 7642, '##dna': 28911, 'bibliography': 24751, 'fewer': 8491, '##ana': 5162, '##ラ': 30257, 'davidson': 12017, 'italianate': 25302, 'shale': 18488, '[unused642]': 647, '##uity': 18518, 'ranging': 7478, 'sulawesi': 29078, 'skyla': 21738, '[unused103]': 108, 'disdain': 25134, 'muir': 23110, 'pip': 28315, 'empire': 3400, 'destruction': 6215, 'maximal': 29160, 'resident': 6319, '##lika': 25421, 'sensitivity': 14639, 'wei': 11417, 'grumbled': 17030, 'depended': 17292, 'jurisdictions': 17370, '##rani': 21578, '[unused174]': 179, 'denote': 19090, 'ritter': 23168, 'asteroids': 26732, 'croft': 28983, 'wednesday': 9317, 'see': 2156, 'struggle': 5998, 'minnie': 26188, 'gain': 5114, 'oblivious': 18333, 'ash': 6683, '28': 2654, 'student': 3076, 'ads': 14997, '##iani': 25443, 'compares': 22963, 'vol': 5285, 'signs': 5751, '1863': 6899, 'nationalism': 14594, '##shing': 12227, 'familiar': 5220, 'luckily': 15798, 'gdp': 14230, 'bolsheviks': 28755, 'rake': 26008, 'horizon': 9154, 'boxing': 8362, 'zeppelin': 22116, 'ᵀ': 1495, '##strel': 26877, 'torches': 24711, 'brace': 17180, 'corps': 3650, '##eta': 12928, 'emerson': 12628, '##kov': 7724, 'carpathian': 26349, 'informing': 21672, 'apologetic': 29352, 'michelangelo': 27701, 'dame': 8214, '[unused404]': 409, 'elliott': 9899, '275': 17528, 'heated': 9685, '[unused689]': 694, 'ք': 1239, 'andre': 7213, 'higgins': 13466, 'restarted': 25606, 'た': 1661, 'town': 2237, 'glue': 25238, '[unused187]': 192, 'creeping': 18266, 'crazed': 28343, '##ich': 7033, 'collecting': 9334, 'choi': 18151, 'boulders': 22177, 'donnelly': 28317, '##久': 30274, 'uprising': 10138, 'grossed': 17500, 'session': 5219, '##jad': 27875, 'heath': 9895, '##iom': 18994, 'employees': 5126, '##ulum': 25100, 'tear': 7697, 'accommodations': 26167, 'charging': 13003, '仮': 1761, '橋': 1885, 'ₕ': 1564, 'offerings': 14927, 'happening': 6230, 'malabar': 28785, '##ak': 4817, 'pharmacy': 13882, 'jordan': 5207, 'marijuana': 16204, 'convicts': 24948, 'cassie': 8869, 'camel': 19130, 'variously': 17611, 'attracting': 15411, '##ᄉ': 29997, 'sketches': 12741, 'designed': 2881, '[unused315]': 320, '##η': 24824, 'consuming': 15077, 'flotilla': 17150, 'payroll': 26854, 'nose': 4451, 'pale': 5122, 'temple': 3379, 'horace': 12757, 'relief': 4335, '##chus': 16194, 'ध': 1326, 'reminded': 6966, 'vegetables': 11546, 'processes': 6194, 'deviation': 24353, 'saxophone': 8283, '1675': 27194, '##ker': 5484, '[unused690]': 695, 'glacier': 10046, 'procession': 14385, 'б': 1181, 'poisonous': 22641, 'cmll': 25395, 'evolution': 6622, 'admired': 12749, '##drich': 23335, '##qing': 19784, '[unused489]': 494, '[unused405]': 410, 'diverted': 18356, 'm2': 25525, 'prominently': 14500, 'penalty': 6531, 'colt': 9110, 'training': 2731, 'aztec': 25245, '##կ': 29774, 'bubbling': 25054, 'labourers': 24043, '##lovic': 29356, 'com': 4012, '63': 6191, '##止': 30413, 'hunters': 9624, 'homicide': 18268, 'nouns': 19211, 'indoor': 7169, '[unused885]': 890, 'patch': 8983, 'solve': 9611, 'peasants': 13193, 'gem': 17070, 'border': 3675, '##rcle': 21769, '254': 22234, 'थ': 1324, '##ハ': 30244, 'halftime': 22589, 'liquidation': 28763, 'fingered': 28842, 'carmel': 19443, 'sigh': 6682, 'awful': 9643, 'nicknamed': 9919, '##け': 30180, 'quantitative': 20155, 'alexei': 21219, 'addict': 26855, 'mansion': 7330, 'yugoslav': 12292, '##ty': 3723, '[unused427]': 432, 'ineligible': 22023, 'fenton': 27096, '##ads': 19303, '950': 20317, '##heater': 27847, '[unused547]': 552, '##thesis': 25078, '##miento': 28883, '1730': 23272, 'wehrmacht': 23443, 'songwriting': 14029, 'incentive': 20438, 'consumption': 8381, 'intend': 13566, 'wiring': 27930, 'maps': 7341, 'associations': 8924, 'jeanne': 14537, 'keynes': 22152, '##yn': 6038, 'afc': 10511, '##fy': 12031, '∆': 1594, 'cade': 18615, 'orthodox': 6244, 'heavily': 4600, 'truce': 18415, 'banquet': 19032, 'homosexual': 15667, '##lce': 23314, 'ignition': 17446, 'chariot': 23507, 'स': 1338, 'adultery': 29169, 'coop': 21859, 'asked': 2356, 'subsidiaries': 20178, 'smelling': 19773, 'steered': 23424, 'protected': 5123, '##ield': 12891, 'mater': 16289, 'sanity': 20039, 'reckon': 29072, 'sikh': 17246, 'auto': 8285, '##elis': 29282, '354': 27878, '[unused59]': 60, '##奈': 30340, '[unused84]': 85, 'respond': 6869, 'scrolling': 28903, 'maintaining': 8498, 'foreigner': 29524, 'vp': 21210, '##arium': 17285, 'legislators': 22680, 'perfect': 3819, 'intriguing': 23824, 'storage': 5527, '軍': 1955, 'strictly': 9975, 'frequency': 6075, '##ali': 11475, 'casey': 9036, 'weapons': 4255, '##utus': 23998, 'corridors': 17506, '##ther': 12399, 'roy': 6060, '1844': 9730, '##sto': 16033, 'bordeaux': 16384, 'conducts': 17976, '##ひ': 30199, 'felicity': 27357, 'tokyo': 5522, 'kowalski': 28874, 'ℓ': 1577, '##neuve': 28104, '##has': 14949, 'slovak': 12747, 'severe': 5729, 'aspect': 7814, 'feat': 8658, 'lakshmi': 21352, '##rgo': 18581, 'upward': 10745, 'moderately': 17844, 'tomorrow': 4826, '##woman': 10169, 'itunes': 11943, 'singles': 3895, 'byrd': 17845, '##sko': 21590, 'overcoming': 27363, 'centimeters': 18119, 'playing': 2652, 'guantanamo': 23094, 'ek': 23969, 'isle': 8842, '122': 13092, 'advertisements': 14389, 'marketing': 5821, 'alpine': 10348, 'require': 5478, 'daily': 3679, 'kitchener': 27154, 'yi': 12316, 'steward': 17946, '[unused335]': 340, 'lambert': 12838, 'against': 2114, '1757': 22621, 'trades': 14279, 'gran': 12604, 'subsidies': 21762, '##ule': 9307, 'round': 2461, 'arresting': 28427, 'owner': 3954, 'variability': 28436, 'calls': 4455, 'sniffed': 18013, 'italian': 3059, 'flew': 5520, 'solitude': 22560, 'montpellier': 28153, 'aachen': 29093, 'fra': 25312, 'sofa': 10682, '##坂': 30329, 'schultz': 22378, 'darkly': 27148, 'countered': 17970, 'popular': 2759, 'paving': 28007, 'communication': 4807, 'seamen': 28671, 'quentin': 15969, 'editor': 3559, 'dresden': 12983, 'morally': 28980, 'ظ': 1287, '[unused626]': 631, 'medal': 3101, 'antony': 16262, 'dina': 26146, '##lithic': 28508, 'bonus': 6781, '##ly': 2135, 'sunglasses': 17072, 'johnston': 10773, 'backlash': 25748, 'macau': 16878, '##lane': 20644, '##imeters': 28136, 'rob': 6487, '1915': 4936, 'evidently': 15329, 'steven': 7112, 'blogger': 28205, '##mers': 16862, '##青': 30501, 'contains': 3397, 'approaches': 8107, 'lobbying': 19670, '##region': 23784, '95': 5345, 'bishop': 3387, 'faust': 24021, 'lagoon': 15825, 'greatly': 6551, 'cornice': 27848, 'reissue': 17173, 'creole': 21414, 'scrubbed': 27820, '##mus': 7606, 'gut': 9535, 'ლ': 1447, 'lena': 14229, 'interact': 11835, 'jarrett': 24503, 'promise': 4872, 'cited': 6563, 'ying': 20879, 'reception': 7684, '##æ': 29667, 'committing': 16873, 'astrid': 27376, 'allen': 5297, 'eton': 21687, 'preference': 12157, '##anal': 27953, 'confusing': 16801, 'twisted': 6389, 'aroused': 18391, 'overcome': 9462, 'puts': 8509, 'accepts': 13385, 'week': 2733, '##ler': 3917, 'misses': 22182, 'balthazar': 25021, 'eun': 26070, '[unused941]': 946, '##gler': 17420, 'erroneously': 29411, 'confess': 18766, 'shareholders': 15337, 'cai': 29080, 'sutra': 26567, 'exceptionally': 17077, 'gala': 16122, 'anatolia': 23747, 'throttle': 24420, '##neck': 18278, 'roosevelt': 8573, 'amused': 11770, 'nunez': 28454, 'his': 2010, 'congregations': 16850, 'shelton': 22230, 'ハ': 1718, 'antwerp': 14003, 'mcdonnell': 23149, 'biographies': 22056, 'instructions': 8128, 'ring': 3614, 'rhodes': 10588, '##lster': 29576, 'parker': 6262, '車': 1954, 'always': 2467, 'dharma': 20669, 'device': 5080, 'chinatown': 22321, 'watts': 11042, '##hey': 14844, '##ː': 23432, 'widely': 4235, '千': 1784, 'dirty': 6530, 'welcoming': 18066, '##bridge': 6374, 'wry': 24639, 'dissemination': 28170, 'instruction': 7899, 'malls': 25943, 'suburbs': 9435, 'trombone': 13914, '316': 23980, '##lay': 8485, '##de': 3207, 'addiction': 13449, 'salts': 23480, '1100': 22096, '[unused256]': 261, 'rating': 5790, '尚': 1830, 'considers': 10592, 'loco': 28046, 'dodd': 21258, 'euroleague': 26093, 'breathing': 5505, 'ය': 1404, 'give': 2507, 'ella': 11713, 'transgender': 16824, '##এ': 29887, 'estimates': 10035, 'wandered': 13289, 'sorrow': 14038, 'creeks': 24974, 'shrinking': 28375, '[unused438]': 443, 'romano': 22070, 'turnover': 20991, '##φ': 29736, 'viva': 20022, 'holidays': 11938, 'laboratory': 5911, 'argentina': 5619, 'accessories': 16611, '##edes': 18352, 'borders': 6645, 'trolley': 20820, 'inherent': 16112, 'secession': 22965, 'essays': 8927, 'present': 2556, '##stock': 14758, 'hearted': 18627, 'ares': 23631, 'halted': 12705, 'disposal': 13148, 'politically': 10317, 'mukherjee': 27040, 'specified': 9675, 'places': 3182, 'fences': 21549, '##pod': 27633, '##lth': 24658, 'nexus': 26041, 'flushed': 12953, 'inclusive': 18678, '家': 1825, 'itf': 27682, '##ج': 29819, '[unused179]': 184, 'ou': 15068, 'nightingale': 21771, 'portion': 4664, 'hooded': 21592, 'gasping': 17054, 'hawaii': 7359, '##br': 19892, '##acies': 20499, '##cock': 13959, 'outreach': 15641, 'margot': 29210, 'bets': 29475, 'offs': 12446, 'cocaine': 16034, '##inia': 23309, 'channels': 6833, 'jerome': 11120, 'furious': 9943, 'hopkins': 10239, '##zon': 11597, 'kn': 14161, 'พ': 1414, '##ysis': 20960, 'porch': 7424, '##gall': 22263, '##sted': 14701, 'museums': 9941, 'razor': 15082, 'leaks': 29324, '##ndon': 19333, 'beds': 9705, 'thoroughly': 12246, '##oza': 25036, 'retreated': 11672, 'enrollment': 10316, '耳': 1937, 'accusations': 13519, 'ᵒ': 1503, 'executive': 3237, '##amine': 19915, 'smoothed': 17966, '##lson': 14881, 'bewildered': 24683, '##ez': 9351, 'clapped': 18310, 'threats': 8767, '[unused289]': 294, 'rico': 7043, '##bery': 22509, '##ن': 15915, 'dwarf': 11229, '☆': 1621, 'enter': 4607, 'rowland': 20539, 'bubble': 11957, '##ull': 18083, '15': 2321, '##tiv': 29068, 'designate': 24414, 'span': 8487, 'wooded': 17172, 'parkway': 12602, '##ythe': 26688, '##有': 30399, 'outset': 26674, 'cyclist': 14199, '##cca': 16665, 'elites': 27021, 'detectors': 25971, 'herzegovina': 11453, 'bey': 20289, '##mation': 28649, '##ᵤ': 30044, 'messed': 18358, '##34': 22022, 'evolving': 20607, 'juventus': 22760, 'film': 2143, 'louise': 8227, 'workforce': 14877, 'cough': 19340, '##gang': 24930, 'muslim': 5152, '1701': 26059, 'cosmopolitan': 24686, 'disorders': 10840, '##mara': 28225, 'assessing': 20077, 'muttered': 6250, 'looked': 2246, 'russians': 12513, 'downed': 20164, '##dication': 25027, 'taxa': 23726, 'sirius': 23466, '##von': 17789, 'trailed': 11145, 'damascus': 16094, 'audiences': 9501, 'lucknow': 23571, '##ʋ': 29699, 'those': 2216, '##mering': 27851, 'baskets': 25946, 'most': 2087, 'braun': 21909, 'reach': 3362, 'eve': 6574, 'contest': 5049, 'offset': 16396, 'harry': 4302, 'mystic': 17477, 'transforming': 17903, 'clash': 13249, 'domesday': 22310, 'biodiversity': 17194, 'fragment': 15778, 'hallway': 6797, 'landowners': 20152, 'projective': 27473, 'nordic': 13649, '##ended': 21945, 'personality': 6180, 'delayed': 8394, 'pinyin': 9973, 'guaranteed': 12361, '##kara': 16566, '##uming': 24270, 'specialising': 23315, '##etched': 29574, 'concerned': 4986, '##runa': 26605, 'lange': 21395, 'mandolin': 17687, 'curate': 27530, 'oricon': 22237, 'correctional': 20873, 'annabelle': 24722, '1899': 6166, '[unused439]': 444, 'amalgamated': 17143, 'bathing': 17573, 'bumper': 21519, 'nonetheless': 9690, 'preferring': 21393, '##>': 29631, 'envoy': 19918, 'fairy': 8867, 'hawker': 23937, 'willard': 19138, 'millie': 19561, '[unused10]': 11, 'attacks': 4491, 'invasions': 23536, '[unused78]': 79, '##lb': 20850, '##寺': 30353, 'cadillac': 20425, 'sanders': 12055, 'taluka': 29512, 'floated': 13715, 'finally': 2633, 'suburb': 7575, 'r': 1054, 'mom': 3566, 'presumed': 14609, 'ardent': 25314, 'archaic': 19261, 'lois': 15815, 'senegal': 16028, 'castes': 26254, 'cole': 5624, 'traction': 16493, '174': 19492, 'singh': 5960, 'whoever': 9444, 'arkansas': 6751, 'approved': 4844, 'zoological': 26168, '##ude': 12672, 'modular': 19160, 'avoidance': 24685, 'arrival': 5508, 'spartan': 20670, '版': 1907, '##icient': 20132, 'pupils': 7391, 'students': 2493, '##fight': 20450, 'parcels': 28998, '##hus': 9825, 'synchronized': 25549, 'amplitude': 22261, 'kenton': 25446, 'guthrie': 19952, 'held': 2218, 'amassed': 22151, 'traps': 16735, 'tourist': 7538, 'havilland': 23994, 'comprehend': 22346, 'wb': 25610, '##boats': 19468, 'cerambycidae': 13168, '108': 10715, '##ecin': 28191, '[unused579]': 584, 'orioles': 18893, 'mcintosh': 29052, '##pate': 17585, 'buffalo': 6901, 'rhyme': 20622, 'mr': 2720, '[unused75]': 76, 'avila': 29079, '伊': 1762, 'evidence': 3350, 'rainbow': 10098, '##filtration': 28674, 'sectional': 27197, '##rice': 17599, '##odies': 27391, 'entertainment': 4024, 'emmett': 25685, 'purse': 8722, 'ucla': 12389, 'mor': 22822, 'uta': 28981, '89': 6486, 'broadband': 19595, 'stalled': 20659, 'merchandise': 16359, 'gothenburg': 22836, 'learners': 26262, 'ascent': 16354, '##⟨': 30155, '##lum': 12942, 'macleod': 23075, 'abs': 14689, 'awarding': 21467, 'wit': 15966, '##yler': 20853, 'housemates': 28152, '##―': 30053, '##kind': 18824, 'ர': 1391, 'remarks': 12629, '111': 11118, '##schen': 23796, 'understands': 19821, '##tute': 24518, 'pulling': 4815, '##ת': 29813, '##9': 2683, 'montenegrin': 24099, 'they': 2027, '##lani': 21141, 'hana': 26048, 'cheng': 15898, 'add': 5587, 'accelerating': 29494, 'flex': 23951, 'ᵘ': 1506, 'phrase': 7655, 'aramaic': 28934, '1646': 28783, 'pastures': 24813, 'penitentiary': 29569, '##ories': 18909, 'hurled': 24025, '##fur': 27942, '##for': 29278, 'psychedelic': 18147, 'specializes': 16997, '##kos': 15710, 'broad': 5041, 'rangers': 7181, '##cia': 7405, 'frustration': 9135, 'sipping': 24747, '』': 1644, 'las': 5869, 'kinase': 21903, '1743': 25615, 'tinged': 22683, 'epoch': 25492, 'patrons': 13497, '[unused831]': 836, '##ᄅ': 29994, 'ᄋ': 1463, 'basel': 14040, 'rated': 6758, 'caused': 3303, 'ruins': 8435, '##uer': 13094, 'dominique': 18165, 'bang': 9748, '1846': 9244, 'catcher': 13795, 'hates': 16424, 'fast': 3435, 'mysteries': 15572, 'comparative': 12596, 'numbers': 3616, 'theatrical': 8900, 'h₂o': 24833, '[unused869]': 874, 'highlighted': 11548, '##iting': 15402, 'suspended': 6731, '##間': 30495, 'elementary': 4732, 'mean': 2812, 'heirs': 15891, 'ш': 1203, '##ª': 29653, 'just': 2074, 'tall': 4206, 'birds': 5055, 'deaths': 6677, 'knowledge': 3716, 'georgetown': 12982, '##tures': 22662, 'slaves': 7179, 'unknown': 4242, '##borough': 10235, 'ecuador': 10378, 'methodist': 8938, 'horton': 18469, 'convent': 10664, 'ballet': 7250, 'warn': 11582, 'hydroelectric': 18541, 'hack': 20578, 'forster': 21316, 'sarawak': 21546, 'prime': 3539, 'struggled': 6915, '##ique': 7413, 'mentioned': 3855, '[unused781]': 786, 'hotel': 3309, '##ך': 29797, 'elegance': 27745, '130': 7558, 'applying': 11243, 'materialized': 27075, '##方': 30389, 'blades': 10491, '##pressed': 19811, 'commencement': 20561, 'laying': 10201, 'santiago': 8728, 'flanks': 23547, 'gutierrez': 20836, 'unprecedented': 15741, 'scotland': 3885, 'graham': 5846, 'preaching': 17979, 'grooves': 25880, 'stabilized': 27697, 'wolverine': 22162, 'delilah': 23006, 'heck': 17752, 'bentley': 15988, 'timothy': 10805, 'reeling': 28515, 'designated': 4351, 'intimate': 10305, 'refurbished': 18662, '##ishly': 19983, '##54': 27009, 'nj': 19193, '##ification': 9031, '##ating': 5844, 'blood': 2668, '##nology': 21020, '[unused82]': 83, 'hating': 22650, 'occurred': 4158, 'amazing': 6429, 'assumption': 11213, 'wreck': 12006, 'satisfying': 17087, 'polynomial': 17505, 'bout': 10094, 'attacker': 17346, 'syllable': 16353, '龸': 1983, 'attached': 4987, 'pmid': 20117, 'atmospheric': 12483, 'weighing': 15243, 'rhodesia': 20340, '##ume': 17897, '[unused675]': 680, 'chef': 10026, 'negative': 4997, 'gp': 14246, 'shores': 13312, 'conscience': 13454, '##ura': 4648, 'chavez': 16860, '[unused923]': 928, 'outlaw': 19104, 'marquette': 24223, 'classed': 27811, '[unused809]': 814, '##田': 30437, 'ろ': 1689, 'canton': 8770, 'hilt': 21184, 'groaning': 24781, '##low': 8261, 'dissertation': 14481, 'effectively': 6464, 'announcer': 14073, 'menace': 19854, 'producers': 6443, '[unused460]': 465, 'path': 4130, '##sin': 11493, 'canon': 9330, 'captured': 4110, 'distinguished': 5182, 'buffy': 18467, 'jc': 29175, '[unused931]': 936, 'empress': 10248, 'seoul': 10884, 'achieve': 6162, 'scooped': 20804, 'stevie': 17458, 'commentaries': 21241, 'booker': 16674, '[unused79]': 80, 'evaluating': 23208, '##llah': 18599, '##brates': 25258, 'anna': 4698, '##rned': 21119, '[unused948]': 953, 'brakes': 13627, 'elemental': 19529, 'desk': 4624, 'proportional': 14267, '文': 1861, 'created': 2580, '##ock': 7432, '##ero': 10624, 'commended': 22429, '1990': 2901, 'ィ': 1694, 'jakob': 19108, '##56': 26976, 'errors': 10697, 'cindy': 15837, 'noses': 27518, '##ang': 5654, 'regularly': 5570, '♯': 1628, '##active': 19620, '##user': 20330, 'mistake': 6707, 'bhp': 22245, '##dding': 27027, 'jelly': 20919, 'wells': 7051, 'odessa': 19693, '[unused490]': 495, 'reprinted': 13603, 'matilda': 17981, '##tries': 21011, 'intellectual': 7789, 'lethal': 12765, 'needs': 3791, 'universite': 22176, 'macarthur': 17719, 'bodily': 20445, 'fen': 21713, 'toxin': 29090, 'sailed': 7434, 'peeking': 28222, 'nancy': 7912, 'alignment': 12139, 'promenade': 26815, 'mohan': 18529, 'swings': 18755, 'simpler': 16325, 'lao': 18805, 'argue': 7475, 'austro': 16951, 'crabs': 26076, 'importantly': 14780, 'fountains': 23497, 'droplets': 27126, 'pitcher': 8070, 'newest': 14751, '##ject': 20614, 'complementary': 21053, 'dvd': 4966, 'jennifer': 7673, 'segments': 9214, '##sic': 19570, 'bright': 4408, 'identification': 8720, 'cornelius': 17354, 'prohibiting': 26325, '##ᴮ': 30027, 'activists': 10134, '##ッ': 30237, 'poetic': 13805, 'treatise': 15326, '##zing': 6774, 'hatred': 11150, 'mobilization': 25580, 'strong': 2844, 'costing': 22173, 'marine': 3884, 'tomb': 8136, 'call': 2655, 'articles': 4790, 'temper': 12178, 'battalion': 4123, '##rich': 13149, '##tius': 22638, 'に': 1668, 'jamaica': 9156, 'obviously': 5525, 'colony': 5701, 'maguire': 26196, 'defines': 11859, 'riddle': 21834, '##uk': 6968, 'favoured': 16822, 'siblings': 9504, 'rudder': 24049, 'fda': 17473, 'lp': 6948, 'giovanni': 9136, 'islam': 7025, 'coherent': 18920, 'christ': 4828, '##ᄐ': 30003, 'harrison': 6676, 'emerald': 14110, 'below': 2917, '1644': 23477, '##★': 30147, '##hore': 16892, 'india': 2634, 'glendale': 27649, '##こ': 30181, 'relies': 16803, 'bay': 3016, 'il': 6335, 'zurich': 10204, 'nippon': 19364, 'derek': 7256, 'avant': 14815, 'diet': 8738, 'hilda': 21589, 'tunisian': 22946, '[unused611]': 616, 'hiss': 19074, 'gonzales': 24334, 'operation': 3169, '也': 1750, 'amusing': 19142, '##uru': 14129, 'changing': 5278, 'carried': 3344, 'athletes': 7576, 'cougars': 26317, '##lity': 18605, 'homeless': 11573, '[unused413]': 418, 'lower': 2896, 'charter': 6111, '##llis': 21711, 'shaved': 20665, 'garner': 18661, 'olaf': 20514, 'practical': 6742, 'comet': 15699, 'preservation': 8347, 'saigon': 24001, 'wall': 2813, 'flinders': 29120, 'opaque': 28670, 'level': 2504, 'wards': 11682, 'credit': 4923, 'northamptonshire': 21367, 'colonies': 8355, 'mg': 11460, '##nco': 15305, 'columns': 7753, 'stopping': 7458, '##ulton': 22145, 'similar': 2714, 'buddies': 24115, 'backup': 10200, 'compose': 17202, 'destined': 16036, 'rotting': 22005, '247': 23380, '##optera': 25324, 'twenty': 3174, 'applied': 4162, 'amusement': 9778, 'roll': 4897, 'evolve': 19852, 'rapidly': 5901, '##isance': 28138, 'lyons': 16642, 'outside': 2648, '[unused136]': 141, 'spaceship': 25516, '##生': 30436, '##ß': 19310, '1980': 3150, 'ul': 17359, 'noon': 11501, 'unsuitable': 25622, 'sorbonne': 28452, 'spheres': 19885, 'elder': 6422, '##aga': 16098, 'russo': 17023, 'suez': 21974, 'chiefly': 15897, 'fiery': 15443, 'conical': 24750, 'choking': 18329, 'connecticut': 6117, 'probability': 9723, 'julian': 6426, '##ল': 29909, 'loops': 15932, '##bold': 27495, 'graduates': 10845, 'bangor': 21895, 'gould': 14913, 'orton': 25161, 'punch': 8595, 'justine': 26377, '##lars': 28695, 'shan': 17137, 'playhouse': 17408, 'cheung': 22632, '##rrard': 29599, '##‰': 30065, 'evolutionary': 12761, 'breton': 16659, 'etched': 20286, '##ship': 9650, 'feminine': 12320, 'ultimately': 4821, 'henderson': 9481, 'antonia': 24272, 'wastewater': 28269, 'glance': 6054, 'addressed': 8280, 'pollution': 10796, 'stella': 11894, 'syed': 19740, '##cable': 21170, 'aisles': 25442, 'closes': 14572, 'handel': 21465, 'whoa': 23281, '##enary': 24553, '##ask': 19895, 'ingredient': 21774, 'yearning': 29479, 'landslide': 20148, 'facebook': 9130, 'qualification': 8263, 'progresses': 22901, '##ptive': 24971, 'ones': 3924, '##dhar': 25632, 'os': 9808, 'bennett': 8076, '##ened': 6675, 'ashton': 13772, 'ס': 1258, '##utable': 23056, 'copies': 4809, 'capacity': 3977, '##reus': 23446, 'money': 2769, 'prototypes': 19599, '##rase': 23797, 'halls': 9873, '##cis': 22987, 'fourteenth': 15276, '##stead': 25647, '##²': 10701, '##edo': 26010, 'springsteen': 26002, 'register': 4236, 'atlas': 11568, '##kti': 22462, '[unused674]': 679, '1621': 27037, 'schleswig': 21173, '##wyl': 27740, '##nick': 13542, 'lurched': 21977, 'ike': 25209, '##漢': 30430, '##─': 30142, 'vacant': 10030, 'carver': 20163, 'pad': 11687, '##oteric': 29112, '##tao': 28555, 'car': 2482, '##ka': 2912, 'preserves': 18536, 'password': 20786, 'foreigners': 15040, 'zip': 14101, 'alias': 14593, 'obligations': 14422, 'farms': 8623, '[unused195]': 200, 'baptised': 28798, 'prohibit': 23469, '##よ': 30210, 'saved': 5552, 'liz': 9056, 'designation': 8259, 'sheikh': 12840, 'substituted': 17316, 'lit': 5507, '283': 25504, 'surroundings': 11301, 'atari': 18978, '##water': 5880, 'figurative': 29407, 'label': 3830, 'cringed': 23952, 'danielle': 18490, 'escalated': 26814, 'advocates': 13010, 'key': 3145, 'daddy': 8600, 'godfrey': 18238, 'lizards': 23898, 'joseph': 3312, 'braking': 24427, 'hyun': 21108, 'promo': 19430, 'immune': 11311, 'added': 2794, '##co': 3597, 'tamil': 6008, 'leicester': 11258, 'threads': 16457, '##cene': 17968, '##gal': 9692, 'bushes': 14568, 'women': 2308, 'relocated': 7448, 'telescopes': 28026, 'ة': 1272, 'adjusted': 10426, 'brooke': 11535, '[unused886]': 891, 'into': 2046, 'stanley': 6156, 'cv': 26226, 'drag': 8011, '##chs': 18069, '##ught': 18533, '##eville': 21187, 'theological': 9208, 'exports': 14338, '71': 6390, 'linden': 22066, '##cott': 13124, '[unused600]': 605, 'points': 2685, 'gleam': 24693, 'worrying': 15366, 'ᄐ': 1467, '[unused409]': 414, 'factor': 5387, 'nazis': 13157, 'し': 1657, 'declared': 4161, '##inates': 28184, 'deux': 24756, 'modulation': 25502, 'allies': 6956, 'breakdown': 12554, '##anga': 18222, 'influential': 6383, 'rumbled': 22257, 'tally': 19552, '##yama': 11613, 'sweetness': 23210, 'intersections': 26540, '##4th': 26724, 'decades': 5109, 'bryson': 26845, 'found': 2179, 'quit': 8046, 'shack': 22200, 'incredibly': 11757, '##run': 15532, 'libraries': 8860, 'glowed': 16497, '方': 1863, 'monthly': 7058, 'tweed': 26922, 'jones': 3557, 'nmi': 22484, 'dutton': 28784, '##lington': 18722, 'directions': 7826, 'ohio': 4058, 'physicians': 11572, 'pollen': 22482, 'frustrating': 25198, 'haven': 4033, '彳': 1845, 'publications': 5523, 'reacted': 14831, 'ᅩ': 1475, 'ocean': 4153, '##ave': 10696, 'implements': 22164, 'canals': 17263, '##iba': 18410, 'anaheim': 20835, 'remy': 22712, '344': 29386, '##平': 30365, 'mariana': 22097, 'derived': 5173, 'orient': 16865, '[unused940]': 945, '##inging': 23180, '##around': 24490, 'weary': 16040, 'nuevo': 22250, 'visionary': 28036, 'willy': 16172, '##ᆯ': 30022, 'magma': 28933, 'exeter': 12869, '##americana': 27026, '##werk': 29548, 'balloon': 13212, 'savannah': 10891, 'rope': 8164, 'physical': 3558, '##lun': 26896, 'æ': 1097, 'willis': 12688, '##tead': 14565, 'mage': 17454, '##भ': 29866, '##qual': 26426, 'hidden': 5023, 'wetlands': 16630, 'stumble': 21811, '##伊': 30288, 'น': 1413, '##ders': 13375, 'absorbing': 20998, 'finely': 22126, 'realistic': 12689, '##lux': 25148, 'aggregate': 9572, '##fall': 13976, 'serialized': 27289, 'テ': 1713, 'shoulder': 3244, 'hostilities': 17601, 'centrally': 25497, 'outs': 21100, 'fein': 27132, 'reinstated': 18671, 'historia': 19891, 'naughty': 20355, 'corrupted': 27279, 'alternate': 6585, 'murders': 9916, '##ional': 19301, 'trident': 26515, 'framing': 20241, 'rum': 19379, 'climatic': 27301, 'care': 2729, '[unused225]': 230, '##⁸': 30077, '##race': 22903, 'dunedin': 22020, 'curve': 7774, 'publisher': 6674, '##ح': 29820, 'palo': 24326, 'rediscovered': 26733, 'stuffing': 28652, '[unused281]': 286, 'va': 12436, '##氵': 30420, 'comb': 22863, 'ringo': 25589, '##cting': 11873, 'haunting': 20161, 'magdalena': 23984, 'intern': 25204, '[unused345]': 350, '113': 12104, 'duchess': 11017, '[unused129]': 134, 'retains': 14567, '##arat': 25879, 'variations': 8358, '##most': 11800, '##加': 30305, 'concourse': 28571, '##ze': 4371, '##cha': 7507, 'erie': 13374, '##〉': 30164, 'caitlin': 27555, 'strode': 11885, 'maximus': 21692, 'nearly': 3053, 'disappeared': 5419, 'royalties': 25335, 'jett': 22962, 'sneakers': 28130, 'conditioned': 22442, 'floors': 8158, '##gated': 11644, 'linguistic': 12158, 'amino': 13096, '##lham': 25968, '##ʿ': 29714, 'coventry': 13613, 'ups': 11139, 'daly': 18509, 'bullying': 18917, 'bree': 21986, '##bana': 19445, '##lling': 13112, 'obliged': 14723, 'tara': 10225, 'caesar': 11604, '##color': 18717, 'debate': 5981, 'wallet': 15882, '##iji': 27821, 'hassan': 13222, 'dispatch': 18365, 'reductions': 25006, '##inus': 13429, 'cooking': 8434, '##claiming': 27640, '[unused586]': 591, 'hatfield': 26853, '##pers': 7347, 'bulletin': 13146, 'billion': 4551, 'integrated': 6377, '##ol': 4747, 'gasp': 12008, '##dance': 25514, 'crossed': 4625, 'amounts': 8310, 'archibald': 15917, '##ivate': 21466, 'creates': 9005, '[unused397]': 402, 'opined': 26084, 'approximate': 15796, 'enslaved': 22216, 'that': 2008, 'longitudinal': 20134, 'ւ': 1238, 'edmond': 21773, 'dali': 29095, 'sorting': 22210, 'explanation': 7526, 'sylvester': 20016, '～': 1995, '##tray': 28473, 'crops': 8765, 'lightweight': 12038, 'sector': 4753, '##yst': 27268, 'til': 18681, 'write': 4339, '##athic': 20972, 'widened': 8723, '1664': 28485, '1764': 21488, 'handling': 8304, 'towed': 18948, 'northbound': 20055, 'coded': 22402, '##rage': 24449, 'injected': 19737, 'aux': 19554, 'semantics': 28081, 'improvisation': 24584, 'informant': 28694, '44': 4008, '[unused110]': 115, '⊂': 1610, '[unused838]': 843, 'remark': 17674, 'monetary': 12194, '##meo': 26247, 'caps': 9700, 'parameter': 16381, 'borg': 28709, '##美': 30461, '##lk': 13687, 'founded': 2631, 'yates': 20356, '[unused482]': 487, 'sanford': 21153, 'duluth': 28218, 'multiplayer': 17762, '##writing': 18560, '[unused162]': 167, '##ன': 29923, '##late': 13806, '##ados': 28118, '[unused389]': 394, 'katharine': 25739, '##ach': 6776, 'suppress': 16081, 'peabody': 20004, 'tribe': 5917, 'saxons': 28267, 'associates': 9228, 'flood': 7186, '211': 19235, 'righteous': 19556, '##nail': 25464, 'involved': 2920, 'vivo': 24269, '，': 1989, 'tasked': 13487, '##lines': 12735, 'reactions': 9597, 'regents': 22832, 'tanya': 19956, '##／': 30518, '##ln': 19666, 'stint': 12116, 'circulated': 17640, 'microphone': 15545, 'scandal': 9446, '##tooth': 19392, '##space': 23058, 'ₓ': 1563, '##ა': 29974, 'baths': 19692, 'boy': 2879, '##eran': 23169, 'freiburg': 22871, 'ನ': 1400, 'underlying': 10318, 'reliefs': 27670, 'pyramid': 11918, 'existing': 4493, 'icy': 13580, '##corp': 24586, 'gifted': 12785, 'plunging': 29059, 'proceeding': 18207, 'outlaws': 23107, 'claremont': 23716, 'chen': 8802, 'scranton': 26123, '[unused190]': 195, 'posterior': 15219, '##ย': 29952, '##tton': 15474, 'stan': 9761, '##lch': 29358, 'returns': 5651, '##roved': 26251, '[MASK]': 103, 'gregg': 18281, 'villain': 12700, 'adjustments': 24081, 'aeronautical': 25010, 'classic': 4438, 'costly': 17047, '##dia': 9032, 'du': 4241, 'territorial': 7894, 'construction': 2810, 'scarlett': 20862, 'correctly': 11178, 'iraq': 5712, '##apple': 23804, '##adi': 17190, 'seizure': 18634, 'darcy': 17685, '##ece': 26005, 'eyed': 7168, 'exists': 6526, 'europeans': 13481, 'buckingham': 17836, 'leads': 5260, '##aan': 14634, 'replicate': 28024, '##元': 30295, 'doug': 8788, '##ᆫ': 30021, 'metropolitan': 4956, 'unpublished': 19106, 'clark': 5215, '##wald': 11191, 'activist': 7423, 'automation': 19309, 'dimensional': 8789, 'antenna': 13438, 'samoa': 15075, 'tripped': 21129, '##dict': 29201, 'gallagher': 17297, 'charles': 2798, 'attitudes': 13818, '##ste': 13473, 'ecological': 12231, 'rodeo': 18936, '##rrick': 24999, 'biplane': 28686, 'nile': 15179, 'kellan': 19697, 'england': 2563, 'ely': 20779, '352': 28906, 'computer': 3274, 'nw': 22064, '##kato': 26020, '##alis': 13911, '[unused98]': 99, '##女': 30341, 'luminous': 25567, 'engulfed': 24692, 'passes': 5235, '¹⁄₂': 18728, '##cb': 27421, 'cleavage': 28691, 'giorgio': 17697, '##ан': 28995, 'apollo': 9348, 'whereby': 13557, 'diver': 17856, 'viral': 13434, 'meat': 6240, 'backyard': 16125, 'prasad': 17476, 'kelsey': 21004, '##gpur': 25758, 'excluding': 13343, '##ieving': 25587, 'rabbis': 25602, 'obsidian': 29042, 'doorway': 7086, 'called': 2170, 'rosen': 21701, 'imperial': 4461, '##tow': 18790, 'patna': 29565, 'cylinders': 18729, 'commissions': 13239, 'precinct': 18761, 'karen': 8129, '##lm': 13728, '##storm': 19718, '##bron': 21337, 'fragrance': 24980, 'humour': 17211, 'dragging': 11920, 'presses': 14616, 'renal': 25125, '##rona': 26788, 'hallmark': 25812, '1919': 4529, 'implied': 13339, '##anor': 27869, 'discovers': 9418, 'syracuse': 11736, 'sponsors': 13162, 'chestnut': 15655, 'engineered': 13685, 'prakash': 22233, 'meade': 26501, 'escort': 8620, 'mph': 5601, 'sabres': 26969, 'sesame': 23605, 'king': 2332, 'publicity': 11845, 'amended': 13266, 'ス': 1707, 'sleeps': 25126, '##hesis': 24124, 'graz': 26918, 'fischer': 13042, 'effort': 3947, 'cognitive': 10699, 'spurred': 22464, 'stayed': 4370, 'genealogy': 26684, 'mcmillan': 27436, 'hugely': 27564, '1930': 4479, 'returned': 2513, 'marching': 10998, 'quarterfinal': 29380, 'vis': 25292, 'protests': 8090, 'rats': 11432, 'fucked': 21746, 'φ': 1176, 'instrumentation': 16015, 'celestial': 17617, 'modifications': 12719, '##mouth': 14359, 'alternately': 23554, 'tissues': 14095, 'guerra': 24698, 'screenwriter': 11167, 'surged': 18852, 'cardboard': 19747, '沢': 1898, 'weighs': 21094, 'flanking': 24958, 'ص': 1284, '##′': 30066, 'piece': 3538, 'disneyland': 25104, 'politician': 3761, 'seizures': 25750, 'responses': 10960, 'directory': 14176, '[unused395]': 400, 'dayton': 14700, 'essex': 8862, 'horned': 26808, '##nis': 8977, 'adequate': 11706, 'oswald': 17411, 'leipzig': 11222, '##rem': 28578, 'siegfried': 25948, 'alfonso': 13591, 'breakout': 25129, 'stables': 16232, 'spitfire': 28323, 'influx': 18050, 'barge': 19398, 'layton': 23103, '##12': 12521, 'affecting': 12473, 'plaid': 26488, '21': 2538, 'winery': 23910, 'exploded': 9913, 'irritating': 29348, 'kaplan': 22990, 'rafe': 15819, 'paragraph': 20423, 'embrace': 9979, '##mide': 24284, 'fist': 7345, 'woodrow': 23954, 'ballast': 28030, 'つ': 1664, 'jin': 9743, 'confrontation': 13111, 'coin': 9226, 'opera': 3850, '##ient': 11638, 'ensuing': 13831, '##mind': 23356, '##nh': 25311, '##dberg': 25190, 'squat': 26783, '##iam': 25107, '##chio': 23584, 'moonlight': 11986, 'boogie': 21495, 'star': 2732, 'scores': 7644, 'upwards': 14873, 'spiders': 14160, 'psalms': 26130, 'nat': 14085, 'residue': 21755, 'tanker': 20135, 'ghost': 5745, 'overly': 15241, '##cend': 23865, 'waistband': 27553, 'bancroft': 29351, '[unused322]': 327, '##59': 28154, '##athing': 22314, 'disbanded': 8532, 'nostrils': 15325, 'clearer': 24509, 'dome': 8514, 'bane': 25163, '[unused197]': 202, 'largely': 4321, 'limit': 5787, 'ᄌ': 1464, 'rocky': 6857, 'gonzalez': 10121, 'prostate': 25086, '##里': 30488, 'も': 1681, '##rone': 20793, '##df': 20952, 'sixties': 22651, 'warwickshire': 18430, 'authorship': 26324, 'tombstone': 26671, 'news': 2739, 'spell': 6297, '##shire': 7430, 'mendez': 28950, 'ranged': 15844, 'sadly': 13718, 'clubhouse': 22067, 'antilles': 27695, 'twist': 9792, '##rov': 12298, 'recorder': 14520, 'furniture': 7390, 'bring': 3288, 'fur': 6519, 'abusive': 20676, 'shopping': 6023, '20th': 3983, 'champ': 24782, 'shillings': 29332, 'listeners': 13810, 'utilities': 16548, 'dinosaur': 15799, '1740': 21757, 'crate': 27297, '1810': 11786, 'exclusion': 15945, 'ontario': 4561, 'generate': 9699, '##thermal': 23367, 'mirror': 5259, 'descent': 6934, '17': 2459, 'pi': 14255, 'selecting': 17739, 'safari': 23591, 'browne': 15005, 'extras': 26279, '##rup': 21531, 'peg': 25039, 'au': 8740, 'veins': 9607, 'sas': 21871, 'breaker': 24733, '1785': 17262, '##sity': 17759, 'fight': 2954, 'lisbon': 11929, '[unused583]': 588, 'nl': 17953, 'microsoft': 7513, 'yamaha': 24031, 'graveyard': 16685, 'gonna': 6069, 'muscled': 24909, '##µ': 29659, 'appropriately': 23263, 'prescribed': 16250, '##ructured': 26134, '##kiewicz': 28563, 'sentences': 11746, 'devastation': 25594, 'employers': 12433, 'multimedia': 14959, 'm³': 14241, '##aba': 19736, 'uppsala': 25720, 'kidd': 25358, '佐': 1764, 'brilliant': 8235, 'tna': 20108, 'congo': 9030, 'prosecutors': 19608, 'sterling': 10933, 'handicap': 15822, '##cake': 17955, 'ee': 25212, 'audition': 14597, 'alexandria': 10297, '269': 25717, '262': 21950, 'candi': 27467, 'smiles': 8451, 'sat': 2938, 'restraint': 19355, 'subsided': 26588, 'bold': 7782, '##ت': 29817, 'chip': 9090, 'implementation': 7375, '385': 24429, 'warrington': 23051, 'guerre': 24613, 'finish': 3926, 'swallows': 26436, '##mund': 25574, 'cost': 3465, 'midwest': 13608, '##ilia': 27861, 'sanctions': 17147, 'garbage': 13044, 'prevalence': 20272, 'silvio': 28107, 'frightening': 17115, 'գ': 1221, 'rotated': 20931, 'sniff': 27907, 'invasion': 5274, 'ashe': 13402, 'instance': 6013, 'honest': 7481, 'declaring': 13752, 'planted': 8461, 'innovative': 9525, '##ids': 9821, 'postseason': 17525, 'micah': 17665, 'crimean': 20516, 'hai': 15030, '##folia': 21710, 'understood': 5319, 'referee': 5330, '##kar': 6673, '##rer': 14544, 'dip': 16510, 'woodstock': 21028, 'standalone': 26609, '[unused852]': 857, 'b1': 29491, 'michigan': 4174, 'inverness': 22937, '##lei': 23057, 'slapped': 11159, '422': 29269, '[unused274]': 279, 'digital': 3617, '##rocity': 21735, 'aviator': 24035, 'lurking': 24261, 'cords': 24551, 'stiff': 10551, 'superb': 21688, 'postage': 22981, '##fs': 10343, 'derivatives': 16942, 'а': 1180, '##mt': 20492, '##urer': 27595, '##街': 30472, '##都': 30487, 'miriam': 16925, '疒': 1913, 'trapped': 7567, 'transplant': 22291, 'tyler': 7482, '##hoe': 14490, 'whitish': 16800, 'remainder': 6893, 'victoria': 3848, 'occupied': 4548, 'arsenal': 9433, 'paused': 5864, 'zeus': 15208, 'oman': 16640, 'daryl': 22514, 'tingling': 23690, 'subdistrict': 24150, 'fused': 19660, 'servants': 8858, 'humanities': 11406, '‿': 1534, 'pathology': 19314, 'chuckled': 10252, 'de': 2139, 'mountain': 3137, 'samson': 18375, 'flees': 24776, 'put': 2404, 'navy': 3212, '英': 1941, 'painting': 4169, '##tte': 4674, 'kimberly': 23729, '##gation': 12540, '##sfield': 15951, 'operate': 5452, '##oted': 27428, 'hispanic': 6696, 'thesis': 9459, '##←': 30111, 'sincerity': 23997, 'sobbed': 25960, 'chevy': 29009, '##bbly': 24200, '##内': 30299, 'grow': 4982, 'baroness': 21479, 'flooded': 10361, 'translates': 16315, '##fare': 17883, 'mn': 24098, '##tula': 28970, 'juarez': 25398, 'repetition': 23318, '31': 2861, '1939': 3912, '##net': 7159, '##lates': 26786, 'emirates': 14041, 'commissioning': 21612, '½': 1092, 'plank': 24000, '##200': 28332, 'realize': 5382, 'decent': 11519, '##let': 7485, 'tonight': 3892, 'mutual': 8203, '##ug': 15916, 'ping': 17852, 'consular': 27326, '##uman': 19042, 'vulnerability': 18130, '[unused560]': 565, 'cash': 5356, '##isha': 24032, 'descendant': 12608, 'leftist': 24247, 'considerations': 16852, '##ø': 16415, 'tuned': 15757, 'detecting': 25952, 'mueller': 26774, 'roger': 5074, 'citizens': 4480, '##idal': 16975, '##oshi': 24303, 'randy': 9744, 'recommendation': 12832, 'montane': 21704, '##kan': 9126, '##eger': 26320, 'cadiz': 26342, '##陽': 30499, '##abia': 29039, 'grassland': 20331, 'baylor': 23950, 'heads': 4641, 'delle': 24121, 'candidate': 4018, 'headlining': 26533, 'control': 2491, 'disadvantaged': 27322, 'chefs': 27828, 'corn': 9781, '##باد': 25799, '1955': 3982, 'show': 2265, 'clashed': 22600, 'levant': 24485, '##no': 3630, 'piedmont': 18873, 'disgusting': 19424, 'maintained': 5224, 'rc': 22110, '##ele': 12260, '##nto': 13663, '##rgy': 22637, 'input': 7953, 'franco': 9341, '1744': 25846, '將': 1828, '##person': 27576, 'ks': 29535, 'pike': 12694, 'barons': 21487, 'break': 3338, '、': 1635, 'shooters': 28310, 'shining': 9716, 'socialism': 14649, '##inga': 28234, 'filmmakers': 16587, '##stad': 16917, 'coronation': 12773, '##dock': 14647, '##～': 30521, 'ejected': 22607, 'blue': 2630, '##ound': 28819, 'headquarters': 4075, 'entitled': 4709, 'sake': 8739, 'faithful': 11633, 'viola': 10482, '##inating': 19185, 'departmental': 28912, 'dortmund': 23912, '[unused773]': 778, 'rivals': 9169, 'reign': 5853, 'blanche': 18158, 'cinemas': 19039, '1917': 4585, 'sheep': 8351, 'booster': 23715, 'routine': 9410, 'capped': 13880, 'lyndon': 23037, 'lombardy': 27728, 'samuel': 5212, '1754': 22593, 'blaming': 24114, 'о': 1193, '##rosis': 29166, 'ashes': 11289, '##ʼ': 29712, '129': 14378, '心': 1849, 'keeping': 4363, 'interactive': 9123, 'creditors': 23112, 'hq': 16260, 'doncaster': 18895, 'mattered': 13836, 'frontal': 19124, '3a': 23842, '##nt': 3372, 'california': 2662, 'quasi': 17982, 'wheat': 10500, 'pursuit': 8463, 'abortion': 11324, '##grating': 28183, 'blink': 12373, 'catalogue': 10161, '[unused94]': 95, 'gaelic': 11196, 'persons': 5381, 'synthesizers': 22211, '##lem': 16930, '##iation': 18963, '##zione': 20574, 'philosophy': 4695, '##fully': 7699, 'price': 3976, '425': 23285, 'illustration': 14614, '##uda': 14066, 'ク': 1702, 'む': 1679, 'manned': 15371, 'iphone': 18059, 'maureen': 19167, 'gunter': 27834, '##erman': 18689, 'trick': 7577, 'ensemble': 7241, 'rick': 6174, 'principally': 16552, 'precedent': 20056, '##atory': 14049, '##inda': 23938, 'dun': 24654, 'phoebe': 18188, 'benjamin': 6425, 'mcc': 23680, 'cramped': 22766, '[unused817]': 822, 'matrices': 21520, 'nfl': 5088, 'colchester': 20115, 'friars': 24037, 'germans': 7074, '##ann': 11639, '[unused317]': 322, 'access': 3229, '495': 29302, 'dagger': 10794, '##rza': 24175, '岡': 1832, 'ton': 10228, 'abruptly': 9225, 'despair': 13905, '375': 18034, '##sume': 23545, '##dened': 24589, '1953': 4052, 'uncanny': 28953, 'alberto': 12007, 'mansfield': 15352, 'arrays': 27448, '##eses': 23072, 'embassy': 8408, '##drive': 23663, 'saline': 28413, 'undergraduate': 8324, '×': 1095, 'samantha': 11415, 'osborne': 16732, 'arbitrary': 15275, '[': 1031, 'grease': 21956, 'sensitive': 7591, 'carol': 8594, 'dresser': 16641, '[unused242]': 247, '##qi': 14702, 'digit': 15340, '1880s': 12751, 'decay': 13121, '##ms': 5244, 'roof': 4412, '##ally': 3973, 'punjabi': 17498, 'ignore': 8568, 'hague': 14575, 'wrestlemania': 28063, 'organizers': 18829, 'hannibal': 24181, 'plug': 13354, 'veto': 22102, 'dickson': 22076, 'dong': 11947, 'squirrels': 29384, 'evicted': 25777, '##ħ': 29672, 'armament': 14410, '1858': 8517, 'apologized': 17806, 'binding': 8031, 'miracle': 9727, 'peaked': 6601, '##phy': 21281, '##ntine': 26730, 'observing': 14158, '##ια': 27432, 'kang': 16073, '##400': 29537, 'commodore': 12957, '##!': 29612, 'index': 5950, '##uri': 9496, '##col': 25778, '##away': 9497, '##cans': 26642, 'narayan': 24331, 'dinamo': 22306, '[unused216]': 221, 'committed': 5462, 'у': 1198, 'intrusion': 24554, '##œ': 29674, '##khand': 25910, '##ins': 7076, 'cass': 16220, 'carrie': 13223, 'toni': 16525, 'hermit': 24308, 'm1': 23290, 'mag': 23848, '##主': 30273, '##pen': 11837, '##aj': 13006, '##44': 22932, '年': 1840, '##ll': 3363, 'steadily': 11328, 'ships': 3719, 'rajasthan': 16815, '##rized': 18425, 'tubular': 25147, 'sheds': 25999, 'vicente': 17280, 'fern': 20863, 'ホ': 1722, 'ե': 1223, 'numerical': 15973, 'montevideo': 22460, '##torm': 20654, '13': 2410, 'telugu': 12180, 'deafening': 28840, 'welsh': 6124, '##„': 30060, 'armchair': 29372, 'converting': 16401, 'rifle': 5883, 'wizard': 10276, 'dragons': 8626, '##son': 3385, 'actress': 3883, 'resembles': 12950, '##at': 4017, 'cast': 3459, '##erative': 25284, 'monastic': 17361, 'suppression': 16341, '##iensis': 27806, '##社': 30450, 'wiggled': 26750, 'vassal': 24351, 'hat': 6045, '[unused161]': 166, '##tered': 14050, 'interpretation': 7613, 'diaspora': 18239, 'clearing': 8430, 'northampton': 15944, '##gt': 13512, 'problematic': 18636, 'fours': 23817, '##lova': 24221, 'themed': 11773, 'baccalaureate': 27802, '[unused907]': 912, 'instantly': 6880, '[unused503]': 508, 'producer': 3135, 'unique': 4310, 'entering': 5738, 'viaduct': 20596, 'eggs': 6763, '##idge': 13623, '##achi': 21046, 'atlantis': 16637, 'invariably': 26597, '##croft': 14716, 'lean': 8155, '##bis': 18477, 'disturbance': 16915, 'tablet': 13855, '書': 1871, 'unable': 4039, 'prism': 26113, 'preach': 25250, 'tuna': 24799, 'groups': 2967, 'ha': 5292, 'bitter': 8618, '##iers': 10136, '##irus': 26013, '##ে': 29917, 'marker': 12115, 'smashing': 21105, 'offence': 15226, 'accomplishment': 24718, '√': 1600, '##ial': 4818, '##gm': 21693, 'hope': 3246, 'embracing': 23581, '・': 1738, 'abolitionist': 29554, 'ste': 26261, '##pw': 28400, '##iously': 19426, 'crossings': 20975, 'am': 2572, 'traffic': 4026, 'interior': 4592, 'sacrifice': 8688, 'chances': 9592, 'armour': 12371, 'a1': 17350, 'badges': 23433, 'hideout': 29588, 'wrinkled': 15968, 'horticultural': 26235, 'insurgency': 23939, 'bitten': 19026, 'process': 2832, 'barrier': 8803, '##icate': 24695, 'recalling': 21195, '##oat': 16503, 'recognize': 6807, 'arrived': 3369, 'arthur': 4300, '##heard': 26362, '[unused70]': 71, 'insistence': 20616, 'outcome': 9560, 'logistics': 12708, 'retiring': 9150, 'recession': 19396, 'inmate': 24467, 'editorial': 8368, '##cy': 5666, 'sunken': 23470, 'attends': 23933, 'autobiographical': 18534, 'shipping': 7829, '##hue': 20169, '##act': 18908, '##wives': 23744, 'miniature': 12973, 'turbines': 17396, 'oppression': 20489, '##∨': 30130, '##na': 2532, '##hesion': 21471, 'setup': 16437, 'faulkner': 25109, 'maya': 9815, '##§': 29650, 'disposition': 22137, 'unemployment': 12163, 'crosses': 7821, '##xt': 18413, 'casper': 24602, '263': 25246, 'ibrahim': 13477, 'shelter': 7713, 'axes': 19589, 'indochina': 27053, 'ridiculous': 9951, 'subset': 16745, 'divides': 20487, 'lego': 23853, 'hardcore': 13076, 'clawed': 22544, 'ramp': 13276, 'shortages': 22623, 'ymca': 26866, 'willie': 9893, '##ibe': 20755, '[unused762]': 767, 'beamed': 22587, 'listen': 4952, 'paddington': 26318, '镇': 1966, 'districts': 4733, 'verse': 7893, 'ancestor': 13032, 'partisan': 14254, 'frontman': 21597, 'cornerback': 26857, '##¢': 29645, 'clarence': 11805, 'suzanne': 15146, '##wind': 11101, 'gay': 5637, 'peaks': 11373, 'approaching': 8455, 'deity': 12764, 'ammonia': 25874, 'versions': 4617, 'shaman': 23610, 'implementing': 14972, '...': 2133, 'lesions': 22520, 'soil': 5800, '##皇': 30443, 'guerrilla': 15722, '##17': 16576, 'transports': 19003, 'piracy': 24386, 'experts': 8519, '[unused705]': 710, 'tolerance': 13986, 'disturbances': 24535, '##ndra': 17670, 'packet': 14771, '⁶': 1540, 'transitional': 17459, 'tunes': 13281, '##ains': 28247, 'relatively': 4659, 'compton': 18592, 'legislative': 4884, '##mler': 18602, 'pressure': 3778, 'architect': 4944, 'replies': 14054, 'surrounds': 20626, 'known': 2124, 'burma': 11050, 'nyc': 16392, 'granny': 19794, '##uce': 18796, 'banging': 22255, 'handed': 4375, 'alto': 10766, 'conduct': 6204, 'telescope': 12772, 'follows': 4076, 'stroke': 6909, 'slogan': 14558, 'paramilitary': 22258, 'sicilian': 22584, 'iq': 26264, '##uet': 23361, 'hospital': 2902, 'ক': 1353, 'cummings': 20750, '##phila': 26083, '##ರ': 29937, 'utilizes': 21852, 'tired': 5458, 'delicious': 12090, '##ru': 6820, 'researcher': 10753, 'sheen': 20682, 'lunatic': 26594, 'englishman': 25244, 'hamburger': 24575, '都': 1961, 'luton': 17807, 'illusions': 24883, 'sustainability': 15169, 'clasp': 23465, 'intra': 26721, '##hn': 7295, 'commercially': 11088, 'trafficking': 11626, 'olympus': 26742, 'abilities': 7590, '##0': 2692, 'earth': 3011, 'resurgence': 26303, 'undergo': 13595, 'trader': 17667, 'hartford': 13381, 'unexpected': 9223, 'colleague': 11729, 'kc': 21117, '[unused142]': 147, 'citrus': 20418, '[unused614]': 619, '##board': 6277, 'shortest': 20047, 'cc': 10507, 'fortification': 23050, 'ⁱ': 1537, '1982': 3196, 'major': 2350, 'suitable': 7218, 'delay': 8536, 'routinely': 19974, '##reate': 29313, 'astronomy': 12799, 'permanently': 8642, 'laura': 6874, 'acheron': 21427, 'outcomes': 13105, 'capitalism': 16498, '[unused772]': 777, '[unused204]': 209, 'calculation': 17208, 'belle': 9852, 'daphne': 16847, 'complaining': 17949, 'trainee': 26758, 'bass': 3321, 'jean': 3744, 'throws': 11618, 'reigned': 21236, 'search': 3945, '##分': 30301, 'breakthrough': 12687, '##kus': 22332, 'clones': 24418, '##dium': 12811, 'venom': 15779, 'philips': 19087, '##tidae': 21861, 'assimilation': 27574, 'inclusion': 10502, 'humanoid': 28051, 'gatherings': 21403, 'gaston': 18572, 'darkness': 4768, 'wrought': 18481, 'contracts': 8311, 'michaels': 17784, '[unused751]': 756, '##anum': 27975, '##ᵉ': 30035, '##●': 30146, '[unused721]': 726, 'maj': 16686, '##roud': 21332, 'pocket': 4979, 'sid': 15765, '##δ': 29722, '##tters': 24168, 'ellie': 10707, 'skip': 13558, 'investigate': 8556, '##bro': 12618, 'loaded': 8209, 'refugee': 13141, '##igate': 28731, 'accusation': 19238, 'desired': 9059, 'tang': 9745, 'larson': 21213, 'pin': 9231, 'semi': 4100, 'nk': 25930, 'dino': 22412, 'proprietary': 16350, 'fiddle': 15888, 'bursting': 21305, 'ore': 10848, 'erected': 7019, 'shattering': 21797, 'oh': 2821, '##deck': 26547, 'promotion': 4712, '##peed': 25599, 'pedal': 15749, '[unused800]': 805, 'pull': 4139, 'brigham': 22727, '##त': 29859, 'atlantic': 4448, '##fen': 18940, 'て': 1665, 'tribunal': 12152, 'armor': 8177, '309': 25048, 'demands': 7670, 'strangely': 13939, '##od': 7716, 'husbands': 19089, 'locating': 26339, 'tirana': 24631, '[unused866]': 871, '1878': 7261, 'tyrol': 27193, 'riga': 17557, 'tomato': 20856, '##vary': 21639, 'recoil': 27429, 'apex': 13450, 'candy': 9485, '##ife': 29323, 'stormed': 16201, 'nik': 23205, 'terrorists': 15554, 'airlift': 20019, 'circulation': 9141, '##mic': 7712, 'grass': 5568, '[unused648]': 653, 'match': 2674, '1975': 3339, 'tome': 21269, '##c': 2278, 'shady': 22824, 'talents': 11725, '1': 1015, 'knowing': 4209, 'relationships': 6550, '##dini': 26039, 'officers': 3738, '।': 1344, 'fungus': 16622, 'raul': 16720, 'ث': 1274, '##wall': 9628, 'squirrel': 18197, '##氏': 30417, 'demise': 13614, 'rebuilding': 14584, '##த': 29921, '##aki': 8978, 'saga': 12312, 'hen': 21863, 'regent': 11315, 'blast': 8479, 'tighten': 21245, 'examination': 7749, 'airfield': 9087, '##rion': 14772, 'poles': 10567, 'noun': 15156, 'tran': 25283, 'fisherman': 19949, 'paranoia': 27890, 'charted': 12568, 'leased': 12019, 'agitation': 22356, 'davey': 20436, '[unused752]': 757, 'twitched': 16767, '[unused910]': 915, 'deep': 2784, 'affiliated': 6989, 'dracula': 18500, 'blanc': 18698, 'pipes': 12432, 'endangered': 10193, '##dha': 17516, 'towel': 10257, 'narrower': 22546, 'foreman': 18031, 'brianna': 25558, '##sen': 5054, '##lal': 13837, '##entes': 26933, '1983': 3172, 'woven': 17374, 'raju': 25098, 'chess': 7433, 'iso': 11163, '##oic': 19419, 'tiny': 4714, '##bey': 19182, 'dear': 6203, '##tlan': 28922, 'lateral': 11457, 'aleksandr': 24020, 'stuart': 6990, 'sm': 15488, 'brno': 28634, 'flights': 7599, 'raider': 27826, 'repeated': 5567, '##ran': 5521, 'depart': 18280, 'clubs': 4184, 'defenders': 12534, 'rail': 4334, '##sea': 17310, 'condemned': 10033, 'attempting': 7161, 'principles': 6481, 'ণ': 1361, 'sharon': 10666, '##taking': 17904, 'resolving': 29304, 'wimbledon': 13411, 'military': 2510, '##ث': 29818, 'massive': 5294, '##nst': 23808, 'mastery': 26364, '##ila': 11733, '[unused128]': 133, 'bruno': 10391, 'subsidy': 28768, 'feminist': 10469, 'concord': 16557, 'content': 4180, '##llen': 12179, '##oured': 16777, 'devoid': 22808, 'grind': 23088, '##kin': 4939, 'luce': 19913, 'duration': 9367, 'tumbled': 18303, '##edge': 24225, 'networking': 14048, 'gan': 25957, '##zyn': 23749, 'applicants': 17362, 'divide': 11443, 'middlesex': 13654, 'knitting': 26098, 'ի': 1225, 'panic': 6634, 'sprinted': 25156, 'timor': 19746, 'pp': 4903, '##nche': 26091, 'fontaine': 25749, '##ogenic': 24278, 'キ': 1701, '##ᶠ': 30047, '##bbled': 12820, '##estinal': 19126, 'gale': 14554, 'components': 6177, 'enveloped': 25407, '##ptic': 20746, '里': 1962, '307': 24559, 'resin': 24604, 'wiley': 18825, 'ethanol': 22886, 'jeremy': 7441, '##ridge': 9438, 'modernism': 27254, 'family': 2155, 'occult': 27906, '[unused367]': 372, 'substance': 9415, 'antrim': 24142, 'stewart': 5954, 'farmhouse': 16870, 'overtime': 12253, 'pa': 6643, 'lam': 16983, 'umm': 26114, 'uncomfortably': 22502, 'transylvania': 20816, 'ᄆ': 1459, 'brant': 29182, '##گ': 29842, 'nah': 20976, 'draped': 15098, 'latham': 28737, 'crumpled': 19814, '明': 1865, 'crossing': 5153, '|': 1064, 'writing': 3015, 'internship': 22676, '##orra': 24285, 'periodically': 18043, 'relying': 18345, '[unused223]': 228, '成': 1854, '2013': 2286, 'recording': 3405, 'emissions': 11768, 'brent': 12895, 'airplane': 13297, 'edge': 3341, '##খ': 29890, 'johanna': 21498, 'waterways': 21938, 'custer': 28888, '##ed': 2098, '##dled': 20043, 'dudley': 12648, '##proof': 18907, 'distances': 12103, 'airport': 3199, '1635': 27426, 'bosnian': 16163, '##rant': 17884, 'dinosaurs': 18148, 'aubrey': 18961, '##sms': 19230, 'listener': 19373, 'relationship': 3276, 'tournaments': 8504, 'kerr': 14884, '##imated': 20592, 'burr': 22715, 'closing': 5494, 'robust': 15873, 'predict': 16014, 'glasses': 7877, '339': 28977, 'revision': 13921, 'fares': 27092, 'convincing': 13359, 'billie': 18210, 'cushion': 22936, 'prospect': 9824, 'net': 5658, '##镇': 30492, 'pol': 14955, 'pamphlet': 19899, '##thed': 23816, 'berwick': 24957, 'abroad': 6917, '1873': 7612, 'usb': 18833, 'monde': 23117, '##स': 29874, 'cordoba': 17986, '##weed': 18041, 'therapeutic': 17261, '##elman': 23830, 'fellowships': 27298, 'moritz': 28461, 'seo': 27457, 'natives': 12493, 'פ': 1261, 'representatives': 4505, 'lewis': 4572, '##vin': 6371, 'quarters': 7728, 'truman': 15237, 'calais': 23116, 'sean': 5977, 'bromley': 27979, 'gupta': 20512, 'broadcasters': 18706, 'cove': 11821, '##dus': 17619, 'tech': 6627, 'leonardo': 14720, 'letting': 5599, 'marilyn': 14749, 'plaintiffs': 23953, 'reiterated': 28960, 'tonic': 28157, '##վ': 29782, 'dewey': 20309, '1641': 25702, '##ting': 3436, 'torment': 21741, '12': 2260, 'unusual': 5866, '##inger': 9912, 'jain': 17136, 'francs': 24313, 'mba': 15038, 'basalt': 26343, 'achilles': 23167, 'dominates': 29532, 'aback': 26575, '##holes': 19990, '##tish': 24788, '##lad': 27266, 'cartoonist': 19659, 'mussolini': 22554, 'verona': 20197, 'extension': 5331, '[unused672]': 677, 'moose': 17716, 'run': 2448, 'payload': 18093, 'judgement': 16646, 'ambrose': 15675, 'exposing': 14944, '↓': 1586, 'jess': 12245, 'enjoyable': 22249, '##ala': 7911, 'erect': 14908, '[unused359]': 364, 'via': 3081, 'restore': 9239, 'jennings': 14103, '##fin': 16294, 'grenade': 17038, 'carthage': 21959, 'duc': 26363, 'billboard': 4908, 'rush': 5481, '##ption': 16790, 'melodies': 16106, '850': 15678, '##osity': 25949, 'section': 2930, '##tch': 10649, 'josiah': 24461, '##you': 29337, 'expansion': 4935, '##anial': 27532, '##60': 16086, 'presenting': 10886, '##lind': 27164, 'millions': 8817, '[unused616]': 621, 'achievements': 10106, 'gaping': 21226, 'landscapes': 12793, 'harness': 17445, 'coughed': 19055, 'topic': 8476, '##cial': 13247, 'bologna': 14102, 'imagining': 16603, 'monte': 10125, '食': 1978, 'prevailing': 19283, 'artefacts': 25762, 'achieved': 4719, '##ram': 6444, 'sr': 5034, 'difference': 4489, 'thirsty': 24907, '570': 24902, '[unused953]': 958, 'october': 2255, 'adolescence': 29101, 'aa': 9779, '[unused933]': 938, 'ferguson': 11262, 'nairobi': 21124, 'gaul': 26522, 'critics': 4401, 'beloved': 11419, 'financed': 13790, 'edgar': 9586, '##rdi': 17080, '[unused718]': 723, 'wish': 4299, 'resist': 9507, 'horizons': 24484, '##ہ': 29845, 'fights': 9590, 'grams': 20372, '[unused824]': 829, '##yoshi': 19196, '1904': 5692, 'kindergarten': 11793, 'property': 3200, 'infection': 8985, '[unused181]': 186, 'cholera': 25916, 'distance': 3292, 'binoculars': 29549, 'startling': 19828, 'tolerated': 25775, '##ttering': 19567, 'wilbur': 26151, 'other': 2060, '##rates': 20370, 'tapes': 13324, 'fractures': 28929, '##rp': 14536, 'symphony': 6189, '##athan': 29246, 'typed': 21189, 'intrinsic': 23807, 'dairy': 11825, '##opers': 27342, 'problem': 3291, '##eto': 18903, '##rous': 13288, 'thomson': 11161, '[unused53]': 54, '1624': 28531, 'mclaren': 18590, 'rewarded': 14610, 'toes': 10393, '##tort': 25485, '[unused682]': 687, 'becker': 15309, 'neutrality': 21083, '⺩': 1632, 'trembling': 10226, 'pillows': 17860, 'electronically': 28926, 'nutrient': 26780, 'upgrade': 12200, 'sousa': 28535, 'drain': 12475, 'suburban': 9282, 'em': 7861, 'coloured': 11401, '145': 13741, 'bai': 21790, '##drum': 21884, 'beg': 11693, 'unsafe': 25135, 'integrate': 17409, 'cu': 12731, '##neil': 27276, 'nationale': 17360, 'schuster': 24253, 'modelling': 19518, 'frankish': 26165, 'counting': 10320, 'narrows': 25142, 'rosalie': 29564, '##ence': 10127, '##_': 29638, 'commuted': 26704, 'dover': 13985, 'exodus': 16388, '##tadt': 18808, 'leaning': 6729, 'adherents': 25712, 'chambers': 8477, 'diaz': 12526, 'roach': 20997, 'violently': 14196, 'gang': 6080, 'graduate': 4619, 'blankets': 15019, 'banking': 8169, '##code': 16044, 'paws': 24392, 'acceptance': 9920, 'godfather': 23834, 'brighter': 16176, 'miner': 18594, 'changes': 3431, '[unused834]': 839, '##shaft': 25679, 'lord': 2935, 'caves': 10614, '##kes': 9681, '戸': 1857, 'girlfriend': 6513, 'licenses': 15943, '##sz': 17112, 'tasks': 8518, 'damaged': 5591, '[unused562]': 567, '##¦': 29649, 'lordship': 20698, 'mayoral': 23578, 'nor': 4496, 'dusk': 18406, '##etta': 16549, '[unused779]': 784, '##ations': 10708, 'buys': 23311, 'gardner': 11764, 'montrose': 24990, '##rled': 27501, 'rook': 28620, 'lublin': 18967, 'difficult': 3697, 'twilight': 13132, '181': 18596, 'hug': 8549, 'hairy': 15892, 'straightening': 27508, 'qing': 13282, '[unused185]': 190, 'football': 2374, 'ratified': 17673, 'hinted': 21795, 'airplanes': 24042, '##hae': 25293, 'yuan': 11237, 'dripped': 22526, '##vance': 21789, 'vermont': 8839, 'da': 4830, 'noticeably': 25327, '##tres': 19168, '[unused160]': 165, '##wn': 7962, 'spine': 8560, 'salvation': 12611, 'richards': 9712, '##gat': 20697, '¶': 1086, 'facts': 8866, 'seven': 2698, 'yin': 18208, 'reprised': 22598, 'grandson': 7631, 'trooper': 28224, 'bathrooms': 28942, 'welded': 29014, 'baltimore': 6222, 'peerage': 18760, 'banning': 21029, 'traded': 7007, 'prep': 17463, '##iq': 18515, 'motor': 5013, '##bbies': 27982, '##པ': 29966, '##仁': 30284, '[unused81]': 82, '##gun': 12734, 'characteristic': 8281, 'bilateral': 17758, 'monica': 9018, 'unconscious': 9787, '##500': 29345, '##mount': 20048, 'courtroom': 20747, 'supper': 15264, '##ged': 5999, 'skinned': 19937, 'cradle': 18293, 'keen': 10326, 'բ': 1220, 'flashback': 21907, 'elsie': 24603, 'sang': 6369, 'internationally': 7587, 'antigua': 26023, 'observation': 8089, '##istan': 23137, 'lexi': 16105, 'openly': 10132, '[unused155]': 160, '。': 1636, 'slopes': 10314, 'hardware': 8051, '##erated': 16848, 'cheers': 21250, 'loyalty': 9721, '##opus': 24676, 'proceed': 10838, '##ically': 15004, '##sy': 6508, '##liga': 14715, 'segregation': 18771, '333': 21211, '##minating': 27932, 'κ': 1164, 'vision': 4432, 'configuration': 9563, 'converge': 28314, 'seismic': 22630, 'barry': 6287, 'mapping': 12375, 'rendering': 14259, 'runner': 5479, 'nasty': 11808, '##rut': 22134, '##metry': 24327, 'leeds': 7873, '##ywood': 26985, 'fixed': 4964, 'closure': 8503, 'fuss': 28554, 'hz': 22100, '##ncies': 14767, '##ience': 13684, '249': 23628, 'security': 3036, 'gown': 11739, 'scully': 25686, 'palma': 23985, 'ट': 1320, 'togo': 23588, '##rina': 11796, 'sixty': 8442, 'demolition': 12451, '##pods': 22925, 'calm': 5475, 'sumatra': 18262, 'billings': 26124, 'hoisted': 27269, '##hani': 23573, 'gertrude': 18734, '22': 2570, 'intervening': 26623, '⊕': 1612, 'oath': 11292, 'ᅳ': 1481, '52nd': 26898, 'donaldson': 23164, '##⋅': 30141, 'their': 2037, 'confines': 25722, 'tri': 13012, '119': 13285, '##ர': 29927, 'benito': 23595, 'zoo': 9201, '[unused622]': 627, 'harmony': 9396, '##oit': 28100, 'rushes': 18545, '##rzy': 28534, 'predator': 15267, 'increases': 7457, 'referencing': 28789, 'fraternity': 13577, 'reindeer': 29495, 'circles': 7925, 'legitimacy': 22568, 'funding': 4804, 'demonstrated': 7645, 'alt': 12456, 'respondents': 25094, 'including': 2164, 'barley': 21569, '##bill': 24457, '##thus': 19877, '##icon': 28524, 'dispute': 7593, 'primarily': 3952, 'personnel': 5073, 'instead': 2612, 'lasting': 9879, 'programme': 4746, '##going': 26966, 'ک': 1304, 'milano': 21613, '##loe': 26846, 'lennox': 21060, '##erty': 15010, 'fatty': 19101, 'sprayed': 25401, '##ն': 29778, 'ி': 1396, '##rata': 14660, 'stirring': 18385, 'henrietta': 20775, 'emerge': 12636, 'cane': 11942, 'remix': 6136, '##oint': 25785, 'developer': 9722, '##‑': 30049, 'catastrophic': 23546, '##rder': 26764, 'highlights': 11637, 'immortal': 12147, 'fists': 10006, '##yle': 12844, 'fears': 10069, 'moniker': 21937, 'rogue': 12406, '##47': 22610, '##alle': 24164, 'thrilled': 16082, 'collaborating': 20295, 'extensively': 8077, 'cdc': 26629, 'electronic': 4816, 'maddy': 20789, 'due': 2349, 'kochi': 27603, 'raises': 13275, 'rise': 4125, 'smirked': 18775, '##lusion': 24117, '##wen': 12449, 'playground': 14705, 'twitch': 19435, 'sort': 4066, 'willoughby': 24919, '##hosis': 25229, 'mcleod': 25363, 'popped': 10538, 'humanist': 24464, '##horn': 9769, '##ふ': 30200, 'recruited': 8733, 'sadler': 29012, 'therapy': 7242, 'books': 2808, '##tta': 5946, 'rigid': 11841, '氵': 1894, '久': 1748, 'establishment': 5069, 'casting': 9179, '[unused531]': 536, 'slams': 25967, 'regulars': 24945, '1770': 17711, 'kb': 21677, 'barney': 15377, 'reminding': 15906, '##nate': 12556, 'shay': 18789, '##efe': 27235, 'rhys': 13919, '法': 1901, 'ᵤ': 1509, 'favorably': 27597, 'conservatives': 11992, 'higher': 3020, 'norfolk': 7735, 'italians': 16773, 'hilly': 22800, '##bread': 27035, 'flair': 22012, '##ो': 29879, '[unused41]': 42, 'prostitution': 15016, '345': 23785, '2010s': 26817, 'θ': 1162, 'memphis': 9774, 'rhine': 10950, 'favorite': 5440, 'pest': 20739, 'spirited': 24462, 'costume': 9427, 'messaging': 24732, 'hang': 6865, 'klein': 12555, 'norm': 13373, '東': 1879, 'cannes': 14775, 'netball': 25034, 'clenched': 8555, 'normandy': 13298, 'pulpit': 23134, 'jarvis': 21072, 'padma': 23731, '14': 2403, '##rrado': 27933, '##la': 2721, 'renumbered': 27855, 'ot': 27178, 'widening': 17973, 'parcel': 20463, '±': 1081, '##sd': 16150, '##rok': 27923, 'reckless': 18555, 'kilda': 22633, 'unstable': 14480, 'switches': 15924, 'soloists': 27516, 'base': 2918, 'malawi': 18137, '##bane': 27543, '##ₚ': 30097, 'maker': 9338, 'ᄅ': 1458, 'domain': 5884, 'imports': 17589, 'falls': 4212, 'ぬ': 1669, 'reading': 3752, 'kylie': 9008, 'lump': 15116, 'intercity': 20651, 'gotham': 22814, '##ɐ': 29676, 'dwarfs': 28984, 'boot': 9573, '##ᅢ': 30007, '##千': 30310, 'mainz': 19876, 'dental': 11394, 'anal': 20302, 'poole': 19107, '##ig': 8004, 'audit': 15727, 'transform': 10938, 'culminated': 16943, 'flow': 4834, 'displaced': 12936, 'hazardous': 17760, 'vibration': 17880, 'keyboards': 6269, 'infectious': 16514, 'inspire': 18708, '[unused695]': 700, '##garh': 13484, 'list': 2862, '##oli': 10893, 'viscount': 12182, '##inski': 19880, 'delgado': 26532, 'malaysia': 6027, 'robbery': 13742, '##oder': 27381, 'celtics': 23279, 'lowers': 24950, '1798': 13036, 'gt': 14181, 'augusta': 13635, 'gazing': 16448, '##dable': 20782, 'rae': 14786, 'si': 9033, '1872': 7572, 'gardiner': 21143, '！': 1986, 'assist': 6509, '##eet': 15558, '##igny': 26978, 'verity': 22178, 'masters': 5972, '##ine': 3170, 'occurrences': 27247, 'possessive': 22105, 'passions': 25289, '##cing': 6129, 'includes': 2950, 'railing': 15747, 'natalie': 10829, 'travelled': 7837, '##uso': 26658, 'wakes': 17507, 'ticked': 26019, 'panel': 5997, 'fuse': 19976, 'pools': 12679, '##chu': 20760, '雄': 1974, 'petersen': 22615, 'chorale': 26561, 'leak': 17271, 'highway': 3307, 'underwood': 22751, 'nassau': 14646, '##rain': 21166, '##ₐ': 30089, 'corona': 21887, 'containers': 16143, '##lick': 25230, 'spill': 14437, 'discontent': 27648, 'hertfordshire': 18889, 'flared': 14937, 'dona': 24260, 'somethin': 27941, 'factors': 5876, 'flush': 13862, '##mani': 20799, 'imagine': 5674, 'closest': 7541, 'imagined': 8078, 'defunct': 11984, 'archaeological': 7611, 'lack': 3768, 'controllers': 21257, 'enamel': 29484, '馬': 1980, 'cinder': 29399, 'techno': 21416, 'slides': 14816, 'hc': 16731, 'heaven': 6014, 'sprinter': 19938, 'contend': 27481, '[unused595]': 600, 'krishna': 10871, 'ty': 5939, 'pod': 17491, '##block': 23467, 'inspecting': 29508, 'plump': 26731, 'dipped': 13537, 'efficacy': 21150, 'australia': 2660, '##balls': 18510, '##bino': 21891, 'woodlands': 19268, 'qu': 24209, 'prediction': 17547, '[unused194]': 199, 'highlight': 12944, 'distinguishes': 27343, 'crying': 6933, 'norman': 5879, '##∅': 30121, 'chemistry': 6370, 'continental': 6803, 'obama': 8112, 'hypnotic': 28322, 'anomaly': 28685, '##back': 5963, 'dismantled': 17293, 'cobalt': 25627, 'groin': 20092, '1618': 29029, 'maverick': 27187, 'ʐ': 1136, '##rons': 28212, 'voting': 6830, 'handing': 13041, 'raf': 7148, '##eu': 13765, 'boiling': 16018, 'storms': 12642, 'toxicity': 22423, '[unused768]': 773, 'ᵈ': 1498, 'inspired': 4427, 'connection': 4434, '##ac': 6305, 'rancho': 18123, 'fracture': 19583, 'diverse': 7578, 'validity': 16406, '##რ': 29987, 'corpses': 18113, 'hindus': 18221, 'stakes': 7533, '[unused936]': 941, '##rick': 11285, '##igh': 18377, 'wheel': 5217, 'vocational': 13099, 'fuscous': 18121, '1727': 25350, 'dc': 5887, 'searching': 6575, 'soto': 22768, 'sylvie': 26009, 'games': 2399, 'conjecture': 22647, 'glaring': 16124, 'improvements': 8377, 'mostly': 3262, '##lating': 22248, 'gentlemen': 11218, 'basement': 8102, 'bundled': 24378, '##tha': 8322, 'shivered': 13927, '##lini': 22153, '##uded': 13936, '168': 16923, 'stolen': 7376, 'trustees': 9360, 'mine': 3067, 'aiden': 15086, '1658': 28944, 'yokohama': 20507, 'pay': 3477, 'assisting': 13951, '##ז': 29793, 'scope': 9531, '[unused641]': 646, '##cken': 19766, 'keynote': 25569, 'can': 2064, 'cabinets': 20053, 'regime': 6939, 'goodness': 15003, 'jeep': 14007, 'drying': 17462, 'petitions': 24320, '##arable': 25236, 'triumph': 10911, 'ν': 1167, 'hypothetical': 25613, '##க': 29918, '##ise': 5562, 'cheryl': 19431, '巿': 1837, 'seventies': 26232, '##ea': 5243, 'landmark': 8637, '[unused597]': 602, 'fragments': 10341, 'reservations': 17829, '##nier': 14862, 'sprawling': 24199, 'kurdish': 15553, '##vb': 26493, '##tile': 15286, 'serbian': 6514, 'duets': 29410, '##tm': 21246, 'confirmation': 13964, '##wear': 16689, 'rebelled': 25183, 'consists': 3774, 'eruptions': 28448, 'enclosure': 17539, 'gujarati': 26428, 'powerplant': 19526, 'maid': 10850, 'characterized': 7356, 'holden': 9988, 'র': 1372, 'prompting': 15870, '##ов': 19259, 'securities': 12012, '##dation': 20207, '##bation': 23757, '##turn': 22299, 'vary': 8137, 'thorns': 28408, 'focuses': 7679, 'poor': 3532, 'pictures': 4620, '[unused542]': 547, 'eager': 9461, '[unused218]': 223, 'limp': 14401, 'brew': 24702, '##take': 15166, 'predictable': 21425, '##meral': 28990, 'finn': 9303, 'guru': 11972, 'fx': 23292, '##frey': 22586, '1797': 14112, '∅': 1593, 'commanding': 7991, 'react': 10509, 'hugs': 24459, '##kumar': 18494, 'आ': 1312, 'donations': 11440, 'look': 2298, 'opening': 3098, 'sunderland': 15518, '290': 17222, 'barrie': 24953, 'pants': 6471, 'unclear': 10599, 'mercenary': 22146, 'agreed': 3530, 'blow': 6271, 'feb': 13114, 'lawn': 10168, 'deer': 8448, '##nel': 11877, 'recruit': 13024, 'blaine': 20002, 'grimm': 24287, 'renaming': 24944, 'gloss': 27068, '##fine': 23460, 'officially': 3985, '##itation': 18557, 'packets': 23730, 'suffers': 17567, 'croydon': 21838, 'ת': 1267, 'prevailed': 19914, '[unused813]': 818, '[unused889]': 894, 'grabbed': 4046, '##sis': 6190, 'volunteers': 7314, 'respective': 7972, 'treason': 14712, 'raj': 11948, 'morgan': 5253, '##uro': 10976, 'carpet': 10135, '##tad': 17713, '##iy': 28008, 'polished': 12853, '319': 26499, 'দ': 1364, 'especially': 2926, 'elongated': 17876, '##than': 21604, 'handgun': 28497, 'bites': 15424, 'system': 2291, 'costumes': 12703, 'predicting': 29458, 'cave': 5430, '##ost': 14122, 'huff': 21301, 'peace': 3521, 'exposition': 13080, 'colloquially': 23992, 'kv': 24888, 'liable': 20090, '##ᅵ': 30019, '##fus': 25608, 'tore': 9538, 'terrestrial': 12350, 'burnley': 23028, 'manipulation': 16924, 'detect': 11487, 'distracting': 25012, '##অ': 29883, 'petite': 20146, 'rescued': 10148, 'myles': 27056, 'bedroom': 5010, 'light': 2422, 'morse': 17107, 'cans': 18484, '##fahan': 28975, 'insistent': 29204, 'cover': 3104, '##ս': 29781, 'convention': 4680, 'lviv': 23814, 'co': 2522, 'clearly': 4415, 'underwent': 9601, 'cite': 21893, 'underworld': 13607, '##木': 30401, 'e': 1041, 'novel': 3117, 'perspective': 7339, 'l': 1048, '[unused89]': 90, 'gliding': 20292, 'shouted': 6626, 'excessive': 11664, 'dowry': 29603, '##unda': 18426, 'majors': 15279, '##rangle': 21476, 'teenage': 9454, '[unused951]': 956, 'darrell': 23158, 'ins': 16021, 'conjunction': 9595, 'aspiring': 22344, '##hood': 9021, 'militant': 16830, 'yan': 13619, '##egan': 20307, 'gallery': 3916, 'vanderbilt': 17066, 'funk': 11962, 'forge': 15681, 'mornings': 16956, 'sutherland': 14274, 'proton': 20843, 'boss': 5795, 'tangible': 24600, 'browser': 16602, 'wed': 21981, 'pierre': 5578, 'leaked': 15748, 'legends': 9489, 'snails': 13482, 'scrap': 15121, 'tight': 4389, 'chromosome': 16706, '##nery': 27415, '##ね': 30196, 'wherever': 11210, '##eron': 26534, '##mad': 25666, 'unfair': 15571, '》': 1640, 'lindsay': 12110, 'compiling': 21953, '##gh': 5603, 'humane': 23369, 'prestige': 14653, 'acquiring': 13868, 'predominant': 21047, 'magical': 8687, 'peak': 4672, 'communications': 4806, 'wynn': 25328, '##olio': 29401, 'participating': 8019, 'gustav': 13430, 'sic': 14387, 'impression': 8605, 'squadron': 3704, 'teresa': 12409, 'intruder': 22841, 'fleetwood': 23866, '##ico': 11261, 'implicit': 24655, 'ideally': 28946, 'reducing': 8161, 'graceful': 19415, '##eras': 24140, 'palestinian': 9302, '##ocene': 25793, 'times': 2335, '##ェ': 30223, 'ھ': 1307, 'epic': 8680, '##rso': 25301, '##ckey': 29183, 'horizontally': 23190, '⇒': 1591, 'brahms': 28419, 'jesse': 7627, 'accidental': 17128, 'tempted': 16312, 'choreographed': 23317, 'premises': 10345, 'tenth': 7891, 'invincible': 25018, 'drugs': 5850, 'laurel': 11893, '##zers': 16750, 'liver': 11290, 'canadians': 16485, '##ajan': 28869, 'sample': 7099, 'chu': 14684, '##tana': 20496, 'literal': 18204, '##ratic': 23671, 'tough': 7823, 'stillness': 29435, 'if': 2065, '##isation': 6648, 'possessions': 13689, 'gradient': 17978, '##ョ': 30256, '1879': 7449, 'idols': 24438, 'active': 3161, 'uniformed': 25189, 'stain': 21101, 'samba': 29086, 'descended': 9287, 'contemporaries': 16682, 'bland': 20857, 'bucket': 13610, 'harassed': 28186, 'amidst': 17171, 'adjustable': 26404, 'tallest': 13747, 'saxony': 13019, 'lowered': 6668, 'slayer': 20005, '##yo': 7677, '##ว': 29955, 'trafford': 26894, 'himself': 2370, 'block': 3796, 'aids': 8387, 'pack': 5308, 'suicidal': 26094, 'syrian': 9042, 'sindh': 20284, '##ross': 25725, 'eighties': 27690, 'sagged': 26719, 'arabian': 13771, 'ferries': 20157, 'handler': 28213, '##poulos': 24662, 'pilgrim': 21214, 'ankara': 20312, 'coated': 15026, 'older': 3080, '217': 20335, 'meredith': 10635, '420': 17442, 'delivered': 5359, 'popping': 20095, 'amounted': 18779, '星': 1866, 'soldiers': 3548, 'adjacent': 5516, '##ige': 25538, 'rare': 4678, 'tips': 10247, 'tt': 23746, 'ﬁ': 1984, '##not': 17048, '1870': 6940, 'neptune': 21167, '##coll': 26895, 'sweetheart': 12074, 'species': 2427, '##fide': 20740, 'curse': 8364, 'sheppard': 24976, 'sooner': 10076, 'green': 2665, '[unused260]': 265, 'enforced': 16348, 'lest': 26693, 'rag': 17768, '##bah': 24206, 'discount': 19575, 'reproduced': 22296, 'massachusetts': 4404, 'resigned': 5295, 'td': 14595, 'welcome': 6160, 'barlow': 17803, '##cular': 15431, 'attaining': 26615, 'spectators': 12405, 'ara': 19027, '##ك': 29835, 'bottle': 5835, 'prejudice': 18024, '立': 1931, 'ky': 18712, '##bund': 27265, 'unanimous': 13604, 'noir': 15587, 'explanations': 17959, 'rowing': 12037, '##yana': 16811, '崎': 1834, '##chon': 24561, 'rattling': 26347, 'rollins': 23502, 'bundles': 26825, '高': 1981, 'cramer': 29433, 'subtropical': 11935, '##љ': 29762, 'hardest': 18263, 'luc': 12776, 'crashed': 8007, 'therapist': 19294, 'angelo': 12262, '1640': 21533, 'synonymous': 22594, 'titanium': 23431, 'freeze': 13184, 'fordham': 27302, 'sphinx': 27311, '60': 3438, '–': 1516, 'criteria': 9181, 'incomes': 29373, '##ј': 29761, 'hybrids': 23376, '##ho': 6806, 'submarines': 12622, 'galloway': 22372, 'universally': 21186, 'manage': 6133, 'corrugated': 27186, 'favourites': 28271, '##uez': 29488, 'cautiously': 15151, 'tor': 17153, 'sturdy': 23073, '##ظ': 29829, 'donnell': 18016, 'overland': 21400, '[unused865]': 870, 'ŋ': 1106, '1922': 4798, '[unused135]': 140, '{': 1063, '41': 4601, 'metabolism': 18089, '##ising': 9355, 'receptionist': 23775, '##mobile': 17751, 'akira': 21616, '##mony': 22847, '[unused228]': 233, 'horizontal': 9876, '##א': 29788, '##es': 2229, 'thief': 12383, 'pumping': 14107, '##sby': 14478, 'devoted': 7422, 'journeys': 19147, 'borneo': 15688, 'keller': 16155, 'funny': 6057, 'hicks': 17221, 'blackish': 22032, 'sexual': 4424, 'narrow': 4867, 'distribution': 4353, 'mellon': 22181, 'entirety': 15700, 'memoirs': 13251, '##lich': 18337, 'deals': 9144, 'environments': 10058, 'concerto': 10405, '艹': 1939, 'road': 2346, 'ignoring': 9217, 'cue': 16091, 'rough': 5931, 'majority': 3484, 'porto': 13809, 'buff': 23176, 'leukemia': 25468, 'visitors': 5731, '##di': 4305, 'damian': 19507, 'settlements': 7617, 'belgarath': 21256, 'tides': 22487, 'v6': 25275, 'winger': 16072, '##bos': 15853, 'cartoon': 9476, 'anson': 25435, 'combatants': 26622, 'rebuilt': 7183, 'fellows': 13572, 'joints': 17651, 'goalkeeper': 9653, '##cence': 29320, 'villages': 4731, 'ignorant': 21591, 'neill': 11511, 'lcd': 27662, '##was': 17311, '##piece': 11198, 'sewage': 19873, '##clops': 28659, 'bonuses': 29563, 'munchen': 25802, '##国': 30325, 'fleeting': 25085, '##aran': 20486, 'invented': 8826, 'ronin': 29249, 'spying': 22624, 'plaques': 28487, 'world': 2088, 'items': 5167, 'interview': 4357, 'coalition': 6056, 'collect': 8145, 'licence': 11172, '##vant': 18941, 'stanza': 29509, 'many': 2116, '##[': 29634, 'stanton': 15845, '##tya': 21426, 'everything': 2673, 'propeller': 15692, 'drains': 18916, 'knot': 12226, 'martian': 20795, 'antiquity': 16433, 'rene': 10731, 'exhaust': 15095, '##hg': 25619, 'ser': 14262, 'po': 13433, 'mandy': 18193, 'mare': 11941, 'secrecy': 20259, 'others': 2500, '103': 9800, '##sily': 26863, 'amazon': 9733, 'retailers': 16629, 'cruises': 23986, 'recital': 25521, 'refuses': 10220, 'varying': 9671, 'panama': 8515, '##bner': 29388, 'boardwalk': 29496, '##v': 2615, '##年': 30366, 'forth': 5743, 'reg': 19723, 'reorganized': 14137, '##moto': 15319, 'yesterday': 7483, 'stereotypes': 22807, 'cosmetics': 25381, 'concentration': 6693, 'swing': 7370, 'candles': 14006, '##usly': 27191, 'unnecessary': 14203, 'import': 12324, 'dunne': 26553, 'jared': 8334, 'expressions': 11423, '##shah': 25611, 'electromagnetic': 17225, 'place': 2173, 'ض': 1285, 'simulated': 23599, 'tbs': 29584, 'saturated': 23489, '[unused167]': 172, '##nos': 15460, '##tf': 24475, 'bosch': 25936, '##antes': 24985, 'ingrid': 22093, '##grant': 18980, '##bo': 5092, '##×': 26306, 'fictional': 7214, 'neighbour': 20065, 'contraction': 21963, '##gi': 5856, '##lli': 6894, 'napoleonic': 18813, 'revisions': 24699, 'greer': 25939, 'graduating': 6800, 'joo': 28576, '##ented': 14088, 'ை': 1399, 'taiwan': 6629, 'knocks': 21145, 'findings': 9556, 'carrot': 25659, 'text': 3793, 'bolt': 10053, 'bishopric': 20664, 'rash': 23438, 'gripped': 9672, 'arc': 8115, 'shadowy': 22801, 'resistance': 5012, '##arty': 23871, '##二': 30278, '##bet': 20915, '##manship': 21530, 'affiliates': 18460, 'story': 2466, 'boxer': 10423, 'obstacle': 18355, 'hits': 4978, 'hour': 3178, '##pit': 23270, 'number': 2193, '八': 1771, 'ethical': 12962, 'thankfully': 16047, '##ա': 29766, 'malcolm': 8861, 'famed': 15607, 'sci': 16596, 'huffington': 26149, 'flight': 3462, 'naomi': 12806, 'marseille': 16766, '##→': 30113, 'feather': 15550, '1960': 3624, 'mormon': 15111, 'abandonment': 22290, 'menu': 12183, 'banks': 5085, 'managers': 10489, 'stared': 3592, 'flags': 9245, 'kidney': 14234, 'assassin': 12025, 'perceive': 23084, '##bour': 25127, 'conquest': 9187, 'sierra': 7838, 'fond': 13545, 'beginnings': 16508, '御': 1847, 'shoots': 11758, '##esh': 9953, '##paper': 23298, '##ssing': 18965, 'admitting': 17927, '##pi': 8197, 'sensed': 10596, '##ones': 21821, 'cheat': 21910, 'deprived': 17676, 'diesel': 7937, '##ब': 29865, 'grinning': 11478, 'powell': 8997, 'retain': 9279, '[unused19]': 20, 'constantine': 12790, 'exclusively': 7580, 'pts': 19637, 'gubernatorial': 19100, 'attorney': 4905, 'rwanda': 17591, 'pitted': 25895, 'boxed': 27554, 'entry': 4443, 'resulting': 4525, 'kolkata': 13522, 'counters': 24094, 'ur': 24471, '1969': 3440, '##gas': 12617, 'scorer': 10835, 'vans': 21994, 'atm': 27218, 'istanbul': 9960, 'advisers': 24205, 'cessna': 27664, 'repairs': 10315, '##graphs': 27341, 'ರ': 1401, 'says': 2758, '[unused535]': 540, 'detonated': 28110, 'falcon': 11684, 'laval': 26205, 'csa': 27804, 'cottages': 18918, 'involving': 5994, '##agh': 17988, '##side': 7363, 'slavery': 8864, '[unused883]': 888, 'tails': 17448, 'salazar': 25315, 'aegean': 27198, 'question': 3160, 'assembly': 3320, '##cise': 18380, 'nosed': 29338, '）': 1988, '香': 1979, 'acute': 11325, '##urn': 14287, 'ezekiel': 28743, 'shoreline': 17721, 'jaipur': 28355, 'peninsular': 22682, '##ela': 10581, 'retired': 3394, 'relied': 13538, 'astonished': 22741, 'jp': 16545, 'olson': 21583, 'tuesday': 9857, '##brook': 9697, '##ava': 12462, 'comrades': 19033, 'paint': 6773, 'azores': 28320, 'butcher': 14998, '発': 1914, 'heroine': 18869, 'ind': 27427, 'produces': 7137, 'managerial': 24465, 'travels': 7930, 'vertical': 7471, '[unused4]': 5, 'mandela': 26887, 'etienne': 16821, 'cas': 25222, 'nomination': 6488, 'oct': 13323, '[unused106]': 111, 'superstructure': 28391, 'faber': 21720, '2000s': 8876, '##永': 30422, 'pays': 12778, 'श': 1336, 'calmed': 20869, 'guernsey': 24640, 'crippled': 24433, '##l': 2140, 'bag': 4524, '[unused463]': 468, '政': 1860, 'technician': 16661, '##kot': 23342, 'libertarian': 19297, 'arte': 16185, '##zo': 6844, 'manifesto': 17124, 'atletico': 16132, '##sil': 27572, '←': 1583, '##ry': 2854, 'nonstop': 25493, '##mei': 26432, 'owes': 24381, 'suddenly': 3402, 'np': 27937, 'mont': 18318, 'house': 2160, '107': 10550, 'holder': 9111, 'downwards': 28457, 'halfway': 8576, 'tattoos': 18395, 'crest': 11146, 'handicapped': 26920, '[unused736]': 741, '[unused527]': 532, 'worker': 7309, 'ronnie': 11688, 'namesake': 17283, 'staffed': 21121, 'contra': 24528, '##usions': 22016, 'crumbling': 24827, 'involves': 7336, 'slowed': 9784, 'trams': 19216, 'hindi': 9269, '##ula': 7068, 'bodyguards': 25681, 'individuals': 3633, '##long': 10052, 'linebacker': 15674, 'pueblo': 18273, '##bek': 24597, 'anhalt': 27088, 'silas': 18553, '##吉': 30319, 'laid': 4201, 'deposition': 19806, 'antoine': 12445, 'ordered': 3641, 'oceanic': 18955, 'steppe': 29096, '##rg': 10623, 'vaughan': 14461, 'align': 25705, '54th': 29570, '##マ': 30249, 'travelling': 8932, '1833': 11040, 'squared': 19942, '##ᅮ': 30014, '[CLS]': 101, 'ile': 17869, 'paralyzed': 22348, 'harvested': 22629, '##sum': 17421, '##jal': 25787, 'ft': 3027, 'diplomatic': 8041, 'hysterical': 25614, 'used': 2109, 'noticing': 15103, '##ghton': 21763, 'sufi': 21845, '##kins': 14322, 'malmo': 23643, 'departments': 7640, 'phoenix': 6708, '##rio': 9488, 'internet': 4274, 'douglas': 5203, 'mandir': 22700, '##kovich': 28195, 'blinds': 28279, 'attach': 22476, 'primitive': 10968, '630': 23609, '1610': 25800, 'defeated': 3249, 'eater': 28496, 'instinct': 12753, 'thoughtfully': 19897, 'scattered': 7932, 'colder': 21399, 'yarn': 27158, 'sacks': 14918, 'motorola': 29583, 'facing': 5307, 'ே': 1398, 'illuminated': 14640, 'dummy': 24369, 'dissolve': 21969, '##ves': 6961, 'occupy': 11494, 'chevalier': 22019, 'unincorporated': 7754, 'sync': 26351, '##pipe': 24548, 'chocolate': 7967, '##verted': 26686, 'ain': 7110, 'varied': 9426, 'conceded': 15848, '##ps': 4523, 'optimization': 20600, 'electors': 19165, 'kits': 18628, '##tter': 12079, '##mber': 21784, 'php': 25718, 'preseason': 20038, 'zimmerman': 27946, 'igor': 14661, 'roderick': 28326, '忄': 1850, 'pen': 7279, '##rber': 20473, 'blurred': 18449, '↔': 1587, '##mia': 10092, '##cle': 14321, 'ushered': 23815, 'blackened': 25788, '165': 13913, 'ρ': 1171, 'grouped': 15131, 'lancaster': 10237, 'color': 3609, 'replacing': 6419, '##bution': 29446, 'nan': 16660, '##mc': 12458, 'insisting': 22604, 'concrete': 5509, '##ils': 12146, 'lesbian': 11690, 'א': 1241, 'row': 5216, 'sands': 13457, 'lacked': 10858, 'sweden': 4701, '##lism': 28235, '##南': 30311, '128': 11899, 'applicable': 12711, 'radiating': 23229, 'wrexham': 27119, 'reefs': 21484, '39th': 22702, 'elevation': 6678, 'invisible': 8841, 'aryan': 26030, 'flint': 13493, '[unused653]': 658, 'steam': 5492, '##van': 6212, 'deepened': 20183, 'stronghold': 16995, 'skate': 17260, '##mm': 7382, '##ি': 29915, 'captivity': 16187, 'joachim': 17286, 'stalking': 20070, 'disrupted': 20275, 'wake': 5256, 'pointed': 4197, 'sympathetic': 13026, 'sabrina': 21876, 'ronald': 8923, 'let': 2292, 'originals': 23728, 'œ': 1107, 'walled': 17692, 'graeme': 21840, '##र': 29869, '[unused327]': 332, '[unused60]': 61, '1661': 24046, 'eureka': 21142, '##abad': 10542, 'waterway': 23668, 'fine': 2986, 'designations': 26672, 'fling': 27655, 'bend': 8815, '##erly': 12561, 'composers': 9929, 'insanity': 19272, 'maclean': 22528, 'neither': 4445, '##ش': 29825, 'ravi': 16806, 'sophie': 8234, 'iconic': 14430, 'persians': 27229, 'bailey': 8925, 'trickle': 26027, 'excelled': 23081, 'dutch': 3803, 'cottage': 9151, '1994': 2807, 'nilsson': 29376, 'nurses': 11500, 'photography': 5855, '##working': 21398, 'shaded': 25273, 'dodged': 28339, '##э': 29756, 'maneuver': 17519, 'targets': 7889, 'pilot': 4405, 'southend': 26104, '[unused922]': 927, 'erasmus': 26809, 'lacking': 11158, '##good': 24146, 'reporters': 12060, 'shove': 14738, 'cara': 14418, '##suit': 28880, 'moor': 16808, '##re': 2890, 'less': 2625, '##zow': 22670, 'worry': 4737, 'declined': 6430, 'violation': 11371, 'chapel': 4970, 'docking': 25776, 'card': 4003, 'burden': 10859, '##berto': 21201, 'christians': 8135, 'rug': 20452, 'pointless': 23100, 'goddamn': 16477, 'probing': 28664, '[unused386]': 391, '##50': 12376, 'sandwiches': 22094, 'stretch': 7683, '##basket': 25351, 'spite': 8741, 'defects': 18419, 'thrown': 6908, 'procedures': 8853, 'accidents': 13436, 'kabul': 21073, '##tation': 12516, 'pau': 29025, '##rson': 17753, '##bus': 8286, 'wizards': 16657, 'kwan': 27741, 'unspecified': 25851, 'gates': 6733, 'turkic': 22926, 'vt': 28879, 'simpsons': 19047, 'perhaps': 3383, 'valleys': 12467, '##川': 30361, '##uter': 19901, 'battleship': 17224, '1927': 4764, '##loh': 24729, 'galley': 27124, 'warmth': 8251, 'sous': 27411, '##sville': 9337, 'կ': 1227, 'kidnapping': 15071, 'quaker': 18844, 'albeit': 12167, 'blush': 16688, 'kiran': 22403, 'winner': 3453, 'barnsley': 25139, 'criminal': 4735, 'physicist': 13702, 'verified': 20119, 'carvings': 22838, '1733': 27230, 'reflections': 16055, 'very': 2200, 'variables': 10857, 'relocate': 20102, 'haired': 10681, 'robinson': 6157, '##print': 16550, '##q': 4160, 'luxurious': 20783, 'office': 2436, 'algeria': 11337, 'sh': 14021, 'ventral': 23420, '##vere': 28943, 'angles': 12113, 'orphan': 18211, 'addition': 2804, '##rations': 28893, 'armenians': 20337, 'feasibility': 24010, 'claiming': 6815, 'certainly': 5121, 'textbooks': 18841, 'zipper': 22082, '##erin': 23282, 'dick': 5980, '##sco': 9363, '1921': 4885, '##dro': 22196, 'masonic': 21152, 'ᴵ': 1493, 'upheld': 16813, 'recently': 3728, '##bs': 5910, 'otago': 21831, '[unused14]': 15, 'marion': 10115, 'councils': 10784, 'frequent': 6976, 'hideous': 22293, 'philosopher': 9667, '##sul': 23722, '[unused189]': 194, 'pitching': 14696, '##f': 2546, '§': 1073, '##mbe': 18552, 'taught': 4036, 'tags': 22073, 'sad': 6517, '##personal': 28823, 'fairbanks': 24502, '##k': 2243, '##zio': 12426, 'aug': 15476, 'jan': 5553, '##⇒': 30119, 'haifa': 21303, '##rrell': 14069, 'auditioned': 23008, 'heavens': 17223, '[unused673]': 678, 'abundant': 12990, 'volunteer': 6951, 'semantic': 21641, '[unused321]': 326, 'emma': 5616, '##sitor': 28307, '##ᵖ': 30040, '##amo': 22591, 'dane': 14569, 'porte': 25927, 'straps': 19702, 'mahogany': 23867, 'stature': 21120, 'tendencies': 20074, 'steering': 9602, 'setting': 4292, 'potomac': 18854, 'crushed': 10560, 'scripture': 18919, 'nearby': 3518, 'southwest': 4943, '[unused775]': 780, 'ancestry': 11377, 'fu': 11865, 'harrington': 19760, 'lawsuit': 9870, 'temporary': 5741, 'evolved': 7964, 'wrestlers': 14039, 'repulsed': 24571, 'downward': 14047, '##hes': 15689, 'nearing': 23454, 'finnish': 6983, 'gdansk': 21942, 'uniquely': 20640, 'inhuman': 29582, '##die': 10265, '##le': 2571, 'outpost': 21080, 'fashion': 4827, '##agger': 27609, '##ʀ': 29693, 'extraordinary': 9313, 'steamer': 18027, '##rooms': 29020, 'guide': 5009, '[unused147]': 152, 'method': 4118, 'knock': 7324, 'vehicle': 4316, 'torre': 22047, 'americans': 4841, '的': 1916, 'nh': 18699, '##aa': 11057, 'sit': 4133, 'vilnius': 20513, 'musique': 25784, 'waterfalls': 24236, 'puppets': 26101, 'glancing': 10167, '[unused730]': 735, 'valves': 17355, 'karl': 6382, 'writer': 3213, '1660': 17954, 'anchors': 24674, '1200': 14840, '[unused652]': 657, 'cruel': 10311, 'notation': 14869, '##86': 20842, 'saddle': 12279, 'beaumont': 16210, 'stepped': 3706, '[unused694]': 699, 'disputes': 11936, 'gujarat': 14288, 'shift': 5670, 'choice': 3601, 'disliked': 18966, 'lei': 26947, '##harat': 28074, 'loosen': 29476, 'sabotage': 20223, 'only': 2069, '[unused337]': 342, 'rank': 4635, 'climbing': 8218, '##vino': 26531, 'transcribed': 26223, 'hokkaido': 20826, 'suspend': 28324, '##™': 30108, '[unused568]': 573, 'danny': 6266, 'inventory': 12612, 'families': 2945, 'esther': 14631, '##oned': 17799, 'popularity': 6217, 'guise': 21980, 'league': 2223, 'algorithm': 9896, 'brandt': 19407, 'airports': 13586, 'injustice': 21321, '##ena': 8189, 'accumulation': 20299, 'shovel': 24596, '##gence': 17905, 'benign': 28378, 'benefited': 19727, 'sitting': 3564, 'governed': 9950, 'rack': 14513, 'iss': 26354, '##hre': 28362, 'jerseys': 28772, '##₀': 17110, 'enlisted': 9417, 'arabs': 14560, 'sinister': 16491, 'fresno': 20840, 'wits': 25433, 'episcopal': 9134, 'reproductive': 15124, 'featherweight': 27145, '##″': 30067, 'kiara': 28870, 'manor': 6952, '##cars': 20745, 'hostages': 19323, 'wembley': 16538, '##agawa': 20812, 'donetsk': 29151, 'keep': 2562, 'rfc': 14645, 'medina': 15761, 'improving': 9229, '##riz': 21885, 'acceleration': 16264, 'ea': 19413, 'murphy': 7104, 'sms': 22434, '##ikh': 28209, 'activities': 3450, 'computers': 7588, '[unused671]': 676, 'trees': 3628, 'sentinel': 16074, 'administer': 21497, '##len': 7770, 'alba': 18255, 'sweating': 18972, 'reunion': 10301, '[unused55]': 56, 'cupboard': 25337, 'madras': 12993, 'exactly': 3599, '1949': 4085, 'nash': 10594, 'credentials': 22496, '##？': 30520, 'village': 2352, 'occurring': 10066, '##dridge': 21482, '1609': 28058, '##udence': 29424, 'trains': 4499, 'joining': 5241, 'lumber': 13891, '##ech': 15937, '##pore': 26691, 'realized': 3651, 'strapped': 18019, '1801': 12410, 'dealings': 24069, '##nan': 7229, 'salvatore': 17485, 'breasts': 12682, 'delightful': 26380, 'taxis': 25964, 'clench': 26753, '##iti': 25090, 'arising': 17707, 'mum': 12954, 'necessary': 4072, 'visitor': 10367, 'hurriedly': 23878, 'signifies': 27353, 'originally': 2761, 'mandated': 16714, 'khmer': 19472, 'exploits': 20397, '##mussen': 29134, 'dripping': 14309, 'annoyance': 17466, '##fell': 23510, 'aquatic': 13582, '##lies': 11983, '##jhl': 29001, '##lib': 29521, 'have': 2031, '##ʻ': 29711, 'according': 2429, 'comedies': 22092, 'pushing': 6183, '##qui': 15549, 'conviction': 10652, '1866': 7647, '1768': 19793, 'inducted': 8120, '##ooped': 20671, 'transmitters': 26288, 'lectures': 8921, '##ious': 6313, '##bat': 14479, 'carnage': 27450, 'nedra': 28240, 'dependent': 7790, '##ར': 29970, '##‘': 30055, '##ruck': 29314, 'cleopatra': 22003, 'service': 2326, 'quartz': 20971, '##ner': 3678, 'bees': 13734, 'madeline': 16974, 'denoted': 19537, '[unused704]': 709, 'learned': 4342, 'foreword': 23059, 'residents': 3901, 'greenwood': 16827, 'hs': 26236, 'speechless': 25146, 'entirely': 4498, 'institutes': 12769, 'stands': 4832, 'near': 2379, 'freshly': 20229, 'secretaries': 23660, 'superhuman': 27061, '##well': 4381, 'hilton': 15481, 'establish': 5323, 'deborah': 15555, 'gameplay': 11247, 'sq': 5490, 'void': 11675, 'turner': 6769, 'rig': 19838, 'þ': 1101, 'ミ': 1724, '[unused178]': 183, '##さ': 30182, 'brushing': 12766, 'miracles': 17861, '##tania': 21013, '##icia': 24108, '##↑': 30112, 'ruben': 19469, 'adopting': 16151, 'compressor': 29329, 'discuss': 6848, 'majestic': 22337, '##(': 29619, 'competence': 22219, '##ₜ': 30099, '##ingly': 15787, '##oides': 25064, '##cating': 18252, 'belts': 18000, 'gaius': 19564, '##ccus': 27631, 'burlington': 15552, '##ailing': 29544, 'paraguay': 13884, 'weeds': 20777, 'swat': 25414, '##like': 10359, 'drink': 4392, 'peters': 12420, '##xton': 14226, 'specifications': 15480, 'animated': 6579, 'guns': 4409, '##venting': 26703, 'employed': 4846, '##baldi': 28807, 'nothin': 24218, '##ischen': 24488, '[unused835]': 840, '##roup': 22107, 'impetus': 27961, 'ག': 1426, 'asleep': 6680, 'resignation': 8172, '[unused2]': 3, 'regimes': 25228, 'loaned': 13190, 'rf': 21792, 'overlooked': 17092, 'shi': 11895, '##bians': 26376, '##meister': 28824, 'nave': 12847, 'understanding': 4824, 'bulgaria': 8063, 'saskatoon': 25447, 'diablo': 28841, 'footage': 8333, 'citation': 11091, '297': 27502, 'crouch': 21676, '1993': 2857, 'museum': 2688, '-': 1011, 'chin': 5413, 'theology': 8006, 'playfully': 22608, '##yl': 8516, 'stating': 5517, 'acknowledge': 13399, '##mp': 8737, 'plantation': 10065, 'nightclub': 15479, 'ornate': 18099, 'ubiquitous': 28498, 'grandmother': 7133, 'troupe': 16017, 'timer': 25309, '##pper': 18620, 'beatles': 11555, '##oma': 9626, 'forgot': 9471, '1845': 9512, 'bypassed': 27539, 'kosovo': 11491, '##ling': 2989, 'besides': 4661, 'elastic': 21274, '##relli': 22948, 'alumnus': 19678, 'soviets': 15269, '##ight': 18743, '158': 17696, '##roids': 29514, 'actions': 4506, '##hra': 13492, 'mick': 10872, 'hive': 26736, 'sickness': 15556, 'development': 2458, 'conform': 23758, 'timmy': 27217, 'satirical': 17251, 'eels': 29317, 'umbrella': 12977, '##ʁ': 29694, 'oversized': 21698, '##bahn': 16052, 'taboo': 27505, 'parrot': 22530, 'civilian': 6831, 'struggling': 8084, 'pharmaceuticals': 24797, 'craft': 7477, 'particularly': 3391, 'mackenzie': 11407, 'actively': 8851, 'salim': 27490, 'privately': 9139, 'protocols': 16744, 'jude': 12582, 'ᄂ': 1456, 'pas': 14674, '##itic': 18291, '##土': 30327, 'clipped': 20144, 'carl': 5529, 'integrating': 22380, 'arrests': 17615, '`': 1036, 'attested': 18470, '##nus': 10182, 'legal': 3423, 'grip': 6218, 'urgency': 19353, 'frost': 10097, 'slice': 14704, 'experiment': 7551, '##all': 8095, 'milwaukee': 9184, 'discus': 26047, 'rs': 12667, 'congratulations': 23156, 'ugly': 9200, 'darting': 24567, '##dies': 18389, 'fees': 9883, 'eden': 10267, 'ი': 1445, 'entered': 3133, 'embedded': 11157, 'tun': 27112, '##龍': 30508, 'ministries': 16410, 'excluded': 12421, 'mars': 7733, '##dicate': 16467, 'vikram': 29063, 'axe': 12946, '##jas': 17386, '##ior': 25346, 'galerie': 17941, 'viewer': 13972, '##eve': 18697, 'sanitary': 25480, 'speeds': 10898, '[unused966]': 971, 'bolivar': 22118, 'trajectory': 22793, 'grill': 18651, '##hwa': 18663, 'mass': 3742, '##orn': 9691, '26th': 14935, 'complain': 17612, 'chemical': 5072, 'epithet': 19626, 'capabilities': 9859, 'sealing': 23038, '288': 24841, 'radiation': 8249, 'labrador': 18604, '75th': 25092, 'advisor': 8619, '[unused415]': 420, 'firmly': 7933, 'negotiate': 13676, '##swick': 27720, 'monitoring': 8822, 'improvement': 7620, 'mandates': 25979, 'presumably': 10712, 'discreet': 29321, 'greens': 15505, 'violated': 14424, 'wow': 10166, 'ɡ': 1116, '##grass': 19673, 'org': 8917, 'amber': 8994, 'simi': 28684, 'conditioning': 14372, 'medication': 14667, 'coins': 7824, '##mable': 24088, '1772': 17483, '目': 1918, 'courageous': 26103, 'chronicle': 9519, '##ted': 3064, 'trade': 3119, 'goodman': 14514, '##rri': 18752, '##uss': 17854, '##ael': 21147, 'kimball': 26659, 'bulbs': 25548, 'futsal': 21921, '[unused325]': 330, 'loaf': 27048, 'arenas': 26434, 'china': 2859, 'hurricanes': 17035, 'prescription': 20422, 'hardship': 26479, 'environmental': 4483, 'crimea': 21516, '##uge': 22890, 'weaponry': 22711, 'historic': 3181, 'unrest': 16591, '##ucible': 21104, '##ভ': 29905, 'forwards': 19390, 'stefano': 19618, 'olivia': 7710, '##cen': 27524, '##lana': 16695, 'simulator': 25837, '##bre': 13578, 'whether': 3251, '57th': 28623, 'garde': 14535, 'horse': 3586, '##29': 24594, 'engage': 8526, '##tance': 26897, '##dier': 24612, 'michele': 15954, '1960s': 4120, '[unused828]': 833, 'shakespeare': 8101, '##bic': 13592, 'wanting': 5782, 'leapt': 13920, 'marrow': 24960, 'inlet': 15824, 'sawyer': 13975, 'trophy': 5384, 'lifelong': 13506, 'vega': 15942, '306': 24622, '##將': 30354, 'converts': 19884, 'uses': 3594, '[unused658]': 663, 'traveling': 7118, '##raction': 25533, 'landing': 4899, '′': 1531, 'scientist': 7155, 'recapture': 27639, '##pr': 18098, 'particles': 9309, 'sentenced': 7331, 'sizable': 25908, 'helicopter': 7739, 'growls': 27825, 'darius': 14861, 'annum': 28907, 'exploitation': 14427, 'м': 1191, 'len': 18798, 'bae': 25818, '1837': 9713, 'harcourt': 22714, 'commander': 3474, 'ま': 1677, 'nhl': 7097, 'yelled': 7581, '##lein': 19856, 'gore': 13638, 'fading': 14059, '##boarding': 21172, 'hades': 23003, 'mentoring': 29192, '##ович': 16198, 'enrico': 21982, 'fits': 16142, 'enabling': 12067, '[unused599]': 604, 'plot': 5436, 'gotta': 10657, '##borne': 19288, 'symphonic': 18957, 'jae': 22770, 'assassination': 10102, 'usable': 24013, 'usaf': 18531, 'alleging': 23294, '1911': 5184, 'nations': 3741, '##پ': 29839, 'fred': 5965, 'cadets': 15724, 'bei': 21388, 'son': 2365, 'nokia': 22098, 'bone': 5923, 'ang': 17076, 'jillian': 27286, 'viet': 19710, 'v8': 15754, '##lund': 18028, '##nding': 15683, 'ق': 1292, 'periphery': 23275, 'xml': 20950, 'foo': 29379, 'agency': 4034, '##cker': 9102, 'mc': 11338, '##ක': 29939, 'significantly': 6022, 'ץ': 1262, 'wreath': 29586, '##offs': 27475, 'lot': 2843, '##石': 30448, 'fabrics': 25123, 'tab': 21628, 'fills': 17469, 'sevilla': 29363, '##mbo': 13344, 'collingwood': 20044, 'boasts': 21979, '##ᵘ': 30042, 'musicians': 5389, 'suffering': 6114, 'tennessee': 5298, 'intense': 6387, 'neighbors': 10638, 'friend': 2767, 'partners': 5826, 'loire': 20399, '[unused964]': 969, 'refit': 27070, 'killers': 15978, 'gerald': 9659, 'spirit': 4382, 'fulfillment': 29362, 'mac': 6097, '##bie': 11283, 'willed': 22705, '##thes': 24138, 'listed': 3205, 'dept': 29466, 'rooster': 27681, 'beck': 10272, 'redesigned': 17051, 'г': 1183, 'experience': 3325, 'breeze': 9478, 'townships': 13991, 'babu': 20948, 'doubles': 7695, 'perpetual': 18870, 'hairs': 13606, '##icide': 21752, 'plates': 7766, 'batteries': 10274, 'incumbent': 7703, 'richardson': 9482, 'dim': 11737, 'malaysian': 11843, 'comparatively': 20172, 'ferrara': 28635, 'tammy': 19971, '##bbon': 27684, 'catalog': 12105, '##reen': 28029, '##rsk': 27472, 'fellow': 3507, 'impulse': 14982, 'heiress': 20020, 'convict': 20462, 'sustaining': 22663, '##ᵇ': 30033, '##borg': 11755, 'attractions': 13051, 'quivering': 26012, 'museo': 19713, 'legs': 3456, '##open': 26915, 'seeing': 3773, '[unused307]': 312, 'overseas': 6931, 'limo': 23338, 'pigment': 28815, '##’': 30056, 'scout': 7464, 'various': 2536, 'ש': 1266, 'sweeping': 12720, 'bruise': 24851, '[unused646]': 651, 'experimentation': 21470, '##rys': 24769, 'stark': 9762, 'skye': 16590, '##ired': 27559, 'eine': 27665, 'deserve': 10107, 'levine': 17780, 'saying': 3038, 'reformer': 24767, 'images': 4871, 'affects': 13531, 'cabins': 20321, 'introductions': 25795, '##fera': 27709, '##ₓ': 30092, 'straight': 3442, 'diminished': 15911, '##vir': 21663, 'believe': 2903, 'konrad': 22958, 'clerk': 7805, 'rochelle': 25649, 'worst': 5409, 'tenderly': 26596, '##cona': 24366, 'longing': 15752, 'vhs': 17550, '##ovich': 12303, 'kendra': 13812, 'thoroughbred': 18359, 'tree': 3392, '##hua': 14691, '60th': 20928, '糹': 1934, 'olivier': 14439, '##drick': 24092, 'lookout': 19052, 'fortress': 7841, 'archival': 22796, 'innate': 25605, 'masterpiece': 17743, 'occasionally': 5681, '##med': 7583, 'heroic': 14779, '£10': 26812, '##դ': 29769, 'theatres': 13166, 'chew': 21271, '##њ': 29763, 'absence': 6438, '##zh': 27922, '##try': 11129, 'capitol': 9424, 'husky': 18758, 'oxide': 15772, 'chennai': 12249, 'settler': 18556, 'musician': 5455, 'bearing': 7682, 'intuition': 26406, 'stairwell': 22109, '##iens': 24836, '##thest': 20515, '##nz': 14191, '##ィ': 30220, 'neutral': 8699, '##ト': 30240, '##ador': 26467, 'expression': 3670, 'researchers': 6950, 'bahadur': 21715, 'lies': 3658, 'patience': 11752, 'atmosphere': 7224, 'cooke': 16546, 'abc': 5925, 'metaphysical': 29081, 'tobacco': 9098, '##rger': 25858, 'ல': 1392, 'masses': 11678, '##nde': 13629, 'numbering': 15200, 'duck': 9457, 'floor': 2723, 'ই': 1349, '[unused99]': 104, '↑': 1584, '[unused618]': 623, 'casa': 14124, 'ל': 1253, 'marjorie': 21562, 'debates': 14379, 'evan': 9340, 'wilderness': 9917, 'insists': 16818, 'robbins': 18091, '##oro': 14604, 'warriors': 6424, 'rallies': 22867, '##ctic': 13306, 'foxes': 24623, 'hansen': 13328, 'confederates': 24627, 'amendments': 16051, '⊗': 1613, 'teased': 13074, 'ran': 2743, '[unused143]': 148, 'enrichment': 27226, '##lis': 6856, 'humphrey': 15462, 'warlock': 28861, 'included': 2443, 'afl': 10028, 'valve': 10764, 'atkinson': 18646, 'risks': 10831, 'locke': 18343, 'pens': 25636, '##onda': 29067, '##₊': 30086, 'whispering': 13550, 'mark': 2928, 'cayman': 26164, '##naire': 20589, 'burnham': 25295, 'uphold': 27329, 'these': 2122, 'interfaces': 19706, 'biennale': 21140, '##₱': 30103, 'sentencing': 23280, '##sb': 19022, '1935': 4437, 'cutting': 6276, 'may': 2089, '<': 1026, 'pact': 14790, 'poses': 22382, '花': 1940, 'single': 2309, '##iating': 15370, 'interred': 13917, 'promoters': 26512, 'household': 4398, 'roast': 25043, 'ritual': 8887, '[unused546]': 551, 'papal': 12156, 'ae': 29347, 'sunday': 4465, 'videos': 6876, 'string': 5164, 'tagged': 26610, 'labor': 4450, 'sheltered': 18304, 'radio': 2557, '##gnan': 28207, 'tortricidae': 27683, 'like': 2066, 'cot': 26046, 'impromptu': 29213, 'タ': 1709, 'albuquerque': 19334, '##dah': 18417, 'sahara': 19604, 'cooperate': 17654, '##ク': 30228, 'rm': 28549, '##inge': 23496, '##ᴬ': 30026, 'utilize': 16462, '##etano': 28752, 'venezuelan': 15332, 'mangrove': 29340, 'bridgeport': 27986, 'bukit': 29007, 'disposed': 21866, 'brushes': 22569, '##broken': 29162, '##ds': 5104, 'cn': 27166, 'violins': 25877, '##i': 2072, 'caucasian': 23368, '##taro': 28160, 'northernmost': 22037, 'welles': 23447, 'career': 2476, 'spy': 8645, '##y': 2100, 'cheltenham': 18763, 'reassigned': 18026, 'km': 2463, 'effect': 3466, 'enzo': 26218, 'exact': 6635, '[unused919]': 924, 'monterrey': 26843, 'wearing': 4147, 'nora': 12306, 'batting': 9640, 'thrusts': 25842, 'mechanic': 15893, 'aaron': 7158, 'governorate': 15162, '##jali': 28948, '##channel': 26058, 'dug': 8655, 'bubba': 19606, 'cuff': 26450, 'cambridgeshire': 24197, 'phones': 11640, '##gard': 13444, 'command': 3094, 'cigarette': 9907, 'slater': 17916, '##nne': 10087, 'kw': 6448, 'oblivion': 24034, 'shirt': 3797, 'jiangsu': 28091, 'brigadier': 9900, 'laird': 21964, 'reagan': 11531, 'rested': 8614, '##placed': 22829, '##ɡ': 29682, 'bureau': 4879, 'yang': 8675, '[unused649]': 654, 'sailor': 11803, '1648': 22533, 'bored': 11471, 'runaway': 19050, 'avoided': 9511, '##⁄₄': 27392, 'ignatius': 26841, 'debating': 20767, 'competes': 14190, 'infrastructure': 6502, 'hovered': 18190, 'crows': 21623, 'meet': 3113, '子': 1816, 'hammersmith': 28420, 'circulating': 22458, 'shin': 12277, 'illustrations': 11249, 'this': 2023, 'residential': 5647, '##riding': 21930, 'bowed': 11489, 'barker': 12852, 'bourbon': 15477, 'sanitation': 18723, 'leaving': 2975, '##inal': 13290, '[unused908]': 913, 'castles': 15618, 'fat': 6638, '[unused829]': 834, 'libyan': 19232, 'scream': 6978, 'anticipation': 11162, 'hess': 23484, '##β': 29720, 'distract': 15886, 'juniors': 16651, '1820': 11102, 'hundreds': 5606, 'bye': 9061, 'rochdale': 26109, 'surgery': 5970, 'ท': 1412, 'controlling': 9756, 'reddy': 18998, '##alla': 25425, '[unused569]': 574, 'cyprus': 9719, '##dam': 17130, 'brendan': 15039, 'persistent': 14516, 'abraham': 8181, 'shrill': 28349, 'owl': 13547, '★': 1620, 'und': 6151, 'considerably': 9839, 'structures': 5090, 'humid': 14178, 'burning': 5255, '##ista': 11921, 'labeled': 12599, 'before': 2077, 'properties': 5144, 'nepal': 8222, 'mei': 19734, 'subfamily': 10946, '##cit': 26243, '##arte': 24847, '##kyu': 23076, 'witnesses': 9390, '##tone': 5524, 'smackdown': 22120, 'fades': 26784, '##mann': 5804, 'waist': 5808, 'alerted': 22333, '##iculate': 24153, 'harm': 7386, '/': 1013, 'carbon': 6351, 'sd': 17371, 'threw': 4711, 'bow': 6812, 'successful': 3144, 'while': 2096, 'heroism': 27117, 'y': 1061, 'documented': 8832, 'deformation': 29130, 'berg': 15214, '##ogist': 22522, 'delegate': 11849, 'demonstrations': 13616, 'dfb': 28894, 'incarcerated': 23995, 'sy': 25353, '##rave': 22401, 'focusing': 7995, 'lgbt': 12010, '##hari': 18428, 'man': 2158, 'begs': 27591, 'cuthbert': 24583, 'unlikely': 9832, 'hum': 14910, 'ismail': 19214, 'trading': 6202, '##bid': 17062, 'spread': 3659, 'madison': 7063, '##خ': 29821, 'santana': 21158, '[unused51]': 52, '##elling': 23918, 'barack': 13857, 'adopted': 4233, 'swimmers': 21669, 'nighttime': 26031, '^': 1034, 'aspirations': 22877, '##lbert': 23373, 'iii': 3523, 'advice': 6040, '##lid': 21273, '##-': 29624, '##main': 24238, '[unused466]': 471, '##zes': 11254, 'gestures': 18327, 'scripted': 22892, 'twitter': 10474, 'disconnected': 23657, '##rdy': 17460, '道': 1957, 'purely': 11850, 'gestured': 11574, '##rral': 24988, 'aromatic': 25496, 'registration': 8819, 'nut': 17490, 'dump': 15653, 'danzig': 26669, '##qu': 28940, 'ᅲ': 1480, 'layout': 9621, 'shuffle': 23046, '##aire': 14737, 'pavement': 14271, 'edible': 21006, '##ress': 8303, 'compliance': 12646, '[unused282]': 287, 'possible': 2825, '1709': 28955, 'aftermath': 10530, '##als': 9777, 'excuses': 21917, 'mojo': 28017, 'sources': 4216, 'vatican': 12111, '##overs': 24302, '##erian': 27549, 'mahmoud': 27278, '##fe': 7959, 'imprisonment': 10219, '##rred': 20529, 'goods': 5350, 'harald': 20966, 'captive': 12481, 'social': 2591, 'sandals': 24617, '##rest': 28533, '##thic': 23048, '##tine': 10196, 'concerts': 6759, '##ien': 9013, 'salty': 23592, 'jimmie': 24671, '206': 18744, 'poised': 22303, 'kanye': 29270, '##〜': 30171, 'raion': 24235, '##rmi': 28550, 'aquarium': 18257, 'vinnie': 24214, 'wounded': 5303, 'ideal': 7812, '##front': 12792, 'ghosts': 11277, 'downs': 12482, '##idi': 28173, 'bust': 13950, '##rang': 24388, 'hahn': 24266, '##tty': 15353, 'organist': 16397, 'elmer': 21464, 'ceiling': 5894, 'visibly': 19397, 'requesting': 17942, 'ӏ': 1218, 'brasil': 21133, 'endless': 10866, '≥': 1609, '##trics': 29392, '##horpe': 22044, 'toe': 11756, 'lid': 11876, 'dem': 17183, 'disappear': 10436, 'pathogen': 26835, 'raymond': 7638, 'ponce': 21085, 'atlanta': 5865, '##therapy': 20900, 'meng': 27955, 'mona': 13813, '##us': 2271, 'spectacle': 21177, '[unused670]': 675, 'compatible': 11892, 'optimistic': 21931, 'inadvertently': 21089, '[unused596]': 601, 'њ': 1216, 'nightly': 22390, 'cyber': 16941, 'adam': 4205, '##cup': 15569, 'shocks': 28215, '##ase': 11022, 'hit': 2718, 'diplomats': 23473, 'likelihood': 16593, 'ashby': 28729, '##arns': 23549, 'barking': 19372, '[unused651]': 656, 'jameson': 22324, 'morley': 20653, 'smug': 20673, 'make': 2191, '##oot': 17206, 'incorporated': 5100, 'asa': 17306, 'electronics': 8139, 'nike': 18368, 'sho': 26822, '##vington': 21827, '##illy': 20577, '##@': 29633, 'enemies': 6716, 'cellular': 12562, 'distributors': 22495, 'nailed': 26304, 'sisters': 5208, 'contents': 8417, 'gloves': 11875, 'fined': 16981, '##₁': 11871, '##ury': 13098, 'dependency': 24394, '##オ': 30225, '##eo': 8780, 'dex': 20647, '660': 20982, 'empty': 4064, 'architects': 8160, 'acton': 28794, 'hearst': 25419, 'traditions': 7443, '##sies': 14625, 'sahib': 23513, 'looted': 27775, 'mouth': 2677, 'forever': 5091, 'υ': 1175, 'exhibitions': 8596, 'drainage': 11987, '##tate': 12259, 'brains': 14332, 'pigs': 14695, 'propelled': 15801, 'welcomed': 10979, '1787': 16057, 'coil': 17085, 'vanilla': 21161, 'pee': 21392, 'fuel': 4762, 'appendix': 22524, 'accessibility': 23661, 'protruding': 23868, 'childhood': 5593, 'dependence': 18642, 'floats': 24885, 'complaint': 12087, 'priority': 9470, '##rdial': 25070, '1940': 3878, 'hence': 6516, '##ld': 6392, '##mori': 24610, 'consecration': 24730, '##rgh': 27172, 'crawled': 12425, 'instability': 18549, '1650': 21875, '##tai': 15444, '##sham': 21010, 'formats': 11630, 'unsuccessful': 7736, '##if': 10128, '##phs': 18757, 'mcpherson': 24332, 'discourage': 28085, 'award': 2400, 'p': 1052, 'edwards': 7380, 'sega': 16562, 'know': 2113, 'cortex': 17132, 'galician': 28830, 'medalist': 12968, 'compound': 7328, 'happy': 3407, 'phd': 8065, 'eventually': 2776, 'anticipating': 26481, '##ʉ': 29697, 'sant': 15548, 'radial': 15255, '##fixed': 23901, 'tq': 28816, 'drummer': 7101, 'bafta': 22284, '##bial': 21102, 'levy': 12767, '##omy': 16940, 'batsman': 13953, 'mid': 3054, 'endemic': 7320, 'favorites': 20672, 'lehman': 28444, 'warm': 4010, '##յ': 29777, 'silky': 18848, 'heir': 8215, 'dobson': 27679, 'christian': 3017, 'anglican': 9437, 'lincoln': 5367, 'stir': 16130, '##tian': 10772, 'injunction': 22928, 'dragon': 5202, 'claimed': 3555, '1946': 3918, '277': 25578, '52': 4720, '##rricular': 21231, 'thrusting': 21468, 'longed': 23349, 'blinked': 7948, 'developing': 4975, '##و': 29836, 'framed': 10366, '良': 1938, 'sophisticated': 12138, 'photos': 7760, 'phased': 21718, '[unused176]': 181, 'singer': 3220, 'geelong': 18664, 'judy': 12120, 'music': 2189, '##tism': 17456, 'lashed': 25694, 'tony': 4116, 'flora': 10088, 'marino': 17185, 'saudi': 8174, '##gingly': 28392, 'dropped': 3333, '##station': 20100, '##ammed': 27479, 'hopper': 20517, 'rainfall': 10101, '[unused971]': 976, 'wines': 14746, 'cutler': 24975, 'brutal': 12077, 'skipping': 25978, '##า': 29958, 'allegiance': 14588, 'campaigning': 18524, 'reformed': 9114, 'hardly': 6684, 'occupation': 6139, '##阝': 30496, 'baku': 16807, 'psychic': 12663, 'celebrates': 21566, '##hire': 20908, 'drunken': 15967, 'sauce': 12901, 'rossi': 18451, 'mary': 2984, 'questions': 3980, 'cries': 12842, 'signed': 2772, 'district': 2212, 'blacks': 10823, '##toy': 29578, '##zer': 6290, '[unused669]': 674, 'peacefully': 21614, 'happens': 6433, 'kings': 5465, 'performers': 9567, 'tabloid': 24173, '650': 13757, 'uci': 14504, '[unused451]': 456, 'filly': 22062, 'laureate': 17656, 'healy': 25706, 'email': 10373, 'bis': 20377, 'smeared': 25400, '1704': 27769, '[unused521]': 526, 'retirement': 5075, 'caucasus': 16512, 'duplicate': 24473, '宮': 1824, 'alcoholism': 25519, 'tarzan': 24566, 'taxi': 10095, 'dawson': 11026, '54': 5139, 'jimmy': 5261, '##rcus': 29006, 'responding': 14120, 'josef': 12947, '##holder': 14528, 'foothills': 18455, 'altitude': 7998, 'liza': 20503, 'wrapping': 12252, '##eit': 20175, 'individually': 14258, '##名': 30321, 'stinging': 22748, 'ribbon': 10557, '[unused441]': 446, 'caroline': 7981, 'ᵗ': 1505, 'chaplin': 23331, '##olf': 28027, 'alabama': 6041, 'hacking': 23707, 'blair': 10503, '##oked': 23461, 'wand': 23967, '##rek': 16816, 'skyline': 21343, 'pamphlets': 24752, 'afforded': 22891, 'regulates': 26773, 'executions': 22679, 'missile': 7421, '##ets': 8454, 'resign': 12897, '##izing': 6026, 'acquisition': 7654, '##som': 25426, 'professionals': 8390, 'prosecutor': 12478, 'infinity': 15579, 'mollusk': 13269, '##isi': 17417, 'tuning': 17372, 'supervisory': 26653, 'travers': 29053, 'portals': 27388, 'burroughs': 25991, 'groove': 14100, 'dice': 18740, 'determination': 9128, 'johor': 25268, 'tractors': 28292, 'discussions': 10287, 'buddy': 8937, 'eng': 25540, 'nylon': 27201, 'followed': 2628, 'standings': 11869, 'unified': 10562, 'brittany': 12686, 'flickered': 15999, 'excavations': 14018, 'chosen': 4217, '##itive': 13043, '[unused584]': 589, 'cheap': 10036, 'galleries': 11726, 'gypsy': 16006, 'officials': 4584, 'dryly': 20825, 'broadway': 5934, 'ride': 4536, 'litigation': 15382, 'wanda': 20848, 'incapable': 19907, 'unicef': 29073, 'zealand': 3414, 'frey': 20068, 'environmentally': 25262, 'ち': 1662, '##ulo': 18845, 'flooding': 9451, 'consistently': 10862, 'costa': 6849, '[unused524]': 529, 'anthropologist': 21571, 'denim': 26762, 'sneaking': 20727, 'burnett': 13829, 'blessing': 13301, 'maldives': 25059, 'details': 4751, 'shiny': 12538, 'seconds': 3823, 'bulk': 9625, 'mori': 22993, 'syn': 19962, 'fluttering': 25001, 'regulators': 25644, '##vinsky': 27704, '##cultural': 29415, 'laughed': 4191, 'intersect': 29261, '[unused152]': 157, 'sami': 17015, 'withdrawn': 9633, '##ψ': 29738, 'fascination': 18987, 'lorenzo': 12484, 'י': 1250, 'judicial': 8268, 'crucial': 10232, '陳': 1972, '．': 1991, 'representation': 6630, 'genetically': 19345, 'sweater': 14329, 'tightly': 7371, '##quest': 15500, 'freddy': 19343, 'corrosion': 24625, 'soldier': 5268, 'watching': 3666, 'liked': 4669, 'alongside': 4077, '##ي': 14498, '##chman': 19944, 'catfish': 23723, 'ganga': 28646, '100': 2531, 'sullivan': 7624, 'counts': 9294, 'delays': 14350, 'appealed': 12068, '##abi': 28518, 'clinical': 6612, 'nottinghamshire': 20126, 'stepmother': 26959, 'ч': 1202, '##yeh': 25673, '##μ': 29728, 'dodge': 11898, '##．': 30517, 'firearms': 13780, 'engraved': 16328, '##dee': 26095, 'enable': 9585, '##pins': 27915, 'milk': 6501, 'courage': 8424, 'danced': 10948, 'odi': 21045, '##ワ': 30262, 'niger': 16842, '##heart': 22375, 'multicultural': 27135, '[unused369]': 374, 'communicating': 20888, '##ologist': 8662, 'cruise': 8592, 'earning': 7414, '##ibly': 17296, 'berry': 10498, 'sienna': 20210, 'formula': 5675, 'yorker': 19095, '##lles': 20434, 'misery': 14624, '1760': 19096, 'amor': 16095, '王': 1909, 'inuit': 25179, '##ab': 7875, 'illustrates': 24899, '⁵': 1539, 'loss': 3279, 'rahman': 14364, 'complications': 12763, 'honey': 6861, 'sensory': 16792, 'briefcase': 21793, '##alic': 27072, 'receiver': 8393, 'wondering': 6603, 'dignitaries': 27960, 'messengers': 28938, 'food': 2833, 'qui': 21864, '##tr': 16344, '®': 1079, '##♠': 30150, 'semifinal': 16797, 'holt': 12621, 'nak': 17823, 'whereupon': 26090, '[unused299]': 304, '[unused338]': 343, 'armoured': 11104, 'crooked': 15274, '##⁷': 30076, '##rd': 4103, 'jokes': 13198, 'purge': 24694, '[unused352]': 357, 'slaughtered': 23044, 'fargo': 23054, 'mx': 25630, 'settle': 7392, 'comical': 29257, 'encoding': 17181, 'josie': 15293, 'corridor': 7120, 'arranger': 13981, 'wealthy': 7272, 'gas': 3806, '[unused572]': 577, 'threatens': 17016, 'falcons': 14929, 'zodiac': 28501, '##ո': 29779, '1864': 6717, 'investor': 14316, 'occupies': 14133, 'hulk': 16009, 'hitting': 7294, 'bodies': 4230, '##ver': 6299, 'cuban': 9642, '##14': 16932, 'burundi': 28836, 'tightened': 8371, 'jefferson': 7625, 'lap': 5001, 'morning': 2851, 'kemp': 20441, 'optimal': 15502, '##河': 30425, '51st': 26017, 'secret': 3595, 'dive': 11529, 'sweetie': 22872, 'east': 2264, 'admit': 6449, 'airing': 10499, 'activate': 20544, '1580': 28278, '##boot': 27927, 'nm': 13221, '##var': 10755, 'broom': 23528, 'anthony': 4938, '##urg': 12514, 'edinburgh': 5928, '911': 19989, 'legislatures': 27977, '##ε': 29723, 'revived': 10570, '##อ': 29957, '##འ': 29969, 'initiated': 7531, '⅔': 1582, 'iraqi': 8956, '##ease': 19500, 'devastated': 13879, '##vos': 19862, 'yours': 6737, 'musa': 23154, '##gna': 16989, 'crawford': 10554, 'indiana': 5242, '##erate': 22139, '##atin': 20363, 'slang': 21435, 'hayes': 10192, 'owe': 12533, 'reed': 7305, 'و': 1298, 'liberal': 4314, 'argument': 6685, 'columbus': 8912, '##que': 4226, '##ュ': 30255, 'africa': 3088, 'emilia': 20417, 'observatory': 9970, 'integers': 24028, '##mura': 16069, 'midday': 22878, '##sius': 24721, 'nitrate': 29607, 'johansson': 26447, 'remotely': 19512, 'spaghetti': 26666, 'barrow': 15355, '##llet': 22592, '##pani': 26569, 'bob': 3960, 'gearbox': 22227, 'မ': 1437, 'auditor': 20964, '[unused620]': 625, 'largest': 2922, 'evening': 3944, 'usl': 22448, 'publishes': 12466, '##kled': 19859, 'delaney': 22101, 'jai': 17410, 'sikhs': 26697, 'rear': 4373, 'proponents': 20401, 'retrieved': 5140, 'racehorse': 25068, '##ᵗ': 30041, '##orous': 25373, 'vila': 23840, 'autumn': 7114, 'solo': 3948, '##film': 23665, 'sammy': 14450, 'tongues': 19677, '1780': 15051, 'ferocious': 27863, 'lilith': 25576, 'bandit': 25334, 'scanner': 26221, 'juliet': 13707, '[unused368]': 373, 'sack': 12803, 'ox': 23060, 'briggs': 15487, 'cancelled': 8014, 'villains': 16219, 'ssr': 20896, 'tasting': 18767, 'class': 2465, 'cavalry': 5945, 'kenya': 7938, 'pope': 4831, 'alien': 7344, 'flux': 19251, 'gable': 13733, '##ster': 6238, '##llary': 24435, 'florian': 29517, 'astronomers': 26357, '##heimer': 18826, 'ellis': 8547, '##woods': 25046, '##iology': 20569, 'spice': 17688, 'sense': 3168, '##sworth': 12255, 'crowds': 12783, 'equilibrium': 14442, 'agriculture': 5237, '##ী': 29916, 'sarcasm': 20954, '##jima': 19417, 'black': 2304, '##idia': 29342, 'walking': 3788, 'reggae': 15662, '199': 20713, 'juliana': 24157, 'royal': 2548, 'parsons': 13505, 'gulped': 25411, 'inside': 2503, 'pre': 3653, 'inactivated': 15663, 'gunmen': 28932, 'kingsley': 22819, 'lands': 4915, 'radioactive': 17669, 'numb': 15903, 'sponsoring': 29396, 'kiel': 20963, 'seeds': 8079, 'between': 2090, 'dedicated': 4056, 'est': 9765, '149': 17332, 'sensors': 13907, 'arguably': 15835, '葉': 1943, 'gender': 5907, 'ign': 16270, 'tape': 6823, 'rainy': 16373, 'blackwell': 18776, 'drastically': 21040, 'bearded': 23905, '##জ': 29894, 'alice': 5650, 'nothing': 2498, '[unused913]': 918, '##bar': 8237, 'unsuccessfully': 11551, '##aux': 13754, 'myers': 13854, 'macmillan': 15434, 'english': 2394, '[unused100]': 105, 'chamber': 4574, 'switching': 11991, 'compare': 12826, '##cula': 19879, 'spoke': 3764, 'dowager': 20508, 'erwin': 22209, 'baha': 13253, 'surreal': 16524, 'pendant': 23152, 'hobbs': 23748, 'modeling': 11643, 'repression': 22422, 'humans': 4286, 'confederation': 11078, 'katz': 20729, '##wark': 24298, 'cornish': 17797, '##inking': 29377, 'paths': 10425, '##≈': 30133, 'virtues': 21560, 'creature': 6492, 'cockpit': 13828, 'erupted': 12591, '[unused156]': 161, 'ncaa': 5803, 'biting': 12344, 'indefinitely': 20733, '302': 22060, 'utc': 11396, 'trois': 25527, 'prevalent': 15157, 'graf': 22160, 'envisioned': 18035, 'tribes': 6946, 'skins': 21049, 'awhile': 19511, '112': 11176, 'francesca': 15409, 'qin': 19781, '##slin': 22908, 'celine': 24550, 'suarez': 22551, '##d': 2094, 'rex': 10151, 'sox': 9175, '##ously': 13453, 'prefecture': 7498, 'missionary': 8696, '1986': 3069, 'taste': 5510, 'slightly': 3621, 'vulnerable': 8211, '##wg': 27767, 'garrett': 9674, 'cancellation': 16990, 'convey': 16636, 'jang': 23769, 'algebra': 11208, 'tunnel': 5234, 'victorian': 6652, 'sergey': 22703, 'liberated': 19553, 'sporadic': 24590, 'billy': 5006, 'roberts': 7031, '##wicz': 21355, 'momentary': 29089, '##oya': 18232, 'precipitation': 13511, 'jessie': 10934, '##hunter': 25629, 'tom': 3419, '##urst': 29402, '[unused249]': 254, 'yamamoto': 28318, 'greenwich': 13861, 'livestock': 11468, 'ballard': 21896, 'ª': 1076, '##ک': 29841, '##gara': 24864, '##ス': 30233, 'electric': 3751, 'waving': 12015, 'affiliation': 12912, 'commit': 10797, 'hydra': 26018, 'scene': 3496, 'upstairs': 8721, 'reverted': 16407, 'wessex': 27886, '218': 20741, 'maggie': 8538, 'punish': 16385, 'arboretum': 25197, 'ordering': 13063, 'supremacy': 22006, 'pieter': 23759, 'child': 2775, '11': 2340, 'cameras': 8629, 'portal': 9445, 'expedition': 5590, '##lav': 14973, 'hosting': 9936, 'antennae': 28624, 'stubborn': 14205, 'sail': 9498, 'illustrator': 13825, 'flop': 28583, 'apes': 27754, '##au': 4887, 'ج': 1275, 'vanguard': 18332, '##rco': 29566, 'creating': 4526, 'nominal': 15087, '1830s': 20400, 'honoured': 15546, 'bully': 20716, 'marge': 25532, 'grenades': 21914, 'celia': 18020, 'possess': 10295, 'brandon': 8825, '##rks': 19987, '[unused882]': 887, '##jm': 24703, '##deh': 25383, 'wolf': 4702, 'brewster': 23009, 'denis': 11064, '##kko': 22426, 'centre': 2803, 'chiang': 17684, 'completely': 3294, '##lai': 19771, 'expo': 16258, 'jaguars': 24017, '1973': 3381, 'adjunct': 20621, '##tale': 22059, '##ower': 25114, 'vampires': 6144, 'task': 4708, '##alo': 23067, 'scented': 25738, 'unnatural': 21242, 'embankment': 22756, 'referees': 25118, 'cafeteria': 16673, 'consonants': 19694, 'µ': 1085, 'asserting': 27644, 'mercantile': 25420, 'contamination': 18701, '##r': 2099, 'option': 5724, 'spilling': 18054, '##nesian': 20281, '##gaard': 18839, 'withdrawing': 21779, '690': 28066, '##np': 16275, '##秋': 30455, 'wharton': 24249, 'trips': 9109, 'transmission': 6726, 'won': 2180, 'horrible': 9202, 'riots': 12925, 'fk': 14352, 'moons': 23377, 'ho': 7570, 'tickets': 9735, 'judith': 12924, '##nden': 25915, 'hiring': 14763, 'callie': 20072, 'seventeenth': 15425, 'parents': 3008, 'remarried': 19316, 'strasbourg': 18104, '##ւ': 29785, '##drop': 25711, '##otide': 26601, '[unused360]': 365, 'slips': 17433, 'bucharest': 14261, 'craved': 25155, '141': 15471, 'tumors': 21434, 'becomes': 4150, 'allegro': 25319, 'jun': 12022, 'seventh': 5066, 'agnes': 11166, '##arian': 12199, 'mai': 14736, '##ng': 3070, 'turrets': 21088, 'deportivo': 23696, '##bly': 6321, 'assurance': 16375, 'revisited': 24354, 'degrees': 5445, 'alternatives': 15955, 'craving': 26369, 'wonder': 4687, 'covering': 5266, 'absent': 9962, 'breadth': 25291, 'worshipped': 22876, '##backs': 12221, 'yong': 18999, 'terminology': 18444, 'bingham': 24579, 'tallinn': 21169, 'strauss': 16423, 'mv': 19842, '##phus': 25255, 'ম': 1370, '##nett': 15361, 'sentai': 28650, 'judas': 25326, 'bush': 5747, 'sts': 8541, '[unused954]': 959, 'advocated': 11886, 'unspoken': 25982, 'alphabet': 12440, 'vintage': 13528, 'rover': 13631, '##ས': 29972, '[unused548]': 553, 'bluff': 14441, 'mongolia': 13906, '##lee': 10559, 'verdi': 20580, 'ballad': 11571, 'insulation': 25710, 'placing': 6885, 'contained': 4838, '##edance': 29605, 'hatch': 11300, 'weakly': 17541, '1731': 28446, 'jimenez': 22790, 'tunisia': 13437, '##ffi': 26989, 'curvature': 25045, 'reviewer': 12027, 'risen': 13763, 'bribery': 27748, '[unused837]': 842, 'ostensibly': 23734, 'neighboring': 8581, '1629': 27882, '##hel': 16001, 'qualified': 4591, 'contracting': 21012, 'tobago': 17247, '##hang': 18003, 'limits': 6537, 'mckay': 16225, '##gley': 22971, 'connie': 16560, 'dwyer': 29394, 'mentioning': 18625, '##bra': 10024, 'η': 1161, '##ಾ': 29938, '[unused485]': 490, 'rachel': 5586, '##arney': 25831, 'nascar': 11838, 'tyson': 19356, '35th': 20198, 'kerala': 8935, 'chemicals': 12141, '##tched': 28265, 'innsbruck': 28914, '##io': 3695, 'economy': 4610, '##ven': 8159, 'niece': 12286, 'om': 18168, 'molded': 27992, '##ius': 4173, '##gn': 16206, 'bundle': 14012, 'guarding': 17019, '[unused371]': 376, 'dave': 4913, '##antly': 15706, 'vile': 25047, 'happen': 4148, 'operational': 6515, 'peel': 14113, 'footprint': 24319, 'kazakh': 25907, 'sherwood': 19427, 'products': 3688, '##lifting': 26644, 'blinking': 15997, 'vitamin': 17663, 'diversion': 20150, '##ours': 22957, 'technical': 4087, 'tracker': 27080, 'honorary': 5756, '1816': 12357, 'hopeless': 20625, 'reapers': 27733, 'constantinople': 11776, '##hh': 23644, '##hia': 12995, '##chfield': 22693, 'lightning': 7407, '##ello': 15350, 'minorities': 14302, 'extinction': 14446, 'saturdays': 18860, 'kicker': 22652, 'armory': 24139, 'paler': 24489, 'graduation': 7665, 'announcement': 8874, 'catholics': 10774, 'hanover': 14393, 'allan': 8926, 'nostalgia': 26968, 'inherited': 7900, 'perspectives': 15251, 'coloration': 28757, 'decision': 3247, 'vegetarian': 23566, 'portrays': 17509, 'port': 3417, '##king': 6834, 'celeste': 21113, 'scandinavian': 17660, 'protagonists': 21989, 'cheated': 22673, '2006': 2294, '##24': 18827, 'elijah': 14063, 'macon': 20025, 'khalid': 21828, '##kr': 21638, 'sweep': 11740, 'artisans': 26818, 'carp': 29267, 'decided': 2787, 'relegated': 7049, 'itv': 11858, '##gram': 13113, 'midfield': 23071, '##titles': 27430, 'stoke': 13299, '##tua': 26302, 'led': 2419, 'tells': 4136, '##kong': 25460, '##eman': 16704, 'located': 2284, 'behavioral': 14260, 'ᅯ': 1479, 'dealers': 16743, 'underwear': 14236, 'investment': 5211, '##fusion': 20523, 'fulham': 21703, 'teaches': 12011, 'outward': 15436, 'cpi': 28780, 'antioch': 19078, 'consolidated': 10495, '##sas': 20939, '##lled': 11001, 'phantom': 11588, '1771': 20708, 'texans': 23246, 'infrared': 14611, 'rumbling': 26670, 'polish': 3907, '##ᅭ': 30013, 'preceded': 11677, '110': 7287, '2020': 12609, 'polymer': 17782, 'mccormick': 23213, 'okinawa': 15052, 'plains': 8575, 'telling': 4129, 'cal': 10250, 'kannada': 13873, '##ge': 3351, 'fuller': 12548, '[unused47]': 48, 'manchester': 5087, 'satan': 16795, 'stack': 9991, 'tanaka': 21829, 'songwriters': 20602, '##lands': 8653, '##aling': 21682, '[unused982]': 987, '##ite': 4221, 'myths': 17218, 'cb': 17324, '##mat': 18900, 'towering': 20314, 'jonny': 26937, 'hitch': 27738, '##nction': 27989, '##ᶜ': 30046, 'bram': 20839, 'category': 4696, '##voking': 22776, '##vu': 19722, 'exploit': 18077, 'waterfall': 14297, 'shut': 3844, 'ukrainian': 5969, 'lauren': 10294, 'beach': 3509, \"'\": 1005, '##tier': 17579, 'volvo': 21074, 'next': 2279, 'lithuania': 9838, 'potentially': 9280, 'pow': 23776, 'overhaul': 18181, 'connecting': 7176, 'penguins': 18134, 'collision': 12365, 'jim': 3958, 'secular': 10644, '##olin': 18861, 'ich': 22564, 'louie': 17438, 'vulcan': 25993, 'ios': 16380, 'ර': 1405, 'geometridae': 24687, 'resides': 11665, 'speculative': 23250, '##cio': 9793, 'aimed': 6461, 'nickel': 15519, 'behaviors': 15592, 'trout': 13452, 'knows': 4282, 'holdings': 9583, '##ih': 19190, 'shop': 4497, 'smirk': 15081, 'names': 3415, '##isch': 19946, '##nzo': 25650, 'hillsborough': 29330, 'objectives': 11100, '##uring': 12228, 'curls': 14484, 'evade': 26399, 'justify': 16114, 'tucson': 17478, '##cin': 15459, '[unused388]': 393, '3000': 11910, '##城': 30330, '##rified': 22618, 'cuckoo': 29010, 'inflicted': 17303, 'spending': 5938, '315': 22904, 'market': 3006, 'rookie': 8305, 'unpaid': 23850, 'bled': 23919, 'linguist': 22978, 'distinctive': 8200, 'pianos': 27864, 'interrupt': 17938, 'medial': 23828, 'happiness': 8404, 'surprise': 4474, 'competition': 2971, 'jane': 4869, '##tov': 26525, 'attackers': 17857, 'reassure': 24647, 'memorabilia': 28663, 'air': 2250, 'kenyan': 20428, 'expensive': 6450, 'announcements': 25674, 'kitchen': 3829, '[unused505]': 510, 'broadcaster': 11995, 'filtered': 21839, 'indifferent': 24436, 'gauge': 7633, '##eum': 14820, 'trinidad': 11856, '##wart': 18367, 'hesitant': 20221, 'penetrating': 22391, 'eastern': 2789, 'referred': 3615, 'widen': 21255, 'karin': 24537, 'likewise': 10655, 'ramps': 24943, 'punjab': 9213, 'protested': 11456, 'objection': 22224, 'strict': 9384, 'profit': 5618, 'seychelles': 27438, 'incident': 5043, '1900': 5141, 'ladder': 10535, 'sally': 8836, '陽': 1973, 'narrowing': 21978, 'highways': 10292, 'fiji': 11464, 'hotter': 22302, 'genuine': 10218, '##kushima': 24917, 'depending': 5834, '[unused963]': 968, 'supplemental': 27024, '士': 1807, 'spent': 2985, 'von': 3854, 'willing': 5627, '##egorical': 27203, 'unconventional': 23693, 'dams': 17278, '##gins': 16529, 'g': 1043, 'taluk': 23140, 'existence': 4598, 'milan': 6954, 'laurent': 14718, 'morality': 16561, 'graphical': 20477, 'visited': 4716, '##enes': 28553, 'engravings': 28611, '##uaries': 22579, 'andrey': 29219, 'architectural': 6549, '[unused742]': 747, 'quebec': 5447, 'competent': 17824, '##ie': 2666, 'moi': 25175, '##rmin': 27512, 'susie': 23917, '##eau': 10207, '[unused924]': 929, 'pets': 18551, 'polled': 26847, 'wrocław': 25160, '##っ': 30189, 'diamond': 6323, 'ag': 12943, '[unused744]': 749, 'wilfred': 26026, 'revenues': 12594, 'eleventh': 11911, 'schoolhouse': 26301, '##quette': 29416, 'frequencies': 13139, 'death': 2331, 'deny': 9772, 'weber': 13351, 'andhra': 14065, 'consideration': 9584, 'fetal': 25972, 'discontinued': 8944, '171': 18225, 'laos': 15786, 'ipa': 24531, '##analysis': 25902, 'pizza': 10733, 'focused': 4208, 'invest': 15697, 'solely': 9578, 'monitor': 8080, 'pilgrimage': 14741, '121': 12606, 'palmer': 8809, '[unused570]': 575, 'allows': 4473, '##rwin': 27349, 'income': 3318, 'prohibited': 10890, 'quotes': 16614, 'possesses': 14882, 'inspiration': 7780, 'amphibious': 16182, 'consultation': 16053, 'message': 4471, 'javanese': 27344, 'curly': 17546, 'lawsuits': 20543, 'furlongs': 26602, '[unused9]': 10, 'handheld': 27291, 'indian': 2796, 'worldwide': 4969, '##ke': 3489, 'darlington': 19667, 'celtic': 8730, 'fatally': 26292, 'hydraulic': 14761, 'representations': 15066, '##md': 26876, 'forms': 3596, '##elled': 21148, 'pony': 15606, 'destroys': 20735, 'espn': 10978, 'denied': 6380, 'grimes': 24865, 'titan': 16537, 'biased': 25352, '[unused58]': 59, 'exotic': 12564, '##beam': 28302, 'turtle': 13170, 'considerable': 6196, 'luck': 6735, 'rabbit': 10442, '##ldon': 19932, 'phillies': 15711, 'turret': 14493, 'philippine': 7802, 'rhetoric': 17871, 'mara': 13955, 'stade': 15649, '##ville': 3077, 'suspect': 8343, 'belt': 5583, 'galactic': 21375, 'direction': 3257, 'emigrated': 11367, 'thumbs': 16784, 'locks': 11223, 'nhs': 17237, 'participants': 6818, 'functional': 8360, 'fascinating': 17160, 'ख': 1316, 'stranded': 15577, 'arrogant': 15818, '[unused116]': 121, 'dion': 19542, 'nsw': 11524, 'tunnels': 10633, 'packers': 15285, '##folding': 21508, 'lad': 14804, 'markets': 6089, 'gravity': 8992, '##uil': 19231, '##hausen': 13062, 'capitalist': 19640, 'origin': 4761, 'te': 8915, 'germany': 2762, '##ffa': 20961, 'settles': 27221, 'harris': 5671, 'rolls': 9372, 'tapestry': 25213, '##test': 22199, 'independence': 4336, '##⊆': 30138, 'infantry': 3939, '##titled': 21309, '1756': 22370, '276': 25113, 'barrett': 12712, 'slipping': 11426, 'hauling': 23113, 'screens': 12117, 'byrne': 14928, '##lins': 24412, 'increasingly': 6233, '##lence': 22717, 'oriented': 8048, '##forms': 22694, 'trauma': 12603, 'frazier': 26551, '[unused873]': 878, 'develop': 4503, 'irrational': 23179, 'lacy': 19959, 'nanny': 19174, '##sef': 20106, '##20': 11387, '##koff': 26126, 'এ': 1351, '189': 20500, '[unused200]': 205, '幸': 1841, '##।': 29880, 'oboe': 22523, 'pistons': 24399, 'imp': 17727, 'inserted': 12889, 'graphics': 8389, 'ia': 24264, 'wicked': 10433, 'loyalists': 26590, 'poetry': 4623, 'traditionally': 6964, 'tank': 4951, '##sta': 9153, 'abduction': 23415, 'ranger': 11505, 'italy': 3304, '内': 1773, '##rano': 20770, '[unused159]': 164, 'fry': 14744, '##list': 9863, 'kelly': 5163, 'perry': 6890, 'catering': 18640, 'oppose': 15391, 'reliable': 10539, 'rooted': 15685, 'ashore': 16145, '##cede': 22119, 'oversaw': 14105, 'attire': 20426, 'lively': 17133, 'atoll': 22292, 'helped': 3271, 'charms': 24044, 'hissing': 26386, '##aith': 22465, 'bonfire': 28698, 'depict': 17120, '##kei': 29501, 'ত': 1362, 'pet': 9004, 'describes': 5577, 'encoded': 12359, 'thereafter': 6920, 'defensive': 5600, 'psychological': 8317, 'piss': 18138, '[unused978]': 983, 'benz': 17770, 'ripley': 25231, '##に': 30194, '193': 19984, 'persist': 29486, '##25': 17788, 'traces': 10279, 'capsule': 18269, 'minor': 3576, 'taylor': 4202, 'acoustic': 6490, 'beliefs': 9029, 'burnt': 11060, '##lining': 16992, '##eb': 15878, 'dissatisfaction': 28237, '##oj': 29147, 'masculine': 14818, 'namely': 8419, 'asset': 11412, 'lyman': 27587, 'nate': 8253, '1980s': 3865, 'measurement': 10903, '##fires': 26332, '##sar': 10286, '##ont': 12162, 'reins': 19222, '[unused417]': 422, 'mistress': 10414, 'arsenic': 29596, '[unused244]': 249, 'sire': 15785, '氏': 1891, 'dew': 24903, 'consciously': 24447, 'bunch': 9129, 'vida': 19830, 'consciousness': 8298, '##opped': 27288, 'cassidy': 13737, '197': 19975, '1883': 7257, 'muslims': 7486, 'baggage': 20220, 'apron': 20376, 'engraver': 27771, '##oire': 26250, 'camp': 3409, 'tropical': 5133, '##hema': 28433, 'fortunately': 14599, 'lens': 10014, '2001': 2541, '##usa': 10383, '##bility': 8553, 'negotiation': 19905, 'hostess': 22566, 'ब': 1329, 'carriage': 9118, '##jet': 15759, 'hemingway': 27299, 'hampstead': 29557, 'afternoon': 5027, 'lamps': 14186, 'play': 2377, 'individual': 3265, '##physics': 15638, 'god': 2643, 'involve': 9125, 'bog': 22132, 'yep': 15624, 'national': 2120, '1808': 13040, 'lucky': 5341, '510': 23475, 'lea': 12203, 'awake': 8300, '##wari': 20031, 'genres': 11541, '##urrent': 29264, '226': 21035, '##iah': 12215, '1954': 4051, 'johann': 8968, 'taipei': 14004, 'average': 2779, 'ப': 1388, 'folds': 15439, '##ahl': 28083, 'explores': 15102, '##nath': 16207, '1570': 28881, '##昭': 30394, 'ச': 1383, 'zero': 5717, 'labelled': 18251, 'demonic': 23170, 'fertile': 14946, '##uate': 20598, 'patriot': 16419, 'new': 2047, 'bluegrass': 21286, '##hner': 28989, '##ago': 23692, 'baptized': 19775, 'wwe': 11700, '49ers': 18156, 'decommissioned': 14394, 'bam': 25307, 'arguments': 9918, 'mural': 15533, 'williamson': 14333, 'wrecked': 18480, 'gleamed': 25224, 'ও': 1352, 'javier': 13824, 'hot': 2980, 'motioned': 13054, 'ripping': 17039, 'psychiatrist': 18146, 'vane': 23334, 'cisco': 26408, 'remodeled': 27170, '之': 1749, 'incense': 28647, '##cation': 10719, '##ia': 2401, 'intro': 17174, 'per': 2566, '[unused757]': 762, 'charlie': 4918, 'advise': 18012, 'positively': 13567, 'colonists': 15526, 'bodied': 22549, 'easter': 10957, '##cent': 13013, 'prefect': 19402, 'midtown': 27219, 'chilled': 23362, 'interference': 11099, '50th': 12951, 'mistakes': 12051, 'denny': 14465, 'vicar': 12340, 'norwood': 22804, '##ல': 29928, '[unused455]': 460, 'enormous': 8216, '[unused575]': 580, 'countless': 14518, 'intentionally': 15734, '##vid': 17258, 'tsar': 17608, 'namibia': 15408, 'films': 3152, 'བ': 1431, 'faults': 19399, 'pulls': 8005, '囗': 1797, 'burmese': 14468, 'bridge': 2958, 'cursing': 19752, 'aunt': 5916, 'purification': 28406, 'pleased': 7537, 'oz': 11472, 'installing': 23658, '[unused396]': 401, 'huskies': 29471, 'testified': 14914, 'coincided': 18616, 'additions': 13134, '##note': 22074, 'outing': 26256, '[unused981]': 986, 'ariel': 16126, '##tley': 18492, '229': 22777, 'leicestershire': 20034, 'strengths': 20828, 'jar': 15723, 'nadu': 10703, 'seek': 6148, '##stellar': 29028, 'farmer': 7500, '##ec': 8586, 'gb': 16351, 'מ': 1255, 'muttering': 22581, 'dod': 26489, '##ள': 29929, '188': 19121, '65': 3515, 'strewn': 25259, '##tsky': 26824, 'attributed': 7108, 'canoe': 14347, '[unused845]': 850, 'cicero': 23080, 'vertices': 18984, 'showcased': 22443, '##odon': 28716, '森': 1882, '##ии': 15414, '1707': 25029, 'newspapers': 6399, '126': 14010, '##rito': 28414, '##put': 18780, '##ბ': 29975, '##ய': 29926, 'confirm': 12210, 'sparkling': 16619, 'ト': 1714, '##zu': 9759, '##oge': 23884, 'tattooed': 26464, 'mutually': 20271, '##nts': 7666, 'assists': 8456, 'saturn': 14784, 'substitutes': 29200, 'hole': 4920, 'cannon': 8854, '##ि': 29877, '##،': 29814, 'tract': 12859, 'hp': 6522, 'stride': 18045, 'immortality': 20107, 'rosenthal': 29062, 'relations': 4262, 'landed': 5565, 'narrowed': 8061, '[unused656]': 661, '##ah': 4430, 'eduard': 20424, 'sermons': 20855, 'rumble': 15658, 'presently': 12825, 'profiles': 17879, '場': 1806, 'encompass': 25281, 'narcotics': 27290, 'mole': 16709, '##emon': 26941, 'diary': 9708, 'clutch': 15357, 'negotiations': 7776, 'jax': 13118, 'superior': 6020, '##enko': 17868, 'rusty': 13174, 'guarantee': 11302, 'mausoleum': 19049, 'bjorn': 24998, 'saratoga': 23136, 'jimi': 27624, 'cdp': 8561, 'statehood': 28000, 'ɫ': 1120, 'stripped': 10040, 'mhz': 11413, 'taxpayers': 26457, 'batted': 12822, 'barnet': 26864, '##^': 29637, 'viceroy': 19007, 'attract': 9958, 'supervising': 21238, '[unused344]': 349, 'slow': 4030, 'shelters': 17177, 'telegram': 23921, '##西': 30473, '[unused719]': 724, 'onward': 15834, 'counselor': 17220, 'bonding': 19882, '##rable': 16670, '瀬': 1905, 'style': 2806, 'later': 2101, 'spun': 7455, 'ბ': 1439, 'professors': 12655, 'flynn': 13259, 'stu': 24646, '##т': 22919, 'panda': 25462, '##部': 30486, 'surname': 11988, '##uni': 19496, '[unused960]': 965, '##pack': 23947, '##oum': 24778, 'casablanca': 24592, 'visual': 5107, 'choices': 9804, 'jazz': 4166, 'speakers': 7492, '##ob': 16429, '##break': 23890, '##和': 30322, '##oll': 14511, '[unused918]': 923, '##oche': 23555, 'vanish': 25887, 'spanned': 18212, '##պ': 29780, 'ψ': 1178, 'gestapo': 26446, 'makes': 3084, 'sponsored': 6485, '##pace': 15327, 'libre': 21091, 'meantime': 12507, 'gaps': 16680, '##ske': 17140, 'arcs': 29137, 'thrilling': 26162, 'rochester': 10541, '##ع': 29830, 'malaria': 19132, 'reprint': 25364, 'bryan': 8527, 'antique': 14361, '1950': 3925, 'fullback': 21803, 'flowers': 4870, 'lille': 22479, '##xon': 22500, 'patrice': 29527, 'luftwaffe': 17740, 'kun': 28919, '[unused378]': 383, '2018': 2760, 'coffin': 13123, 'interrupted': 7153, 's': 1055, '##》': 30166, '##tonic': 25009, 'uneven': 17837, 'bug': 11829, '##aka': 11905, 'annoying': 15703, '##cule': 21225, '⁰': 1536, '[unused636]': 641, 'revival': 6308, 'instituted': 14948, 'tucker': 9802, '##stra': 20528, 'mini': 7163, 'proportions': 19173, 'separately': 10329, '##健': 30294, 'unwilling': 15175, 'counted': 8897, '##town': 4665, 'konstantin': 23989, '##polis': 17699, 'apart': 4237, 'wrestled': 20615, 'substitute': 7681, '[unused972]': 977, 'idol': 10282, '580': 23712, '1962': 3705, '##«': 29654, 'networks': 6125, 'biography': 8308, 'gandhi': 12338, 'rallied': 24356, 'wore': 5078, 'abd': 19935, 'moving': 3048, 'myrtle': 21381, 'hounds': 27772, '##iu': 17922, 'condemn': 28887, '##gami': 26517, 'lincolnshire': 16628, 'bela': 20252, 'aces': 20232, '##udy': 20217, 'appearance': 3311, 'tale': 6925, 'monitored': 17785, 'illustrious': 26150, 'inflammation': 21733, 'pleasures': 26552, '##enham': 23580, '##stan': 12693, '武': 1889, 'monumental': 15447, 'za': 23564, 'mo': 9587, '##t': 2102, 'fiction': 4349, 'siren': 19558, 'abdomen': 13878, 'kissing': 7618, 'icc': 16461, 'terribly': 16668, 'retracted': 28214, 'corporation': 3840, 'vocalist': 8032, 'reassured': 26350, '##hem': 29122, 'hectare': 20406, 'months': 2706, 'classify': 26268, 'protocol': 8778, 'good': 2204, '1800': 9807, 'supervised': 13588, 'midi': 22265, 'f': 1042, 'belinda': 24574, 'wrist': 7223, 'creator': 8543, 'valuation': 26004, 'judgments': 26186, '##dick': 24066, 'theory': 3399, 'rents': 28206, 'priests': 8656, 'train': 3345, 'witchcraft': 21599, 'purported': 27023, 'bruins': 18159, 'solids': 26778, 'x': 1060, 'easier': 6082, 'ड': 1321, 'cycle': 5402, '##marine': 21966, 'penny': 10647, '[unused118]': 123, '##yya': 19903, 'magician': 16669, 'starter': 11753, 'space': 2686, 'mexico': 3290, 'counsel': 9517, 'beaux': 20290, 'credible': 23411, '[unused113]': 118, 'consisting': 5398, 'motivation': 14354, 'north': 2167, 'chaired': 12282, 'emphasis': 7902, '##bang': 25153, '##neas': 26737, 'syndrome': 8715, '##ret': 13465, 'seminar': 18014, 'hoffmann': 24437, '吉': 1793, 'effective': 4621, '##moor': 17622, '##金': 30490, 'carlson': 22226, '##eil': 24359, 'entity': 9178, 'barrels': 13826, 'lumpur': 17761, 'horst': 28565, '##sf': 22747, 'doyle': 11294, 'kissed': 4782, 'seasonal': 12348, 'ᆯ': 1486, 'soaking': 22721, '1699': 28718, 'st': 2358, 'jose': 4560, 'buddhist': 7992, 'clan': 6338, '##key': 14839, 'shortly': 3859, 'assisted': 7197, 'stalin': 13125, 'biographical': 16747, 'stirling': 15597, 'entrusted': 18011, 'orthodoxy': 26582, 'migrants': 16836, 'আ': 1348, 'reduces': 13416, '##sonic': 18585, '##ddled': 28090, 'issn': 23486, 'plotted': 27347, 'wilkinson': 16237, 'analytic': 23521, '##sty': 21756, '##eborg': 27614, 'distillery': 24870, '##cut': 12690, 'wrote': 2626, 'bp': 17531, 'gabriel': 6127, '##holz': 28094, 'smoke': 5610, 'nursing': 8329, 'ridge': 5526, 'convergence': 19143, 'marred': 24563, 'punched': 11696, '##yman': 17906, 'rattled': 19252, 'physiological': 19389, '##40': 12740, 'sacrificed': 20268, 'guinness': 17323, 'hamburg': 8719, 'herr': 23506, 'campground': 29144, 'depicting': 10775, 'indicates': 7127, 'summers': 10945, 'tokugawa': 22065, '##pol': 18155, 'tasted': 12595, 'estates': 8707, 'eyebrow': 9522, '1851': 8792, '##hdi': 22960, '##orum': 20527, 'wentworth': 20572, 'laser': 9138, 'sinking': 10186, 'sofie': 26359, 'photon': 26383, 'wing': 3358, '##め': 30206, 'roared': 14242, 'possessing': 18840, 'bond': 5416, 'turk': 22883, 'streaming': 11058, 'lectured': 19206, '[unused285]': 290, 'tipperary': 17333, 'comune': 21130, '##uation': 14505, 'watson': 7908, 'pipe': 8667, 'mocking': 19545, '214': 19936, 'gina': 17508, '##ロ': 30261, 'painter': 5276, 'outsider': 21002, 'sets': 4520, 'helpful': 14044, 'sleeper': 24372, 'cowan': 25752, '##ு': 29933, 'explicitly': 12045, 'mit': 10210, '##ssed': 29417, '##met': 11368, 'noah': 7240, '##广': 30368, 'inch': 4960, 'else': 2842, '[unused342]': 347, 'regulated': 12222, 'hurst': 26405, 'played': 2209, 'maris': 23787, 'limestone': 9771, 'tragic': 13800, '井': 1754, '[unused925]': 930, '##ts': 3215, 'statements': 8635, 'over': 2058, '##ions': 8496, 'deed': 15046, 'lingered': 17645, 'marched': 9847, 'claus': 19118, 'mort': 22294, '##ayo': 28852, '[unused947]': 952, 'fleet': 4170, '##iro': 9711, '[unused681]': 686, 'has': 2038, 'sentence': 6251, 'eaton': 17642, 'packard': 24100, '[unused624]': 629, '176': 18561, 'makeshift': 19368, '[unused16]': 17, 'juvenile': 11799, '宗': 1821, 'お': 1650, 'dominican': 10104, 'kathryn': 20484, 'knife': 5442, 'everett': 15160, 'consecrated': 12299, '##hc': 16257, '##vered': 25896, 'tighter': 12347, '##ema': 14545, 'annexation': 18985, 'prodigy': 28334, 'mccoy': 16075, 'pouring': 13053, '##crow': 24375, '[unused71]': 72, '##ray': 9447, 'overcame': 26463, 'succession': 8338, 'delaware': 8452, 'overlook': 27590, '##atter': 20097, 'cycles': 12709, 'citizen': 6926, 'implications': 13494, 'socrates': 26772, 'cretaceous': 18122, 'housing': 3847, '[unused115]': 120, 'strips': 12970, 'onto': 3031, 'indirect': 14958, 'capt': 14408, 'candidacy': 17057, 'visiting': 5873, 'makers': 11153, 'ticking': 28561, 'running': 2770, 'lazy': 13971, 'forgiveness': 17213, '##logic': 27179, 'primary': 3078, 'portage': 25140, 'otto': 8064, 'pristine': 27375, 'baffled': 29088, '本': 1876, 'shelly': 28360, 'altar': 9216, 'knots': 9439, 'zack': 13658, 'dizzy': 14849, '##ffle': 18142, 'came': 2234, 'hip': 5099, 'cecilia': 18459, 'nix': 23330, '##lash': 27067, 'seriously': 5667, 'homo': 24004, 'retention': 20125, '##tou': 24826, 'tops': 13284, 'starred': 5652, 'poultry': 22468, '##nga': 13807, 'crossover': 16335, 'monty': 18446, 'visit': 3942, '##lam': 10278, '##sford': 17658, 'aziz': 21196, 'lifeboat': 23450, '##相': 30445, '##cture': 14890, 'fuselage': 13478, 'models': 4275, '•': 1528, 'ideology': 13165, 'carpenter': 10533, 'synod': 15374, '##oor': 16506, 'playful': 18378, 'musica': 21137, 'scotch': 18937, 'zaragoza': 25744, 'masjid': 27779, 'ding': 22033, 'ponder': 29211, '##cki': 18009, 'reorganisation': 24934, '1732': 27582, '°f': 8157, 'residence': 5039, 'essence': 11305, 'victory': 3377, 'dynamite': 21719, 'eastwood': 24201, 'observed': 5159, 'radius': 12177, 'commenting': 15591, 'aboard': 7548, '1682': 29478, 'stifled': 27146, 'thence': 23166, 'fountain': 9545, '[unused149]': 154, '1881': 7005, 'teddy': 11389, 'quite': 3243, 'descends': 23328, 'senior': 3026, 'unite': 15908, 'vi': 6819, 'frigates': 22833, 'educating': 25088, 'marianne': 19887, 'fool': 7966, 'trier': 25487, 'kingdom': 2983, '##typical': 27086, 'forensic': 15359, 'operates': 5748, 'zhao': 15634, '##pura': 21228, 'wounds': 8710, 'purity': 18433, 'gerard': 11063, 'dysfunction': 28466, '##ι': 18199, 'castile': 15656, '##ulous': 16203, '15th': 6286, 'disappointing': 15640, 'decorated': 7429, '##gate': 5867, '##the': 10760, 'touch': 3543, 'horses': 5194, 'modernization': 20181, 'motorway': 13119, '[unused500]': 505, '##火': 30432, 'personal': 3167, '扌': 1859, 'sciences': 4163, 'elements': 3787, 'pm': 7610, 'guardians': 14240, 'rouen': 27030, '##प': 29864, 'boone': 15033, 'elaine': 15263, 'tehran': 13503, 'heaving': 23907, 'republic': 3072, '140': 8574, 'collapsing': 22724, '##մ': 29776, 'jealous': 9981, 'staples': 24533, '##leen': 24129, '##ד': 29791, 'rulers': 11117, 'mauritius': 18004, 'amsterdam': 7598, '##tlement': 24007, 'strawberry': 16876, '[unused347]': 352, 'dining': 7759, 'madagascar': 11934, '##ω': 29739, 'fund': 4636, 'vie': 20098, '##trip': 24901, 'frederick': 5406, 'sarcastically': 23800, 'backwards': 11043, 'clown': 15912, 'bothered': 11250, '1766': 21410, 'constructions': 21913, 'deliveries': 23534, '##15': 16068, 'walters': 19097, '##nified': 25201, '[unused64]': 65, '##position': 26994, '[unused747]': 752, 'reclamation': 27389, '##lices': 29146, '##司': 30317, '##ater': 24932, 'adapted': 5967, 'recognise': 17614, '[unused605]': 610, 'pledged': 16970, 'harmonica': 16527, '##禾': 30453, 'ahmed': 10208, 'legislator': 22964, 'flash': 5956, 'ο': 1169, '##foil': 24323, '##sr': 21338, 'bent': 6260, '1841': 9840, 'otis': 18899, '##yt': 22123, 'caliph': 22733, '##cheon': 28099, 'pianist': 9066, 'remembers': 17749, '##lts': 21593, '##ways': 14035, 'shielding': 25553, 'storm': 4040, 'back': 2067, 'fifth': 3587, 'accompanying': 10860, 'brick': 5318, '102': 9402, 'orion': 18747, '##iver': 16402, 'austen': 24177, 'plausible': 24286, 'allergic': 27395, '##wood': 3702, 'acceptable': 11701, 'herring': 22103, 'kayla': 26491, 'estimated': 4358, 'enhanced': 9412, '##enting': 26951, 'lok': 13660, 'μ': 1166, 'sheets': 8697, 'paterson': 19162, '##pment': 24073, 'খ': 1354, '##kata': 29123, 'cain': 11557, 'moments': 5312, '##ision': 19969, 'rowan': 14596, 'endings': 21306, 'temperate': 16868, 'shrug': 13409, 'burger': 15890, 'eddy': 16645, 'patton': 18090, 'heartland': 27399, 'jonah': 15617, 'acting': 3772, 'diner': 15736, '##ನ': 29936, 'cabinet': 5239, 'tumbling': 21552, '##დ': 29977, '##genic': 16505, 'treasure': 8813, 'johnnie': 27716, 'fins': 18564, 'piccolo': 29368, 'bitterly': 19248, 'mute': 20101, '##lt': 7096, 'tripoli': 21841, 'benefit': 5770, 'patterns': 7060, '江': 1897, 'wage': 11897, 'roland': 8262, 'commune': 5715, 'erosion': 14173, 'clapton': 24705, 'drop': 4530, 'instinctively': 16607, 'cam': 11503, 'ode': 24040, 'framework': 7705, 'tools': 5906, '##vana': 27313, 'cypress': 22183, 'scales': 9539, 'z': 1062, 'speed': 3177, 'colonial': 5336, 'municipality': 3250, 'haley': 16624, 'pal': 14412, 'arden': 26225, 'opposes': 29158, '##ew': 7974, 'widows': 24835, 'matrix': 8185, 'tequila': 26791, 'biographer': 17121, 'plymouth': 10221, 'eyre': 26975, 'protection': 3860, 'organizing': 10863, '##ega': 29107, 'ensure': 5676, 'nelly': 29498, '[unused294]': 299, 'fog': 9666, 'dotted': 20384, 'respectful': 26438, 'als': 25520, 'correspondence': 11061, 'height': 4578, 'logging': 15899, '##食': 30504, 'prop': 17678, 'jamie': 6175, 'pipeline': 13117, 'ambitions': 19509, '##uf': 16093, 'disqualification': 27782, 'overseen': 22485, 'fertility': 17376, 'orbit': 8753, '##ely': 26006, 'herman': 11458, 'ciudad': 20759, 'cloth': 8416, '[unused95]': 96, '##sack': 25607, 'frankie': 12784, 'sucking': 13475, 'disappearance': 13406, 'scholars': 5784, 'sargent': 27599, 'ツ': 1712, 'clockwise': 22839, '[unused476]': 481, 'stole': 10312, 'abandoning': 19816, 'nic': 27969, '##oles': 29111, '##dent': 16454, 'proceeded': 8979, 'demoted': 25692, 'interpretations': 15931, 'fc': 4429, '##rac': 22648, 'ascended': 19644, 'elton': 19127, 'trunk': 8260, 'champaign': 28843, 'anzac': 29224, '[unused467]': 472, 'nikki': 14470, '##nov': 16693, 'legitimate': 11476, '##roller': 26611, 'soundtracks': 24245, 'hu': 15876, 'compared': 4102, 'coached': 8868, 'explored': 10641, 'veronica': 13133, 'risked': 22374, 'gottfried': 26142, 'phases': 12335, '##ᄎ': 30001, 'responsible': 3625, 'yuki': 24924, '[unused577]': 582, '[unused815]': 820, 'lava': 13697, 'wolfe': 14212, 'expose': 14451, '##ʷ': 29709, 'serra': 22737, '##puri': 24661, 'mason': 6701, 'mi': 2771, 'brood': 25565, 'grossing': 18244, 'detector': 19034, 'clemens': 27277, 'vittorio': 25914, '##jak': 18317, '##耳': 30463, 'explore': 8849, 'masked': 16520, 'wink': 16837, 'died': 2351, 'marsh': 9409, 'looting': 29367, '##liness': 20942, 'commissioned': 4837, 'target': 4539, 'encouraging': 11434, '##dale': 5634, 'upland': 25770, '##前': 30302, 'maddox': 22730, '1950s': 4856, 'uptown': 28539, '##rator': 16259, 'francais': 22357, 'dots': 14981, '[unused715]': 720, 'schmidt': 12940, '##havan': 24652, 'exceed': 13467, 'trek': 10313, '[unused17]': 18, 'negro': 12593, 'imperative': 23934, 'intercollegiate': 21587, 'pairing': 22778, 'riches': 26768, '##gina': 20876, '##ciation': 23247, 'plunged': 16687, 'workings': 24884, '##vis': 11365, 'geschichte': 28299, '##dgets': 28682, 'ignores': 26663, 'helix': 25743, '##øy': 27688, 'ecumenical': 23540, '##43': 23777, '208': 18512, 'etat': 17997, 'statistical': 7778, 'dumped': 14019, 'stubble': 26816, '##endra': 19524, 'projects': 3934, 'fis': 27424, '70th': 26934, '##erving': 25164, 'vibrant': 17026, '##llar': 17305, 'wrestle': 25579, 'goats': 17977, 'quick': 4248, 'sobs': 21503, 'mccann': 28641, 'detention': 12345, 'precisely': 10785, 'shadows': 6281, 'leather': 5898, 'terminates': 28790, '##合': 30318, 'slacks': 27786, 'aarhus': 29173, 'prolific': 12807, 'globe': 7595, '##cap': 17695, 'incompatible': 25876, 'slug': 23667, '1794': 13199, 'sloping': 24724, 'ロ': 1735, 'خ': 1277, '##g': 2290, '1779': 17616, 'prophecy': 14951, 'influence': 3747, 'japanese': 2887, 'mecklenburg': 22007, 'barron': 23594, 'transmitting': 23820, 'gibbs': 15659, '[unused250]': 255, '1792': 13414, 'segunda': 19071, 'pathways': 16910, 'amnesia': 29222, '##ja': 3900, 'rejection': 13893, '##osaurus': 25767, 'ˣ': 1153, 'meets': 6010, 'riff': 24808, 'essay': 9491, 'syndicate': 16229, 'outsiders': 22361, 'competitor': 12692, 'dropping': 7510, 'sensor': 13617, 'enclave': 25867, 'sustained': 8760, 'condemnation': 26248, '##ok': 6559, '##&': 29617, 'ignorance': 18173, 'peat': 23366, '##ti': 3775, 'inspector': 7742, 'nouveau': 25272, 'installation': 8272, '46': 4805, 'howe': 13358, 'academie': 19669, '代': 1760, 'basics': 24078, 'reborn': 24910, 'kit': 8934, '##tur': 20689, 'giuseppe': 12574, 'third': 2353, 'looking': 2559, 'delegates': 10284, 'servant': 7947, 'demonstrating': 14313, 'ग': 1317, 'clinched': 18311, 'smoking': 9422, 'oneself': 25763, 'linear': 7399, 'temporarily': 8184, 'rat': 9350, 'statute': 11671, 'anchored': 14453, 'dresses': 14464, 'utah': 6646, '[unused320]': 325, 'expeditions': 15014, '¼': 1091, 'い': 1647, 'shared': 4207, 'administrations': 27722, '##on': 2239, '##ote': 12184, 'reportedly': 7283, 'verve': 29230, '##mbs': 29232, 'confronts': 17628, '##ic': 2594, 'persuasion': 27577, 'dell': 12418, '##ichi': 11319, 'buzzed': 21377, '##nsor': 29577, '##ands': 29560, '##dr': 13626, 'simple': 3722, 'apology': 12480, 'booking': 21725, 'oldham': 18285, '##watch': 18866, '##vik': 13309, 'endurance': 14280, 'prisons': 15996, '##53': 22275, 'presidency': 8798, 'context': 6123, 'gospels': 24131, 'bicycles': 21066, '##lessly': 10895, 'randall': 12813, 'perceived': 8690, 'grandfather': 5615, 'mosque': 8806, 'palms': 9486, 'lure': 17256, 'honeymoon': 19227, 'tx': 19067, '##urt': 19585, 'lights': 4597, '[unused97]': 98, '1625': 26263, 'ruined': 9868, 'producing': 5155, 'absently': 21284, 'businessmen': 17353, 'satisfaction': 9967, '##achal': 24409, '##⟩': 30156, 'hospitality': 15961, 'limited': 3132, 'boston': 3731, 'talked': 5720, 'bastard': 8444, 'playboy': 18286, 'risking': 26875, '403': 28203, 'enthusiast': 29550, 'bunker': 15742, 'current': 2783, 'cleaner': 20133, 'galileo': 21514, 'bret': 25626, '##rte': 19731, 'trolls': 27980, '##»': 29663, '3': 1017, 'brett': 12049, 'prosperity': 14165, 'tick': 16356, 'slippery': 22274, 'tragedy': 10576, 'reeves': 17891, 'organizations': 4411, 'feet': 2519, 'receive': 4374, '##kki': 24103, 'isabel': 11648, 'minimum': 6263, 'thompson': 5953, 'uss': 7234, '##gned': 19225, 'rightful': 27167, '##「': 30167, 'agreement': 3820, '##erie': 17378, 'shafts': 20542, 'erskine': 27139, 'hesitated': 9369, 'dark': 2601, 'railways': 7111, 'finalist': 9914, '223': 20802, 'parted': 10277, 'cerebral': 18439, 'welterweight': 20882, 'westphalia': 22952, 'perfection': 15401, 'multiple': 3674, 'cleric': 29307, 'wang': 7418, 'ripple': 24644, 'duct': 23245, 'collapse': 7859, 'axle': 17290, 'protect': 4047, 'bandages': 27993, 'messages': 7696, 'marrying': 13516, 'chaplain': 14011, '##eza': 28640, 'outlet': 13307, 'glass': 3221, '##tos': 13122, 'sand': 5472, 'builder': 12508, 'nipple': 14298, 'worcestershire': 20218, '##tively': 25499, 'explicit': 13216, 'curtiss': 26431, '##ए': 29850, 'es': 9686, 'fuels': 20145, '##±': 29657, 'edit': 10086, 'configurations': 22354, 'werewolf': 12797, 'contemplating': 25247, 'seize': 15126, 'heels': 8265, 'nad': 23233, '##nall': 22270, 'apostle': 20121, 'overgrown': 26433, '[unused67]': 68, '##hor': 16368, '谷': 1951, 'wait': 3524, 'distracted': 11116, '##ffed': 15388, 'servicemen': 26714, 'inca': 27523, 'hailed': 16586, 'slumped': 14319, 'cream': 6949, 'theodore': 10117, 'virtual': 7484, '民': 1892, 'greene': 11006, 'fulfilled': 16829, 'odyssey': 18735, 'nights': 6385, 'inhaled': 15938, 'lin': 11409, 'banded': 25264, 'generosity': 26161, '##mut': 28120, 'campaigned': 16196, 'settling': 9853, 'excused': 28512, 'groves': 21695, 'pluto': 26930, 'indy': 18214, 'futuristic': 28971, 'wet': 4954, 'follow': 3582, '##ren': 7389, 'martin': 3235, 'sorts': 11901, 'bas': 19021, 'accelerator': 23468, '##rval': 26585, 'u2': 23343, 'amos': 13744, 'symphonies': 29355, 'virginia': 3448, 'bartholomew': 19866, 'spire': 19823, 'neil': 6606, '##erved': 25944, 'resisting': 22363, 'seahawks': 21390, 'losses': 6409, 'continued': 2506, 'shields': 11824, 'proprietor': 21584, 'portuguese': 5077, 'mildly': 19499, 'posters': 14921, 'constantly': 7887, 'foods': 9440, 'benoit': 21721, 'elected': 2700, '[unused407]': 412, 'feud': 13552, '[unused202]': 207, 'notes': 3964, 'zombies': 14106, 'exposure': 7524, 'claude': 8149, 'corners': 8413, 'esteem': 19593, 'nellie': 25365, '##¡': 29644, '##hot': 12326, '##sure': 28632, '##smith': 21405, 'menon': 25111, '##caster': 25490, 'civilians': 9272, 'twin': 5519, 'sam': 3520, 'mafia': 13897, 'spins': 23371, 'irregularities': 28868, 'fucking': 8239, 'squire': 21263, 'edict': 24754, '##尚': 30356, 'scared': 6015, 'npr': 21411, 'orchestral': 13533, 'attic': 14832, '##ˡ': 29716, 'sentimental': 23069, 'fruit': 5909, 'supervisors': 22565, 'unofficially': 29322, 'spouse': 18591, 'petition': 9964, 'melanie': 13286, 'tears': 4000, '##ulia': 20922, 'backbone': 21505, 'marie': 5032, 'breeds': 15910, 'и': 1188, 'simmons': 13672, 'hodge': 23148, 'gilded': 23880, '##tically': 25084, 'erection': 13760, 'coma': 16571, '##ution': 13700, '##ք': 29786, 'tracts': 22069, 'alliance': 4707, 'memorials': 22899, 'decides': 7288, 'baritone': 14458, 'veteran': 8003, '[unused452]': 457, 'moses': 9952, 'ceded': 19705, 'khz': 17737, 'anarchy': 26395, 'marguerite': 15334, 'ein': 16417, 'thorpe': 20249, 'feminism': 20050, 'disappears': 17144, 'dispersal': 27602, 'artery': 16749, '##ო': 29986, 'crew': 3626, 'veil': 15562, 'arrangement': 6512, 'yerevan': 23383, 'imperfect': 29238, 'ctv': 28720, 'shaun': 16845, 'respectable': 19416, 'american': 2137, '##ba': 3676, 'laszlo': 28226, 'rams': 13456, 'damp': 10620, 'chained': 22075, 'fingerprints': 27556, '##market': 20285, '##th': 2705, 'caring': 11922, '##oki': 23212, '##vres': 24790, '##zan': 13471, '##б': 29740, 'く': 1653, 'diagnosed': 11441, 'borne': 15356, 'pleading': 16418, 'offenses': 25173, 'contemplated': 23133, 'democratic': 3537, '##tto': 9284, 'porter': 8716, 'mundo': 25989, 'installment': 18932, '##ח': 29794, 'censorship': 15657, '##香': 30505, '##ة': 19433, 'malvern': 28082, 'thinks': 6732, 'reared': 23295, 'autonomous': 8392, '##rah': 10404, 'children': 2336, 'casual': 10017, '##ending': 18537, '##aina': 27971, 'restricting': 26996, 'vale': 10380, 'initial': 3988, 'kidnapped': 11364, '##lou': 23743, '100th': 16919, 'negatively': 19762, '##sle': 25016, '##bag': 16078, 'duval': 23929, '##人': 30282, 'petrol': 17141, 'zen': 16729, 'hood': 7415, 'س': 1282, 'touching': 7244, '##vor': 14550, '司': 1791, 'failed': 3478, 'enemy': 4099, 'nipples': 28124, 'married': 2496, '##bain': 29148, 'each': 2169, 'rugged': 17638, 'angle': 6466, 'pictured': 15885, 'hanna': 10579, 'empathy': 26452, '##rbin': 27366, '₂': 1549, 'limousine': 28012, '##hampton': 21946, 'rift': 16931, 'slump': 28702, '[unused615]': 620, 'noting': 9073, 'delight': 12208, 'greet': 17021, '##rmed': 29540, 'softer': 19013, '##uka': 15750, '##fu': 11263, 'deficit': 15074, 'rumours': 19200, 'nicely': 19957, 'rom': 17083, '##hy': 10536, 'translation': 5449, 'slated': 18517, '[unused336]': 341, 'mlb': 10901, 'gently': 5251, 'contributor': 12130, 'whiskey': 13803, '##ntal': 15758, 'wnba': 25554, 'praise': 8489, 'scratching': 20291, '##ini': 5498, 'serpent': 16517, 'dalai': 28511, 'webber': 19861, '##nig': 25518, 'accent': 9669, '##nami': 28987, 'stones': 6386, 'inclination': 21970, '##ф': 29749, 'concentrated': 8279, 'subgenus': 25356, '##more': 5974, 'cd': 3729, '##lev': 20414, 'kyushu': 25885, '##mmy': 18879, 'replacement': 6110, 'joel': 8963, '224': 19711, 'fed': 7349, '##ographer': 26145, 'traced': 9551, 'determine': 5646, 'execute': 15389, 'defender': 8291, 'portions': 8810, 'pill': 17357, '##lot': 10994, '##গ': 29891, '212': 18164, 'precedence': 23359, 'archives': 8264, '164': 17943, 'traditional': 3151, '[unused44]': 45, 'pia': 24624, '##dal': 9305, '##ick': 6799, 'ducked': 13781, '##yat': 26139, '##kie': 11602, '##rum': 6824, 'azerbaijani': 18325, '##eer': 11510, 'responsive': 26651, 'scrambled': 13501, 'subsidiary': 7506, 'yo': 10930, 'appearances': 3922, 'fashioned': 13405, '##sett': 21678, '[unused666]': 671, '##ous': 3560, 'farm': 3888, 'julie': 7628, 'lived': 2973, 'commemorated': 19131, 'commotion': 23960, 'barnard': 22266, '##quin': 12519, 'molina': 25601, '##・': 30264, 'multinational': 20584, 'lev': 23310, 'chanting': 22417, '##masters': 27751, '分': 1775, 'engineer': 3992, 'fairness': 26935, '##urbed': 29595, '[unused857]': 862, 'freak': 11576, 'manually': 21118, '##sg': 28745, '##pathy': 20166, 'carved': 7844, 'nfc': 22309, 'schubert': 24645, 'quiet': 4251, 'immense': 14269, '##se': 3366, '₄': 1551, 'dentistry': 26556, 'paranoid': 19810, 'regrets': 23161, '##ples': 21112, 'trip': 4440, 'castro': 11794, 'マ': 1723, 'preparing': 8225, 'precautions': 29361, 'universal': 5415, 'ya': 8038, 'generous': 12382, 'coastline': 15458, '##chemical': 15869, 'sc': 8040, 'djs': 23837, '##德': 30374, 'scrambling': 25240, '##ᄃ': 29993, 'amplifier': 22686, '139': 16621, 'apocalyptic': 27660, 'oxfordshire': 20124, 'guinea': 7102, '##hoot': 23416, '##ener': 24454, 'hacked': 28719, 'flock': 19311, 'sato': 20251, 'gamer': 27911, '##eth': 11031, '##ova': 7103, 'moans': 24675, 'mug': 14757, 'recovered': 6757, 'elevators': 19406, '##eric': 22420, 'watershed': 12547, 'fe': 10768, '39': 4464, '光': 1770, 'turbine': 14027, 'reached': 2584, 'terminate': 20320, '##pet': 22327, 'every': 2296, 'landings': 16805, 'sparse': 20288, 'curious': 8025, 'providers': 11670, 'jolted': 28801, '##leaf': 19213, 'ʑ': 1137, '##jit': 18902, '長': 1967, 'munster': 11348, '##ка': 28598, '##³': 18107, 'first': 2034, '##ki': 3211, 'terminal': 5536, 'collar': 9127, 'robe': 11111, '##uted': 12926, 'je': 15333, 'pennant': 22690, '115': 10630, '##igan': 10762, 'pearl': 7247, 'thumping': 25996, 'finale': 9599, 'lloyd': 6746, 'communist': 4750, 'co₂': 20190, 'planner': 24555, 'grandpa': 15310, 'programmed': 16984, 'official': 2880, '##ral': 7941, 'դ': 1222, 'lawrence': 5623, 'ralph': 6798, '##shed': 14740, 'notwithstanding': 26206, 'chuck': 8057, 'cop': 8872, '∇': 1595, 'poll': 8554, '1789': 13739, 'diffuse': 28105, 'distribute': 16062, '##eyer': 20211, 'nikita': 29106, 'linked': 5799, 'wipe': 13387, 'swung': 7671, '35': 3486, '##adan': 26335, 'lads': 29126, 'dates': 5246, 'what': 2054, 'pulse': 8187, 'isolation': 12477, 'needing': 11303, '##uba': 19761, 'cocoa': 22940, 'self': 2969, '##encies': 15266, 'lend': 18496, 'expelled': 10016, 'atv': 29108, '##ized': 3550, 'alvin': 17348, 'dialed': 21300, 'clasped': 16763, 'hong': 4291, '66': 5764, 'marche': 28791, 'distributed': 5500, 'gia': 27699, 'toro': 23790, 'phillip': 10852, 'plays': 3248, '##ᄂ': 29992, 'electron': 10496, 'subjective': 20714, 'vince': 12159, 'myspace': 24927, 'daggers': 24210, 'ע': 1259, 'vase': 18781, 'ghent': 24202, '##iss': 14643, 'unlock': 19829, 'imported': 10964, 'cyclic': 23750, 'は': 1672, '##nish': 24014, 'court': 2457, 'thirds': 12263, 'impressions': 19221, '##rgen': 25892, 'diseases': 7870, 'descend': 18855, 'slim': 11754, 'isa': 18061, 'user': 5310, 'luca': 15604, 'smallpox': 25765, 'sandra': 12834, 'educated': 5161, 'dough': 23126, 'doctor': 3460, 'fossils': 11954, 'knighted': 19572, 'tina': 11958, 'johannes': 12470, 'chick': 14556, 'loves': 7459, '[unused928]': 933, 'celebration': 7401, 'manipulate': 17708, 'fraser': 9443, 'began': 2211, 'illegal': 6206, '94': 6365, '09': 5641, 'ambition': 16290, '##ger': 4590, 'carving': 18441, '##hiko': 22204, 'nominally': 24207, 'bellamy': 25544, 'binary': 12441, 'dissatisfied': 25956, '##郡': 30485, '##sca': 15782, 'jeopardy': 26604, '[unused50]': 51, 'amazed': 15261, '[unused861]': 866, 'lyon': 10241, '##nnis': 27803, 'mod': 16913, '407': 28941, 'dartmouth': 16960, '##orro': 29459, 'ethnicity': 18240, '##rug': 26549, '##rogate': 21799, 'tn': 28286, 'rms': 29311, 'hub': 9594, '##imi': 27605, 'stocked': 24802, 'comfortably': 18579, '華': 1942, '1898': 6068, '1820s': 28504, 'adulthood': 20480, 'prelate': 26595, '##ecure': 29150, 'save': 3828, 'typhoon': 15393, '##oon': 7828, '##llan': 19036, 'smoothly': 15299, 'workers': 3667, 'defend': 6985, 'luciano': 24512, 'bracket': 21605, 'motif': 16226, 'unreasonable': 29205, 'tremble': 20627, '##ɔ': 29679, 'tier': 7563, '##ls': 4877, 'derbyshire': 15207, '1887': 6837, '[unused392]': 397, 'heavenly': 16581, 'repaired': 13671, 'adorable': 23677, 'topographic': 24254, 'whilst': 5819, 'caller': 20587, 'breed': 8843, 'ce': 8292, '##pia': 19312, 'lu': 11320, '小': 1829, 'quantities': 12450, '##cape': 19464, 'uno': 27776, 'genus': 3562, 'noticed': 4384, 'ambassador': 6059, 'exterior': 8829, 'parchment': 22433, 'roanoke': 25899, 'buses': 7793, 'emery': 24294, 'aren': 4995, '##end': 10497, 'proudly': 18067, '##line': 4179, '##rnet': 26573, '##kam': 27052, '320': 13710, 'exhaled': 16242, '##ud': 6784, '（': 1987, 'identities': 15702, 'proficiency': 26293, 'export': 9167, 'sha': 21146, 'thinner': 23082, 'distraught': 25348, 'demonstrate': 10580, 'nonsense': 14652, 'henri': 8863, '##tling': 15073, 'georg': 12062, '32nd': 20628, 'oppressive': 28558, '##！': 30512, 'backstage': 20212, '##fi': 8873, 'vietnamese': 9101, 'bliss': 13670, 'pistols': 18248, '國': 1800, 'interested': 4699, '##お': 30176, 'prank': 26418, '##itung': 28813, '[unused920]': 925, 'み': 1678, 'encompassing': 19129, '##zal': 16739, '##rva': 19146, 'spies': 16794, '##km': 22287, 'unicode': 27260, 'weakness': 11251, '##lim': 17960, 'bit': 2978, 'ɔ': 1112, 'watches': 12197, '##static': 16677, 'gael': 28151, '1643': 25534, '##rret': 27032, '[unused901]': 906, '##oca': 24755, 'sweeney': 21178, 'couple': 3232, 'nebraska': 8506, '260': 13539, '##rained': 27361, 'handled': 8971, '##mel': 10199, 'lust': 11516, 'bernhard': 21432, '##ifer': 23780, 'babe': 11561, 'hardened': 15015, '##cliffe': 18115, 'ruth': 7920, '##mada': 23574, 'smack': 21526, '##forth': 15628, '##cala': 25015, '##kian': 28545, 'wheeling': 29142, 'donegal': 25415, 'photo': 6302, '##55': 24087, 'affirmed': 19768, 'qualifications': 15644, '##books': 17470, 'lattice': 17779, 'orphaned': 27093, 'legendary': 8987, 'purchasing': 13131, 'forrest': 16319, '##あ': 30172, 'immigration': 7521, 'luxembourg': 10765, '##tzer': 29546, 'issuing': 15089, '##ч': 29752, 'sides': 3903, 'convenience': 15106, 'forte': 24898, 'acidic': 24171, '##dt': 11927, 'oder': 27215, 'hunter': 4477, 'accommodated': 28959, 'smacked': 19203, 'respecting': 27818, 'jozef': 23258, '##yam': 14852, 'clicked': 13886, 'topped': 9370, 'bogota': 21240, 'wasn': 2347, '##正': 30414, 'regiment': 3483, 'pornographic': 26932, '##sw': 26760, 'reserve': 3914, 'bowing': 26690, 'babylon': 17690, 'introduced': 3107, 'bottled': 26071, '[unused796]': 801, 'snuggled': 26867, '社': 1924, 'migrant': 20731, 'planning': 4041, 'priest': 5011, 'accused': 5496, 'municipalities': 7602, '[unused272]': 277, 'glory': 8294, 'mega': 13164, 'prowess': 26120, 'ნ': 1449, 'record': 2501, '##mona': 21781, '##otte': 28495, 'thomas': 2726, 'typical': 5171, 'padded': 20633, '295': 21679, '80s': 16002, '##ians': 7066, 'miocene': 26926, 'riders': 8195, 'ionic': 24774, '200': 3263, 'median': 3991, 'emotional': 6832, 'baxter': 14664, 'prove': 6011, '##ege': 24746, '##ening': 7406, 'yanking': 25716, 'condoms': 29094, 'consolation': 24831, '##tive': 6024, 'squealed': 26175, 'frankfurt': 9780, 'flawed': 25077, 'radiant': 23751, 'nightfall': 28018, 'graduated': 3852, 'emotionally': 14868, 'cult': 8754, 'tsunami': 19267, 'forerunner': 23993, 'unaware': 11499, 'leave': 2681, 'awesome': 12476, '1753': 23810, '##新': 30388, 'powder': 9898, '##pies': 13046, '##rogated': 26565, 'tia': 27339, '555': 29541, 'restroom': 28249, 'issue': 3277, '##ks': 5705, 'staged': 9813, 'eligible': 7792, 'farther': 8736, 'brass': 8782, '173': 19410, '##nese': 14183, 'wince': 29585, 'hated': 6283, 'rebecca': 9423, 'ann': 5754, '##vili': 21661, 'complied': 26946, 'savoy': 16394, 'disabilities': 13597, 'portsmouth': 10913, 'bedford': 12003, '##glia': 20011, 'artworks': 22000, 'suited': 10897, 'zinc': 15813, '##lese': 24527, 'bribe': 26470, 'gustave': 27973, 'reconstruction': 8735, '##º': 29662, '##ל': 29799, '≡': 1607, '1893': 6489, 'encompassed': 24058, '[unused39]': 40, 'rematch': 18229, 'mediterranean': 7095, 'restrict': 21573, 'renowned': 8228, 'enters': 8039, 'pulled': 2766, 'blazed': 25590, 'bordered': 11356, 'eritrea': 26040, 'standardization': 28648, '##sal': 12002, '700': 6352, 'gorgeous': 9882, 'skiing': 12701, 'defeats': 14222, 'concludes': 14730, 'sarajevo': 18354, 'discoveries': 15636, '’': 1521, 'outlook': 17680, 'kisses': 8537, 'chairperson': 19072, 'ambitious': 12479, 'comforting': 16334, 'ⁿ': 1546, 'globalization': 24370, '1989': 2960, 'galilee': 28928, '[unused576]': 581, 'nelson': 5912, 'interiors': 20769, 'whimper': 28544, '##ц': 29751, '†': 1526, '1809': 12861, 'ether': 28855, 'organisations': 8593, 'bonnet': 24367, 'coal': 5317, 'bumps': 18548, '##mere': 13759, 'rave': 23289, 'nets': 16996, '[unused836]': 841, 'kyoto': 15008, '##မ': 29973, 'croats': 26222, '##ff': 4246, 'internationale': 21339, 'drummond': 19266, '1747': 24522, '##ɒ': 29678, 'initiate': 17820, 'bullets': 10432, 'correlated': 23900, '##erson': 18617, 'powerhouse': 24006, 'dormant': 22170, 'ward': 4829, 'distinct': 5664, 'clinic': 9349, 'aerospace': 13395, '##vi': 5737, '##raph': 24342, 'contacted': 11925, 'around': 2105, 'keenan': 26334, 'unaffected': 24720, 'conclusion': 7091, 'noelle': 22336, 'curran': 19649, 'pursuits': 23719, 'engaged': 5117, 'preface': 18443, '501': 16202, '##cie': 23402, '350': 8698, '##dre': 16200, 'partnership': 5386, 'ct': 14931, 'vacated': 15348, 'streaks': 21295, 'streams': 9199, '¦': 1072, 'hungarian': 5588, 'spines': 20352, 'pandit': 29331, 'vent': 18834, 'ironic': 19313, 'ᵉ': 1499, 'private': 2797, '[unused350]': 355, 'materials': 4475, '[unused208]': 213, '##wy': 18418, '##furt': 24428, 'edith': 13257, 'seductive': 23182, '[unused437]': 442, 'skills': 4813, 'locomotives': 7830, 'accepted': 3970, 'concentrating': 16966, 'miscellaneous': 25408, '元': 1769, 'responds': 16412, '##table': 10880, '[unused598]': 603, 'livingstone': 27656, '216': 20294, 'doha': 26528, 'জ': 1358, 'sociology': 11507, '##cey': 25810, 'coaches': 7850, '##ison': 10929, 'wouldn': 2876, 'domenico': 18638, 'injuring': 22736, '##kla': 26086, 'operators': 9224, '##nton': 15104, 'vow': 19076, 'males': 3767, '##ه': 14157, 'exclaimed': 12713, 'lead': 2599, 'den': 7939, 'pedro': 7707, '1603': 25625, '##བ': 29967, 'α': 1155, 'ideas': 4784, '##rie': 7373, 'nintendo': 10022, 'reproduction': 14627, '2500': 25108, '[unused112]': 117, 'shadowed': 25843, 'jalan': 28410, 'derivation': 29280, 'midland': 12240, '平': 1839, 'alyssa': 26442, '##les': 4244, 'odin': 26195, 'ochreous': 23474, 'れ': 1688, 'almighty': 26668, '##ぬ': 30195, 'zombie': 11798, 'assaulted': 17536, 'sacrifices': 18502, 'representing': 5052, '[unused934]': 939, 'conde': 24707, 'chants': 29534, 'premio': 23031, '##lino': 25226, '##wer': 13777, 'avenue': 3927, 'historian': 5272, 'candace': 22905, 'takahashi': 25585, 'image': 3746, 'marek': 29318, 'critique': 16218, '##tery': 20902, 'earl': 4656, '##ani': 7088, '##um': 2819, 'exclusive': 7262, 'halloween': 14414, '土': 1801, 'samurai': 16352, 'unification': 16905, 'campeonato': 17675, 'spores': 23763, 'liturgy': 21176, 'cartoons': 13941, '##imov': 25299, 'oslo': 9977, 'helps': 7126, 'scanned': 11728, 'ventura': 21151, 'do': 2079, '##fa': 7011, 'novels': 6002, 'ai': 9932, 'stroking': 14943, '[unused318]': 323, 'tre': 29461, '##mata': 21022, 'somalia': 14717, 'drawn': 4567, 'furthermore': 7297, '##nie': 8034, 'melt': 14899, 'sends': 10255, 'usually': 2788, 'combat': 4337, 'unrelated': 15142, '[unused268]': 273, 'basilica': 13546, 'algerian': 16953, 'mocked': 24195, '##words': 22104, 'trigger': 9495, 'shielded': 25474, 'ب': 1271, 'rang': 8369, '禾': 1927, 'lined': 7732, 'curses': 23897, 'arrive': 7180, 'relevant': 7882, 'issued': 3843, '##me': 4168, 'patrols': 14703, 'foliage': 19624, 'affected': 5360, 'leningrad': 15930, 'swept': 7260, '##wheel': 22920, '##gold': 21270, 'cocky': 24995, 'similarly': 6660, 'উ': 1350, 'apply': 6611, 'serie': 8668, '##stone': 9221, 'operas': 14281, 'snarled': 15022, '##ocating': 27483, 'benefits': 6666, '##icus': 14239, 'ito': 23333, 'pads': 19586, 'screwing': 29082, 'accuse': 26960, 'priorities': 18402, '##rith': 24292, 'tapped': 10410, 'dai': 18765, 'genocide': 14052, 'bureaucracy': 25934, 'og': 13958, 'bought': 4149, 'scholastic': 24105, 'efficiently': 18228, '##雄': 30500, 'earthquake': 8372, 'overseeing': 19642, 'fundraiser': 28536, 'early': 2220, 'valentine': 10113, '1891': 6607, 'advised': 9449, '##tee': 17389, '##द': 29861, 'replacements': 23936, 'season': 2161, '49': 4749, 'elliptic': 29413, 'greek': 3306, 'sovereign': 11074, '##sie': 11741, 'instrumentalist': 29253, 'continuously': 10843, '##00': 8889, 'compact': 9233, 'ो': 1343, 'lightly': 8217, 'interactions': 10266, '99': 5585, 'accepting': 10564, '##ɬ': 29687, 'symptoms': 8030, '##box': 8758, '##ᵢ': 19109, 'ₒ': 1562, 'log': 8833, 'shout': 11245, 'continually': 14678, 'manipulating': 26242, 'managing': 6605, 'seminal': 20603, 'alvaro': 24892, 'へ': 1675, 'robyn': 28047, 'teams': 2780, '##mont': 9629, 'plaster': 15673, 'extremes': 28800, 'plain': 5810, 'adjusting': 19158, '郡': 1959, 'excitement': 8277, 'pedersen': 27409, 'infected': 10372, 'systems': 3001, 'bike': 7997, 'implement': 10408, 'novgorod': 24338, 'diocese': 5801, 'zhu': 15503, 'fifteen': 5417, 'pushed': 3724, 'enforce': 16306, '##bird': 9001, '##ggles': 24989, 'instructor': 9450, 'ᴮ': 1491, '##sti': 16643, 'throbbing': 17061, '##ʻi': 26444, 'follower': 22399, 'chant': 16883, 'convoy': 9549, 'sought': 4912, 'malta': 9933, 'impacts': 14670, '##pre': 28139, 'undercover': 16382, '##）': 30514, '##anu': 24076, '##gaon': 27073, 'substantial': 6937, '##ection': 18491, 'lim': 18525, 'encourages': 16171, 'rental': 12635, 'albanians': 25267, 'motionless': 19917, 'lenses': 15072, 'embarrassment': 14325, 'slender': 10944, '1638': 27497, '##kra': 22272, 'rust': 18399, 'reactivated': 28577, 'casts': 23942, '∞': 1601, 'transformation': 8651, 'उ': 1313, 'dwellers': 28857, 'cyclone': 11609, '[unused287]': 292, 'magistrate': 14351, '##ix': 7646, 'cap': 6178, 'uniforms': 11408, '[unused29]': 30, '##nse': 12325, 'fix': 8081, '##meter': 22828, 'alcoholic': 14813, 'dorsey': 27332, '##ols': 27896, 'dormitory': 21538, 'shuffling': 24770, '##mal': 9067, 'controversies': 25962, '##uram': 27618, 'mind': 2568, 'so': 2061, '##vating': 26477, 'borrowed': 11780, 'ₛ': 1570, '##zam': 20722, '##court': 13421, 'tai': 13843, 'fraudulent': 27105, '##kers': 11451, 'inauguration': 17331, 'pena': 19409, '##nock': 18057, 'slept': 7771, 'aristotle': 17484, 'borrowing': 23733, 'cairns': 21731, 'cody': 13326, 'yoo': 26823, 'brazil': 4380, 'traits': 12955, 'varieties': 9903, 'witch': 6965, 'relentless': 21660, 'smallest': 10479, 'enrolled': 8302, 'casimir': 24701, 'shrugged': 6345, 'telecommunication': 25958, 'weigh': 17042, 'joking': 16644, '##eria': 11610, '3d': 7605, 'remastered': 19484, '##ored': 19574, 'ս': 1234, 'cis': 20199, '##em': 6633, 'breakaway': 26321, 'joshi': 26645, 'sock': 28407, 'produce': 3965, 'bonded': 20886, 'examinations': 14912, '##（': 30513, 'transmit': 19818, '##ム': 30251, 'cranes': 27083, '##vision': 17084, 'wood': 3536, 'glowing': 10156, 'symbol': 6454, '##class': 26266, 'rink': 18416, 'ngc': 27645, 'is': 2003, '##kovsky': 23829, 'clergy': 11646, '600': 5174, '##eers': 22862, 'fitting': 11414, 'pablo': 11623, 'swell': 18370, 'repeat': 9377, 'whereabouts': 18913, '[unused224]': 229, '[unused854]': 859, 'subspecies': 11056, 'table': 2795, 'lay': 3913, 'parent': 6687, 'poison': 9947, '##lland': 26087, 'allocated': 11095, '##udge': 15979, '##glio': 29480, 'loosened': 22456, 'blur': 14819, 'turquoise': 28653, '##一': 30266, '##70': 19841, 'boys': 3337, '##head': 4974, '##ct': 6593, 'allie': 16944, '[unused356]': 361, 'ʲ': 1141, 'your': 2115, 'marked': 4417, 'kg': 4705, 'boundary': 6192, 'thom': 19438, 'ernesto': 22428, 'congregational': 20809, 'cl': 18856, '##eering': 20550, '##ز': 29823, 'screening': 11326, 'tam': 17214, 'softened': 16573, 'infants': 16725, 'postwar': 20013, '##rium': 18802, 'somerville': 27209, 'pinning': 22866, 'initiatives': 11107, 'carr': 12385, 'torn': 7950, '##や': 30208, '##uously': 28078, '[unused567]': 572, 'broke': 3631, '##op': 7361, '##mana': 24805, 'consumers': 10390, 'otter': 22279, 'options': 7047, ',': 1010, 'spider': 6804, '1670': 26253, 'approximation': 20167, 'logical': 11177, 'resume': 13746, 'grupo': 26678, 'empowerment': 23011, 'oceania': 20526, 'disrepair': 27799, 'budget': 5166, 'ᅭ': 1477, 'carpets': 27997, 'pier': 10356, 'utterly': 12580, '##zz': 13213, '##loid': 27710, '##zzled': 17269, 'source': 3120, '##€': 30102, 'patient': 5776, '[unused377]': 382, '##ides': 8621, 'leto': 24543, 'girl': 2611, '##⁵': 30074, '##σ': 29733, 'swallowing': 18468, 'fated': 27442, 'concepts': 8474, 'counter': 4675, 'visuals': 26749, 'ක': 1403, 'pv': 26189, '##insky': 20894, '##promising': 25013, 'cr': 13675, '1688': 24082, 'group': 2177, '##gical': 26715, '##dur': 24979, 'grid': 8370, '##ʲ': 29707, 'powerful': 3928, 'sighting': 29426, '##logist': 10727, 'pakistan': 4501, 'adi': 27133, 'adviser': 11747, 'painters': 12499, 'bids': 20723, '[unused251]': 256, 'christina': 12657, 'feral': 18993, 'insects': 9728, '##lion': 18964, 'yankee': 17652, '##waite': 29601, '1895': 6301, 'gradually': 6360, '##立': 30457, 'vest': 17447, 'barrister': 19805, '##ouring': 27897, 'lending': 18435, 'psychology': 6825, 'bermuda': 13525, 'fran': 23151, 'displeasure': 28606, 'proves': 16481, '##tham': 22536, 'premiere': 6765, 'advisors': 18934, 'advantages': 12637, 'weiss': 17889, 'sylvia': 13378, 'simone': 14072, 'joked': 19700, 'degraded': 26131, 'drown': 19549, '##itical': 26116, '##天': 30337, '##vel': 15985, 'contender': 20127, '##ono': 17175, 'identify': 6709, 'marius': 20032, 'denying': 16039, 'preston': 8475, '##ax': 8528, 'compressed': 16620, 'ordnance': 14445, '##ssa': 11488, 'ability': 3754, 'boyer': 23456, 'greta': 26111, '[unused710]': 715, '風': 1977, 'saxe': 24937, '##tp': 25856, 'moderate': 8777, 'descending': 15127, 'hanging': 5689, 'mines': 7134, '##gger': 13327, 'references': 7604, 'birthplace': 14508, 'resigning': 24642, 'continent': 9983, 'insider': 25297, 'refused': 4188, '##logies': 21615, 'advent': 13896, 'madhya': 20841, 'daniels': 13196, 'fluids': 20989, '##wk': 26291, 'sofia': 8755, 'stem': 7872, 'त': 1323, 'maybe': 2672, 'ron': 6902, 'doris': 15467, 'torah': 16297, 'array': 9140, 'narrative': 7984, 'blending': 23293, 'gabled': 26182, '##rney': 27114, 'motorcycle': 9055, 'earthly': 29520, 'anyone': 3087, 'courtyard': 10119, 'appointments': 14651, 'bum': 26352, 'quieter': 27486, 'ק': 1264, 'decisive': 13079, 'providing': 4346, 'reversed': 11674, 'dissolution': 12275, 'outputs': 27852, '[unused930]': 935, 'shuttle': 10382, '[unused305]': 310, 'laundering': 28289, 'retrieval': 26384, 'convinces': 19480, 'harpsichord': 26504, '##dlow': 27002, 'scorpion': 22015, 'thousands': 5190, 'discusses': 15841, 'sited': 28603, 'members': 2372, 'race': 2679, 'doctoral': 11316, 'plasma': 12123, 'determining': 12515, '松': 1880, '##gr': 16523, '1710': 23841, '[unused124]': 129, 'netherlands': 4549, 'apartheid': 17862, 'astronaut': 19748, 'lineman': 24062, 'substantially': 12381, 'busiest': 20530, 'ceased': 7024, 'outlets': 11730, 'establishing': 7411, '##carbon': 26190, 'sleep': 3637, 'practitioners': 14617, 'thumped': 28963, 'malibu': 29047, 'adjective': 24931, 'dismay': 20006, '[unused5]': 6, '[unused254]': 259, 'dso': 22951, 'rented': 12524, '1862': 6889, '[unused419]': 424, 'specific': 3563, 'sculpted': 19921, 'ko': 12849, 'butler': 7055, 'auxiliary': 9830, 'tugs': 29306, 'maria': 3814, 'ᵃ': 1496, 'herbs': 17561, 'thug': 26599, '##आ': 29848, 'crates': 27619, 'achieving': 10910, 'buckley': 17898, '##j': 3501, 'notorious': 12536, 'backdrop': 18876, 'plans': 3488, 'pending': 14223, '##sh': 4095, 'eliminating': 15349, 'ndp': 21915, '##inn': 23111, 'blooms': 29037, 'metal': 3384, 'invested': 11241, 'regret': 9038, 'radiated': 23000, 'migrating': 28636, 'foe': 22277, 'finishing': 5131, 'values': 5300, 'wexford': 23121, 'indus': 27746, '##thy': 16921, '##01': 24096, 'specialization': 28031, 'structural': 8332, 'var': 13075, 'variants': 10176, 'click': 11562, '##quitable': 28298, 'dorm': 19568, 'gu': 19739, 'craftsman': 26286, '##比': 30416, '[unused270]': 275, 'deck': 5877, 'happier': 19366, 'undead': 17315, 'humor': 8562, 'alfa': 22989, 'clumsy': 22902, '##21': 17465, 'siena': 23754, '##nder': 11563, 'walkers': 22559, 'vance': 16672, 'conserve': 27749, 'impoverished': 25488, 'detachment': 11009, 'rna': 12987, 'wagons': 14525, 'ligand': 27854, 'appeared': 2596, 'words': 2616, 'tortured': 12364, 'reprise': 16851, '前': 1776, 'racing': 3868, 'predicted': 10173, '[unused203]': 208, 'mccarthy': 12584, '##中': 30272, 'ballots': 17069, 'knox': 11994, 'shropshire': 19322, 'highness': 17114, 'qur': 23183, '##erton': 20995, '##bh': 23706, '合': 1792, 'instructional': 23219, 'argyle': 26503, 'inquiry': 9934, 'embarrassed': 10339, 'baseball': 3598, 'barton': 12975, 'bahn': 17392, '##ition': 22753, 'restricted': 7775, 'thierry': 26413, 'relaxing': 19613, 'gen': 8991, 'vineyard': 18621, '[unused712]': 717, '©': 1075, '##eck': 11012, '##actic': 28804, 'austria': 5118, 'distributor': 16632, '⟩': 1630, 'parked': 9083, 'institution': 5145, 'gate': 4796, 'cadet': 12738, '1832': 10212, '##ס': 29804, 'division': 2407, 'navigation': 9163, 'influenced': 5105, 'suspension': 8636, 'specials': 19247, 'swedish': 4467, 'dale': 8512, 'porcelain': 17767, 'goal': 3125, '001': 25604, '337': 28489, 'milo': 20359, 'previous': 3025, 'sipped': 17735, 'buds': 26734, 'silent': 4333, '215': 17405, '##)': 29620, 'excellent': 6581, 'wiltshire': 17045, 'pursuant': 27081, 'supporter': 10129, 'acquaintances': 29409, 'outer': 6058, '##ntes': 17340, 'roller': 11220, 'undermine': 25174, '##し': 30183, 'ridley': 20608, 'vastly': 24821, 'chong': 24008, 'disguise': 14249, '²': 1082, 'adjutant': 20690, 'school': 2082, 'paternal': 15112, '##smo': 25855, 'seizing': 24681, '##kovic': 14733, 'philanthropic': 25321, 'unreleased': 13270, '##psy': 18075, 'polling': 17888, 'kept': 2921, 'morris': 6384, 'goose': 13020, 'reasonable': 9608, 'fielded': 20988, 'tutor': 14924, 'african': 3060, '[unused958]': 963, '##chy': 11714, 'contrast': 5688, 'verdict': 14392, 'investigators': 14766, '625': 22810, 'specimen': 11375, 'famine': 15625, '##national': 25434, 'loretta': 28493, '2d': 14134, '[unused731]': 736, '##idae': 6096, 'confesses': 22826, 'tourism': 6813, '##tama': 28282, '##ו': 29792, 'parasite': 21198, 'inward': 20546, 'hushed': 24033, 'safeguard': 28805, 'tyres': 24656, 'undertaken': 10607, '1836': 10081, '##fc': 11329, 'lodge': 7410, 'ᶠ': 1512, 'loyola': 21580, '38th': 22051, 'tricked': 24929, '##anna': 25789, 'pg': 18720, '##flight': 28968, 'necessitated': 29611, 'lastly': 22267, '«': 1077, '##nas': 11649, 'marches': 20691, 'capitals': 15433, 'rap': 9680, 'holocaust': 11513, '[unused205]': 210, 'urged': 9720, 'enthusiastically': 24935, 'pere': 23976, 'falling': 4634, 'performances': 4616, 'auction': 10470, 'hotels': 9275, 'richer': 26108, 'treaties': 16013, 'contributes': 16605, '##leader': 19000, '##京': 30281, 'earle': 23888, 'math': 8785, '##ark': 17007, 'sums': 20571, '##ヒ': 30245, 'shops': 7340, 'amir': 18904, '##by': 3762, '##dust': 24220, 'guillermo': 21070, 'adherence': 29235, 'ہ': 1308, 'cares': 14977, 'blinding': 19709, 'mcconnell': 28514, '##rim': 20026, 'automotive': 12945, 'possibility': 6061, 'bali': 20222, 'ronan': 18633, '÷': 1099, 'dragoons': 28069, 'cameroon': 13841, '##kis': 14270, 'young': 2402, '1850': 7973, '[unused657]': 662, 'kazan': 22001, 'expand': 7818, 'minerva': 27383, 'ण': 1322, 'celebrity': 8958, 'recreated': 29414, '##yr': 12541, 'joaquin': 16877, 'fireplace': 13788, 'fluttered': 18360, 'trenches': 19874, 'llc': 11775, '##ghan': 22218, '[unused365]': 370, 'alterations': 16705, 'rubble': 17538, 'younger': 3920, '##unced': 22392, 'hours': 2847, 'hips': 6700, '[unused769]': 774, 'mingled': 25959, 'flavour': 28126, 'groundbreaking': 23222, '“': 1523, 'associate': 5482, 'handy': 18801, 'latvian': 14698, '##uising': 28580, 'push': 5245, 'salmon': 11840, '##quay': 25575, 'cote': 17155, 'reaches': 6561, 'moved': 2333, 'gettysburg': 22577, '##gai': 23805, '##andra': 29159, 'cantata': 23629, '[unused980]': 985, '##49': 26224, 'powered': 6113, 'connect': 7532, '##ressed': 16119, 'delivery': 6959, '##opsis': 22599, 'consumed': 10202, 'enhancement': 22415, 'watkins': 17055, 'fai': 26208, 'deeply': 6171, 'exceptions': 11790, 'contempt': 17152, '##bbs': 17226, '##stis': 29472, '[unused629]': 634, 'artemis': 19063, 'bills': 8236, '##ncy': 9407, 'dt': 26718, 'research': 2470, 'literary': 4706, '##work': 6198, 'historians': 7862, 'square': 2675, 'isn': 3475, 'suffered': 4265, 'attributes': 12332, 'chases': 29515, 'garlic': 20548, '999': 25897, 'faye': 19243, '[unused458]': 463, '£': 1069, 'say': 2360, 'trusts': 20278, 'montana': 8124, 'hamid': 24811, '##rika': 23778, 'bianca': 18051, 'marriage': 3510, 'testament': 9025, 'assumed': 5071, 'following': 2206, 'ames': 19900, 'reports': 4311, 'nba': 6452, '##ء': 29815, '##sm': 6491, 'sublime': 28341, 'realising': 27504, 'glover': 20012, '門': 1968, 'communicated': 24162, 'ist': 21541, 'todd': 6927, '[unused833]': 838, 'collins': 6868, 'dismiss': 19776, 'conferred': 15186, 'difficulty': 7669, 'camille': 16675, 'marissa': 22767, 'paris': 3000, 'soccer': 4715, '[unused450]': 455, '[unused782]': 787, '##jevic': 26782, 'ள': 1393, 'physically': 8186, 'pembroke': 21457, 'remade': 26943, 'abyss': 22159, '我': 1855, 'unfit': 29439, '[unused955]': 960, 'dyed': 28432, 'wrists': 12150, 'rubin': 20524, 'ಾ': 1402, 'amendment': 7450, 'neural': 15756, 'christoph': 21428, 'people': 2111, '##hine': 14014, 'encouragement': 15846, '[unused606]': 611, '##abas': 27537, 'sponsor': 10460, 'sedimentary': 25503, 'woman': 2450, 'practicing': 12560, '##sper': 17668, 'thought': 2245, '##nous': 18674, 'dani': 19522, 'ill': 5665, 'technology': 2974, 'đ': 1102, 'frequently': 4703, 'ting': 28642, '##movable': 25661, 'iona': 22347, '##osi': 20049, 'daemon': 12828, 'caste': 14542, 'arts': 2840, '1925': 4849, '##wehr': 27156, 'cfa': 28125, '##tang': 26067, 'chrysler': 17714, '1649': 26538, '##hic': 16066, 'she': 2016, 'goin': 17249, 'exchanges': 15800, 'libby': 19533, '##oven': 25592, 'campaign': 3049, '##helm': 24546, 'chieftain': 26625, 'packed': 8966, 'george': 2577, '##lco': 22499, 'gland': 25320, 'bernie': 15941, 'ported': 27650, 'moss': 10636, '##oh': 11631, 'shave': 27545, '##yas': 16303, 'brother': 2567, 'onions': 24444, 'afar': 28978, '部': 1960, 'pass': 3413, '[unused230]': 235, 'precious': 9062, 'shifted': 5429, 'greedy': 20505, '59': 5354, 'melting': 13721, 'mu': 14163, 'astonishing': 26137, '##郎': 30484, 'gloom': 24067, 'piercing': 14628, 'test': 3231, 'hitler': 8042, 'patiently': 19080, 'knicks': 27817, 'beautifully': 17950, 'vishnu': 17647, '##old': 11614, '##rasia': 27839, 'made': 2081, 'polymers': 27216, 'flies': 10029, '##sha': 7377, 'administering': 28965, 'crop': 10416, '##bian': 15599, 'pausing': 20490, 'monster': 6071, 'practice': 3218, 'voltage': 10004, '##nam': 13129, '##ror': 29165, 'triangles': 27189, 'featuring': 3794, 'proceedings': 8931, 'valencia': 13083, 'nagasaki': 27107, 'choose': 5454, 'tones': 12623, '##grapher': 18657, 'package': 7427, 'manuel': 7762, '##oa': 10441, 'sud': 19219, '[unused748]': 753, '##escence': 28964, '[unused804]': 809, '##izan': 27334, 'skipped': 16791, 'engine': 3194, 'vents': 28287, 'prepares': 20776, 'nervously': 12531, '##rat': 8609, 'dies': 8289, 'belmont': 17449, 'tanks': 7286, 'game': 2208, 'warsaw': 8199, 'fangs': 11738, '##6th': 25994, '##3d': 29097, 'resistant': 13070, '##pel': 11880, 'mirage': 21483, '##nist': 26942, 'jayne': 24408, 'ᴺ': 1494, 'paula': 13723, '##ense': 16700, 'भ': 1330, 'helens': 24074, 'emir': 23434, '##ilo': 22360, 'cambodian': 24417, 'ok': 7929, 'imaging': 12126, 'necklace': 13016, 'connacht': 27062, 'biscuits': 27529, 'wat': 28194, '[unused461]': 466, '##eal': 15879, 'accession': 16993, '7': 1021, '29': 2756, 'edwin': 10049, '##₹': 30104, 'dir': 16101, 'collapsed': 7798, 'regeneration': 20045, 'abundance': 14531, '##mins': 21266, 'acknowledges': 28049, '##aby': 21275, 'ワ': 1736, 'martial': 7761, 'evident': 10358, 'infections': 15245, 'vows': 16495, 'cuffs': 24347, 'opponent': 7116, 'rica': 11509, 'offer': 3749, '##ith': 8939, 'appealing': 16004, '264': 21611, '##nut': 24072, '##mian': 20924, 'topological': 24309, 'jena': 27510, '##tma': 29418, '##№': 30106, '[unused311]': 316, 'intervened': 21116, '1903': 5778, 'birthday': 5798, '##fication': 10803, 'newport': 9464, 'ས': 1436, '##cel': 29109, 'recess': 28290, 'koch': 15259, 'warwick': 13283, 'serene': 25388, 'chinese': 2822, '##holders': 17794, 'adapting': 25357, 'lo': 8840, '[unused298]': 303, 'avail': 24608, 'horn': 7109, '##hane': 28006, 'cousins': 12334, 'dinah': 26562, 'promotes': 14067, 'keeps': 7906, 'basket': 10810, '##pped': 11469, '##ifier': 18095, '18': 2324, 'sol': 14017, 'admitted': 4914, 'guides': 12468, 'crook': 19302, 'luis': 6446, 'panthers': 12915, '##iny': 24300, '##bel': 8671, 'treated': 5845, 'prosecution': 11537, 'grinding': 16153, 'hair': 2606, 'winds': 7266, '―': 1518, 'primetime': 18474, '##ardo': 24649, '25th': 10965, '##bine': 16765, 'gallant': 26984, '##nitz': 22792, '##eux': 20860, '##olar': 19478, '##:': 29627, 'madeira': 27309, 'cross': 2892, 'proved': 4928, '##its': 12762, '##iction': 28097, 'indies': 9429, 'favour': 7927, 'springer': 17481, 'stockholm': 8947, 'stalk': 23899, '##lative': 26255, 'amulet': 25087, 'urgently': 25478, 'compartments': 27998, 'nickname': 8367, 'boots': 6879, 'geneva': 9810, '##pine': 19265, 'schumann': 29448, 'apprehension': 25809, '##kai': 11151, '##『': 30169, 'unleashed': 22416, 'punished': 14248, 'overrun': 24672, 'slope': 9663, '##bled': 23242, 'britannia': 24677, 'kyle': 7648, '##22': 19317, 'そ': 1660, 'philippe': 11169, 'splitting': 14541, 'draught': 25349, 'centers': 6401, 'bail': 15358, 'nonfiction': 25753, 'ivo': 28346, 'filmed': 6361, 'causes': 5320, 'inn': 7601, '##yed': 20821, 'slapping': 22021, 'algiers': 22947, '##ming': 6562, '1966': 3547, 'tommy': 6838, 'ʁ': 1128, 'nottingham': 11331, 'going': 2183, '30th': 13293, 'lamar': 19756, 'appalled': 29279, 'grounds': 5286, '##☆': 30148, 'questioned': 8781, 'scholarships': 15691, '##क': 29851, 'dessert': 18064, '##♥': 30152, 'growth': 3930, 'cruelty': 18186, 'spaniards': 20999, 'art': 2396, 'ricky': 11184, 'cora': 17195, 'rod': 8473, 'belize': 18867, 'developed': 2764, 'breuning': 18177, 'duane': 27319, '##phon': 20846, 'tactical': 8608, '##isches': 27239, 'pickering': 27078, '##ragan': 28905, 'murderer': 13422, '##bi': 5638, 'silk': 6953, 'equivalent': 5662, 'freshwater': 12573, 'fault': 6346, 'immigrants': 7489, 'octopus': 24318, 'transportation': 5193, 'besieged': 17923, 'expanse': 22944, '##otted': 26174, '##ef': 12879, 'sorcerer': 23324, 'grammar': 8035, '44th': 26409, 'fourteen': 7426, '##imo': 16339, 'hickory': 28158, 'rituals': 13549, '##ipe': 15457, 'cargo': 6636, 'blanchard': 26870, '1652': 28853, 'marks': 6017, 'cfl': 18830, '##26': 23833, '[unused210]': 215, '1848': 7993, 'preventing': 10723, 'attendants': 26727, 'canyon': 8399, '1729': 28449, 'liberties': 18271, '##att': 19321, '##₤': 30100, 'pointe': 26119, 'targeting': 14126, 'roles': 4395, 'preacher': 14512, 'enough': 2438, 'courier': 18092, 'depicts': 11230, 'perennial': 14638, 'gripping': 13940, 'scripts': 14546, '##itch': 20189, '##lard': 20822, 'leopard': 16240, 'clemson': 25906, '[unused390]': 395, 'bother': 8572, 'montreal': 5548, 'organism': 15923, 'slovenia': 10307, '₇': 1554, 'testimony': 10896, 'hon': 10189, 'mock': 12934, '[unused183]': 188, 'constrained': 27570, 'downstream': 13248, 'staff': 3095, 'townsend': 16139, 'mcgrath': 23220, 'deluxe': 15203, 'futile': 24495, 'boon': 23264, 'boasted': 23390, 'hose': 21290, '##ος': 15297, 'peoples': 7243, 'dogs': 6077, 'wherein': 16726, '[unused793]': 798, 'slalom': 19617, 'showtime': 23811, 'mumbled': 11567, 'identifies': 14847, 'monument': 6104, '##ski': 5488, 'ua': 25423, '[unused792]': 797, '[unused220]': 225, 'simply': 3432, '1981': 3261, 'pencil': 14745, '##ej': 20518, '##巿': 30363, 'angel': 4850, 'annually': 6604, 'platinum': 8899, '##a': 2050, 'fatigue': 16342, 'copying': 24731, 'neolithic': 19061, 'imply': 19515, '1910s': 28088, 'archer': 11024, 'alive': 4142, '##xx': 20348, 'invoked': 24959, 'pods': 26723, '##holding': 23410, 'discomfort': 17964, '##folk': 29284, 'rte': 20375, 'pharaoh': 22089, '##dou': 26797, 'transaction': 12598, 'imposed': 9770, 'desperation': 15561, 'impress': 17894, 'gingerly': 26961, 'romanesque': 17135, 'canopy': 14582, 'satellite': 5871, 'roofs': 15753, '##lean': 20898, 'rains': 15811, '##ctors': 24817, '##ₛ': 30098, '##yah': 17560, 'ranking': 5464, 'dd': 20315, 'newell': 28052, 'banco': 28678, 'flavor': 14894, '1716': 28204, '##oth': 14573, 'airspace': 29357, 'totally': 6135, 'asbestos': 29191, 'titus': 18828, '##夏': 30334, '1937': 4347, 'sweet': 4086, 'asiatic': 23983, 'impressed': 7622, 'chunks': 24839, 'pumps': 15856, '##ats': 11149, '##link': 13767, 'ζ': 1160, 'soared': 29127, 'mustang': 18851, 'u': 1057, 'leach': 24520, 'lucille': 28016, 'donors': 17843, 'interstate': 7553, '[unused341]': 346, 'henrik': 18745, 'korean': 4759, 'shooting': 5008, '##↓': 30114, 'acknowledged': 8969, 'penetrated': 21653, 'intercontinental': 18725, 'ithaca': 27939, 'slower': 12430, 'maurice': 7994, 'garnered': 13056, 'stripping': 23987, 'ァ': 1692, '##gens': 21230, 'fictitious': 23577, 'mouthed': 20521, 'colored': 6910, 'ए': 1314, '##cks': 10603, 'stylized': 19551, 'prophets': 23172, 'eponymous': 15248, '##kia': 21128, 'emancipation': 22656, 'auguste': 20758, 'brightest': 26849, 'auburn': 12704, 'communism': 15523, 'reviewed': 8182, 'corporate': 5971, 'pavel': 18635, 'owens': 14824, '##el': 2884, 'amateurs': 24361, '##fers': 24396, 'zachary': 19474, 'nicholas': 6141, '##rell': 16230, 'further': 2582, 'attempt': 3535, 'watch': 3422, 'sioux': 16615, 'loser': 10916, 'geoff': 14915, 'ா': 1395, 'balfour': 27560, 'compliment': 19394, 'grim': 11844, 'xinjiang': 25904, 'advertising': 6475, '##nc': 12273, 'utrecht': 18361, 'wise': 7968, '##eving': 23559, 'overturned': 17068, 'ignited': 22395, 'honoring': 21494, '##wave': 16535, '▪': 1618, 'pious': 25020, 'hay': 10974, 'markers': 16387, 'assemble': 21365, 'cs': 20116, 'liberia': 18039, '1784': 16496, '[unused573]': 578, '##aurus': 17342, 'ethiopian': 15101, '[unused125]': 130, 'carroll': 10767, 'credibility': 21553, 'suggest': 6592, '##lace': 19217, 'litter': 19070, '190': 11827, '##−1': 27944, 'buckinghamshire': 21712, 'shatter': 27271, 'フ': 1720, 'fa': 6904, 'curtis': 9195, 'nero': 19212, '##rail': 15118, 'hoc': 21929, '##uid': 21272, '##ξ': 29729, 'customized': 28749, 'error': 7561, 'eclectic': 20551, 'previously': 3130, 'surrounding': 4193, 'horatio': 27752, 'pew': 29071, 'cunning': 23626, 'elevations': 18166, 'proximity': 10039, '##tre': 7913, 'deposits': 10042, 'wagon': 9540, '##raf': 27528, '##gua': 19696, '##ores': 16610, 'dolls': 14421, '##ى': 29837, 'antics': 27440, 'showcasing': 27696, 'accustomed': 17730, '##game': 16650, 'severn': 24753, '##ify': 8757, 'wah': 22894, 'revelations': 22191, 'answered': 4660, 'begins': 4269, 'divers': 18612, '1749': 24704, 'yves': 19687, 'glee': 18874, 'haul': 14655, 'timely': 23259, 'ge': 16216, '##growth': 26982, 'comprised': 11539, 'vibe': 21209, 'sidekick': 29240, 'trash': 11669, 'renewed': 9100, 'dude': 12043, 'mal': 15451, 'steamship': 23869, 'breakers': 24742, '##iography': 26535, 'patrolling': 24248, 'inspections': 29589, '##ett': 6582, 'use': 2224, 'scratched': 15047, '##lea': 19738, 'wyoming': 10622, 'jacksonville': 13057, '##nka': 25804, 'sourced': 23184, 'acres': 4631, 'brooding': 28902, '##eira': 21302, 'fairfax': 17833, 'surplus': 15726, '##lastic': 28723, 'salem': 10389, '##キ': 30227, 'mobile': 4684, '西': 1947, 'productions': 5453, 'staying': 6595, '[unused333]': 338, '##lves': 20899, 'metric': 12046, 'saint': 3002, '##udes': 22087, 'quiz': 19461, '##ographic': 13705, '##ar': 2906, 'wilkes': 20635, 'pouch': 21445, '1776': 13963, '##י': 29796, 'pillars': 13766, 'respect': 4847, 'homer': 11525, 'recipes': 19328, 'surviving': 6405, '##ν': 16177, 'thicker': 19638, '##ured': 12165, 'bedside': 19475, 'godzilla': 26631, '##era': 6906, 'inning': 12994, 'spared': 16891, 'engages': 24255, 'quoting': 27394, '##fia': 22749, 'champs': 29008, '##o': 2080, 'sleeves': 15114, 'bert': 14324, 'injuries': 6441, 'novi': 21574, 'sculpture': 6743, 'investigating': 11538, 'totals': 21948, 'aloud': 12575, '1736': 28192, 'whisper': 7204, 'irregular': 12052, 'complement': 13711, 'induce': 19653, 'xiii': 15031, 'lenny': 19065, 'maxim': 20446, 'saints': 6586, 'russ': 18072, '1735': 26063, '2014': 2297, 'bombed': 18897, 'filed': 6406, 'fire': 2543, '##ein': 12377, 'slap': 14308, '₍': 1558, 'toys': 10899, '##isman': 20257, '##morphic': 18078, '##fulness': 20938, 'ivy': 7768, 'relaxation': 23370, 'poems': 5878, '74': 6356, 'indigenous': 6284, 'stares': 14020, '##combe': 14149, 'nicaragua': 15448, 'kappa': 16000, '##boards': 15271, 'childless': 26983, 'employee': 7904, 'influences': 8092, 'silva': 11183, 'pursue': 7323, 'instances': 12107, '##pool': 16869, '[unused478]': 483, 'emotion': 7603, 'peering': 16740, 'significance': 7784, '##tner': 18885, 'weston': 12755, 'philadelphia': 4407, 'morocco': 9835, 'ibm': 9980, 'grassroots': 23299, '[unused799]': 804, '23': 2603, '##ount': 21723, '##fire': 10273, 'beverages': 21705, 'chronological': 23472, 'remington': 25282, 'biceps': 27947, 'cut': 3013, 'connects': 8539, 'rotor': 18929, '##flower': 14156, 'goodbye': 9119, 'stacy': 18566, '##90': 21057, '240': 11212, '267': 25491, '##uma': 12248, '♦': 1626, 'silenced': 25030, 'resolutions': 18853, 'draws': 9891, 'crowned': 10249, 'vimes': 28482, 'formally': 6246, 'rune': 23276, 'swap': 19948, '[unused222]': 227, 'pioneer': 7156, '##uve': 22909, 'israeli': 5611, '##bley': 29538, 'mechanics': 9760, 'boom': 8797, 'bigger': 7046, 'percussion': 6333, 'stronger': 6428, 'boulevard': 8459, '##ashi': 12914, 'characterization': 23191, 'forgotten': 6404, 'malaga': 27382, 'distributing': 20083, 'throbbed': 27714, '##otype': 26305, 'threatening': 8701, 'cried': 6639, 'pills': 15345, '##gli': 25394, 'republicans': 10643, 'mounting': 15986, '##ouin': 25058, 'johns': 11545, '##明': 30391, 'manufactures': 22027, 'resultant': 28573, 'eh': 15501, 'laborers': 23428, 'calendar': 8094, 'melissa': 9606, 'skinny': 15629, 'divisional': 14167, 'maru': 26280, '154': 16666, '##aton': 22436, '##ill': 8591, 'greeted': 11188, 'clinton': 7207, '##ku': 5283, 'bala': 21451, 'kei': 26679, 'and': 1998, 'predominantly': 9197, '夫': 1813, 'rain': 4542, 'as': 2004, 'privileges': 14310, 'clue': 9789, '##ट': 29856, '[unused827]': 832, 'nautical': 11339, '帝': 1838, '##rly': 21194, 'layer': 6741, 'scandals': 29609, 'ח': 1248, '##ᅩ': 30011, 'exception': 6453, 'leslie': 8886, 'figure': 3275, '##ched': 7690, 'relics': 16712, 'colbert': 23928, 'tornado': 11352, '##ɨ': 29684, '[unused464]': 469, 'nicolas': 9473, '##raya': 29539, 'dumping': 23642, '[unused680]': 685, 'mold': 18282, 'bonaparte': 20830, 'spokesman': 14056, '##lore': 20186, '##ucci': 16835, '™': 1580, 'clifton': 16271, '1829': 11523, 'county': 2221, 'secured': 7119, 'footed': 28856, 'quarterback': 9074, 'launch': 4888, 'aground': 29406, '##ange': 22043, 'penalties': 12408, ']': 1033, 'navigate': 22149, 'tacoma': 22954, '[unused844]': 849, 'thrive': 25220, 'railway': 2737, 'synonym': 10675, 'countryside': 10833, '##unt': 16671, '־': 1240, 'fusiliers': 27207, 'marty': 12578, '##economic': 23035, 'almond': 26011, 'configured': 26928, 'cruz': 8096, 'bollywood': 16046, 'packaged': 21972, '##apa': 22068, 'alma': 11346, 'jock': 23407, 'catastrophe': 25539, 'societe': 18341, '##cted': 10985, 'melvin': 20993, 'restart': 23818, 'linger': 26577, 'commercial': 3293, 'condom': 20910, 'grotesque': 27707, 'ku': 13970, 'monkeys': 17059, 'siegel': 27996, 'profession': 9518, 'tertiary': 13553, 'seas': 11915, 'robbers': 28019, '⟨': 1629, '##fp': 22540, 'albany': 10283, 'tug': 12888, 'patterson': 12424, 'ன': 1387, 'since': 2144, 'agra': 29542, 'funds': 5029, 'scarlet': 11862, '[unused122]': 127, 'claudius': 25017, 'ineffective': 20694, '##む': 30205, 'mohammad': 12050, '##base': 15058, 'pronunciation': 15498, '##rish': 18774, 'dmitry': 22141, 'reconstructed': 14858, '##門': 30494, 'melinda': 23721, '##ь': 23742, 'prompt': 25732, 'starved': 26042, 'auckland': 8666, 'kilometers': 7338, 'headache': 14978, 'fiat': 18550, 'summon': 18654, 'wendell': 24526, 'sb': 24829, 'rural': 3541, '##ན': 29965, '222': 19015, '₈': 1555, 'discovering': 13648, 'knocked': 6573, 'quickly': 2855, 'gao': 17377, 'spraying': 29035, 'chopper': 28057, 'rowe': 20538, 'adept': 26398, 'opened': 2441, 'alone': 2894, 'sodium': 13365, 'blasting': 26239, 'promoted': 3755, 'analog': 11698, 'statewide': 13486, 'pulitzer': 17618, '##witz': 15362, 'zhejiang': 26805, 'sadness': 12039, 'cesar': 14923, '##aneous': 17191, 'snort': 26759, 'tire': 12824, 'yards': 4210, 'widespread': 6923, 'deacon': 14845, 'parramatta': 22820, 'limerick': 15679, 'admits': 14456, 'zoe': 11199, 'sexes': 21024, 'acronym': 20137, 'br': 7987, 'quietly': 5168, '[unused211]': 216, '##grade': 24170, 'mile': 3542, 'arranging': 19018, 'collection': 3074, 'mt': 11047, '##ares': 17933, 'liu': 8607, 'disney': 6373, '##lord': 19980, 'machinery': 10394, 'thy': 15177, 'exasperated': 24379, 'hygiene': 19548, 'encompasses': 13974, 'secretly': 10082, 'maiden': 10494, 'hyderabad': 13624, 'fame': 4476, '1934': 4579, 'reddish': 14182, 'ras': 20710, 'chose': 4900, 'rebuild': 14591, 'pudding': 29593, 'stages': 5711, '##ᵣ': 30043, 'mayors': 21941, 'dustin': 24337, '##ibility': 13464, '##ང': 29963, 'period': 2558, '##isms': 22556, 'robbie': 12289, 'node': 13045, 'item': 8875, 'な': 1667, 'typing': 22868, 'inducing': 29290, 'champagne': 12327, 'booklet': 19271, '[unused406]': 411, 'ヘ': 1721, 'adored': 28456, '##ggio': 20650, 'covert': 19813, '2000': 2456, 'originated': 7940, 'kai': 11928, 'some': 2070, 'obvious': 5793, 'flipped': 9357, 'ɛ': 1115, 'analogous': 19639, 'thrill': 16959, 'strive': 29453, 'northeastern': 8763, 'infinite': 10709, '##—': 30052, 'religious': 3412, 'steamboat': 24897, 'cohen': 9946, 'tentative': 19943, 'drums': 3846, '410': 19151, 'dead': 2757, '##藤': 30470, 'bricks': 14219, 'classification': 5579, 'ɯ': 1122, 'melee': 27868, 'corresponded': 27601, '##kker': 23793, 'trapping': 22977, 'bullet': 7960, 'sway': 17812, 'guards': 4932, 'saharan': 24505, 'turnbull': 26439, '##ʂ': 29695, 'audrey': 14166, 'protecting': 8650, 'cpu': 17368, 'hornets': 24855, '1725': 25651, '史': 1790, 'curtain': 11002, 'rejected': 5837, '##iled': 18450, 'employ': 12666, 'meaningful': 15902, 'deteriorated': 20111, '##tase': 18260, 'amp': 23713, '##gative': 26792, 'skirmish': 27264, 'meal': 7954, 'centered': 8857, 'owed': 12232, 'chains': 8859, '##nard': 16564, '##mics': 22924, 'host': 3677, '##ney': 5420, 'portray': 17279, 'joey': 9558, '##cek': 25368, 'ourselves': 9731, 'sava': 28350, '⺼': 1633, '##makers': 12088, 'vascular': 21449, '##い': 30173, 'ᄏ': 1466, 'bellowed': 27386, 'spin': 6714, 'milky': 21582, 'airs': 14369, 'aerial': 9682, 'boring': 11771, '##straße': 24967, 'nasser': 26099, '##dav': 29045, '##ख': 29852, 'specializing': 14055, 'doherty': 23798, 'are': 2024, 'fi': 10882, 'propped': 16863, 'remembering': 10397, '##tro': 13181, '##mming': 25057, '##tructing': 26310, 'will': 2097, '##ggle': 24679, 'verbs': 16025, '##kley': 22315, 'deeper': 6748, 'tack': 26997, 'lenin': 17497, '##ndi': 16089, 'duo': 6829, 'ɨ': 1118, 'friar': 25287, 'ᄇ': 1460, 'overthrow': 16857, '##graph': 14413, '[unused639]': 644, 'shipyard': 13858, 'turmoil': 17930, 'hazards': 22010, 'racial': 5762, 'blamed': 11248, 'sabha': 11200, 'tattoo': 11660, '##pan': 9739, 'benson': 11999, 'zach': 12397, 'gabrielle': 16988, 'buddhism': 11388, 'history': 2381, '##gil': 20142, '##mament': 28119, '##rdon': 28176, 'focus': 3579, 'subtly': 28797, 'caravan': 17184, '##ᄑ': 30004, 'paralympics': 15600, '##lon': 7811, '##uke': 15851, 'inspection': 10569, 'sent': 2741, 'tumor': 13656, 'modernist': 19770, 'judah': 23022, '##champ': 25450, '##ᴵ': 30029, 'maternal': 11062, 'anderson': 5143, 'tavern': 13090, 'hangar': 18284, 'thickness': 14983, 'nirvana': 26530, 'camped': 27077, 'ː': 1150, 'publicly': 7271, 'guatemala': 11779, 'salisbury': 13817, 'overwhelmed': 13394, 'pounds': 7038, '##dermott': 29370, 'radios': 22229, 'spells': 11750, 'bunk': 25277, 'studying': 5702, '1990s': 4134, 'harmless': 19741, 'services': 2578, 'huts': 23326, '##צ': 29809, 'imaginary': 15344, 'mining': 5471, 'permitting': 24523, '1712': 28460, 'き': 1652, 'nationalist': 8986, 'rapids': 12775, 'pulp': 16016, 'rival': 6538, '##id': 3593, '##irs': 18894, 'unless': 4983, 'wealth': 7177, '##endez': 28787, 'tenor': 9534, 'striped': 17983, 'wrestler': 10706, 'leases': 29597, 'pretend': 9811, 'flanders': 13998, 'ₐ': 1560, 'merely': 6414, 'funnel': 25102, 'souza': 26598, 'warped': 25618, 'raphael': 12551, 'falkland': 25257, 'garrison': 8427, 'mortgage': 14344, 'stray': 15926, '##belt': 21561, 'lakeside': 28701, 'thematic': 23539, 'eclipse': 13232, '##cho': 9905, 'archipelago': 13888, 'drafts': 28967, '##ops': 11923, 'header': 20346, 'truths': 23019, 'expenses': 11727, 'impairment': 25172, 'stein': 14233, '##bir': 17706, '1805': 13126, '##ering': 7999, 'archaeologists': 19254, 'honda': 11990, '##瀬': 30431, 'cardiac': 15050, 'cups': 10268, '##atal': 27815, 'sociological': 24846, '[unused483]': 488, 'farmland': 16439, 'commandos': 25144, '[unused154]': 159, 'commissioner': 5849, 'accompany': 12673, 'approached': 5411, 'ム': 1725, 'demonstrates': 16691, 'cats': 8870, 'jewish': 3644, 'bin': 8026, 'aggression': 14974, 'barges': 27712, 'outfits': 22054, 'disaster': 7071, 'reside': 13960, 'beforehand': 25828, 'die': 3280, '##esthesia': 25344, 'spilled': 13439, 'finite': 10713, '##gno': 26745, 'incorrectly': 19721, 'lemon': 14380, 'ம': 1389, 'avalanche': 18846, 'bari': 22466, '##nity': 22758, '##•': 30063, '##ق': 29834, 'julius': 10396, '村': 1878, '##gan': 5289, '##chang': 22305, '##ᅡ': 30006, 'benny': 11945, 'megan': 12756, 'coined': 13279, '[unused408]': 413, 'mediated': 19872, '[unused667]': 672, 'ten': 2702, 'gentry': 20262, 'melody': 8531, 'potsdam': 26554, 'injured': 5229, 'rosy': 26851, 'relating': 8800, 'infiltrate': 29543, 'tub': 14366, '##drome': 29171, 'opus': 16895, 'stationary': 17337, '[unused36]': 37, 'pirates': 8350, 'server': 8241, 'fork': 9292, '[unused486]': 491, '##lly': 9215, 'durban': 25040, '##dhi': 19114, '##inen': 21820, 'pic': 27263, 'stimulate': 23216, 'ballads': 18456, 'dominating': 21949, 'punishments': 29115, 'tenderness': 24605, 'exempt': 11819, '##ieg': 28872, 'energetic': 18114, 'lionel': 14377, '430': 19540, 'hopes': 8069, 'rep': 16360, 'swiped': 24452, 'reveals': 7657, 'licensed': 7000, 'jets': 9924, '##bad': 9024, 'ol': 19330, 'criticisms': 19628, 'jonas': 10680, 'republican': 3951, 'identity': 4767, 'isolate': 27152, 'bragg': 23678, 'affectionately': 28734, '160': 8148, 'napkin': 20619, 'pleasure': 5165, '##aer': 27867, '380': 17014, 'dialogue': 7982, 'woke': 8271, 'alexis': 13573, 'hobby': 17792, '##mbit': 28878, 'ট': 1359, '*': 1008, 'navarre': 21260, '##coe': 16288, 'tornadoes': 22668, 'outraged': 23558, 'snarl': 24845, 'rib': 19395, '##ber': 5677, '[unused749]': 754, 'roses': 10529, 'uniform': 6375, 'gathering': 7215, 'tess': 15540, 'buster': 18396, 'immaculate': 19532, 'bounds': 19202, 'superfamily': 24169, '##д': 29742, 'stevens': 8799, 'genetic': 7403, 'topping': 22286, 'slit': 18036, 'tchaikovsky': 25900, 'wharf': 16435, 'hercules': 15067, '##lette': 27901, 'bakery': 18112, 'answers': 6998, '##ices': 23522, 'dedication': 12276, 'female': 2931, 'attacking': 7866, 'landowner': 20270, '01': 5890, '##ified': 7810, 'possessed': 8679, 'lucinda': 28261, 'highland': 10983, 'ह': 1339, 'application': 4646, '##hed': 9072, 'massey': 21402, 'resources': 4219, 'intercourse': 23198, 'vip': 21722, 'texture': 14902, '##gc': 18195, 'venice': 7914, 'distinguish': 10782, 'lear': 26511, 'wants': 4122, 'cow': 11190, 'lal': 21348, '##ホ': 30248, 'cy': 22330, 'cousin': 5542, 'scowled': 17474, 'hooper': 23717, 'cakes': 22619, 'enhance': 11598, 'rovers': 9819, 'thud': 20605, '##rath': 27362, 'sip': 10668, 'former': 2280, 'pounded': 13750, 'archery': 21383, 'trieste': 25199, '##sberg': 11711, 'raged': 28374, 'opportunity': 4495, '1822': 12307, 'skier': 21294, '##emy': 26662, 'aided': 11553, 'futures': 17795, 'alvarez': 16309, 'xbox': 12202, '[unused144]': 149, 'bitch': 7743, 'ruby': 10090, 'ski': 8301, 'theaters': 12370, 'when': 2043, 'polo': 11037, 'contractor': 13666, 'grain': 8982, '##anda': 13832, 'consume': 16678, 'atkins': 21087, 'wellness': 25860, 'ready': 3201, '##onic': 12356, 'insignia': 16751, '##ð': 29668, 'null': 19701, 'toulon': 27160, 'skeletons': 24365, '##trum': 24456, '##uppe': 29547, 'democrats': 8037, 'dating': 5306, 'gunner': 14850, 'merritt': 26701, 'flashed': 8373, 'chili': 20238, 'users': 5198, '##ɕ': 29680, 'celebrated': 6334, '[unused976]': 981, '##ou': 7140, 'cedric': 26170, 'bladder': 24176, '##ර': 29941, 'scoffed': 26326, '##pis': 18136, '##pa': 4502, 'waited': 4741, 'wider': 7289, 'jagged': 18187, 'troops': 3629, 'microwave': 18302, '##nated': 23854, '中': 1746, 'chloe': 9318, '##fo': 14876, 'carey': 11782, 'larsen': 20094, 'thereof': 21739, 'conor': 20545, '289': 27054, 'bait': 17395, 'traverse': 20811, '『': 1643, '[unused477]': 482, '##ice': 6610, 'plush': 27729, 'roberto': 10704, '##lase': 25002, 'conflict': 4736, 'choreographer': 17334, 'usual': 5156, '1896': 6306, '##lton': 13947, 'horsepower': 15149, 'barns': 25684, 'strands': 14119, 'monopoly': 15404, 'heinrich': 10952, '131': 14677, '##hof': 14586, 'calculations': 16268, '##pic': 24330, 'softness': 27012, 'gastropod': 10953, '##張': 30370, 'heavy': 3082, 'sign': 3696, '##zily': 28431, '[unused519]': 524, 'might': 2453, 'be': 2022, 'invertebrates': 25700, 'islamabad': 26905, 'cantonese': 22241, 'canberra': 13107, 'top': 2327, 'fought': 4061, 'commendation': 23039, 'lip': 5423, '1834': 10700, '##ban': 8193, 'peach': 18237, 'chun': 20979, 'reinforce': 19444, '875': 27658, 'predictions': 20932, 'oxford': 4345, 'promoter': 15543, '##kura': 28260, 'mahmud': 25886, 'app': 10439, 'kahn': 19361, 'encouraged': 6628, '308': 24232, 'communities': 4279, 'lawyers': 9559, '285': 21777, 'pry': 29198, 'designers': 11216, 'echoed': 10187, 'solving': 13729, 'interviewed': 10263, '##dana': 25917, 'cafes': 23812, 'mp3': 23378, '##nda': 8943, '##kit': 23615, 'loudly': 9928, 'mosquito': 22529, 'snowfall': 26043, 'murdering': 21054, 'threaded': 26583, 'seaside': 20276, 'tiles': 13262, 'newscast': 20306, 'fingernails': 21243, '天': 1811, '##ques': 10997, 'peers': 12746, 'tigers': 7600, '##cal': 9289, '街': 1946, '##enas': 26474, 'arises': 18653, 'intensity': 8015, 'illegally': 17800, '1700': 16601, 'uganda': 10031, 'lavender': 20920, 'properly': 7919, '[unused849]': 854, 'grant': 3946, 'geology': 13404, '[unused146]': 151, 'macbeth': 25182, '26': 2656, '[unused134]': 139, '##rag': 29181, '[unused370]': 375, 'actors': 5889, '##ars': 11650, '1936': 4266, 'conservative': 4603, 'flourishing': 29571, 'ছ': 1357, 'thereby': 8558, 'partnered': 12404, '⁴': 1538, 'force': 2486, 'gears': 19456, 'diana': 8805, '1795': 13397, 'results': 3463, 'balancing': 20120, '##farlane': 23511, 'ict': 25891, '1604': 28754, 'ark': 15745, 'thong': 27468, 'ultra': 11087, '[unused277]': 282, 'olympiad': 25225, 'photograph': 9982, 'е': 1185, '185': 15376, 'commodity': 19502, '##ₑ': 30090, 'from': 2013, 'mathematician': 13235, 'specifies': 27171, 'trap': 8132, 'є': 1212, '##falls': 28067, '[unused23]': 24, 'mtv': 8692, '521': 26963, 'moth': 5820, 'petroleum': 11540, '##vat': 22879, 'husband': 3129, '##lta': 24458, '##bl': 16558, 'garment': 19002, 'miniseries': 13612, '[unused68]': 69, '##cus': 7874, '[unused283]': 288, 'woods': 5249, 'jupiter': 13035, '##uki': 14228, 'fascist': 14870, 'fundraising': 15524, 'clarkson': 18648, 'exile': 8340, 'dax': 27116, 'lullaby': 29149, 'cheney': 23745, 'clive': 14675, '[unused364]': 369, 'media': 2865, '##eim': 12112, '##uta': 13210, 'translations': 11913, '[unused515]': 520, 'iucn': 20333, 'taiwanese': 16539, 'christi': 21359, 'footing': 22849, 'ensures': 21312, 'frames': 11048, 'protects': 18227, 'dozen': 6474, '##rro': 18933, 'homme': 26574, 'feature': 3444, 'acid': 5648, '70': 3963, 'inches': 5282, 'views': 5328, 'alexandra': 10481, 'tenant': 16713, 'drawer': 13065, 'disturb': 22995, '##হ': 29913, 'amenities': 19870, 'ル': 1733, 'powerless': 25192, 'rustling': 29188, 'productivity': 15836, 'carry': 4287, 'rotating': 13618, 'ɣ': 1117, 'springs': 6076, 'symmetrical': 23476, 'threat': 5081, 'albans': 26311, 'writ': 25697, 'truck': 4744, '##oke': 11045, 'nikolay': 28494, '##च': 29854, '##eral': 21673, 'births': 18250, 'ف': 1291, 'paddle': 20890, '##wai': 21547, 'manager': 3208, '[unused443]': 448, '1824': 11617, '→': 1585, 'pressured': 25227, '##iling': 16281, '417': 27519, '##upt': 29441, 'instructed': 10290, 'rourke': 24400, 'nes': 24524, 'renee': 17400, 'mergers': 28585, 'reduced': 4359, 'broadly': 13644, 'decrease': 9885, 'uniformly': 27423, '##ச': 29919, 'stainless': 18676, 'wyatt': 12986, 'ա': 1219, 'pricing': 20874, '151': 16528, 'character': 2839, '##itha': 26054, 'airways': 13095, 'sequels': 25815, '##chai': 24925, '##games': 26393, 'seventy': 10920, 'projections': 21796, 'operations': 3136, '##tor': 4263, '[unused581]': 586, 'manners': 14632, 'desires': 14714, 'ankles': 15392, 'acquitted': 18538, 'cavern': 16679, 'marley': 20326, 'silesia': 21872, 'guitarist': 5990, 'hillside': 18573, 'participant': 13180, 'erica': 19295, '##dae': 6858, 'stories': 3441, '##bourg': 20431, 'cipher': 27715, 'sixteen': 7032, 'dukes': 16606, 'funeral': 6715, 'unseen': 16100, 'obituary': 20815, 'comprehension': 26683, '##jah': 18878, 'gorilla': 23526, 'dislike': 18959, 'reef': 12664, 'interaction': 8290, '##ulates': 18969, '₉': 1556, '##pt': 13876, 'zionist': 21379, 'prequel': 28280, 'carole': 24348, '##ake': 13808, 'estimate': 10197, 'default': 12398, 'exchanging': 25620, '##ports': 25378, 'liability': 14000, '##lellan': 25839, '1665': 27676, 'ה': 1245, '[unused428]': 433, 'bearings': 21714, 'crews': 10604, '##hli': 27766, 'scraped': 20378, 'tango': 17609, '##tel': 9834, '##diment': 21341, 'knoxville': 20021, 'pc': 7473, 'catching': 9105, 'famously': 18172, 'littered': 24777, '##ieu': 17301, '##竹': 30459, '[unused801]': 806, 'obsessed': 15896, 'hiatus': 14221, 'junk': 18015, '##xy': 18037, 'tinted': 25577, 'catholic': 3234, 'pro': 4013, 'unveiled': 11521, 'parc': 27985, 'muzzle': 17750, 'dictated': 23826, 'martini': 24480, '##walk': 17122, 'valerie': 14264, 'fearing': 14892, 'playwright': 11170, 'baronetcy': 25071, 'doping': 23799, 'neighborhoods': 11681, 'restored': 5854, 'councillors': 13189, 'beans': 13435, 'nineteenth': 9137, '##lessness': 24913, 'confirmed': 4484, '355': 26271, 'ལ': 1435, '##sai': 15816, 'obscene': 27744, 'destroy': 6033, 'andes': 18096, 'amherst': 19850, 'lockheed': 17646, 'skyscraper': 24581, 'navigable': 28538, 'superiority': 19113, 'asthma': 26180, '##stle': 22516, 'automobiles': 19207, 'chips': 11772, '‖': 1519, '##歌': 30412, 'doubts': 13579, 'weekends': 13499, 'medium': 5396, 'towers': 7626, 'shear': 18330, '##per': 4842, 'aperture': 18892, 'vii': 8890, 'party': 2283, 'cod': 19429, '##lian': 15204, '##ply': 22086, 'unmarried': 17204, 'satisfactory': 23045, 'princes': 12000, 'deities': 17091, 'ford': 4811, 'farmers': 6617, 'protectorate': 20394, 'nail': 13774, 'automatically': 8073, 'stokes': 18716, 'newborn': 20662, '##tock': 17406, 'ed': 3968, '##ου': 26789, 'contiguous': 25177, 'fun': 4569, 'wilder': 18463, 'bryant': 12471, 'relic': 24933, 'millennia': 27620, '07': 5718, 'skeleton': 13526, 'bassist': 9858, '##bbe': 19473, 'tal': 21368, '##cic': 19053, 'johnny': 5206, 'crusade': 16282, 'heaved': 19970, 'buckle': 22853, 'persistence': 28297, '##hana': 15788, '##mini': 25300, 'jay': 6108, 'tracks': 3162, 'provides': 3640, 'leary': 22986, 'band': 2316, 'villas': 27317, 'economies': 18730, 'jogged': 27812, 'heroin': 19690, 'burrows': 22009, 'vegetation': 10072, 'choosing': 10549, 'teeth': 4091, 'metropolis': 18236, 'freyja': 7075, '##dote': 23681, 'tangled': 14170, 'corporations': 11578, 'wellington': 8409, 'liberals': 13350, 'kali': 19924, 'audible': 19525, 'offenders': 19591, '##eles': 26741, '##本': 30402, 'geological': 9843, 'guy': 3124, 'recipients': 15991, '##aar': 26526, 'astro': 28625, '[unused12]': 13, '##bular': 28808, 'interpreter': 19555, 'oblique': 20658, 'neighbourhood': 10971, 'hurried': 9520, '##sphere': 23874, '##nd': 4859, '##ried': 11998, 'andres': 15614, 'aggregation': 28041, 'olive': 9724, 'clearance': 14860, '[unused164]': 169, 'mounts': 19363, '##ony': 16585, '##wise': 14244, 'domes': 26193, 'pots': 18911, 'clarity': 15563, 'beneficial': 15189, 'sensing': 13851, 'ali': 4862, 'gwen': 11697, 'stress': 6911, 'lucia': 12337, '##nies': 15580, 'swiss': 5364, 'experiences': 6322, 'dynamo': 17205, 'yer': 20416, '##rand': 13033, 'loved': 3866, 'artist': 3063, 'beats': 10299, '##pired': 21649, 'societal': 23382, 'う': 1648, '##raphic': 20721, 'nay': 29349, 'developers': 9797, 'behaved': 26979, '白': 1915, '##yer': 10532, 'tangle': 24453, 'qualify': 7515, 'climates': 24734, '1745': 21809, '324': 27234, 'intersecting': 27294, '2015': 2325, 'bible': 6331, 'comedic': 21699, 'tries': 5363, '##enay': 27727, '##nstein': 15493, 'accountability': 17842, 'grace': 4519, 'mortimer': 17416, '[unused329]': 334, '##oys': 27153, 'circuit': 4984, 'poe': 18922, 'verification': 22616, '##ntino': 25318, 'incorporation': 16935, 'homeland': 10759, 'archdiocese': 12658, '##uel': 16284, 'than': 2084, 'coconut': 16027, 'toulouse': 17209, '##ois': 10054, 'transitions': 22166, 'snow': 4586, 'drifted': 10070, '##pl': 24759, 'mcintyre': 24564, '¿': 1094, '##ents': 11187, 'squinted': 17425, 'kgb': 25467, 'hackney': 28425, 'morrison': 9959, 'xavier': 10062, 'concluded': 5531, '##real': 22852, 'o': 1051, '[unused422]': 427, 'belarus': 12545, 'rutherford': 18472, 'appointed': 2805, 'sexually': 12581, 'hush': 20261, 'angela': 10413, '##zza': 20715, 'publish': 10172, 'wallis': 24029, 'soul': 3969, 'saul': 16897, '##riders': 28116, 'doubt': 4797, 'triad': 25355, 'obtaining': 11381, '智': 1869, '##shida': 25541, 'aggressive': 9376, 'barred': 15605, 'tending': 25069, 'delhi': 6768, '1818': 12094, 'herds': 28822, '##osta': 28696, '##rix': 17682, 'holy': 4151, '##nova': 13455, 'walnut': 18489, 'burned': 5296, 'bangkok': 12627, '福': 1926, 'conventions': 12472, 'accordion': 19060, '##oes': 22504, 'johannesburg': 15976, 'inorganic': 28256, '##igen': 29206, 'refining': 28596, 'crusaders': 18831, 'seam': 25180, '##hya': 17915, '1796': 13885, 'redesign': 25136, 'mercedes': 10793, 'decrees': 28966, 'symbolic': 12613, 'presents': 7534, 'wrestling': 4843, '##ulsion': 23316, 'finals': 4399, '##je': 6460, '##uated': 16453, 'lana': 16554, 'failure': 4945, 'physician': 7522, '##ssler': 23385, 'flopped': 24723, 'uttar': 14940, 'stimuli': 22239, '##≡': 30134, '[unused634]': 639, 'receives': 8267, 'unnoticed': 22719, '780': 28601, '##ா': 29931, 'kneeling': 16916, 'trucks': 9322, 'absolute': 7619, 'zane': 6944, 'semester': 13609, 'farrell': 16248, '207': 19843, 'sniffing': 27646, 'darkening': 27862, 'amara': 28599, 'ʷ': 1143, 'witness': 7409, '堂': 1805, 'increasing': 4852, '健': 1768, 'receivers': 19278, 'dumont': 25830, 'deliberately': 9969, 'remain': 3961, 'signal': 4742, 'seaman': 21626, '##logie': 27095, 'sustain': 15770, 'remind': 10825, '##ido': 13820, 'tap': 11112, 'yank': 23178, '2021': 25682, 'joyah': 27098, '##ades': 18673, 'textiles': 18762, '♭': 1627, 'manning': 11956, '##פ': 29807, 'stockton': 19161, 'marcelo': 24984, '##°': 7737, '##can': 9336, 'faculty': 4513, 'crowley': 20748, '##astic': 20875, '##yne': 9654, 'ginger': 14580, 'console': 10122, 'fatal': 10611, '~': 1066, '124': 13412, 'seller': 14939, 'sewer': 22365, 'gaines': 28645, 'it': 2009, '##gia': 10440, '34': 4090, 'historical': 3439, 'bias': 13827, '##太': 30338, 'craig': 7010, '##face': 12172, '50': 2753, 'flanked': 15874, 'levi': 11902, '##ten': 6528, 'german': 2446, 'book': 2338, '[unused691]': 696, 'afraid': 4452, 'insurance': 5427, 'maharashtra': 12434, 'breaths': 12938, 'innocence': 12660, '##dorf': 11592, 'admiralty': 14179, 'accumulated': 14830, 'lass': 27333, '##verse': 16070, '48th': 27787, 'jacobs': 12988, 'regatta': 26848, 'ghana': 9701, 'unexpectedly': 14153, '[unused393]': 398, 'mri': 27011, '##oco': 24163, '##vish': 24968, '##স': 29912, 'sap': 20066, 'clayton': 11811, '[unused893]': 898, 'immersion': 27013, 'researching': 20059, 'nanjing': 21455, 'specifically': 4919, '##ᅧ': 30010, '⅓': 1581, 'ね': 1670, '₅': 1552, '##giri': 23243, '1813': 12169, 'commitments': 17786, '257': 24368, 'fir': 21554, '##lynn': 27610, 'nod': 7293, 'strap': 16195, 'slave': 6658, 'glorious': 14013, 'archeological': 23005, 'excavated': 15199, 'munitions': 21061, 'kam': 27829, '##mer': 5017, 'lexington': 14521, 'orchards': 22976, 'convinced': 6427, '##orough': 26388, '##ং': 29882, '[unused617]': 622, 'ancestral': 14947, '##kka': 15714, 'roofed': 26080, '##ute': 10421, 'scrape': 26988, '##cision': 28472, '##谷': 30477, '##las': 8523, 'romani': 23982, '##ttle': 26328, '[unused538]': 543, 'globally': 16452, '[unused96]': 97, '##忄': 30376, '[unused400]': 405, 'herb': 12810, '##м': 29745, 'tully': 25724, 'leon': 6506, 'drugged': 25483, 'pensacola': 26073, 'insult': 15301, '341': 28358, 'stink': 27136, 'nutritional': 28268, 'soils': 13622, '##redo': 23417, 'pirate': 11304, 'outrageous': 25506, '##formed': 29021, 'avengers': 14936, 'arcade': 10877, 'tenants': 14665, 'beasts': 15109, 'ringing': 13060, 'ric': 26220, 'disabled': 9776, 'cleaned': 12176, 'ja': 14855, '−': 1597, 'believes': 7164, '##book': 8654, 'ramsey': 15092, '##rana': 16737, 'restaurants': 7884, '##;': 29628, 'su': 10514, '##dget': 24291, '##cked': 18141, 'blushing': 25771, 'swinging': 11820, 'liam': 8230, '[unused903]': 908, 'langdon': 15232, 'fleets': 25515, '女': 1815, 'tactic': 19717, 'migrated': 13447, '##♣': 30151, 'stumbling': 19730, 'residual': 21961, 'clare': 11152, 'diagnostic': 16474, '##cuting': 29163, 'flaws': 21407, '##building': 25820, '##aldi': 24657, 'sk': 15315, 'jolt': 22538, '##gnant': 27881, 'deliver': 8116, 'owned': 3079, 'hodgson': 26107, 'netting': 26909, '730': 28004, '##hers': 22328, 'silvery': 21666, 'grape': 14722, 'cabbage': 28540, 'twice': 3807, 'angola': 13491, 'sex': 3348, '##hawk': 17998, 'manifest': 19676, '[unused821]': 826, 'normally': 5373, 'cake': 9850, 'stemming': 29217, '[unused990]': 995, '##ished': 13295, 'carlo': 9758, '##mon': 8202, 'commonplace': 27550, 'reaction': 4668, 'cleveland': 6044, 'fabric': 8313, '[unused758]': 763, 'evans': 6473, 'pigeon': 16516, '336': 27954, '##そ': 30186, 'edged': 13011, 'share': 3745, 'them': 2068, 'surrendered': 10795, 'wolverhampton': 20085, 'land': 2455, '##lage': 20679, 'frigate': 15437, 'deciduous': 22411, 'favored': 12287, 'natural': 3019, '[unused469]': 474, 'schwartz': 16756, 'iron': 3707, 'fortunes': 18023, '##ক': 29889, '##tose': 22282, 'safely': 9689, 'downloaded': 22817, 'herbal': 27849, 'kathy': 14986, 'implying': 20242, 'bolshevik': 24477, 'observe': 11949, 'onstage': 23542, '##hak': 20459, 'dumb': 12873, 'watt': 15231, 'er': 9413, 'demon': 5698, 'elite': 7069, 'mongolian': 17855, 'settlers': 7322, 'management': 2968, 'plateau': 9814, 'dual': 7037, '##ath': 8988, 'ٹ': 1301, '##chel': 15721, 'justices': 19867, '[unused808]': 813, 'liberalism': 26505, 'deutsche': 11605, 'intercepted': 16618, 'darted': 14051, '##acious': 20113, 'notification': 26828, '##una': 9521, 'caliber': 15977, 'positioning': 19120, 'entrance': 4211, '##ddle': 20338, 'aldo': 28163, 'arm': 2849, '弘': 1843, 'universities': 5534, 'teens': 13496, 'baton': 15302, 'caine': 19881, '##軍': 30481, '##ey': 3240, '##ib': 12322, 'nocturnal': 23789, '##crest': 25313, 'goth': 25976, 'レ': 1734, 'pig': 10369, 'debuting': 24469, '##bach': 7693, '##cr': 26775, '[unused703]': 708, 'adaptation': 6789, 'vinyl': 8877, 'suggesting': 9104, '##ص': 29826, 'puebla': 27452, '[unused255]': 260, 'touring': 6828, '##chemist': 24229, 'regarded': 5240, 'join': 3693, 'stations': 3703, 'headlined': 25214, '##ron': 4948, 'bucks': 14189, '##宗': 30347, '##ᵏ': 30037, 'sighted': 19985, 'mixed': 3816, 'esq': 25325, 'marlon': 25861, 'anand': 18887, 'implementations': 24977, 'judaism': 13725, 'condensed': 25011, '1752': 24736, 'adolf': 12500, '##∪': 30132, 'stats': 26319, '[unused306]': 311, '[unused351]': 356, 'summit': 6465, 'bravery': 16534, 'abuses': 21078, 'hissed': 11402, 'suffix': 16809, 'sparrow': 19479, 'metis': 26057, 'matter': 3043, '##aud': 19513, '175': 12862, 'idaho': 9795, '##rrigan': 28706, 'habitat': 6552, 'afterlife': 25115, 'tiger': 6816, '阝': 1970, 'endeavour': 26911, '##ʑ': 29703, '##tat': 29336, '1886': 6929, 'mcguire': 23872, '##ned': 7228, '##bow': 18912, 'gin': 18353, '[unused308]': 313, 'seminars': 17239, 'shredded': 29022, '##.': 29625, 'よ': 1684, 'fill': 6039, 'year': 2095, 'areas': 2752, 'rotherham': 27456, 'opposition': 4559, 'oclc': 12258, 'cosmetic': 25536, 'presenters': 25588, 'vinegar': 29387, '##ered': 6850, 'mar': 9388, 'grandparents': 14472, 'draper': 23641, 'clarissa': 25260, 'applause': 20737, 'crotch': 28629, 'མ': 1432, ')': 1007, '[unused219]': 224, 'victorious': 13846, '##ntation': 23436, 'philanthropy': 29291, '[unused921]': 926, '[unused554]': 559, 'asylum': 11386, 'hendrix': 20645, 'nino': 20801, 'judd': 20128, 'shrink': 22802, '##nat': 19833, 'utilizing': 16911, 'volatile': 20606, 'irritation': 17373, 'southwestern': 8772, 'mca': 22432, 'screen': 3898, 'briefing': 27918, 'huffed': 25014, 'mist': 11094, '[unused214]': 219, 'quran': 21288, '##ages': 13923, 'rock': 2600, 'coarse': 20392, 'extraction': 14676, 'flowing': 8577, 'healing': 8907, 'triggering': 29170, 'moral': 7191, 'rosie': 15820, 'stance': 11032, 'spears': 13957, 'brownish': 19437, '[unused15]': 16, 'inclined': 13050, 'playoffs': 7555, 'listening': 5962, 'lighting': 7497, 'hayden': 13872, 'singled': 25369, 'flanagan': 26558, '##男': 30438, '⇌': 1590, 'ส': 1420, 'grimly': 22561, 'succumbed': 25642, 'gibson': 9406, 'pumpkin': 25730, 'moffat': 28528, 'fuck': 6616, 'ra': 10958, 'janeiro': 11497, '##inium': 27585, 'poem': 5961, 'muscular': 13472, 'vibrations': 22755, 'dolores': 21544, 'undone': 25757, '##貴': 30479, '##ttes': 14581, '##dp': 18927, '##tour': 21163, 'wc': 15868, 'clients': 7846, '[unused65]': 66, '⊆': 1611, 'monique': 26194, 'expecting': 8074, 'gateway': 11909, 'gundam': 26152, '##ady': 18632, 'disagreements': 23145, 'fresh': 4840, 'dealt': 9411, 'low': 2659, 'brady': 10184, 'appoint': 16823, 'teller': 21322, 'untouched': 22154, '##kat': 24498, 'bingo': 27137, 'ears': 5551, 'armenia': 10110, 'guitarists': 28523, '##প': 29903, 'federation': 4657, 'sequel': 8297, 'gaa': 19930, '##gga': 23033, 'parish': 3583, 'seth': 6662, '##fort': 13028, 'hussain': 20093, 'gillespie': 21067, '##culus': 28703, 'harding': 15456, 'republished': 24476, '##tea': 27058, '##ץ': 29808, 'brien': 9848, 'ye': 6300, '##ri': 3089, '##ston': 7106, 'dos': 9998, 'mothers': 10756, 'recall': 9131, 'takeover': 15336, 'musically': 21385, 'hidalgo': 24715, 'wildly': 13544, 'argyll': 27365, '3rd': 3822, 'bulky': 27446, 'enraged': 18835, 'pronouns': 26028, 'validation': 27354, '##cm': 27487, 'sloop': 24786, '[unused411]': 416, 'portland': 6734, 'cinematography': 16434, '##vation': 21596, 'somali': 16831, 'allowed': 3039, '##ot': 4140, 'abe': 14863, 'stat': 28093, 'centuries': 4693, '[unused410]': 415, 'sergeant': 6722, '##宇': 30345, 'sung': 7042, 'moan': 13673, 'susquehanna': 26361, 'affection': 12242, 'marquez': 27320, '##ノ': 30243, 'busy': 5697, '298': 27240, 'ropes': 14607, 'meadow': 13244, 'negligence': 27988, 'abdul': 10298, 'sf': 16420, 'hector': 10590, 'tremor': 27734, 'navigator': 20532, 'boyfriend': 6898, 'alderman': 18977, 'trenton': 17148, '##weather': 28949, 'rodents': 28156, 'seldom': 15839, 'textile': 12437, 'shapes': 10466, 'vancouver': 6930, 'adjoining': 13562, '191': 19871, 'basins': 22739, 'applies': 12033, 'pat': 6986, 'signing': 6608, 'spoon': 15642, '[unused232]': 237, '[unused714]': 719, 'probe': 15113, 'piers': 16067, 'routing': 16972, 'reciprocal': 28309, '##her': 5886, 'tempered': 22148, 'scientology': 23845, 'kia': 27005, '##itative': 29293, 'killings': 16431, 'motion': 4367, 'bishops': 8414, 'athens': 7571, 'agree': 5993, '6th': 5351, 'calder': 19347, 'extent': 6698, 'breathed': 8726, '##nay': 16741, 'improves': 24840, 'bien': 29316, 'rendered': 10155, 'scare': 12665, 'fraction': 12884, '##nary': 24041, 'greatest': 4602, '##龸': 30509, 'michael': 2745, '[unused446]': 451, 'madonna': 11284, 'jump': 5376, '##ign': 23773, 'academics': 15032, 'orr': 26914, '[unused424]': 429, 'immigrated': 17352, 'regular': 3180, 'dylan': 7758, '[unused545]': 550, 'formidable': 18085, 'crypt': 19888, '##ml': 19968, '##65': 26187, '##⊂': 30137, 'spark': 12125, '大': 1810, 'ב': 1242, 'lynn': 9399, 'administration': 3447, 'suzy': 28722, '##stems': 29189, 'rude': 12726, 'davy': 23255, 'nape': 23634, 'mclaughlin': 23720, '[unused822]': 827, 'ferns': 25715, 'probation': 19703, '2007': 2289, 'emptiness': 23397, 'ultraviolet': 26299, 'grenada': 29153, 'update': 10651, 'bulldog': 28628, 'rann': 26312, 'tanner': 12780, 'latitude': 15250, '##pu': 14289, '##ffy': 16329, 'hail': 16889, 'kickoff': 25233, 'palm': 5340, '##iest': 10458, 'gallo': 25624, 'matthias': 17885, 'available': 2800, 'nebula': 25677, '##belle': 25766, 'wardrobe': 17828, 'ტ': 1453, 'practised': 20439, 'cheerleading': 25721, '##et': 3388, '##wu': 16050, 'southland': 29121, '##章': 30458, 'adventures': 7357, 'toronto': 4361, 'whimpered': 26184, '##weight': 11179, '##bn': 24700, '##athi': 25457, 'realizes': 10919, 'slash': 18296, 'junior': 3502, 'titles': 4486, '[unused803]': 808, 'plate': 5127, 'giving': 3228, 'regain': 12452, 'tonga': 20188, 'provisions': 8910, 'joke': 8257, 'mainland': 8240, '##kali': 28613, '[unused499]': 504, 'maintain': 5441, '##ologies': 20792, '##uls': 28426, '##bit': 16313, 'harold': 7157, 'shade': 8703, '##is': 2483, '##gata': 26589, 'staffordshire': 17052, 'longitude': 20413, '[unused625]': 630, 'coincidence': 16507, 'ley': 25866, 'we': 2057, 'belief': 6772, 'camilla': 26902, 'guarantees': 21586, 'booth': 9065, 'performing': 4488, 'reuben': 17294, 'specialist': 8325, 'warily': 23625, 'brunei': 18692, 'hereford': 21136, '##jord': 24876, '[unused273]': 278, 'clothed': 24963, 'waters': 5380, 'bars': 6963, 'travelers': 15183, 'streetcar': 21420, 'calculate': 18422, 'reconcile': 21063, '##virus': 23350, 'prom': 20877, '##oue': 27872, '##lvis': 28530, 'nutrition': 14266, 'feasible': 22945, 'エ': 1698, 'harvest': 11203, 'circled': 14867, '520': 19611, 'broughton': 29187, 'spectra': 29237, '##ona': 7856, 'estate': 3776, '360': 9475, 'islander': 12544, 'definitely': 5791, 'whip': 11473, 'sparhawk': 13709, 'unison': 18732, 'nana': 17810, 'গ': 1355, 'municipal': 4546, 'video': 2678, 'persuade': 13984, 'theft': 11933, '##jer': 20009, 'sued': 12923, '[unused876]': 881, 'scenery': 17363, 'dickens': 19675, 'medications': 20992, 'location': 3295, '##hong': 19991, '390': 20024, '##春': 30393, '1870s': 14896, 'rip': 10973, '##iser': 17288, '##scent': 27654, 'grassy': 22221, '##pts': 22798, 'sober': 17358, 'institute': 2820, '##wes': 18192, 'beverage': 19645, 'communal': 15029, 'lanes': 10914, '##lius': 15513, 'theoretically': 22634, 'compassion': 15398, '##þ': 29670, '[unused25]': 26, '1884': 6988, 'burt': 18611, 'rapper': 10687, 'gunshot': 22077, 'cassandra': 15609, 'warning': 5432, 'jerk': 12181, 'maha': 24404, 'suicide': 5920, 'paz': 18183, 'mani': 23624, 'winthrop': 28974, 'exceptional': 11813, 'blankly': 26344, '力': 1778, 'apparently': 4593, 'eel': 24315, '##zhou': 9791, 'rios': 25836, 'wash': 9378, 'multitude': 20889, 'high': 2152, 'cultural': 3451, 'organization': 3029, '720': 22857, '##vious': 24918, 'reliability': 15258, 'manitoba': 10512, '##lde': 17920, 'wisdom': 9866, 'earliest': 5700, 'genes': 9165, 'volleyball': 7454, 'brussels': 9371, 'lifestyle': 9580, '##rnik': 26437, 'geese': 28519, '##psis': 18409, 'lever': 15929, '##kowski': 15449, 'cascade': 16690, 'drafted': 7462, 'strata': 22913, '##ype': 18863, '€': 1574, 'mysterious': 8075, 'formulated': 19788, 'monsieur': 21380, 'entourage': 25342, 'teen': 9458, 'idiot': 10041, 'stamps': 12133, '1976': 3299, 'couch': 6411, 'tablets': 17596, 'feedback': 12247, 'yorkshire': 7018, 'stall': 13498, 'warmly': 22775, 'walden': 24834, '##გ': 29976, 'revive': 17995, '##nio': 27678, 'artillery': 4893, 'battling': 17773, 'eccentric': 18080, 'sundance': 20140, '[unused783]': 788, 'successes': 14152, 'tires': 13310, '##‿': 30069, '##gating': 16961, 'geographical': 10056, '[unused57]': 58, '##ub': 12083, '##stituting': 21532, 'gospel': 8036, 'plato': 18858, 'freely': 10350, 'gasoline': 13753, 'ʼ': 1146, 'longer': 2936, 'ruin': 10083, '##bb': 10322, 'sable': 23492, '##ives': 24653, '[unused497]': 502, '[unused711]': 716, 'oakland': 9182, 'correction': 18140, '##tens': 25808, 'lobbied': 26421, '##kova': 18810, 'elias': 14579, 'poured': 8542, 'talented': 10904, '[unused607]': 612, 'holds': 4324, '##apes': 29040, 'ebert': 22660, '##lent': 16136, 'reeve': 20726, 'cum': 13988, 'pyrenees': 22696, '[unused117]': 122, 'awarded': 3018, 'corbett': 24119, 'junction': 5098, 'albrecht': 25542, '##я': 17432, 'yawned': 27626, 'done': 2589, 'ions': 15956, 'flute': 8928, '251': 22582, '##valent': 24879, 'connell': 17199, '##fleet': 27657, '##《': 30165, '[unused665]': 670, '##beau': 26401, '##∗': 30125, 'employer': 11194, 'gardener': 19785, '1828': 11517, 'kurdistan': 23627, 'ana': 9617, '##cion': 10446, 'salsa': 26509, 'honestly': 9826, 'protections': 28548, 'pollard': 25513, '##under': 20824, '##ov': 4492, 'accredited': 11459, 'unused': 15171, 'rosenberg': 21069, 'braves': 13980, 'imitation': 20017, 'potential': 4022, 'scalp': 21065, 'databases': 17881, 'houses': 3506, '##physical': 23302, 'reaching': 4285, 'adelaide': 7364, 'lynne': 26938, 'strung': 23509, 'jumping': 8660, 'disgust': 12721, '[unused297]': 302, 'gunnery': 27919, 'derelict': 28839, 'prairie': 10996, 'rappers': 28257, 'range': 2846, '行': 1945, '##ctive': 15277, '##sons': 23345, '##া': 29914, 'separation': 8745, 'canvas': 10683, 'anthropological': 28395, 'lsu': 21849, 'risky': 19188, 'eighteenth': 12965, 'zev': 27909, '##won': 19291, 'mart': 20481, 'radically': 25796, 'invaded': 10836, 'viable': 14874, 'zenith': 28672, 'insane': 9577, '##tera': 14621, 'related': 3141, '##vs': 15088, 'simplicity': 17839, '1550': 26245, '##dates': 27122, 'malley': 25271, 'slabs': 28761, 'leonard': 7723, '##over': 7840, 'lance': 9993, 'championship': 2528, 'wind': 3612, 'akron': 22735, 'blockade': 15823, '##fted': 18915, 'nevada': 7756, 'mikhail': 11318, 'move': 2693, 'hillary': 18520, 'penguin': 13987, 'buren': 29470, '[unused859]': 864, '##ately': 28239, 'selected': 3479, 'selling': 4855, 'pastor': 9220, 'seasoned': 28223, 'forecast': 19939, 'remembrance': 19451, 'dangling': 18737, 'unease': 27880, 'secondary': 3905, '##elial': 24587, 'impossible': 5263, 'online': 3784, '##yde': 18124, 'sneered': 28098, 'goddess': 7804, 'analytics': 25095, '##blood': 26682, '##ay': 4710, 'centred': 16441, 'verbal': 12064, '1958': 3845, '##iec': 23783, 'denise': 15339, 'feudal': 16708, 'bryce': 19757, 'hodges': 19772, '##bby': 14075, 'boil': 26077, 'coa': 28155, 'carlisle': 13575, '##herton': 27949, 'phosphorus': 25473, 'shorter': 7820, '##<': 29629, 'ᄒ': 1469, 'suggestion': 10293, '##cci': 14693, 'sheffield': 8533, '##illon': 20343, 'hk': 22563, 'shower': 6457, 'repertoire': 13646, '##nable': 22966, '##ert': 8743, 'iroquois': 23015, 'jihad': 24815, 'simulate': 26633, 'orbiting': 26472, '##⁴': 30073, 'themselves': 3209, 'los': 3050, 'barren': 20225, 'duff': 21019, 'cox': 9574, 'worms': 16253, 'affirmative': 27352, 'institut': 17126, 'limb': 15291, 'nodded': 3368, 'destroyer': 9799, 'militia': 8396, 'danish': 5695, 'reluctantly': 11206, 'manny': 16320, 'fever': 9016, 'fungi': 15289, 'movement': 2929, '##dry': 21190, 'becky': 14407, 'traveller': 21916, '##ﬁ': 30510, 'trumpeter': 28220, 'silesian': 20453, 'elsa': 23452, 'valkyrie': 26696, '##ntial': 19909, 'wonderland': 20365, 'ching': 19992, 'revelation': 11449, 'cumbria': 25559, 'applications': 5097, 'conditional': 18462, 'pity': 12063, 'counseling': 17041, 'vomiting': 24780, 'advance': 5083, 'signatures': 16442, 'harvard': 5765, 'chicks': 20649, 'announced': 2623, '1941': 3874, 'underwater': 11564, 'statutory': 15201, 'aft': 16638, '##issa': 21205, '##mute': 26746, 'expressive': 22570, '##nen': 10224, '##♦': 30153, 'firm': 3813, 'rods': 19485, 'daisy': 10409, '##ლ': 29983, 'squid': 26852, '##nko': 16107, '[unused729]': 734, 'guzman': 22789, 'fitch': 26062, 'houston': 5395, 'files': 6764, 'launched': 3390, '##za': 4143, 'programs': 3454, 'dana': 11271, 'assessed': 14155, 'refer': 6523, 'rite': 14034, '##gen': 6914, 'mackay': 17090, '1826': 11931, '[unused468]': 473, 'ি': 1379, 'kenny': 8888, 'bolted': 18088, 'puzzles': 19672, 'computational': 15078, 'doo': 20160, 'peanut': 21443, '##mare': 24376, 'branding': 16140, '##tek': 23125, 'thayer': 25563, '##eno': 16515, 'gonzaga': 26840, 'gareth': 20243, 'itself': 2993, '##rae': 16652, 'notch': 18624, 'diversity': 8906, '1860': 7313, 'perth': 9300, '##bant': 29604, 'poisoned': 17672, 'restrain': 28467, '##che': 5403, 'aw': 22091, 'rabbits': 20403, 'sloppy': 28810, '##‒': 30050, 'nobel': 10501, 'provisional': 10864, '忠': 1852, 'cornell': 10921, 'harlow': 22545, '##ᄏ': 30002, 'parting': 20254, 'wainwright': 25305, '##rew': 15603, 'picnic': 12695, 'bilbao': 25427, 'drift': 11852, 'toned': 27604, '[unused765]': 770, 'ample': 20851, 'handles': 16024, 'blunt': 14969, 'pondered': 28347, '##balance': 26657, 'cylinder': 7956, 'assassins': 18364, '##esis': 19009, 'tradition': 4535, 'inspected': 20456, 'jenks': 15119, 'rocco': 28167, 'ongoing': 7552, 'lang': 11374, '##目': 30444, '##宀': 30344, '##otho': 29288, 'daughter': 2684, '##uch': 10875, 'mohawk': 22338, 'continues': 4247, '##lf': 10270, 'township': 3545, 'nehru': 23556, '##dra': 7265, 'nausea': 19029, 'govern': 21208, 'swan': 10677, 'misty': 15167, 'pune': 16920, 'rained': 28270, 'voter': 14303, '29th': 16318, '##iance': 28335, '##loaded': 17468, '412': 25873, '##rl': 12190, 'acids': 12737, '##bbing': 23200, 'soluble': 24345, 'scares': 29421, 'sax': 19656, '##og': 8649, 'boosted': 28043, 'landscape': 5957, 'ole': 15589, '##aint': 22325, 'coco': 25033, 'pontifical': 22362, '[unused750]': 755, 'bella': 12101, 'cables': 15196, 'demonstration': 10467, 'luther': 9678, 'gretchen': 21625, 'emergency': 5057, 'prague': 8634, 'paved': 12308, 'methodology': 16134, 'overlapping': 20567, 'treating': 12318, 'pr': 10975, 'skies': 15717, 'elves': 16980, 'dar': 18243, 'karim': 25461, '[unused544]': 549, 'construct': 9570, '##quent': 15417, '##eased': 25063, 'chrome': 18546, 'sapphire': 21965, 'harmed': 25596, 'timber': 7227, '##ης': 29155, '##h': 2232, 'der': 4315, 'depends': 9041, 'received': 2363, 'maintains': 9319, '##∂': 30120, 'atrocities': 23482, '##hima': 16369, 'grimaced': 19014, 'cruising': 22206, 'guo': 22720, 'matthews': 12351, 'resorts': 16511, 'elephants': 16825, '##ulata': 18060, '##while': 19927, '450': 10332, '[unused45]': 46, 'positions': 4460, 'organised': 7362, 'mundane': 24684, 'brigade': 4250, '##chi': 5428, '##rdan': 26992, 'designer': 5859, 'evenly': 18030, 'leveled': 22915, '##nished': 28357, 'edmonton': 10522, 'mail': 5653, 'epidemic': 16311, 'marina': 9985, '##kawa': 13069, 'bombings': 20109, 'borrow': 17781, '##rland': 18324, 'situation': 3663, 'clement': 12223, 'armored': 10612, 'pot': 8962, '[unused987]': 992, 'uncover': 26944, '##onne': 18256, '相': 1919, 'rose': 3123, '##ʊ': 29698, 'lutheran': 10034, 'chest': 3108, 'stock': 4518, 'avery': 12140, 'liquor': 13207, '##wana': 21761, '##−': 22543, 'mavericks': 28330, 'sustainable': 9084, 'ambiguous': 20080, 'july': 2251, 'dorchester': 27252, 'mutants': 23892, 'large': 2312, 'zulu': 27359, '##room': 9954, 'kaitlyn': 28584, 'whose': 3005, 'photographer': 8088, 'socks': 14829, '##uck': 12722, 'hawthorne': 23440, 'overture': 25052, 'skepticism': 27936, 'blacksmith': 20987, 'utmost': 27917, 'theirs': 17156, 'freezing': 12809, 'laden': 14887, 'controversy': 6704, 'preview': 19236, '##ister': 12911, 'introducing': 10449, '1630': 24994, 'rights': 2916, '##eed': 13089, 'alarm': 8598, 'shelby': 15294, 'maize': 21154, 'buffet': 28305, 'arise': 13368, '##rna': 12789, 'pdf': 11135, 'recovery': 7233, '##zar': 9057, 'reputation': 5891, '##dina': 18979, 'pali': 28619, 'ᵖ': 1504, '184': 19681, 'organizes': 22013, 'flaps': 26570, 'era': 3690, '##cs': 6169, 'arrows': 12563, 'bald': 13852, 'vendors': 17088, 'grit': 24842, '##ain': 8113, 'hannover': 29209, 'glove': 15913, '##ط': 29828, 'midway': 12213, '##iga': 13340, 'basketball': 3455, 'ad': 4748, '[unused917]': 922, 'tolerant': 23691, 'chapters': 9159, 'ruling': 6996, 'constituencies': 13315, 'dangers': 16796, 'bounty': 17284, '##nostic': 28199, 'toward': 2646, 'informs': 15670, 'secrets': 7800, '##born': 10280, 'spiritual': 6259, 'completing': 7678, 'hostility': 18258, 'schedules': 20283, '##erik': 27350, 'briefly': 4780, 'fortune': 7280, '##dara': 25329, 'pear': 28253, 'inner': 5110, '##tus': 5809, '1985': 3106, 'urbana': 27929, 'dat': 23755, 'herself': 2841, 'cremated': 28376, 'robert': 2728, '##■': 30144, 'alpha': 6541, 'variety': 3528, '##dder': 20791, '##福': 30452, 'eaten': 8828, 'rupert': 14641, '[unused239]': 244, '##rsten': 19020, '900': 7706, 'bowled': 19831, '##efined': 28344, 'scan': 13594, '106': 10114, '##আ': 29884, 'chamberlain': 13904, '##mans': 15154, '235': 17825, 'flipping': 18497, '##dda': 25062, 'wilde': 18575, 'ᅴ': 1482, 'urdu': 12454, 'joyce': 11830, 'startup': 22752, 'ு': 1397, 'lingering': 15304, '##31': 21486, 'brat': 28557, '##mg': 24798, 'croatia': 8097, 'tax': 4171, 'sarah': 4532, '##ggy': 22772, 'ष': 1337, 'ruler': 7786, 'whistling': 24350, 'benedictine': 21139, 'colorado': 5169, 'displays': 8834, 'sven': 21313, 'harley': 13653, 'recycling': 17874, 'pakistani': 9889, '##rchy': 29389, 'farming': 7876, 'indexed': 25331, '##eng': 13159, 'landfill': 28382, 'true': 2995, 'hubert': 15346, 'levels': 3798, 'wanderers': 14237, '##hope': 26441, '##cide': 27082, 'jogging': 28233, 'appears': 3544, 'blocked': 8534, '##tary': 18219, '##ذ': 29822, '##0s': 16223, 'wrong': 3308, 'suspicious': 10027, 'institutional': 12148, 'ɬ': 1121, 'someone': 2619, 'conglomerate': 22453, '##hun': 17157, 'freaked': 22783, '##bio': 26282, '##tics': 14606, 'banjo': 16698, 'justice': 3425, 'independents': 23756, 'hoop': 27669, 'aimee': 23551, '##twined': 21077, 'apartment': 4545, 'arabia': 9264, 'comics': 5888, 'angels': 7048, 'hospitalized': 24735, 'batter': 23801, 'cryptic': 26483, 'creaked': 28068, 'plainly': 24250, 'zambia': 15633, 'palatine': 22202, 'divisions': 5908, 'patrol': 6477, 'quantum': 8559, '##tford': 26341, 'booths': 27612, 'avoids': 26777, '##ods': 20620, 'edouard': 21627, '##llo': 7174, 'exploring': 11131, '##yes': 23147, 'selena': 19166, 'palazzo': 18482, 'celebrities': 12330, '##戦': 30382, '[unused530]': 535, 'seemed': 2790, 'gifts': 9604, 'daring': 15236, '¡': 1067, 'foreign': 3097, 'controls': 7711, 'costs': 5366, 'confident': 9657, '##nia': 6200, 'ports': 8831, 'principality': 18018, 'steamed': 21734, 'christensen': 24189, 'violet': 8766, '239': 23688, 'sima': 26769, 'cochran': 28506, 'slots': 19832, 'shrine': 9571, 'nails': 10063, 'atoms': 13353, 'hastings': 12296, 'wolff': 23498, '##burg': 4645, 'undeveloped': 29341, 'magic': 3894, 'kathmandu': 28045, 'trent': 7990, 'colleagues': 8628, '##essed': 23656, 'comrade': 27661, 'providence': 11293, 'utilized': 12550, 'clinging': 17892, 'orthogonal': 28721, 'motto': 12652, 'songs': 2774, 'standards': 4781, 'blocking': 10851, '##rissa': 26571, 'ვ': 1443, '##mental': 26901, 'hardy': 9532, 'mills': 6341, '##tze': 23102, 'shattered': 10909, 'czechoslovakia': 12833, 'pierce': 9267, 'surround': 15161, 'calderon': 28129, 'ケ': 1703, 'shu': 18454, '##57': 28311, '[unused414]': 419, '##christ': 26654, '[unused62]': 63, '[unused888]': 893, 'garden': 3871, '1867': 7517, 'mathematical': 8045, 'espana': 20678, 'privacy': 9394, 'stilled': 22791, 'disclosure': 19380, 'squad': 4686, 'calgary': 10112, '##ট': 29895, 'assigned': 4137, 'scaled': 18953, '##bina': 21114, 'johan': 13093, 'disciples': 17102, '##ensis': 9911, '##kon': 19648, 'semitic': 20604, 'southernmost': 21787, 'carmine': 18791, 'ς': 1172, '##ays': 22916, 'entropy': 23077, 'rags': 26346, 'mis': 28616, 'teachers': 5089, '##ı': 11722, 'tong': 15740, 'loki': 24143, 'por': 18499, 'goldman': 17765, 'bengal': 8191, 'stalked': 15858, '148': 16459, 'stampede': 29375, '##nched': 19282, 'ratification': 27369, '##∈': 30124, 'equal': 5020, 'nee': 7663, 'formal': 5337, 'york': 2259, '##shi': 6182, 'regency': 15647, 'disks': 23999, 'commuter': 14334, '##laise': 25122, 'goddard': 22547, '#': 1001, 'park': 2380, 'send': 4604, 'montgomery': 8482, '13th': 6122, 'luigi': 15153, 'opposite': 4500, 'sexy': 7916, 'flinch': 23224, 'stitches': 25343, 'kawasaki': 27324, 'coat': 5435, 'prizes': 11580, '242': 22431, 'date': 3058, 'parry': 20803, 'modern': 2715, '##play': 13068, '##cko': 19665, '##yk': 15922, '##pour': 27757, '##lc': 15472, 'soaring': 23990, 'throw': 5466, 'composure': 23619, '##gui': 25698, 'clube': 24852, 'commodities': 21955, 'humiliated': 26608, 'algae': 18670, 'hamlet': 8429, 'lease': 10084, 'wesley': 11482, '180': 8380, 'covers': 4472, 'bean': 14068, 'behind': 2369, 'director': 2472, 'urges': 23876, 'ventured': 20510, 'chihuahua': 28480, 'abnormal': 19470, 'rhea': 24775, 'hermitage': 24708, 'wheeled': 17320, 'academia': 16926, 'adele': 17623, '##aro': 10464, '氷': 1895, 'electrical': 5992, 'isles': 14195, 'zu': 16950, 'liberation': 7931, 'aragon': 16146, 'elena': 9060, 'vegetable': 15415, 'ira': 11209, '[unused196]': 201, '##wash': 28556, 'hooked': 13322, 'visas': 24487, 'encourage': 8627, 'cancel': 17542, '1840s': 19308, 'lawson': 14577, '##hurst': 10510, '123': 13138, 'տ': 1236, '太': 1812, 'ʿ': 1148, 'aberdeen': 12080, '91': 6205, 'passport': 12293, '##uration': 18924, 'hamas': 22129, 'designing': 12697, 'squadrons': 11435, '##bound': 15494, 'lucas': 6326, 'destroyers': 13242, '鈴': 1965, '##tec': 26557, '##arion': 27548, '##政': 30386, 'rodgers': 15652, '[unused173]': 178, 'issues': 3314, 'waves': 5975, 'ballroom': 14307, 'evergreen': 16899, '##き': 30178, 'slip': 7540, 'raped': 15504, 'dan': 4907, 'craters': 27561, 'shaft': 9093, '##⺩': 30158, '[unused961]': 966, 'clean': 4550, 'appreciated': 12315, '##lder': 16502, 'sunlight': 9325, 'jumps': 14523, '##ergy': 24395, 'receptor': 10769, 'excerpt': 28142, 'timing': 10984, '##pone': 29513, 'its': 2049, 'proud': 7098, '£3': 28182, '##nidae': 29135, '##ttered': 14795, 'nagoya': 24425, 'ref': 25416, '##eva': 13331, 'climbs': 18881, 'equity': 10067, 'shocked': 7135, '##roid': 22943, 'kramer': 16322, 'navajo': 22440, 'vaccines': 28896, 'minneapolis': 11334, '##3': 2509, '##は': 30198, 'disputed': 11621, 'अ': 1311, '[unused853]': 858, 'transported': 9687, 'aim': 6614, 'shock': 5213, '##▪': 30145, 'cactus': 23265, 'javelin': 23426, '##ngo': 16656, 'pain': 3255, 'crush': 10188, 'calcutta': 13419, 'vox': 29450, 'connections': 7264, 'academies': 20994, 'rendezvous': 20558, 'blockbuster': 27858, 'epsilon': 28038, 'elliptical': 27213, 'tone': 4309, 'arms': 2608, '303': 19988, '##vard': 25911, 'external': 6327, 'smiling': 5629, 'ross': 5811, 'keepers': 24018, '68': 6273, '##mate': 8585, 'impose': 17607, 'indicative': 24668, '[unused313]': 318, 'antibodies': 22931, 'governor': 3099, 'dorothea': 24052, 'anatomy': 13336, 'abandoned': 4704, '##и': 10325, 'munro': 20923, 'befriended': 23386, 'flown': 10583, 'eagle': 6755, '##ort': 11589, 'alison': 12684, 'knelt': 12804, 'nowadays': 13367, 'photographed': 16164, '##sive': 12742, 'puppy': 17022, 'ape': 23957, 'days': 2420, 'cone': 13171, 'abstraction': 24504, 'too': 2205, 'rebels': 8431, '##ass': 12054, 'mainline': 20575, '##leg': 23115, 'wires': 14666, '##icle': 25128, 'pursuing': 11828, 'eruption': 17259, 'deaf': 12419, '[unused433]': 438, '##kari': 20224, '##bered': 22408, '##terol': 27833, '##vill': 26548, 'stricken': 16654, 'sugar': 5699, 'metals': 11970, 'myth': 10661, 'beirut': 15335, 'brewery': 12161, '##orf': 16347, '##pg': 26952, 'flatly': 24295, '##$': 29615, '##nds': 18376, 'charity': 5952, 'stevenson': 13636, 'smoky': 20629, '##曲': 30396, 'connor': 6720, '##ear': 14644, 'learn': 4553, 'rb': 21144, 'havana': 15278, '##45': 19961, 'banker': 13448, 'cater': 23488, 'patches': 13864, 'thieves': 15862, 'illnesses': 24757, 'backside': 25065, 'telenovela': 25754, '##zier': 21548, 'spike': 9997, 'indicate': 5769, '##pee': 28084, '##oko': 16366, 'rosa': 9508, 'amore': 26297, 'tits': 25671, 'yun': 22854, 'mercenaries': 19331, 'disappointment': 10520, 'fencing': 15962, '##ested': 17944, '##viere': 27087, 'agustin': 26889, 'creepy': 17109, '##ancy': 11656, 'serbs': 16757, 'fiercely': 16265, 'violence': 4808, '##ada': 8447, '##mity': 16383, '[unused860]': 865, 'nobleman': 18487, '##aus': 20559, 'jagger': 25827, 'remains': 3464, 'visa': 9425, 'sighed': 5489, 'analysts': 18288, 'concerning': 7175, 'nobles': 13969, 'dominate': 16083, '##uing': 25165, 'trophies': 22236, 'dev': 16475, 'regression': 26237, 'committees': 9528, 'persona': 16115, '##jean': 27687, 'vapor': 20064, 'earrings': 27212, '##nza': 16786, 'theme': 4323, 'jia': 25871, '##vate': 16952, 'facto': 13743, '##atable': 27892, '##er': 2121, 'admiration': 17005, 'alexandre': 16971, 'software': 4007, '##ites': 7616, 'lucrative': 21115, '252': 22898, 'advocating': 20324, 'consist': 8676, 'pursed': 21954, '##dl': 19422, 'sound': 2614, '##rnier': 28484, 'yourself': 4426, '##ants': 11390, 'transatlantic': 26617, 'fighting': 3554, 'word': 2773, 'wired': 17502, 'warming': 12959, '[unused791]': 796, 'songwriter': 6009, 'walton': 14290, '##ا': 25573, '##rb': 15185, 'humidity': 18213, '##tl': 19646, 'quarries': 27069, '[unused38]': 39, 'badger': 24186, 'confronting': 26964, 'hiding': 6318, 'barak': 27739, '##nine': 19105, 'approval': 6226, 'constantin': 27991, 'chan': 9212, '##ibar': 26656, 'escaping': 13002, '##oop': 18589, 'homestead': 14473, 'policy': 3343, 'soap': 7815, 'brush': 8248, '##usion': 14499, 'sprint': 9043, '##lden': 28476, 'slashed': 23587, 'measuring': 9854, 'shutting': 17521, 'intention': 6808, 'defamation': 27652, '##tion': 3508, 'cinema': 5988, '##vac': 24887, 'rosemary': 18040, '##mps': 25370, '69': 6353, 'descriptions': 13271, 'academic': 3834, 'pd': 22851, '##fies': 14213, '##ures': 14900, '##enstein': 21819, 'obesity': 24552, 'indefinite': 25617, '##ifying': 11787, '[unused339]': 344, 'contribute': 9002, '##holm': 18884, '同': 1794, 'madman': 28441, 'grown': 4961, 'ᵥ': 1510, 'praises': 27128, 'variable': 8023, 'beast': 6841, '##rey': 15202, 'regan': 16964, 'moaned': 12557, 'looks': 3504, 'comedy': 4038, '##ricting': 28827, 'maestro': 25270, '##с': 29747, 'processed': 13995, 'thirteen': 7093, 'commentator': 12268, 'boise': 23193, 'nuns': 16752, 'hummed': 26747, 'camden': 13267, 'spruce': 19893, '[unused662]': 667, 'leighton': 26873, 'initiating': 26616, 'cheered': 25471, 'overshadowed': 28604, 'hook': 8103, '##花': 30466, 'tbilisi': 22406, 'desire': 4792, 'behold': 27541, '##eurs': 26744, 'madden': 24890, 'ʔ': 1139, 'satisfied': 8510, '232': 20666, '##din': 8718, 'seals': 13945, 'sinclair': 11881, 'siding': 17326, 'galveston': 26630, 'ear': 4540, 'leroy': 19103, 'trusted': 9480, 'launcher': 22742, 'lulu': 21744, '二': 1752, 'background': 4281, '加': 1779, 'trusting': 19836, '##lous': 15534, 'routines': 23964, 'innocent': 7036, 'among': 2426, 'を': 1690, 'ᆸ': 1488, 'bolivian': 26075, 'ე': 1442, 'anything': 2505, '##zzo': 12036, 'released': 2207, 'avalon': 18973, 'anymore': 4902, 'idf': 24011, 'babylonian': 26990, 'encircled': 25759, 'hilarious': 26316, 'archie': 13255, 'psi': 17816, 'cleansing': 26799, 'madly': 29179, 'uh': 7910, '林': 1881, 'jenny': 8437, 'concurrent': 16483, '##neo': 23585, '##yuan': 17236, 'thru': 27046, '##vd': 16872, '760': 24643, '##ropriation': 26121, 'verandah': 26153, 'whichever': 29221, '##ζ': 29724, 'hans': 7003, 'leigh': 11797, 'launching': 12106, 'locations': 5269, 'for': 2005, 'nineteen': 11977, '##gee': 18372, 'bandage': 24446, '##tees': 28313, 'parachute': 13561, 'denial': 14920, '##ological': 10091, 'exiles': 27127, 'perception': 10617, '##wing': 9328, 'zheng': 20985, 'outspoken': 22430, '›': 1533, '##ments': 8163, 'analysis': 4106, 'disagreed': 18335, 'directed': 2856, '##lidae': 21595, 'tyrone': 18770, 'chorus': 7165, 'filled': 3561, 'corsica': 27047, 'alps': 13698, 'dentist': 24385, '##ground': 16365, 'vaughn': 18220, '[unused902]': 907, 'turbulence': 29083, 'lame': 20342, 'ao': 20118, '##zawa': 19708, 'comfortable': 6625, 'nationwide': 9053, 'surge': 12058, '##م': 22192, 'furry': 28662, '##せ': 30185, 'neglected': 15486, '宇': 1819, 'jung': 11810, 'albums': 4042, '##af': 10354, 'titanic': 20753, '##ম': 29906, 'damon': 11317, '##kell': 18690, 'cp': 18133, 'adding': 5815, '##ₘ': 30096, 'lauded': 26507, 'plane': 4946, 'respectively': 4414, 'kingdoms': 12028, 'lila': 19286, '##bham': 25522, 'inadequate': 14710, 'tasha': 25448, 'score': 3556, 'stony': 16104, '##miya': 29312, 'wendy': 12815, 'workshops': 9656, 'bard': 22759, 'sending': 6016, 'eva': 9345, 'sculptors': 28417, 'lately': 9906, 'geo': 20248, 'oaks': 13396, 'handwriting': 24149, '[unused353]': 358, '##16': 16048, 'intervals': 14025, 'observes': 24451, '##yce': 29297, 'chilling': 27017, 'tapping': 15135, '##ˢ': 29717, 'sport': 4368, '##江': 30423, '##見': 30474, 'rebel': 8443, '[unused627]': 632, 'invading': 17657, 'patriotic': 14314, 'corp': 13058, 'wylie': 29152, 'forbidden': 10386, '##anza': 16076, '##ory': 10253, 'quinlan': 28451, '##lge': 28875, 'ministerial': 18645, '480': 17295, 'undoubtedly': 17319, 'amelia': 11556, 'broncos': 14169, 'publicized': 24928, 'annapolis': 24889, 'porsche': 16099, 'studio': 2996, 'pilasters': 26752, 'archangel': 28185, 'magnificent': 12047, 'dictatorship': 18944, 'figuring': 23218, 'saunders': 15247, 'central': 2430, 'conveyed': 21527, 'echoes': 17659, 'alta': 23647, '##hala': 19531, '##unds': 26698, 'sheldon': 19369, '##cope': 16186, '##cre': 16748, '##iki': 17471, '##52': 25746, 'horseshoe': 23449, '[unused846]': 851, 'watery': 28259, 'yanked': 10963, 'learnt': 20215, 'bar': 3347, '[unused676]': 681, 'prussian': 10734, 'duffy': 17971, 'successor': 6332, 'grows': 7502, 'cubic': 11919, 'alloy': 17564, 'jumper': 21097, 'diabetes': 14671, 'რ': 1451, 'knuckles': 13513, 'kimberley': 25004, '¢': 1068, 'bessie': 29298, 'あ': 1646, '[unused213]': 218, 'coke': 14492, 'ং': 1346, '##im': 5714, 'united': 2142, 'off': 2125, 'fantastic': 10392, 'mesa': 15797, '##vita': 28403, 'southward': 22161, 'bavarian': 15097, 'saxophonist': 19977, 'labyrinth': 24239, 'consulted': 17535, 'peptide': 25117, '##istles': 28738, '##成': 30380, 'rather': 2738, 'step': 3357, 'shoot': 5607, '##rites': 28884, 'drew': 3881, '[unused850]': 855, '##tree': 13334, '##tility': 18724, 'heightened': 21106, '##rts': 21217, '##ili': 18622, '1765': 20797, 'riley': 7546, 'cello': 10145, 'allmusic': 10477, '##fly': 14151, '##lice': 13231, 'sore': 14699, 'letters': 4144, '1750': 18171, 'rewards': 19054, 'presentation': 8312, 'equals': 19635, 'clinics': 17865, 'feast': 9831, '##gus': 12349, '##hand': 11774, 'ars': 29393, '[unused619]': 624, 'chat': 11834, 'aquitaine': 24973, 'oddly': 15056, 'showing': 4760, 'santo': 11685, '1860s': 15914, '##ය': 29940, 'crap': 10231, '##iface': 29164, 'vikings': 13468, 'hutton': 20408, 'ceremonies': 10912, 'buffer': 17698, '1815': 10679, 'earthquakes': 17932, 'ozone': 26443, 'january': 2254, '##ง': 29946, '##vani': 27760, 'flicked': 12777, '##ists': 5130, 'somewhere': 4873, 'encore': 19493, 'commands': 10954, 'forbid': 27206, 'sweeps': 26981, '##izzly': 29266, 'puppet': 13997, '##ato': 10610, 'needed': 2734, 'processor': 13151, '1695': 29140, 'vectors': 19019, 'impaired': 18234, 'poorly': 9996, 'chatter': 24691, 'witnessing': 24740, 'plume': 26888, 'generals': 11593, 'bargaining': 21990, 'equality': 9945, 'peggy': 14911, 'chimed': 27460, 'lucius': 15639, 'selections': 16310, 'assets': 7045, 'lair': 21039, 'earn': 7796, 'damaging': 15011, '##tz': 5753, 'charities': 15430, 'sinks': 23462, 'clerks': 26706, '##ament': 24996, 'birth': 4182, 'variance': 23284, '##org': 21759, 'theta': 23963, 'clone': 17598, '##avia': 15164, 'venue': 6891, '[unused826]': 831, '##চ': 29892, '##₉': 30085, 'binds': 20817, 'rampant': 25883, 'chancellor': 7306, '525': 25621, 'zoom': 24095, 'terminals': 17703, 'possibly': 4298, 'stranger': 7985, 'ා': 1408, '##lium': 21816, 'stood': 2768, '##raphy': 26228, 'plastered': 21832, 'ditch': 14033, 'norris': 15466, 'economic': 3171, 'cf': 12935, 'influencing': 25870, 'liaison': 14975, '##oslav': 23085, 'childish': 24282, '張': 1844, '##oese': 23379, 'statues': 11342, 'tremendous': 14388, 'comic': 5021, 'definitions': 15182, 'hugh': 6621, 'fiona': 13073, 'introductory': 23889, 'healthcare': 9871, '##vn': 16022, 'ར': 1434, '##lho': 28061, 'compositions': 9265, 'charcoal': 18872, 'constituted': 11846, 'bro': 22953, 'reported': 2988, 'dad': 3611, 'pace': 6393, 'singing': 4823, 'longtime': 11155, 'snake': 7488, '##ogan': 21131, 'sold': 2853, 'plata': 19534, 'shipment': 22613, 'nils': 27282, '1902': 5774, 'last': 2197, 'regardless': 7539, '##dity': 25469, '##bri': 23736, 'napoleon': 8891, 'analytical': 17826, 'characters': 3494, 'handball': 12378, 'supposed': 4011, 'ankle': 10792, 'sacrament': 28958, 'bamboo': 15216, 'sealed': 10203, 'seminole': 28909, 'unmistakable': 24219, 'glucose': 18423, 'homecoming': 20772, 'fishes': 21995, 'lush': 16299, 'daniela': 28541, 'mainstream': 7731, 'euclidean': 25826, 'derive': 18547, '##也': 30276, 'indira': 28232, 'crewe': 22188, 'ieee': 15368, '##nob': 25083, 'alzheimer': 21901, 'technologies': 6786, '##nya': 17238, 'laude': 21602, 'whistle': 13300, '##mme': 20058, 'insert': 19274, '##jana': 18803, '[unused423]': 428, 'abbas': 17532, '1912': 4878, 'bloomberg': 22950, '##tained': 28055, 'compete': 5566, 'carter': 5708, 'thai': 7273, 'having': 2383, '1882': 7131, 'solicitor': 15468, 'گ': 1305, 'gear': 6718, 'never': 2196, '##usia': 24118, 'smile': 2868, 'foyer': 16683, '1971': 3411, '##ocks': 25384, 'frontiers': 28750, '##ο': 29730, 'sweetly': 22557, 'lesley': 23920, 'vibrating': 24987, '[unused105]': 110, '##scoe': 28147, 'commentary': 8570, 'archduke': 27463, '##س': 29824, 'often': 2411, '405': 23988, 'tossed': 7463, 'tobias': 16858, 'libretto': 17621, '[unused881]': 886, 'mercer': 13081, 'adrian': 7918, 'neared': 20832, 'skating': 10080, 'relevance': 21923, 'nz': 20008, 'riaa': 22716, 'antiques': 27763, 'benedict': 12122, 'collarbone': 29600, 'ᴬ': 1490, 'spain': 3577, 'ambulance': 10771, 'meetings': 6295, '##law': 14919, '##hear': 26560, 'novella': 20674, '##scopic': 24895, 'amnesty': 16154, '505': 28952, 'ss': 7020, '##he': 5369, '[unused566]': 571, 'digger': 28661, 'brute': 26128, 'magnum': 19691, '##write': 26373, 'warehouses': 23319, 'obtain': 6855, '##fields': 15155, 'soho': 23771, 'learning': 4083, '[unused261]': 266, 'nantes': 25387, 'bombardier': 29143, 'salt': 5474, 'urging': 14328, 'endowment': 15108, 'engagements': 20583, '##kir': 23630, 'willingness': 19732, '##jo': 5558, 'dread': 14436, 'labeling': 28847, 'babies': 10834, 'peeled': 20956, 'android': 11924, 'governance': 10615, '##ean': 11219, 'sundays': 14803, 'portrayal': 13954, 'great': 2307, 'interdisciplinary': 18593, 'pseudo': 18404, 'mbe': 20301, '##ม': 29951, 'shri': 14880, 'temeraire': 23611, 'mans': 16042, '1880': 6756, '96': 5986, 'jason': 4463, '##nets': 22781, '[unused111]': 116, 'brigades': 14589, 'reds': 12281, 'waltz': 17569, 'glued': 22805, 'serge': 21747, 'shield': 6099, 'tory': 17117, 'dietary': 23444, 'brutality': 24083, 'guys': 4364, '##,': 29623, 'hopping': 26397, 'cardinal': 7185, '##艹': 30465, 'opener': 16181, '1800s': 19878, '##a1': 27717, '##lateral': 28277, 'capable': 5214, '##plin': 28296, 'grinned': 6973, '##truct': 18300, 'edison': 17046, '##isa': 14268, 'economist': 11708, 'climbed': 6589, 'tests': 5852, 'berman': 29482, '0': 1014, 'agrarian': 23226, '##ी': 29878, 'recognized': 3858, 'touched': 5028, 'marlins': 27055, 'tar': 16985, 'appreciate': 9120, 'optic': 22816, 'tehsil': 20751, 'tributary': 8914, '415': 24690, '##ག': 29962, 'again': 2153, 'sacramento': 11932, 'lettering': 25782, 'march': 2233, 'animal': 4111, 'acquire': 9878, '[unused394]': 399, 'guessing': 16986, 'pits': 14496, '1620': 24259, '##武': 30415, 'afi': 28697, 'government': 2231, 'descendants': 8481, 'bleeding': 9524, 'blaze': 15347, 'pickup': 15373, '##zuka': 22968, 'cartridges': 22998, 'portfolio': 11103, 'organs': 11595, '##nica': 12782, 'ceramic': 14692, '##pole': 15049, 'emilio': 18644, '##lva': 22144, 'gallon': 25234, 'latch': 25635, 'volunteered': 14382, '##ह': 29875, 'oversight': 15709, 'begged': 12999, 'boating': 23639, 'marx': 13518, 'argued': 5275, '9': 1023, 'bahia': 22474, 'whitaker': 27049, 'democracy': 7072, '##urus': 20089, 'militants': 17671, '287': 23090, 'prussia': 10724, 'klaus': 16536, '##%': 29616, '##sari': 22740, 'sensations': 21378, 'guitar': 2858, 'cloud': 6112, 'insufficient': 13990, 'beyonce': 20773, '##aday': 28039, 'aleppo': 22973, 'highlands': 11784, 'stalks': 29594, 'la': 2474, 'distressed': 24305, 'hagen': 24047, 'hungary': 5872, '##mit': 22930, 'baroque': 10456, '81': 6282, 'ascribed': 28025, 'tunic': 23002, 'reclaim': 24104, 'gaulle': 28724, 'huddersfield': 19715, 'bronx': 14487, '##ddy': 14968, 'prostitutes': 20833, 'sudanese': 25603, 'loving': 8295, 'chandra': 16469, 'ˢ': 1152, 'copied': 15826, 'microscopy': 29105, '##buch': 25987, 'reworked': 27575, 'snapped': 5941, '##tra': 6494, '永': 1896, '##azi': 16103, '介': 1759, 'suppose': 6814, 'paradise': 9097, 'northward': 17192, '≈': 1606, 'ages': 5535, 'root': 7117, 'howard': 4922, '[unused756]': 761, 'query': 23032, 'hosts': 6184, 'cushions': 27205, '##19': 16147, '##llin': 21202, 'bronze': 4421, 'closely': 4876, 'swallowed': 7351, 'furnished': 19851, 'tourists': 9045, 'workplace': 16165, 'pri': 26927, '##min': 10020, 'hormone': 18714, 'foul': 12487, '##orio': 27416, 'や': 1682, 'mental': 5177, '##rada': 28510, 'assault': 6101, 'charlotte': 5904, 'second': 2117, 'dilemma': 21883, 'fragmentation': 28424, 'hostile': 10420, 'roots': 6147, 'logos': 25571, 'heroes': 7348, 'attracts': 17771, 'sinai': 20837, 'slovakia': 10991, 'vortex': 23463, 'duly': 25073, 'satin': 19412, '16th': 5767, '##ault': 23505, '##umi': 12717, 'reappeared': 23040, 'ruled': 5451, 'runway': 9271, 'downfall': 22252, 'piper': 11939, '[unused13]': 14, 'torch': 12723, 'criticised': 10648, '##hta': 22893, 'mendes': 27916, 'stretched': 7121, 'inability': 13720, 'estranged': 24211, 'vault': 11632, '##phones': 19093, 'graves': 9729, 'beard': 10154, 'bowler': 14999, 'hampered': 22532, 'keeper': 10684, 'whale': 13156, 'subgroup': 20576, 'whales': 17967, 'transmissions': 21670, 'motorsport': 21044, '##uto': 16161, 'interceptor': 24727, 'asphalt': 16295, 'machines': 6681, '##lty': 24228, 'gunn': 22079, 'stockings': 26412, 'ir': 20868, '[unused188]': 193, 'sherlock': 20052, 'shutter': 28180, '750': 9683, 'malicious': 24391, 'propagation': 20594, 'modest': 10754, 'lives': 3268, 'bhutan': 18768, 'chloride': 19057, '♥': 1625, 'beta': 8247, 'rca': 12639, 'pseudonym': 13881, 'neo': 9253, 'olds': 19457, 'brightness': 18295, 'communists': 13009, 'stammered': 28532, 'holland': 7935, 'sparta': 21251, '##held': 24850, 'arid': 17511, '1719': 28162, 'plots': 14811, 'wears': 11651, '##学': 30343, 'bank': 2924, '##gni': 29076, '##hara': 11077, 'disrupt': 23217, 'bradford': 9999, '##ios': 10735, 'cartel': 21680, 'm3': 29061, 'plea': 14865, 'edition': 3179, 'evie': 26400, 'giggling': 21783, 'perched': 17335, 'helium': 22764, '##riation': 18769, '##ots': 12868, 'drilled': 24311, 'twinkle': 29038, '##下': 30269, 'religions': 11822, 'terraces': 25633, 'psycho': 18224, 'wooden': 4799, 'websites': 11744, 'similarities': 12319, 'pitched': 8219, 'packages': 14555, 'speculation': 12143, 'packing': 14743, '##evic': 17726, 'lucivar': 21401, 'nestor': 25397, 'barrio': 25676, 'blanket': 8768, '##bolic': 18647, 'advertised': 17099, 'aiming': 13659, 'parasites': 23996, '##fied': 10451, 'م': 1295, 'stallion': 19122, 'bmg': 24499, '##dome': 26173, 'benfica': 26542, 'moist': 11052, 'orange': 4589, '[unused912]': 917, 'brown': 2829, 'rolled': 4565, 'howl': 22912, '1978': 3301, 'pitchers': 23232, 'memorable': 13432, 'confiscated': 17182, 'cebu': 23312, 'das': 8695, 'ව': 1407, 'snorted': 15235, '##gp': 21600, 'ד': 1244, '##ocytes': 28788, 'kerman': 24500, 'bs': 18667, 'archived': 9749, 'demanded': 6303, '##iary': 17302, 'mateo': 19327, 'დ': 1441, 'elle': 15317, '[unused788]': 793, '##olis': 20872, 'furiously': 20322, 'fastened': 24009, 'oil': 3514, '##poo': 24667, '##e': 2063, 'whom': 3183, '##haus': 13821, '##etz': 26327, 'centro': 18120, 'day': 2154, 'inverted': 20037, '##da': 2850, 'scouts': 10158, '##sov': 23230, 'bound': 5391, 'behavior': 5248, 'storylines': 22628, 'bao': 25945, 'marines': 9622, '[unused851]': 856, '1908': 5316, '##ɲ': 29689, '##疒': 30439, 'theresa': 14781, 'conner': 17639, '##vina': 17948, 'dixie': 18910, '##bolt': 22803, 'nueva': 24536, '##mis': 15630, 'policies': 6043, 'burton': 9658, 'cigar': 19135, '##sche': 22842, 'deception': 17575, 'alan': 5070, 'gary': 5639, 'hurting': 11878, 'worm': 15485, '##just': 29427, 'christy': 21550, 'andersen': 18308, '##ham': 3511, 'guangdong': 21287, '132': 14078, 'crust': 19116, '##ⱼ': 30157, 'appropriate': 6413, 'permits': 14245, 'revolving': 24135, '1690': 24765, 'return': 2709, '[unused991]': 996, 'ludwig': 10302, 'grille': 26192, 'quake': 27785, 'dreamer': 24726, 'sri': 5185, 'dockyard': 24874, 'orientation': 10296, 'soaked': 13077, 'inscriptions': 12920, 'shorts': 9132, 'hartley': 20955, '##walker': 26965, 'shipbuilding': 16802, 'factory': 4713, 'leaders': 4177, '##lers': 12910, 'genuinely': 15958, 'undertake': 16617, 'heats': 18559, '##ayton': 29319, 'campo': 22339, '##har': 8167, '##tar': 7559, 'violating': 20084, 'confederate': 8055, 'conservation': 5680, 'reduce': 5547, 'ortiz': 19439, 'authenticity': 21452, '¬': 1078, '£5': 27813, 'ceremony': 5103, 'denomination': 18683, '##rzburg': 29136, 'lasted': 6354, 'bacon': 11611, 'kamal': 21911, 'synthesizer': 13564, 'mice': 12328, 'features': 2838, 'enabled': 9124, 'nc': 13316, 'reactive': 22643, 'eliminate': 11027, '116': 12904, 'screaming': 7491, 'mated': 28795, 'garage': 7381, 'telecom': 18126, '##mpton': 26793, 'valet': 27238, 'shire': 13182, 'bismarck': 22029, 'checks': 14148, 'together': 2362, 'marriott': 26490, 'tailored': 21727, 'ireland': 3163, '##opa': 29477, '##ider': 18688, 'rpm': 11575, 'converted': 4991, 'rid': 9436, '##any': 19092, 'commemorating': 20646, 'hierarchical': 25835, '##plane': 11751, 'damn': 4365, 'represents': 5836, 'practiced': 9051, 'wolfgang': 13865, '##els': 9050, 'exercises': 11110, 'convenient': 14057, 'skopje': 29255, 'nina': 9401, 'fans': 4599, 'accidentally': 9554, 'ti': 14841, '##bor': 12821, '##nea': 22084, 'charts': 6093, 'loosely': 11853, 'ʸ': 1144, 'ptolemy': 23517, 'tres': 24403, 'uncle': 4470, 'zhang': 9327, 'boundaries': 7372, 'empowered': 26480, '##plify': 28250, '##schaft': 26527, 'granite': 9753, 'clips': 15281, 'angrily': 14136, 'enacted': 11955, 'bends': 23394, 'raleigh': 15842, '##tative': 27453, 'operative': 12160, 'bayou': 24818, 'reflect': 8339, '##itude': 18679, 'conscription': 26329, 'siberian': 21822, 'teenager': 10563, '##ticus': 29587, '[unused842]': 847, 'aesthetic': 12465, 'discretion': 19258, '##ₒ': 30091, 'clarinet': 12089, '##ide': 5178, '135': 11502, 'importance': 5197, 'puddle': 25081, 'discovered': 3603, '##hon': 8747, 'fathers': 11397, '##mbling': 29256, 'mozart': 13177, 'sharif': 20351, 'offered': 3253, 'restaurant': 4825, '##葉': 30469, 'tatum': 25913, 'parliament': 3323, 'mild': 10256, 'pt': 13866, 'pei': 26850, '##itas': 24317, 'swapped': 29176, 'sweaty': 19889, 'fauna': 15018, 'seat': 2835, 'cornerstone': 23354, 'dominated': 6817, 'soloist': 16504, 'haste': 24748, 'climax': 14463, 'design': 2640, 'composer': 4543, 'facilitate': 10956, 'dom': 14383, 'perpendicular': 19581, 'subdued': 20442, '##onate': 21149, 'leaps': 29195, 'versatile': 22979, 'swarm': 21708, 'vineyards': 20597, 'urine': 17996, 'speedy': 26203, 'dow': 23268, 'visitation': 28877, 'forums': 21415, '##imating': 22835, 'mommy': 20565, 'reggie': 17963, 'halves': 23672, '##す': 30184, 'plastic': 6081, '##ウ': 30222, '2a': 23409, 'super': 3565, 'scraping': 23704, 'inched': 25330, 'lowlands': 26411, '##izations': 22318, 'laboratories': 12030, 'mutations': 14494, 'rivers': 5485, 'crashing': 12894, '441': 28015, 'ə': 1114, '₤': 1572, 'merged': 5314, '##tom': 20389, 'bookstore': 21785, '##ₕ': 30093, 'socio': 17522, 'paced': 13823, 'loan': 5414, 'audience': 4378, 'breakfast': 6350, 'mbc': 27262, 'bulb': 20581, 'choral': 14564, '##odle': 26156, 'refusal': 13948, 'adversary': 27248, 'mississippi': 5900, '##‖': 30054, '##yx': 17275, '270': 13756, 'about': 2055, '1943': 3826, 'proposal': 6378, 'tent': 9311, 'flourish': 24299, 'hunted': 14682, 'shannon': 10881, 'bump': 16906, '手': 1858, '##島': 30359, 'emanuel': 21135, 'peruvian': 15432, 'dirt': 6900, 'debuted': 6006, 'type': 2828, 'leger': 26523, 'observer': 9718, 'typically': 4050, 'pictorial': 28304, '##eries': 28077, 'disappointed': 9364, 'reaper': 19559, '##load': 11066, 'sketch': 11080, 'requests': 11186, '##avio': 28471, 'important': 2590, '[unused905]': 910, 'diving': 9404, '##pling': 14353, 'foremost': 16097, '##lga': 27887, 'hbo': 14633, '##tangle': 23395, '24th': 13386, 'mall': 6670, '##cky': 17413, 'j': 1046, 'chart': 3673, 'duncan': 7343, 'flowering': 10902, 'enrique': 15769, '##chev': 16179, 'hyper': 23760, 'calming': 23674, '38': 4229, '[unused440]': 445, 'voices': 5755, 'lonely': 9479, 'meiji': 23214, 'horde': 21038, 'jamestown': 27435, 'inaugural': 7725, 'xi': 8418, 'wright': 6119, 'branched': 21648, '##gizing': 28660, '##ractive': 26884, '##ai': 4886, 'define': 9375, 'arousal': 18905, 'observations': 9420, 'licking': 17033, 'orchard': 15623, 'ahmad': 10781, 'minh': 19538, 'angelica': 19458, '##zy': 9096, 'approximately': 3155, 'lords': 8140, 'rape': 9040, '##logram': 24915, 'stuck': 5881, '##uber': 21436, '##zuki': 24015, 'vodka': 21092, 'pamela': 17217, 'repairing': 26296, 'onset': 14447, 'invites': 18675, 'release': 2713, '##ward': 7652, 'romans': 10900, 'geraldine': 26987, 'firefighters': 21767, 'greater': 3618, 'aura': 15240, '##weiler': 22384, '[unused316]': 321, 'fighter': 4959, 'specialty': 12233, '##iae': 19001, '##finder': 23695, '##vr': 19716, 'generic': 12391, 'confirming': 19195, 'mia': 8764, 'sword': 4690, 'yearbook': 24803, 'announces': 17472, 'viking': 12886, 'cinderella': 21686, 'qb': 26171, 'ernie': 14637, '##代': 30286, 'inherently': 26096, 'network': 2897, 'galaxies': 21706, 'anytime': 15933, 'soyuz': 29412, 'resolved': 10395, 've': 2310, 'reptiles': 20978, 'brow': 8306, 'time': 2051, 'cooks': 26929, 'exploration': 8993, 'queen': 3035, 'depot': 8470, 'knob': 16859, 'patrick': 4754, 'm²': 7030, 'almeida': 29555, '##¥': 29648, 'locker': 12625, 'editions': 6572, 'trimmed': 21920, 'ᄑ': 1468, 'commission': 3222, 'mastered': 15682, 'dreadful': 21794, '[unused229]': 234, 'filthy': 18294, 'privateer': 26790, 'norway': 5120, 'inference': 28937, 'riding': 5559, 'beetles': 14538, 'federally': 20892, 'achievement': 6344, 'company': 2194, '区': 1782, 'consultancy': 24853, 'supporters': 6793, 'computed': 24806, 'ecosystems': 20440, '##hom': 23393, 'recordings': 5633, 'originate': 21754, '##wick': 7184, 'rhymes': 24468, 'jaw': 5730, 'samsung': 19102, 'glossy': 19504, 'bounce': 17523, 'suit': 4848, 'prestigious': 8919, 'structured': 14336, 'speaking': 4092, 'fanny': 17813, '104': 9645, 'barked': 17554, 'imagery': 13425, '##hiti': 27798, 'living': 2542, 'master': 3040, 'organize': 10939, 'bavaria': 11606, 'corpus': 13931, 'quartet': 8530, 'incorrect': 16542, 'furrowed': 20555, 'fearless': 22518, 'groceries': 26298, 'condor': 29260, 'neurons': 15698, 'outfielder': 21033, 'surface': 3302, 'kick': 5926, 'unpleasant': 16010, 'aus': 17151, 'bytes': 27507, 'additive': 29167, 'chronicles': 11906, '##tology': 23479, 'endured': 16753, 'punt': 18975, 'papa': 13008, '##glass': 15621, '##ush': 20668, '82': 6445, 'res': 24501, 'cheer': 15138, 'merlin': 15993, 'depression': 6245, 'truncated': 25449, 'rhythm': 6348, 'bonn': 19349, 'belongings': 20033, '##ation': 3370, 'patronage': 15694, '##kney': 26993, 'librarian': 13850, 'kris': 19031, 'asteroid': 12175, 'safer': 13726, 'recovering': 13400, 'worried': 5191, '##rid': 14615, 'lace': 12922, 'lai': 21110, '##awa': 10830, 'customer': 8013, '##英': 30467, 'handle': 5047, 'spanning': 13912, 'lagos': 16738, 'hailey': 21664, 'kindness': 16056, 'morton': 11164, '##mbled': 23931, '##uh': 27225, 'unacceptable': 21873, 'stick': 6293, '##uven': 27346, 'douglass': 27485, 'barclay': 23724, '##adt': 18727, 'peer': 8152, 'shoe': 10818, '辶': 1956, 'create': 3443, 'discrete': 16246, 'excel': 24970, 'biomass': 28148, '##ի': 29772, 'wcw': 24215, 'developments': 8973, 'investigation': 4812, 'fancy': 11281, 'с': 1196, 'noticeable': 17725, 'haynes': 21805, 'loft': 19459, 'cocktail': 18901, '##rose': 13278, '##ative': 8082, '##ほ': 30202, 'tan': 9092, '[unused686]': 691, 'gorge': 14980, 'penang': 21812, '美': 1935, 'complex': 3375, 'het': 21770, '##tet': 22513, 'intently': 16170, '1741': 25280, 'osman': 28609, '##grounds': 28951, '##hin': 10606, 'richter': 20105, 'memorandum': 20336, 'revolutionaries': 24517, 'gambia': 26728, 'nature': 3267, '##zee': 23940, 'visible': 5710, '1728': 26833, '1835': 10150, 'abbot': 11428, 'emphasizing': 22671, 'gemma': 19073, '[unused296]': 301, 'stealing': 11065, 'cocked': 13179, 'chapels': 24130, 'entertainer': 21751, 'glared': 9400, 'patron': 9161, 'possession': 6664, '##tight': 26143, '##lov': 14301, 'breast': 7388, '##ex': 10288, 'collectors': 14256, 'relay': 8846, 'darmstadt': 28381, 'চ': 1356, '##rita': 17728, 'fiancee': 19455, 'jewellery': 21545, 'contestant': 10832, '##nic': 8713, 'mildred': 25452, 'flashing': 12659, '##eti': 20624, 'marathon': 8589, 'undefeated': 15188, 'ornaments': 24005, 'thatcher': 21127, 'sebastian': 6417, 'fails': 11896, 'det': 20010, 'carolina': 3792, 'splits': 19584, 'sale': 5096, '##rs': 2869, '55': 4583, '##sus': 13203, 'extant': 12905, 'gnome': 25781, 'sinatra': 19643, 'hooks': 18008, 'anybody': 10334, 'stripes': 13560, 'correlation': 16902, 'efforts': 4073, 'plagued': 17808, 'disbelief': 12537, 'dhaka': 16479, 'vita': 19300, 'karma': 19902, 'survivor': 12084, 'hamilton': 5226, 'completion': 6503, '##rius': 13245, 'slicing': 26514, '1906': 5518, 'hopefully': 11504, 'sharma': 14654, 'ck': 23616, 'controller': 11486, 'dispatched': 14501, 'blogs': 23012, 'four': 2176, '[unused241]': 246, 'choked': 12444, '##pac': 19498, 'replaced': 2999, 'during': 2076, 'airplay': 15341, 'bed': 2793, '##ausen': 25701, 'sediment': 19671, 'precursor': 14988, 'entrepreneurs': 17633, 'banner': 9484, 'facade': 8508, 'racist': 16939, 'suffer': 9015, 'mature': 9677, '##biotic': 26591, 'grading': 26886, 'spring': 3500, 'noted': 3264, 'pulsing': 23139, '##cot': 24310, 'nadia': 14942, 'wavy': 23098, 'shifter': 18189, 'purcell': 26429, 'groan': 13001, '##公': 30298, '##nte': 10111, 'sky': 3712, 'hasn': 8440, 'lavish': 22689, '##guard': 18405, 'audi': 20075, '་': 1424, '##bha': 22655, 'risk': 3891, 'west': 2225, 'dementia': 28767, 'colliery': 19462, 'darker': 9904, 'teammate': 10809, '##orth': 28610, 'eased': 10987, 'elf': 17163, 'tents': 17732, 'centimetres': 13935, '##ometer': 18721, 'changed': 2904, 'deposited': 14140, 'electrons': 15057, 'scary': 12459, 'personally': 7714, 'qaeda': 18659, 'whitehead': 23257, '[unused650]': 655, 'departed': 7745, '##oin': 28765, '[unused157]': 162, 'turnpike': 17116, 'occasional': 8138, '[unused375]': 380, 'messiah': 22112, 'caf': 24689, '##page': 13704, 'hawk': 9881, '[unused880]': 885, 'turning': 3810, 'continuity': 13717, 'keyboardist': 20173, '##pathic': 25940, 'richmond': 6713, 'tak': 27006, 'nacional': 10718, 'rations': 29559, 'mw': 12464, '##uts': 16446, '##帝': 30364, '##vich': 12933, 'facilitated': 19601, 'containing': 4820, 'molecule': 13922, 'checking': 9361, 'cortes': 22242, 'savior': 24859, 'code': 3642, 'hammer': 8691, '##en': 2368, 'sphere': 10336, 'please': 3531, '##igraphy': 23132, 'toured': 7255, 'horsemen': 25431, '##venor': 26806, 'case': 2553, 'collective': 7268, 'labels': 10873, 'regard': 7634, '##lby': 14510, 'biochemistry': 22419, 'university': 2118, '##チ': 30236, '##ca': 3540, 'barber': 13362, '##ण': 29858, 'youth': 3360, 'vacuum': 11641, 'ᆫ': 1485, '[unused382]': 387, 'dundee': 14252, 'surf': 14175, 'mounds': 19503, 'susan': 6294, 'fabulous': 18783, 'leah': 14188, 'qualifying': 6042, 'winked': 14217, '##dron': 19440, 'blah': 27984, 'bruce': 5503, 'pearson': 12874, '##“': 30058, 'percent': 3867, 'gloria': 10778, 'jurgen': 23171, 'opportunities': 6695, '1500': 10347, 'pound': 9044, '##onzo': 29452, '##mir': 14503, '##yp': 22571, '[unused276]': 281, 'visibility': 16476, 'reacts': 27325, '56': 5179, 'inaccessible': 29104, '##do': 3527, 'obey': 15470, 'bowman': 19298, 'cheerfully': 27403, 'dunlop': 27458, '₃': 1550, '##pd': 17299, '##worth': 5172, 'defective': 28829, '##dden': 17101, '161': 17365, '1823': 12522, 'schooner': 21567, '14th': 6400, 'odds': 10238, 'interrogation': 16871, 'euros': 19329, 'weaver': 14077, 'mandarin': 15831, 'bargain': 17113, 'differences': 5966, 'imam': 17189, 'sonic': 12728, 'mandate': 11405, 'postmaster': 20707, 'francois': 8173, 'colonization': 18962, 'admissions': 20247, 'kristen': 22435, '##shima': 24772, 'reverse': 7901, 'nathaniel': 13961, 'walked': 2939, 'tracked': 12808, 'layers': 9014, 'distress': 12893, '##ctions': 22014, 'lighter': 9442, '[unused473]': 478, '80': 3770, 'jammu': 21433, 'goals': 3289, '740': 25833, '##rrow': 28597, 'goldberg': 18522, '##風': 30503, 'paired': 12739, 'fiba': 13651, '##58': 27814, 'governing': 8677, 'malice': 28238, '##sia': 8464, 'extreme': 6034, 'psyche': 25774, 'payment': 7909, 'leopold': 12752, '##elo': 18349, '[unused145]': 150, '##rn': 6826, 'distinction': 7835, 'garfield': 20170, 'usher': 20774, '1964': 3546, 'amount': 3815, 'sheng': 25981, 'wil': 19863, 'toss': 10055, 'hourly': 21462, '##wee': 28394, 'overheard': 20443, 'dunkirk': 29467, 'bethlehem': 19360, 'spoiled': 19582, 'express': 4671, 'ne': 11265, 'stephanie': 11496, 'purdue': 19749, 'amtrak': 20208, '##itarian': 25691, 'hall': 2534, 'ago': 3283, 'muscle': 6740, 'challenged': 8315, '##tie': 9515, 'failures': 15428, 'grips': 24758, '##linson': 25051, 'altered': 8776, 'pinned': 11807, 'gymnasium': 11288, 'pradesh': 7970, 'bore': 8501, '##plate': 15725, '##emann': 17545, 'clamped': 18597, 'donate': 21357, 'manila': 9011, 'grateful': 8794, 'contributed': 5201, 'muster': 20327, 'crafted': 19275, 'minsk': 20790, 'basil': 14732, '##ua': 6692, '##zhi': 19436, 'performed': 2864, 'roma': 12836, '##tting': 13027, 'minogue': 27736, 'minerals': 13246, 'financing': 12135, '90': 3938, '##yar': 13380, 'onboard': 27120, 'orders': 4449, 'climb': 7105, 'worries': 15508, '##boys': 24916, '##ement': 13665, 'pendulum': 28300, 'dances': 11278, 'leap': 11679, 'este': 28517, 'defaulted': 17265, 'elm': 17709, 'confuse': 28679, 'blessed': 10190, 'prince': 3159, 'leyte': 27214, 'rule': 3627, 'readiness': 19822, 'meals': 12278, 'honolulu': 16598, 'c1': 27723, 'professional': 2658, 'headlights': 18167, 'trance': 16588, '[unused191]': 196, 'barnes': 9957, '136': 15407, '365': 19342, 'lukas': 23739, 'doll': 10658, '##iner': 26455, 'commercials': 12698, 'magnate': 27470, '##sch': 11624, 'endure': 18094, 'moravian': 28131, 'kuala': 15490, '1645': 26081, 'https': 16770, 'windows': 3645, 'julio': 15090, 'panels': 9320, 'doomed': 20076, 'occupations': 23374, 'field': 2492, '##edd': 22367, '[unused841]': 846, 'blanca': 27538, 'jamal': 24132, 'strongly': 6118, 'і': 1213, 'lifetime': 6480, 'accomplished': 8885, 'pops': 16949, 'responsibility': 5368, 'surely': 7543, '##clusive': 23633, 'greaves': 27808, '}': 1065, 'limitations': 12546, '1861': 6863, 'oxidation': 19577, 'bazaar': 22774, '##sur': 26210, '##ni': 3490, 'discouraged': 22585, 'sparked': 13977, '##tory': 7062, '[unused943]': 948, 'assam': 15181, 'armed': 4273, 'expectation': 17626, '1659': 28288, 'something': 2242, '##ica': 5555, 'staten': 24161, 'speeding': 21485, 'memo': 24443, 'interacting': 21935, 'cheyenne': 17778, 'landon': 20642, 'swam': 16849, '¨': 1074, '##n': 2078, 'ammunition': 9290, 'wedge': 17632, 'nationality': 10662, '[unused878]': 883, 'sa': 7842, '[unused430]': 435, 'indicating': 8131, 'georgina': 27358, '##cina': 28748, 'map': 4949, 'pointing': 7302, '##mb': 14905, 'reconciliation': 16088, 'retreating': 17512, 'sunshine': 9609, 'peeling': 28241, '##oni': 10698, 'institutions': 4896, '##rigues': 22934, 'contributors': 16884, 'month': 3204, 'rutgers': 18607, 'movies': 5691, '##some': 14045, 'frantic': 15762, '[unused170]': 175, 'tianjin': 26216, 'workshop': 8395, 'canadiens': 21247, '##aru': 15728, 'creations': 20677, 'exiting': 22371, 'divine': 7746, 'mono': 18847, '##res': 6072, 'apparatus': 14709, '1790': 13393, 'delaying': 29391, 'scratches': 25980, 'blushed': 19370, '…': 1529, 'non': 2512, 'shame': 9467, '[unused728]': 733, 'trained': 4738, 'imagination': 9647, 'specially': 11974, 'sonata': 14681, '##time': 7292, 'bjp': 24954, '##ford': 3877, 'frozen': 7708, 'mayfair': 28387, 'प': 1328, 'authors': 6048, '川': 1835, 'heal': 11005, '##白': 30441, 'inmates': 13187, '##66': 28756, 'bethel': 19375, '##–': 30051, 'verge': 16079, 'compelling': 17075, '1871': 7428, 'accessory': 25339, 'endorsed': 11763, 'bungalow': 27563, 'provinces': 6941, 'backs': 10457, '##pus': 12207, '男': 1912, 'titled': 4159, 'khalifa': 27925, 'putin': 22072, 'passive': 13135, '##rar': 19848, '##ility': 15148, 'establishes': 21009, '[unused66]': 67, 'plastics': 26166, '[unused956]': 961, 'clair': 17936, 'tastes': 16958, '##wi': 9148, 'cheshire': 13789, 'flushing': 23519, '##ington': 7853, 'substrates': 23725, 'instructors': 19922, 'monarchs': 19799, 'unchanged': 15704, 'adventurous': 29118, '##史': 30316, 'federico': 20493, 'objective': 7863, '##vers': 14028, 'ˈ': 1149, 'biotechnology': 20353, '[unused209]': 214, 'wages': 12678, 'tuck': 18029, 'flag': 5210, 'vampire': 4393, '##lke': 28143, 'dorian': 16092, 'व': 1335, 'napoli': 29485, 'haydn': 25595, '##朝': 30400, '1839': 10011, 'branded': 11180, '421': 29403, 'dove': 10855, '##ssar': 25556, 'mention': 5254, '##one': 5643, 'posts': 8466, '8': 1022, '##zbek': 28733, 'freddie': 15528, 'coordinates': 12093, 'winding': 12788, 'sob': 17540, 'value': 3643, 'mischievous': 25723, 'soda': 14904, 'aliens': 12114, '1788': 15622, '##ɴ': 29690, 'millennium': 10144, '##ining': 24002, 'appearing': 6037, 'inhabit': 21490, 'hires': 28208, 'threatened': 5561, 'symmetry': 14991, 'ideological': 17859, 'would': 2052, '##ito': 9956, 'solved': 13332, 'チ': 1710, 'pour': 10364, 'villiers': 25333, 'brian': 4422, 'gavin': 9448, '##⺼': 30159, 'transverse': 18323, 'panchayat': 20079, 'doubtful': 21888, '##示': 30449, '1889': 6478, 'bowls': 15220, 'lydia': 14076, 'think': 2228, '##writer': 15994, 'nominee': 9773, 'marxism': 27255, 'premature': 21371, '##flict': 29301, 'shimmering': 22349, '##ulsive': 23004, 'cuba': 7394, 'tackled': 26176, 'assyrian': 19950, 'jody': 27562, 'shake': 6073, 'been': 2042, 'flores': 17343, 'redesignated': 11836, 'depth': 5995, 'woodland': 11051, 'katie': 9734, 'isla': 25340, 'fable': 28458, 'petersburg': 8062, 'maharaja': 21609, '##isson': 24077, '°c': 6362, '1685': 25053, 'offshore': 12195, 'standard': 3115, 'mcgill': 17919, 'thornton': 14630, 'defected': 26330, 'singleton': 28159, '##fan': 15143, 'speaker': 5882, 'bu': 20934, 'thoughtful': 16465, 'inter': 6970, 'noise': 5005, 'rub': 14548, '##uli': 15859, 'irish': 3493, 'keys': 6309, 'undergoes': 29129, '##cos': 13186, '##enne': 24336, 'shelves': 15475, '##ioms': 29178, 'roughly': 5560, 'melville': 20154, '♣': 1624, 'theo': 14833, '##4': 2549, 'episodes': 4178, 'consumer': 7325, 'signature': 8085, '##uled': 18696, 'kunst': 28145, '##rle': 20927, 'tables': 7251, 'eyes': 2159, 'connolly': 21018, 'palais': 22113, 'watered': 27129, 'celebrating': 12964, 'remember': 3342, '##red': 5596, 'shuddering': 25735, '[unused169]': 174, 'festivities': 21206, 'multi': 4800, '##ska': 8337, '##π': 29731, 'frederic': 15296, 'module': 11336, 'naive': 15743, 'mir': 14719, 'paul': 2703, 'scottish': 4104, 'final': 2345, 'astoria': 29285, 'bare': 6436, '##dilly': 28832, 'comprehensive': 7721, '木': 1875, 'snatched': 14177, 'lafayette': 14425, 'insights': 20062, 'returning': 4192, 'positioned': 10959, 'lacks': 14087, '##tman': 22942, 'blond': 8855, '##po': 6873, 'austin': 5899, '##ako': 20411, '##rot': 21709, 'म': 1331, 'thoughts': 4301, 'supports': 6753, 'motorcycles': 18580, 'kan': 22827, 'franciscan': 19420, 'vijay': 17027, 'genesis': 11046, 'real': 2613, '##rock': 16901, '##ivated': 21967, 'margaret': 5545, 'fisher': 8731, 'mika': 27857, 'armand': 20371, 'represented': 3421, 'sports': 2998, '1667': 27643, '##事': 30277, 'goran': 28356, 'yue': 27163, 'ラ': 1731, 'clarify': 25037, 'geographic': 9183, '##ก': 29945, 'covenant': 16077, 'estonia': 10692, '##ே': 29934, 'motions': 15323, 'taft': 19911, 'marvin': 13748, '野': 1963, 'slightest': 15989, '##智': 30395, 'auf': 21200, 'vegas': 7136, 'piston': 16733, '233': 22115, '1722': 26689, 'prohibits': 25822, 'definition': 6210, 'sliding': 8058, 'geography': 10505, 'rebound': 27755, 'marcus': 6647, 'favors': 21191, 'candidates': 5347, 'tavi': 24283, 'pregnancy': 10032, 'scala': 26743, 'friction': 15012, '##ignant': 25593, 'aluminium': 14794, 'patented': 16719, 'ethernet': 26110, 'fascinated': 15677, 'illicit': 25049, '##wice': 23425, 'tabitha': 21581, 'realities': 22213, '##ら': 30211, '##シ': 30232, 'havre': 28890, 'concurrently': 15442, 'billions': 25501, 'offences': 18421, 'helmets': 22674, 'forbade': 27315, 'mourning': 16236, 'rein': 27788, 'communion': 15661, 'ordinary': 6623, 'padre': 28612, '##veda': 28614, '[unused510]': 515, '##eye': 17683, 'vanishing': 24866, 'oliver': 6291, 'predecessor': 8646, 'adolph': 28564, 'clauses': 24059, 'assaults': 22664, 'heller': 25038, '##cter': 21162, 'insulted': 23637, 'derives': 12153, 'prone': 13047, 'legged': 15817, 'sampson': 22041, 'wartime': 12498, 'had': 2018, 'charged': 5338, 'coloring': 22276, 'gaunt': 27534, '##anies': 28064, 'committee': 2837, 'aquino': 24183, '##mah': 25687, '##ae': 6679, 'foley': 17106, '##rove': 17597, 'believers': 20373, 'lobe': 21833, '##jong': 21958, '##yd': 25688, 'exposed': 6086, 'whitman': 21311, 'best': 2190, 'nodes': 14164, '■': 1617, 'creep': 19815, 'occurs': 5158, 'gigs': 20929, '##eki': 26576, '##jan': 8405, 'tampa': 9925, 'salon': 11090, '##tagram': 23091, 'laurence': 10883, 'toby': 11291, 'barracks': 10492, 'sucks': 19237, '55th': 29075, 'makeup': 5789, '##mie': 9856, 'unix': 19998, 'maternity': 23676, 'fort': 3481, 'ip': 12997, 'deleted': 17159, 'viktor': 13489, 'fumble': 19576, '##mission': 25481, '##plex': 19386, '##arians': 28369, 'sampling': 16227, 'theater': 4258, 'percentage': 7017, 'starting': 3225, 'offices': 4822, '1806': 12518, 'commissioners': 12396, 'sophomore': 13758, '##₅': 30081, 'periodical': 21802, '[unused42]': 43, '##vah': 21927, 'generators': 16937, 'ties': 7208, 'merger': 7660, 'carolyn': 15611, '72': 5824, 'un': 4895, '34th': 20460, 'berkeley': 8256, '##ags': 26454, 'nathan': 7150, 'perfectly': 6669, 'jerked': 8245, '##ency': 11916, '##oning': 13369, '##hly': 27732, 'ต': 1411, 'irina': 25404, 'container': 11661, 'republics': 24822, 'saxon': 10038, 'cathedral': 5040, '##matic': 12644, 'velocity': 10146, 'gritted': 17307, 'automated': 12978, 'steer': 20634, '##flow': 12314, 'statue': 6231, 'vital': 8995, 'hire': 10887, '[unused612]': 617, 'petit': 17268, 'keane': 27228, 'romney': 19615, '1781': 16788, 'confront': 14323, 'bullock': 25200, 'consistency': 18700, '[unused35]': 36, '##si': 5332, 'scouting': 14299, '2nd': 3416, 'countess': 11716, 'possibilities': 12020, 'seniors': 15995, 'apparel': 26278, 'franchises': 22506, 'н': 1192, '150': 5018, 'struck': 4930, '[unused291]': 296, 'transformers': 19081, '##ה': 29128, 'commemorate': 13415, 'flour': 13724, 'gunfire': 16978, 'nigerian': 11884, '[unused295]': 300, 'comte': 19758, 'ie': 29464, '##mple': 23344, 'directors': 5501, 'whipped': 12428, '##der': 4063, '⁺': 1544, 'life': 2166, 'managed': 3266, '##ego': 20265, '##gf': 25708, '##文': 30387, 'specialised': 17009, 'scotia': 9676, 'stripe': 18247, '##naut': 24619, 'ceasefire': 26277, 'hobart': 14005, 'waivers': 28654, 'crossroads': 16760, '##₈': 30084, 'win': 2663, 'closer': 3553, 'performance': 2836, '114': 12457, '1817': 12529, 'compiled': 9227, 'sabine': 18578, '##ف': 29833, '##vine': 20534, '##ric': 7277, 'medieval': 5781, 'recipient': 7799, 'dubai': 11558, '##encia': 27742, 'vuelta': 21441, 'uk': 2866, 'grasp': 10616, 'ensembles': 21528, 'railroads': 16197, 'lucie': 28831, 'modeled': 14440, 'essen': 29032, 'explained': 4541, 'declan': 16494, '[unused766]': 771, 'huntsville': 28492, '[unused512]': 517, '##och': 11663, 'iberian': 21988, 'neal': 11030, 'avenge': 24896, 'deter': 28283, '##zie': 14272, 'brows': 11347, 'pageant': 12438, 'reminder': 14764, 'branching': 23346, 'copeland': 27303, 'cha': 15775, '##tv': 9189, '##wide': 22517, '##gre': 17603, '##pid': 23267, 'include': 2421, '[unused623]': 628, '1737': 26738, 'gasped': 8535, '##dley': 14232, 'famous': 3297, 'gravitational': 19790, 'helmut': 24515, 'narration': 21283, '⁷': 1541, 'freedom': 4071, 'halt': 9190, 'syria': 7795, 'papacy': 25097, 'winfield': 24739, '##gu': 12193, '##nah': 15272, 'hepburn': 22004, 'sanctioned': 14755, '##till': 28345, 'exploited': 18516, 'locality': 10246, 'dunes': 17746, 'mysore': 20761, 'poking': 21603, 'fitzroy': 21870, 'trembled': 14823, 'policing': 21107, 'lyle': 21971, 'hamish': 28859, 'shankar': 17429, 'pauline': 15595, 'unauthorized': 24641, '##eley': 22352, 'lynx': 22636, 'す': 1658, 'md': 9108, '##vitt': 28656, 'louvre': 25110, 'gloucester': 13370, 'treasures': 17605, 'simulcast': 20525, 'allotted': 23932, 'motorsports': 20711, 'commonly': 4141, 'resemble': 13014, 'poly': 26572, '##using': 18161, '阿': 1971, 'ngo': 17895, 'shuffled': 18764, '##ruff': 26919, 'jian': 29214, 'contracted': 11016, 'recent': 3522, 'pittsburgh': 6278, 'lebanon': 8341, 'downstairs': 10025, 'serving': 3529, 'concordia': 24982, 'gall': 26033, 'sculptures': 10801, '·': 1087, 'brightened': 28996, 'mythological': 21637, '[unused412]': 417, 'fiber': 11917, 'genius': 11067, 'guilt': 8056, 'lecture': 8835, '##iated': 15070, 'weaken': 23021, 'undertaker': 27568, 'ω': 1179, 'trust': 3404, 'neighbouring': 9632, '##xley': 20959, 'skin': 3096, 'erased': 23516, 'autonomy': 12645, 'nasal': 19077, 'susannah': 20471, '##ђ': 29758, '357': 26231, 'trim': 12241, 'drills': 28308, 'parma': 19177, '##vot': 22994, 'voss': 24878, 'wilhelm': 9070, 'recreational': 10517, 'newscasts': 24482, '##ᄆ': 29995, '##tok': 18715, 'yucatan': 28631, 'chests': 28717, '##illa': 9386, 'comparing': 13599, '4a': 26424, 'disciplined': 28675, 'flu': 19857, 'vengeance': 14096, 'gone': 2908, '##had': 16102, '##cate': 16280, 'remote': 6556, 'marco': 8879, 'detection': 10788, 'peculiar': 14099, 'future': 2925, '##xin': 20303, '##roll': 28402, '271': 25103, '[unused755]': 760, '230': 11816, '##hing': 12053, 'order': 2344, '36': 4029, 'sash': 24511, '##oses': 27465, 'savanna': 22323, 'メ': 1726, 'כ': 1252, 'filmmaking': 24466, 'venus': 11691, 'surveyor': 17169, 'faa': 17032, 'praising': 15838, '##ited': 17572, 'amanda': 8282, 'psalm': 22728, 'vogel': 27063, 'liszt': 26273, 'oklahoma': 5858, 'ri': 15544, 'categorized': 20427, 'davis': 4482, 'willow': 11940, '[unused334]': 339, 'started': 2318, 'alaska': 7397, 'licked': 11181, 'mckenna': 21749, 'shrieked': 22383, '##rdo': 20683, 'jacobite': 28725, 'seville': 18983, 'peter': 2848, 'rocks': 5749, 'annual': 3296, 'preached': 21491, '[unused536]': 541, 'ferreira': 26135, '##nally': 26827, 'drone': 18465, '##amen': 27245, 'cultivation': 13142, 'heterosexual': 28229, 'whore': 17219, 'asha': 24595, 'revised': 8001, 'stepping': 9085, 'ramon': 12716, 'lab': 6845, 'louder': 10989, 'sutton': 11175, '##mill': 19912, 'leiden': 20329, 'hardcover': 20990, 'kennedy': 5817, '##hid': 27511, 'equivalence': 27841, '328': 25256, 'mushroom': 18565, '##eiro': 17166, 'said': 2056, 'macquarie': 23903, 'carla': 17081, 'hanson': 17179, 'guam': 16162, 'chassis': 11832, '##year': 29100, 'infused': 29592, '止': 1887, 'luke': 5355, 'γ': 1157, 'ী': 1380, 'helm': 16254, 'ben': 3841, 'differential': 11658, '##aver': 22208, '##odes': 19847, 'streak': 9039, 'privy': 14452, '##az': 10936, '[unused580]': 585, 'atom': 13787, 'transformations': 21865, 'loose': 6065, 'metre': 7924, 'lineup': 10515, 'tie': 5495, 'humanity': 8438, 'deportation': 23702, 'fray': 25975, 'beaming': 27910, 'debt': 7016, 'passengers': 5467, 'faux': 29276, '##hein': 26496, 'lore': 19544, 'overview': 19184, '##॥': 29881, 'horror': 5469, 'reintroduced': 28263, 'completed': 2949, 'hurts': 13403, '##bari': 25990, 'guerrillas': 26955, 'bce': 10705, 'tis': 22320, '間': 1969, 'saw': 2387, 'cows': 17188, 'papers': 4981, 'given': 2445, 'dennis': 6877, 'brain': 4167, 'consequently': 8821, 'differ': 11234, 'sufficiently': 12949, 'finances': 16156, 'supplements': 25654, 'domingo': 15586, '37th': 23027, 'variation': 8386, '##iidae': 15648, 'electrified': 18042, 'stressed': 13233, '##xed': 19068, 'helplessly': 24942, 'squinting': 29156, 'punk': 7196, '##tia': 10711, 'propulsion': 16404, 'sumner': 23922, 'mustered': 21900, 'businesses': 5661, 'brisk': 28022, 'magnolia': 24659, 'dressed': 5102, 'cavendish': 23570, '∨': 1603, 'benji': 27231, 'whaling': 23687, '##tic': 4588, 'murmured': 7152, 'schemes': 11683, 'conductors': 23396, 'ottawa': 8166, 'vicious': 13925, 'rockefeller': 16696, 'nonlinear': 27400, 'orchid': 15573, 'steals': 15539, '1600': 14883, '1951': 4131, 'blew': 8682, '##gging': 12588, 'definitive': 15764, '##~': 29643, '##tions': 9285, 'persia': 16667, '##sbury': 18065, '##ers': 2545, '##ی': 24830, '##ced': 11788, '##cco': 21408, 'doin': 24341, '.': 1012, '##x': 2595, 'tide': 10401, 'ᄃ': 1457, 'privatization': 23966, 'cook': 5660, '##aman': 23093, 'barrington': 26469, 'brink': 20911, 'which': 2029, 'lester': 14131, 'annoyed': 11654, '##rlin': 19403, 'martina': 23508, 'states': 2163, '##int': 18447, '##body': 23684, 'mapped': 17715, 'echoing': 17142, 'accordingly': 11914, 'vessel': 6258, 'industry': 3068, 'quarry': 12907, '19th': 3708, 'lifeless': 22185, 'naming': 10324, 'cumberland': 12310, 'dungeon': 16633, '##hide': 26100, 'inhabited': 9613, 'henson': 27227, '##ⁱ': 30072, 'catalan': 13973, 'davenport': 16273, '##¤': 29647, 'gordon': 5146, 'δ': 1158, 'rent': 9278, 'presbyterian': 10133, 'prescott': 20719, 'spreads': 20861, 'mesopotamia': 25889, 'elisa': 29490, 'religion': 4676, 'cooper': 6201, '##uses': 25581, 'radar': 7217, 'angular': 16108, 'propose': 16599, '₹': 1576, 'bankrupt': 17482, 'cw': 19296, 'intentional': 21249, 'thugs': 24106, '##umble': 26607, 'kumar': 9600, '802': 23908, 'sexuality': 13798, 'yellowish': 17804, 'combustion': 16513, 'nu': 16371, 'olympics': 3783, 'heights': 7535, 'superman': 10646, 'nap': 18996, 'armada': 25306, '##train': 23654, 'relinquished': 26566, 'ono': 21058, 'voodoo': 21768, '##bee': 11306, 'soo': 17111, '##pha': 21890, 'doorbell': 25422, '##abe': 16336, 'werner': 14121, '[unused363]': 368, 'sliced': 15920, 'survived': 5175, 'union': 2586, 'stadiums': 28244, 'someplace': 24956, 'styx': 21856, 'invalid': 19528, 'any': 2151, 'liga': 8018, '[unused786]': 791, 'libya': 12917, '[unused31]': 32, 'brotherhood': 12865, 'template': 23561, 'playoff': 7808, 'crisp': 15594, 'morales': 17103, 'torres': 13101, 'basically': 10468, 'suck': 11891, 'bicycle': 10165, 'empirical': 17537, '[unused236]': 241, '##xing': 19612, '530': 23523, 'operator': 6872, 'sparkle': 26831, 'thread': 11689, 'kate': 5736, 'miners': 11257, 'scratch': 11969, 'fall': 2991, 'timeline': 17060, 'caretaker': 17600, 'uefa': 6663, 'fully': 3929, '##oja': 22918, 'ј': 1214, '##ryn': 18143, 'grocery': 13025, 'compassionate': 29353, 'rockets': 12496, 'oversee': 17467, 'bowl': 4605, '##cana': 28621, '[unused472]': 477, 'gleaming': 18242, '##qa': 19062, 'starvation': 22611, 'ryu': 19367, '##oia': 25463, 'astronauts': 25881, 'tongue': 4416, 'jesuit': 13840, 'ր': 1237, '##rel': 16570, '##子': 30342, '##standing': 24911, '##linger': 23101, 'apostles': 19178, '##fle': 21031, 'popularly': 16071, 'christmas': 4234, '##estra': 26199, 'fcc': 14420, '##ette': 7585, 'benches': 19571, '##ifolia': 29244, '[unused929]': 934, '##foot': 13064, 'pl': 20228, '##場': 30332, 'agony': 12812, '##zone': 15975, 'neighbourhoods': 27535, 'panties': 12095, 'touchdowns': 10183, 'hell': 3109, 'incorporate': 13265, 'oakley': 28876, 'canary': 17154, 'disclosed': 21362, 'nikola': 24794, 'hold': 2907, 'yao': 23711, 'contests': 15795, 'unanimously': 15645, 'strikeouts': 20813, 'fox': 4419, 'conceal': 19819, 'sectors': 11105, 'profits': 11372, 'garry': 21507, 'stemmed': 27674, 'declares': 18806, 'plunge': 25912, 'ending': 4566, 'reluctant': 11542, 'siberia': 16881, '京': 1755, 'tiberius': 28411, 'joan': 7437, 'fair': 4189, 'cardiff': 10149, 'reversal': 23163, 'conducted': 4146, '##ups': 22264, 'guggenheim': 20402, 'bose': 21299, 'consequences': 8465, 'charlemagne': 27257, 'renovation': 10525, 'tends': 12102, 'barbecue': 26375, '58': 5388, 'overwhelming': 10827, '[unused206]': 211, 'slovenian': 16583, '178': 19289, 'orphanage': 18504, 'steiner': 21264, 'quincy': 16141, 'physiology': 16127, 'panther': 15133, 'shapiro': 24630, 'xp': 26726, 'gilbert': 7664, 'creators': 17277, 'giggle': 17565, 'alike': 11455, 'gesture': 9218, 'attempted': 4692, '##pe': 5051, 'jen': 15419, 'attended': 3230, 'public': 2270, 'out': 2041, '##craft': 10419, 'projection': 13996, 'suppliers': 20141, 'terre': 25170, '##vocation': 19152, 'recover': 8980, 'broadcasts': 8960, 'orderly': 23589, 'irritated': 15560, 'sect': 17831, '##ess': 7971, 'factual': 25854, 'rings': 7635, 'massage': 21881, 'grazed': 26627, '##⇌': 30118, 'electorate': 13694, 'glazed': 19724, 'bassett': 28303, 'carefully': 5362, 'pleaded': 12254, 'inaccurate': 24949, 'elizabeth': 3870, '##jos': 19929, 'sculptor': 10160, 'mckinley': 22121, 'healthy': 7965, 'texas': 3146, 'angus': 13682, '##berger': 14859, 'appalachian': 19682, '##gy': 6292, 'william': 2520, 'prints': 11204, '‚': 1522, 'niall': 21889, 'miguel': 8374, '[unused73]': 74, 'established': 2511, 'fulton': 17049, 'redemption': 18434, '##chison': 27795, 'hollow': 8892, 'behaviour': 9164, 'earnest': 17300, 'insurrection': 27860, 'immortals': 26796, 'grafton': 28680, 'relocation': 18344, 'lilly': 14765, '##wright': 26460, 'feelings': 5346, 'ᅧ': 1474, '##del': 9247, '##tric': 12412, 'enthusiasm': 12024, 'gradual': 16612, 'despised': 26626, '##uo': 19098, 'mps': 12616, 'yard': 4220, 'banana': 15212, 'clustered': 25221, 'tufts': 25252, 'confluence': 13693, 'tailor': 22701, 'records': 2636, 'solemn': 19487, 'pussy': 22418, 'anarchist': 18448, 'minors': 18464, 'stump': 22475, '##nburg': 22642, 'essential': 6827, 'splash': 17624, 'dress': 4377, 'albania': 10407, '[unused74]': 75, '1842': 10008, '##hone': 27406, 'taxonomic': 27691, 'rational': 11581, 'indication': 12407, 'antarctic': 10227, 'displacement': 13508, 'platt': 28005, '##不': 30270, 'द': 1325, '##stor': 23809, 'degradation': 16627, 'criminals': 12290, 'pilots': 8221, '##ent': 4765, 'destiny': 10461, 'synth': 24203, '317': 26628, 'soon': 2574, 'blurted': 18751, 'cotton': 6557, 'traversed': 27797, 'upright': 10051, '##baum': 14898, '##duk': 28351, '##tam': 15464, '##國': 30326, 'resumed': 7943, 'alert': 9499, 'sardinia': 21594, 'lara': 13679, 'missing': 4394, 'attribute': 17961, 'tune': 8694, 'ec': 14925, '##beat': 19442, 'mined': 21846, 'greed': 22040, 'coven': 25248, 'criticizing': 21289, 'virgin': 6261, 'charlton': 17821, '[unused474]': 479, 'desmond': 16192, 'docked': 25727, 'foot': 3329, '318': 27003, '1967': 3476, 'hungry': 7501, 'lil': 13451, '##rda': 13639, 'unusually': 12890, '##ᅥ': 30008, 'minas': 21750, 'bracing': 25919, 'moines': 23286, 'oaxaca': 26923, 'troopers': 26246, 'events': 2824, 'exhibit': 8327, 'viii': 9937, 'birmingham': 6484, 'bremen': 16314, 'nesting': 21016, '[unused504]': 509, '##inator': 23207, '##⁶': 30075, 'playstation': 9160, 'philharmonic': 12355, '##ப': 29924, 'nightmares': 15446, 'sabah': 22515, '[unused693]': 698, 'appeal': 5574, 'embarrassing': 16436, 'kilometre': 13214, 'drafting': 21168, 'gratitude': 15531, 'dangerously': 20754, '[unused93]': 94, 'larkin': 25570, 'challenges': 7860, 'luxury': 9542, 'nato': 10079, \"##'\": 29618, 'defendant': 13474, '##crats': 23423, 'bloody': 6703, 'specification': 12827, '1875': 7466, 'ground': 2598, 'boo': 22017, '90s': 17233, 'tiled': 26510, '##very': 27900, 'purchase': 5309, 'monday': 6928, 'rudy': 18254, '##dler': 21222, 'serious': 3809, 'movements': 5750, '##bing': 10472, 'auditorium': 11448, 'plus': 4606, 'amc': 21962, 'proficient': 27029, 'bartender': 15812, 'clyde': 12085, 'withstand': 19319, '##arm': 27292, 'holloway': 20977, 'lynch': 11404, 'stability': 9211, '##の': 30197, 'camera': 4950, 'asahi': 29465, 'gather': 8587, 'count': 4175, 'becoming': 3352, 'suriname': 25050, 'mud': 8494, 'elbow': 8999, 'existent': 25953, 'unit': 3131, 'հ': 1228, 'yearly': 12142, 'botanic': 27761, 'fin': 10346, 'runways': 24578, 'grouping': 19765, 'radha': 26498, '##к': 23925, 'cork': 8513, '⁄': 1535, '1970': 3359, '##use': 8557, '##p': 2361, 'martins': 19953, 'debbie': 16391, 'structurally': 29060, '##sa': 3736, 'misconduct': 23337, 'raven': 10000, 'adjustment': 19037, 'forests': 6138, 'bayer': 26367, 'pins': 16300, '##ˣ': 29718, 'patent': 7353, 'se': 7367, 'betsy': 22396, '##ncia': 22750, 'jace': 10352, 'tooth': 11868, 'backward': 8848, 'satire': 18312, 'little': 2210, 'native': 3128, '宀': 1818, 'ר': 1265, 'emmy': 10096, '##省': 30446, 'denotes': 14796, 'ego': 13059, '##ais': 15593, '##ල': 29942, '[unused660]': 665, 'utilization': 27891, 'irresistible': 27149, 'geographically': 20969, '##nn': 10695, '##د': 15394, 'attache': 29489, '##ux': 5602, 'windy': 27370, 'portugal': 5978, 'remnants': 11270, 'core': 4563, 'eagles': 8125, '1663': 29016, '21st': 7398, 'gotten': 5407, '##his': 24158, '##zzy': 28753, 'portrait': 6533, 'interviewing': 27805, 'by': 2011, '##zed': 5422, '##е': 15290, '404': 24837, 'exercise': 6912, 'འ': 1433, 'dipping': 23427, 'headwaters': 25345, 'ন': 1366, 'eli': 12005, 'david': 2585, '##urse': 28393, 'uttered': 20947, 'fence': 8638, 'oral': 8700, '##enberg': 11029, 'lauderdale': 23520, 'blazers': 28513, 'successfully': 5147, '№': 1578, 'courtesy': 14571, '##ٹ': 29838, '##მ': 29984, 'finds': 4858, 'prospective': 17464, '241': 22343, 'illusion': 12492, 'naples': 10553, 'decorate': 29460, '##frame': 15643, 'crowns': 24364, '##st': 3367, '##gree': 28637, 'aisle': 12485, 'nine': 3157, 'aj': 19128, '##lance': 23078, '##scia': 27210, 'spit': 13183, 'rainforest': 18951, '##ʔ': 29705, 'bus': 3902, '##itan': 25451, 'ebook': 26885, 'robson': 23698, 'pawn': 19175, '##』': 30170, 'minimal': 10124, '##oud': 19224, 'tea': 5572, '##，': 30515, 'presented': 3591, 'tease': 18381, 'baldwin': 10970, 'thigh': 10120, 'n': 1050, '##notes': 20564, '##sions': 27466, 'ruining': 27853, '##勝': 30306, 'lorraine': 13895, '##ize': 4697, 'cooked': 12984, 'martinez': 10337, 'payne': 13470, '##imeter': 19198, 'visits': 7879, 'chargers': 18649, 'hospitals': 8323, '[unused550]': 555, '##set': 13462, 'trailers': 21389, '##ppy': 27659, 'boats': 6242, 'hurry': 9241, 'moran': 17866, 'molecular': 8382, 'tau': 19982, 'lasers': 23965, 'drastic': 23956, 'analogue': 21800, 'custom': 7661, 'senate': 4001, 'ð': 1098, 'convicted': 7979, 'mayer': 14687, 'blind': 6397, 'intersects': 17349, 'unidentified': 20293, 'jordanian': 26276, 'printing': 8021, 'vinci': 23765, 'schwarz': 29058, '[unused502]': 507, 'hampson': 21780, 'spot': 3962, '##力': 30304, 'mickey': 11021, '79': 6535, 'shone': 14707, '##cycle': 23490, 'bottles': 11015, 'introduce': 8970, 'h': 1044, 'male': 3287, 'demons': 7942, '[unused243]': 248, 'daytona': 18226, 'mistakenly': 20706, '##ism': 2964, '[unused720]': 725, 'advanced': 3935, '##resh': 21898, '##vo': 6767, 'cites': 17248, 'christophe': 23978, '##発': 30440, 'enthusiasts': 20305, '##rwood': 20941, 'relatives': 9064, 'voice': 2376, 'executives': 12706, 'commons': 7674, 'probably': 2763, 'rq': 28134, '##hered': 27190, 'apples': 18108, 'mons': 29408, 'fossil': 10725, 'store': 3573, 'confusion': 6724, 'vessels': 6470, 'membrane': 10804, 'tariffs': 26269, 'polytechnic': 14466, 'minnesota': 5135, 'stud': 16054, 'statesman': 17689, 'heron': 22914, '##コ': 30230, '##oration': 21223, 'manages': 9020, 'phonetic': 26664, 'sorority': 27600, 'demo': 9703, 'franklin': 5951, 'lipstick': 22359, 'spoil': 27594, 'starters': 29400, '##ɯ': 29688, 'swirling': 16499, '##75': 23352, '##宣': 30349, 'morrow': 19084, '[unused172]': 177, 'victim': 6778, 'redding': 26687, 'addresses': 11596, 'wavelength': 19934, '##omic': 22026, '##osition': 19234, 'superiors': 22983, 'unnamed': 13294, 'ada': 15262, 'crack': 8579, 'tucking': 25056, 'negotiated': 13630, '##nir': 29339, 'hoping': 5327, 'northeast': 4794, '57': 5401, 'mused': 22335, 'yellowstone': 29231, 'jet': 6892, 'rescues': 26001, 'ᵢ': 1507, '‒': 1515, 'homosexuality': 15949, '##minate': 19269, '[unused280]': 285, 'refuge': 9277, 'duchy': 11068, 'woodward': 19133, 'scenic': 12916, 'collapses': 25938, '[unused911]': 916, 'gomez': 12791, 'responsibilities': 10198, '[unused177]': 182, 'angled': 18756, '##ango': 23422, 'buy': 4965, 'kilkenny': 17131, 'secretive': 28607, '##olate': 19425, 'gables': 27008, '##pala': 19636, 'extract': 14817, '##anta': 26802, 'woolf': 29572, 'fremont': 22550, 'violent': 6355, 'resisted': 13335, 'friendships': 28956, 'weight': 3635, 'courtney': 14139, '##tell': 23567, 'administratively': 23710, '##fish': 7529, 'grandchildren': 13628, 'bates': 11205, 'firms': 9786, 'regulatory': 10738, 'xii': 14371, 'admire': 19837, '##hen': 10222, '##מ': 29801, '##phine': 20738, '##ading': 23782, 'spare': 8622, 'frontier': 8880, 'reference': 4431, 'lambda': 23375, 'complexion': 28838, 'detective': 6317, 'investigations': 9751, 'octagonal': 22340, 'michelle': 9393, 'eucalyptus': 27111, 'harp': 14601, '##8th': 28264, 'assert': 20865, 'there': 2045, '[unused374]': 379, '[unused290]': 295, 'texts': 6981, '##scan': 29378, '137': 14989, 'fractured': 21726, 'ps': 8827, 'profoundly': 28089, 'cherry': 9115, '[unused150]': 155, 'summons': 24814, 'scroll': 17186, 'sherman': 11011, 'goodwill': 22875, 'aesthetics': 20749, 'pierced': 16276, 'marquis': 13410, 'collided': 17745, 'yugoslavia': 8936, 'dialogues': 22580, 'severity': 18976, 'bears': 6468, '##れ': 30214, 'weird': 6881, 'syllables': 20732, 'ₖ': 1565, 'laugh': 4756, 'lions': 7212, 'grabs': 13273, '##nted': 14706, 'tributaries': 15777, '[unused21]': 22, 'composed': 3605, '##tique': 28437, 'vulture': 27588, 'beacon': 14400, '##mail': 21397, 'nara': 27544, 'claudia': 13479, '##istic': 6553, 'leone': 13363, 'raids': 11217, 'contenders': 27236, 'myself': 2870, 'incoming': 14932, 'defined': 4225, 'redeveloped': 28053, 'frankly': 19597, 'parameters': 11709, 'coordinate': 13530, 'players': 2867, 'ness': 23384, 'guildford': 26353, 'stage': 2754, 'burke': 9894, 'wandering': 13071, '##bai': 26068, 'locomotive': 8098, 'reel': 15934, 'freed': 10650, '##cats': 19588, '##uca': 18100, 'niche': 18111, 'bridget': 19218, '1920': 4444, 'afford': 8984, '##xes': 20156, 'wilton': 26407, 'wai': 23701, 'island': 2479, '##ton': 2669, 'ك': 1293, 'consecutive': 5486, 'nam': 15125, 'intercept': 19115, '9th': 6280, 'multiplied': 28608, '##lake': 14530, 'anatomical': 28141, 'skinner': 17451, 'naga': 26539, 'interest': 3037, 'facility': 4322, '##ミ': 30250, 'cores': 25562, 'vc': 18315, 'middletown': 28747, '##林': 30407, 'symbols': 9255, 'western': 2530, 'astronomer': 15211, '##rde': 25547, 'shiver': 13277, 'local': 2334, 'winnipeg': 11093, '1654': 27445, 'mission': 3260, 'norms': 17606, 'accounts': 6115, '75': 4293, 'rocked': 14215, 'wasting': 18313, 'unconstitutional': 20454, 'tito': 20712, 'signalling': 21919, 'clancy': 24530, 'bladed': 27402, 'fairs': 21947, 'markus': 23030, 'ᵇ': 1497, 'attitude': 7729, 'さ': 1656, '[unused699]': 704, 'bulgarian': 7643, 'classroom': 9823, 'steel': 3886, 'jessica': 8201, 'browning': 18778, 'nicky': 20158, '[unused735]': 740, 'uranium': 14247, 'theodor': 21448, 'lobes': 24423, 'winslow': 26600, '##hian': 14204, 'rouse': 27384, '##”': 30059, '300': 3998, 'inviting': 15085, 'trillion': 23458, 'frowned': 7335, 'biomedical': 20906, 'skipper': 23249, '##girl': 15239, '1995': 2786, 'belgrade': 10291, 'gangs': 18542, '##mpt': 27718, 'lie': 4682, 'defining': 12854, 'astonishment': 23819, 'receiving': 4909, 'ensign': 19890, 'line': 2240, 'settlement': 4093, 'lark': 23404, '##崎': 30360, '##omorphic': 28503, 'heartbeat': 12251, '##with': 24415, 'strikers': 26049, '##ball': 7384, 'onion': 20949, 'enriched': 25202, 'talks': 7566, 'laptop': 12191, 'nico': 19332, 'stephan': 15963, 'cochrane': 22329, 'intermediate': 7783, 'posture': 16819, 'joins': 9794, '[unused764]': 769, 'thee': 14992, 'proposed': 3818, 'marne': 25823, '##nee': 24045, '##gement': 20511, 'filmmaker': 12127, 'naacp': 26155, 'fled': 6783, 'ধ': 1365, 'burkina': 23089, 'dal': 17488, 'brutally': 23197, 'brock': 13899, '[unused436]': 441, 'warehouse': 9746, 'argent': 23157, '##powering': 23948, '##rella': 21835, 'vigor': 24474, '##rook': 25399, '##omics': 25524, '##ety': 27405, 'solutions': 7300, 'principle': 6958, '##icides': 22698, 'unsure': 12422, 'tata': 23236, '51': 4868, 'tulsa': 16631, 'rankings': 10385, '##ま': 30203, 'ours': 14635, 'cells': 4442, 'logic': 7961, 'expulsion': 18272, 'georgia': 4108, 'spikes': 19547, '##ssee': 29522, 'cinnamon': 21229, '##claim': 25154, '##lic': 10415, '##vale': 17479, 'apps': 18726, '##غ': 29831, 'nearest': 7205, 'studies': 2913, '##ons': 5644, 'modification': 14080, 'balkan': 17581, 'vista': 13005, 'compulsion': 27638, 'seeming': 16064, 'localities': 19664, 'fearful': 19725, 'tumble': 28388, '1987': 3055, 'recounted': 22906, '147': 16471, 'anita': 12918, '##eber': 22669, 'badly': 6649, '##ahu': 21463, '[unused727]': 732, 'strength': 3997, 'π': 1170, '漢': 1904, 'approve': 14300, '##cles': 18954, '[unused366]': 371, 'constitutes': 17367, '##rti': 28228, 'asian': 4004, 'everybody': 7955, 'lds': 17627, 'felix': 8383, 'weighted': 18215, 'encryption': 21999, '##gue': 9077, 'rays': 9938, '##fting': 26169, 'abbess': 28010, 'mound': 12048, '##bard': 21458, 'suffrage': 15178, 'ricardo': 13559, 'floating': 8274, 'consensus': 10465, 'worcester': 12539, '##vable': 12423, 'abused': 16999, '##hony': 27629, 'filter': 11307, 'illustrate': 19141, '##dio': 20617, 'orchestra': 4032, 'automatic': 6882, '##khov': 25495, '[unused884]': 889, '##ths': 26830, 'patting': 26085, 'junta': 22450, 'compliant': 24577, '貴': 1953, 'aviation': 5734, 'bless': 19994, 'painfully': 16267, 'indirectly': 17351, 'khorasan': 23225, 'cree': 27831, 'explosion': 7738, '##py': 7685, 'report': 3189, 'organizational': 13296, 'undrafted': 21347, 'hooves': 28399, '##erus': 21608, 'feel': 2514, 'name': 2171, 'product': 4031, 'denounced': 17787, 'netflix': 20907, 'angry': 4854, 'epstein': 26646, 'taxation': 14952, 'announce': 14970, 'programmes': 8497, 'tip': 5955, 'revolves': 19223, 'experimented': 20918, 'gs': 28177, 'casket': 25864, 'snyder': 17840, 'attention': 3086, '##rily': 11272, 'badminton': 14618, 'shifts': 12363, 'snack': 19782, 'flowed': 13230, 'sabre': 22002, '##itz': 8838, '[unused496]': 501, 'mmm': 25391, 'overhead': 8964, 'echo': 9052, 'ribbons': 22688, 'bain': 28477, 'sensible': 21082, '##∆': 30122, '##良': 30464, 'biological': 6897, 'centennial': 15483, '##aging': 16594, 'fabrication': 25884, 'voluntary': 10758, 'majesty': 9995, '##atic': 12070, 'partnerships': 13797, 'towels': 24213, 'domino': 23968, 'proposals': 10340, 'sanjay': 29590, 'freeway': 10846, 'ʃ': 1130, 'posting': 14739, 'highlighting': 20655, 'utopia': 26425, 'successors': 18530, '##inho': 29344, '##hman': 13890, 'judgment': 8689, '##iche': 17322, 'reassuring': 21093, 'recaptured': 23838, 'escaped': 6376, '1605': 28202, 'allah': 16455, 'stellar': 17227, 'ᵣ': 1508, 'clermont': 27956, 'sole': 7082, '##ł': 18818, 'function': 3853, 'deposit': 12816, 'serum': 20194, 'haitian': 21404, 'vanity': 18736, 'lost': 2439, 'smelled': 9557, 'heart': 2540, '##ika': 7556, '##ges': 8449, '[unused644]': 649, 'nord': 13926, 'translate': 17637, 'unto': 19662, 'mcdowell': 25005, '∩': 1604, 'everyone': 3071, '##35': 19481, 'dismissed': 7219, 'milford': 22622, 'troubles': 13460, 'differs': 12980, 'cervical': 28711, 'hostage': 13446, '##愛': 30379, 'jaenelle': 20757, 'restoring': 16487, 'ﬂ': 1985, 'disciple': 17849, 'zeta': 23870, '1791': 14362, 'poke': 26202, 'anonymous': 10812, '##51': 22203, 'labs': 13625, 'decide': 5630, 'showered': 23973, 'harmon': 25546, 'growing': 3652, 'inhabitants': 4864, 'elevator': 7764, 'hurrying': 27951, 'crater': 11351, '1931': 4739, '##nta': 12380, '⁻': 1545, '[unused633]': 638, '##aine': 18175, 'posed': 13686, 'mountains': 4020, 'animator': 25132, 'middle': 2690, '##owe': 29385, 'shape': 4338, 'alfredo': 19423, 'nigel': 12829, 'snapping': 15790, '##ध': 29862, '##gist': 24063, '四': 1798, 'cristina': 22246, 'merge': 13590, 'overboard': 27089, 'spinning': 9419, 'mats': 22281, 'convertible': 22840, '##ـ': 29832, 'mata': 22640, 'automobile': 9935, 'acclaimed': 10251, 'freedoms': 22467, '##ui': 10179, 'rounds': 6241, 'reset': 25141, 'islamist': 27256, '##х': 29750, '##－': 30516, '主': 1747, 'lover': 7089, 'peru': 7304, 'rhythmic': 14797, 'aston': 14327, 'botany': 17018, 'brethren': 17937, 'renew': 20687, 'chromosomes': 26874, 'economics': 5543, 'ن': 1296, '##pas': 19707, 'brackets': 19719, '1708': 27337, 'travellers': 19284, 'salaries': 20566, 'sea': 2712, 'position': 2597, '##icated': 17872, 'equatorial': 21333, '##tri': 18886, 'wander': 17677, 'peanuts': 27613, 'decatur': 27783, 'ʎ': 1135, 'thank': 4067, '##ough': 10593, 'luggage': 17434, '##外': 30335, 'dickinson': 17590, '[unused868]': 873, 'broker': 20138, '##tangled': 27898, 'missions': 6416, 'glimpse': 12185, 'printer': 15041, 'vargas': 20556, '##wski': 10344, '1761': 21364, 'bosses': 23029, 'fifa': 5713, 'bros': 10243, '162': 17832, 'praying': 14488, 'me': 2033, 'complained': 10865, 'missouri': 5284, 'men': 2273, 'pained': 22295, 'faso': 22773, 'solar': 5943, 'oro': 20298, 'heidelberg': 16793, 'oven': 17428, 'incline': 27461, 'poked': 16826, 'winning': 3045, '[unused706]': 711, '##ӏ': 29765, '##へ': 30201, 'rich': 4138, 'sur': 7505, 'goat': 13555, 'ursula': 20449, 'contrasts': 23347, 'compute': 24134, '##ilation': 29545, 'spaces': 7258, '[unused108]': 113, 'somebody': 8307, 'fished': 26281, '##ivar': 28739, 'gillian': 22358, '##ァ': 30218, 'whereas': 6168, 'rests': 16626, 'ax': 22260, '##asco': 28187, '[unused645]': 650, 'tricky': 24026, 'accountants': 29114, '##ndo': 15482, 'ت': 1273, '##dar': 7662, 'fernandez': 12023, 'continents': 17846, 'theories': 8106, '##max': 17848, 'textual': 25304, '##escu': 19434, 'crafts': 14030, 'fjord': 26158, 'rubs': 28860, 'rev': 7065, 'jase': 18626, 'dial': 13764, '16': 2385, 'hunger': 9012, 'femme': 26893, 'cpc': 28569, 'moreover': 9308, 'climate': 4785, 'proof': 6947, 'balls': 7395, 'shakes': 10854, '##hai': 10932, 'seamus': 24993, 'upset': 6314, 'drank': 10749, 'internally': 16058, 'conceptual': 17158, '##tarian': 28897, '##castle': 23662, 'berries': 22681, 'brief': 4766, '##ssen': 14416, '2017': 2418, 'stream': 5460, 'en': 4372, 'beaten': 7854, '##kas': 13716, 'sara': 7354, 'guard': 3457, 'oceans': 17401, 'years': 2086, 'lecturer': 9162, 'uneasy': 15491, 'reno': 17738, 'punching': 19477, 'brand': 4435, '##hm': 14227, 'localized': 22574, 'walt': 10598, '##lio': 12798, '143': 16065, 'plead': 25803, 'speculated': 15520, 'brittle': 24650, 'mattress': 13342, '##∩': 30131, 'fielding': 15452, 'orbits': 20347, 'teenagers': 12908, '##firm': 27972, 'torpedoes': 18544, 'lausanne': 22256, 'desktop': 15363, 'boroughs': 21413, '##gl': 23296, '[unused926]': 931, 'leonid': 27316, 'ave': 13642, 'reservoir': 8071, 'stagecoach': 26025, 'indonesian': 9003, '##urity': 25137, 'mcqueen': 29265, '##olt': 27914, 'reserves': 8269, 'sultan': 7544, 'rampage': 29216, 'abbey': 6103, 'lund': 21860, 'helping': 5094, '##dorff': 26559, 'devotion': 13347, 'promoting': 7694, 'abbreviated': 12066, 'shortened': 12641, 'ninety': 13568, 'cautious': 17145, '##星': 30392, 'favourite': 8837, '##ayan': 25868, 'jeremiah': 17526, '1726': 28342, 'ornamental': 18200, '[unused34]': 35, 'generally': 3227, 'maple': 11035, 'constraints': 14679, '##spar': 27694, 'reporting': 7316, 'chord': 13924, 'mario': 7986, 'temples': 8436, 'freight': 8441, 'warrant': 10943, 'spokane': 21878, 'feeder': 21429, '1626': 28818, 'jasper': 14791, 'particular': 3327, 'rutland': 27780, 'relative': 5816, 'fabricated': 24212, 'clutches': 29497, 'tenure': 7470, '##anto': 21634, 'easing': 24070, 'societies': 8384, 'curled': 8188, 'beaches': 12212, 'donnie': 28486, 'monterey': 19860, 'proportion': 10817, 'scarcely': 20071, 'mecca': 21340, 'angeles': 3349, 'gibraltar': 12272, '##itia': 29050, 'logistical': 28961, 'martha': 9246, 'needle': 12201, '##llus': 20159, 'gymnast': 25055, '##ows': 15568, 'vivid': 14954, 'mead': 19855, 'airship': 27636, 'luna': 12909, 'leaflets': 27306, 'promises': 10659, 'ousted': 28368, 'element': 5783, 'ancestors': 10748, '1977': 3355, 'unfamiliar': 16261, 'roaring': 17197, 'pond': 8644, 'soothe': 28384, 'mathew': 25436, '##ari': 8486, '##zia': 12871, 'volkswagen': 18817, 'subjects': 5739, '##lone': 27165, 'lifted': 4196, '##oting': 20656, 'bo': 8945, 'lent': 15307, '##rling': 22036, 'spirituality': 21244, 'sitcom': 13130, '##cious': 18436, '##vity': 24872, 'eighteen': 7763, '##bula': 28507, 'schools': 2816, 'fusion': 10077, 'ᄎ': 1465, 'parana': 28383, 'singular': 13048, '##unas': 23904, 'terence': 22677, 'vein': 12818, 'procurement': 21423, '##omp': 25377, '##gut': 27920, 'transparent': 13338, 'mortally': 26495, 'vial': 28475, 'requires': 5942, 'jeans': 6312, 'pillow': 10005, '##tes': 4570, 'felony': 24648, 'superseded': 19886, 'bachelor': 5065, 'glare': 10982, 'anxiety': 10089, '[unused582]': 587, '##ր': 29784, '978': 4891, 'vowed': 18152, 'impatiently': 19951, 'marcia': 22548, 'augustus': 11668, 'africans': 18076, '##tosis': 25950, 'equipped': 6055, 'canadian': 3010, 'such': 2107, 'alexia': 21683, 'handbook': 14812, 'didn': 2134, 'bahrain': 15195, 'powers': 4204, 'organisms': 11767, 'referendum': 9782, 'slade': 18048, 'landmarks': 16209, 'refined': 15514, '〉': 1638, '##port': 6442, 'lawyer': 5160, '##dling': 14423, 'thankful': 18836, 'kangaroo': 21652, 'sinn': 26403, 'agile': 29003, 'ethiopia': 11154, 'sins': 15516, 'harshly': 21052, 'ß': 1096, 'gerhard': 21037, 'max': 4098, 'comedian': 9971, '##iko': 12676, '##wny': 22251, '512': 24406, 'artificial': 7976, 'whistled': 26265, '##ky': 4801, 'mule': 20568, 'promote': 5326, 'curving': 21439, 'sheath': 21867, '##夫': 30339, 'twenties': 18946, '##vre': 12229, 'inspiring': 18988, '،': 1268, 'redskins': 17461, '##isan': 29196, 'westward': 15165, '公': 1772, 'quay': 21048, '[unused498]': 503, 'intimately': 29024, 'terra': 14403, '1714': 25241, 'gruff': 27038, 'act': 2552, '313': 22997, 'one': 2028, 'gathered': 5935, 'prayers': 12583, 'offended': 15807, 'catalyst': 16771, 'tu': 10722, '[unused654]': 659, '##tage': 26702, '##ъ': 29755, 'constructing': 15696, 'emerges': 19391, 'wheelchair': 13204, '##ingham': 16445, 'nests': 17415, 'illumination': 21203, 'inquiries': 27050, '##ᴺ': 30030, '##brand': 23544, '##dim': 22172, 'vine': 15351, 'kuwait': 13085, '##coat': 16531, 'mentor': 10779, 'sharpened': 26694, 'swans': 26699, 'turkmenistan': 25432, 'isil': 19600, 'campos': 26925, 'hammered': 25756, '##lowe': 27663, '?': 1029, 'jana': 23341, 'toilet': 11848, 'obstacles': 15314, 'containment': 29174, 'dwight': 14304, 'subscribers': 17073, 'allegheny': 21192, 'worsened': 27622, 'coldly': 24745, '[unused523]': 528, 'must': 2442, 'cher': 24188, 'capcom': 26861, 'wagner': 10304, 'requirements': 5918, 'angelina': 23847, 'pitch': 6510, 'effortlessly': 29483, 'claw': 15020, '##ect': 22471, 'greenland': 16128, '[unused37]': 38, 'grunt': 20696, 'glide': 21096, 'cloudy': 24706, 'kenneth': 8856, 'placed': 2872, 'sting': 12072, 'paralympic': 17029, 'rhino': 24091, 'bonds': 9547, 'ষ': 1375, 'anguish': 21782, 'nasa': 9274, 'yacht': 12187, 'torso': 15190, '[unused201]': 206, 'structure': 3252, '##ters': 7747, 'cinematic': 21014, 'spelling': 11379, 'perform': 4685, 'amadeus': 27185, 'commemorative': 17524, 'showers': 23442, 'landscaping': 28404, 'starving': 18025, '[unused457]': 462, '##ere': 7869, 'attachment': 14449, 'guardian': 6697, '##も': 30207, '[unused890]': 895, 'nsa': 23971, '##aca': 19629, 'geologic': 22125, '##oric': 29180, '##bers': 17198, 'offering': 5378, 'frustrated': 10206, '##ntly': 20630, 'least': 2560, 'forgetting': 17693, '##cc': 9468, 'leaped': 16900, 'whig': 21632, 'whites': 12461, 'an': 2019, 'populous': 20151, 'shipped': 12057, 'ratings': 8599, 'ı': 1104, 'hybrid': 8893, 'geologist': 21334, 'versus': 6431, '1885': 6571, '##zko': 29002, '##type': 13874, 'regiments': 10435, '##elli': 13348, 'bombing': 8647, '747': 25374, 'why': 2339, '##sma': 26212, 'convection': 23849, 'dorset': 15367, '[unused8]': 9, '##unce': 17457, 'moderator': 29420, 'placement': 11073, 'paradox': 20506, 'appointing': 28050, 'prehistoric': 14491, '156': 16734, 'strain': 10178, 'exploding': 20728, '##fast': 24333, 'indo': 11424, '##uld': 21285, 'め': 1680, 'crimson': 11466, 'fairies': 20182, '##日': 30390, 'stadium': 3346, 'weekdays': 19759, 'glenn': 9465, 'parallels': 18588, 'pattern': 5418, '[unused349]': 354, '243': 22884, 'passionate': 13459, 'grains': 17588, 'pagoda': 27387, 'humanitarian': 11470, 'abdullah': 14093, 'foil': 17910, '##ials': 26340, '##eche': 27635, 'decimal': 26066, 'subterranean': 28811, '##teacher': 24741, '[unused91]': 92, 'adequately': 23613, 'rescue': 5343, 'harvey': 7702, '##vie': 13469, '1924': 4814, 'burns': 7641, 'screw': 11224, 'lough': 29504, 'advocate': 8175, 'beckett': 20599, 'balloons': 22163, 'bodo': 28137, 'captures': 19566, 'apr': 19804, '170': 10894, '##nant': 16885, '##hall': 9892, 'tractor': 16358, 'alternative': 4522, 'signaled': 22319, 'outright': 13848, 'herrera': 23527, 'immediate': 6234, '##イ': 30221, 'respected': 9768, '2004': 2432, '##rt': 5339, 'irwin': 17514, 'manufacture': 9922, 'pillar': 14809, 'mast': 15429, '##erry': 19826, 'iris': 11173, 'panting': 15601, 'transition': 6653, '##oxide': 28479, 'aspects': 5919, '##down': 7698, 'dolphins': 13600, 'disagreement': 18185, '##cards': 17965, 'opinions': 10740, 'sticking': 13423, '83': 6640, 'turf': 14585, 'silicon': 13773, '##ners': 16912, '##pse': 29251, 'editors': 10195, 'sixteenth': 14683, '##itis': 13706, 'temperature': 4860, '##liest': 21292, 'rotate': 24357, 'deteriorating': 28440, '1963': 3699, 'vera': 12297, 'cerro': 25498, 'ュ': 1729, 'juan': 5348, 'knights': 7307, 'replace': 5672, 'destroying': 9846, 'windmill': 25367, '##eda': 11960, '##loading': 18570, 'engined': 21235, 'mimic': 23150, 'popularized': 20339, 'af': 21358, 'mutiny': 19306, '##itate': 17570, 'selma': 28112, 'amalgamation': 21968, '[unused137]': 142, 'chords': 18495, 'curved': 9203, '##phone': 9864, '##top': 14399, 'fitz': 27706, 'hometown': 9627, '##cloth': 23095, 'universe': 5304, 'lobby': 9568, 'baptiste': 17329, '##tite': 23096, 'せ': 1659, '1a': 20720, 'diagrams': 26309, 'junctions': 27169, 'philip': 5170, '##lewood': 26580, 'papyrus': 29575, 'dying': 5996, '##ye': 6672, '##hole': 11484, 'unpopular': 19657, '##sc': 11020, 'plantations': 14979, 'legislature': 6372, 'playable': 16854, 'brunswick': 9192, 'democrat': 7672, 'nerve': 9113, 'varsity': 11710, '[unused516]': 521, 'submit': 12040, 'hedges': 25840, 'jorge': 10853, 'deutschland': 28668, 'inflated': 29561, '##yte': 17250, 'diaries': 18707, '##grave': 12830, '##uit': 14663, '##logue': 24277, '坂': 1803, 'immensely': 24256, 'monastery': 6408, 'freelance': 15919, 'messing': 22308, 'agatha': 23863, 'croatian': 7963, 'barre': 23189, 'վ': 1235, '##ord': 8551, '##romatic': 23645, '##ym': 24335, '##laya': 22923, 'howie': 28211, 'inactive': 16389, '1988': 2997, 'williams': 3766, '##tled': 14782, 'umar': 27981, '27th': 15045, 'maze': 15079, 'interlude': 25347, '##oss': 15094, 'isaac': 7527, 'steal': 8954, '1738': 28009, 'dynasty': 5321, '##ace': 10732, 'friedman': 18486, '5000': 13509, 'enoch': 28178, '##ata': 6790, 'golden': 3585, 'overdose': 26641, 'lane': 4644, 'gentleman': 10170, 'acre': 7456, 'turnout': 15512, 'sub': 4942, 'sl': 22889, 'adverse': 15316, '##off': 7245, '##dington': 21504, 'millimetres': 13388, '[unused101]': 106, 'wars': 5233, 'serena': 14419, 'palaces': 22763, 'segregated': 24382, 'casinos': 27300, 'altering': 22552, 'པ': 1430, 'regards': 12362, 'proposes': 17146, 'pines': 17527, 'plaque': 11952, '##jiang': 21786, 'success': 3112, 'refers': 5218, 'surfing': 19967, 'adaptations': 17241, 'ellington': 21630, 'friendly': 5379, 'hanoi': 24809, 'translit': 28468, 'چ': 1303, 'olympic': 4386, 'determines': 16463, 'commerce': 6236, 'columbian': 25882, '##etto': 20082, 'planting': 14685, '##dun': 27584, 'longest': 6493, 'hints': 20385, 'tariff': 23234, 'newcomers': 24159, 'irony': 19728, 'concessions': 20638, '##say': 24322, 'supplies': 6067, '##ist': 2923, '1901': 5775, 'melodic': 17187, '寺': 1827, 'deng': 26957, 'change': 2689, '1850s': 16488, 'peng': 26473, '##dy': 5149, 'ripe': 22503, 'desirable': 16166, '##bies': 20536, 'feeling': 3110, 'trails': 9612, 'amelie': 25285, '##ћ': 29764, '##ᅲ': 30016, 'smaller': 3760, 'macedonian': 13037, 'infringement': 20701, 'wave': 4400, '##ald': 19058, '##bury': 4917, 'minus': 15718, 'granting': 15080, '##nson': 15551, 'epilogue': 24297, 'breached': 25769, 'ka': 10556, 'giggles': 26466, '##music': 27275, 'transitioned': 23946, 'implemented': 7528, 'ryder': 11731, 'kildare': 24275, 'interpreting': 25455, '##tan': 5794, 'arctic': 10162, 'corrected': 13371, 'buyers': 17394, 'hungarians': 24027, '##lete': 25890, 'abnormalities': 28828, '##land': 3122, 'theorem': 9872, 'nr': 17212, 'regal': 21279, 'comets': 27138, 'board': 2604, 'door': 2341, '##obe': 20891, '22nd': 13816, 'billed': 14843, 'concepcion': 27331, 'genoa': 15771, 'factories': 11123, '##ooping': 29046, '[unused384]': 389, '##inatory': 28230, 'dieter': 27976, 'preferences': 18394, 'tribal': 8807, '[unused537]': 542, '##onga': 26356, 'object': 4874, 'fluctuations': 28892, 'coding': 16861, 'incidents': 10444, '##gren': 13565, 'terrifying': 17082, 'corrections': 20983, 'fidelity': 22625, '##り': 30212, 'exceeded': 14872, '##ue': 5657, '##sk': 6711, 'matt': 4717, 'romanized': 7651, 'uncovered': 14486, '##hp': 22269, '[unused52]': 53, 'hawkins': 13835, '1793': 12387, '##aves': 21055, 'mondays': 28401, 'ceo': 5766, 'transferring': 14391, '##un': 4609, 'radicals': 23618, 'scooted': 24289, 'surfaced': 15791, 'similarity': 14402, 'marcos': 14810, 'alec': 9752, 'capital': 3007, '[unused49]': 50, 'inspectors': 28421, 'enzyme': 9007, '##fect': 25969, 'ga': 11721, 'mesh': 20437, 'bruises': 18438, 'apartments': 9620, 'mortal': 9801, '$': 1002, '##hir': 11961, 'yd': 21076, 'tolkien': 23602, 'strings': 7817, '##zziness': 29212, 'geek': 29294, '##inated': 15833, '##ᵈ': 30034, '##ian': 2937, '##fighter': 20027, 'envy': 21103, 'toledo': 13163, '##yla': 23943, 'merit': 7857, 'barrage': 19359, '##pta': 22799, 'advantage': 5056, 'galway': 14370, 'alton': 26374, '##bon': 11735, '##ₙ': 19110, 'purpose': 3800, '1859': 8165, 'programmers': 28547, '[unused713]': 718, 'rothschild': 19712, 'jeff': 5076, 'layne': 22452, 'telecast': 28803, 'slipped': 5707, '##kson': 26579, '##tein': 9589, 'shivers': 28464, 'labour': 4428, 'sunny': 11559, 'pak': 22190, 'coca': 16787, 'shifters': 21537, 'grunted': 14922, 'grins': 20237, 'eurasian': 23399, 'hears': 14994, 'alfred': 6152, 'referring': 7727, '[unused33]': 34, 'captives': 21496, 'surfer': 27747, 'calves': 28023, 'shortlisted': 21353, '##我': 30381, 'botanical': 13194, 'textbook': 16432, 'dario': 26800, 'მ': 1448, '##kic': 29493, 'acquired': 3734, 'weekday': 16904, 'lehigh': 23241, 'bandwidth': 20235, 'strike': 4894, 'kassel': 27884, '##codes': 23237, 'penn': 9502, 'literacy': 8433, 'jeffrey': 10799, 'cosmic': 14448, 'raised': 2992, '##ager': 17325, '∪': 1605, '##士': 30333, 'series': 2186, 'topics': 7832, 'realm': 8391, '##dong': 17679, 'paisley': 23321, '##sling': 28886, 'stained': 9702, '##‚': 30057, 'iranian': 7726, 'mouths': 15076, '1st': 3083, 'cope': 11997, 'ng': 12835, 'folk': 5154, 'momentarily': 16032, '##erence': 20935, 'beers': 18007, 'reader': 8068, 'polar': 11508, 'territory': 3700, 'plant': 3269, 'amid': 13463, 'finley': 29194, '##itor': 15660, '1968': 3380, 'clues': 15774, 'nicole': 9851, '##∞': 30128, '一': 1740, '国': 1799, '##ন': 29902, 'maori': 12600, '##oons': 27174, 'advancing': 10787, '##ir': 4313, '##野': 30489, '[unused945]': 950, 'tracking': 9651, 'oversees': 22558, '[unused767]': 772, 'enfield': 25821, 'friday': 5958, '##are': 12069, 'complicated': 8552, 'vhf': 27527, 'hull': 6738, 'tilting': 21788, 'heat': 3684, 'threaten': 15686, 'theatre': 3004, 'colors': 6087, '1819': 12552, 'wept': 24966, 'inventions': 21644, 'venues': 9356, 'response': 3433, 'aeronautics': 27459, 'yielding': 21336, 'fundamental': 8050, 'jug': 26536, 'companions': 11946, 'holes': 8198, 'trainer': 10365, 'switzerland': 5288, 'negotiating': 18875, 'eats': 20323, 'ironically': 18527, 'leo': 6688, 'judged': 13224, '##dine': 10672, 'pisa': 25472, '∈': 1596, '##ronia': 28140, '259': 25191, '##powered': 27267, 'hash': 23325, '##ump': 24237, 'offender': 25042, 'did': 2106, '##手': 30384, '##38': 22025, '##olo': 12898, 'ale': 15669, 'cases': 3572, 'compilations': 22754, 'erebidae': 25875, 'title': 2516, '##tsu': 10422, '##phic': 17926, '##ctable': 23576, 'coast': 3023, 'ph': 6887, 'plague': 11629, 'fluorescent': 22184, 'nestled': 22704, 'tristan': 9822, 'systematically': 23087, '1769': 20663, 'styling': 20724, 'macintosh': 22228, 'darkest': 23036, 'edited': 5493, 'violinist': 16609, 'sparks': 12300, 'hughes': 8099, 'batman': 8942, '##hal': 8865, 'compromised': 20419, 'devils': 13664, 'christopher': 5696, '##rea': 16416, 'bea': 26892, 'hammond': 11309, 'jaya': 24120, 'vowel': 12710, 'ₙ': 1568, 'chased': 13303, 'teammates': 13220, '##eptive': 22048, '1683': 27414, '##iform': 22631, 'accessed': 11570, '##ums': 18163, 'distorted': 19112, 'aviv': 12724, 'kathleen': 14559, '##gle': 9354, 'predecessors': 16372, 'wurttemberg': 16346, 'forts': 15421, 'thrash': 27042, 'accuracy': 10640, 'fifties': 25942, '[unused138]': 143, 'gm': 13938, '##ses': 8583, '##ᵥ': 30045, 'though': 2295, 'yell': 14315, 'juicy': 28900, 'protestant': 8330, '##scribe': 29234, 'anime': 8750, 'gilles': 21717, 'livery': 18475, '##ko': 3683, '##lab': 20470, 'marvelous': 28851, 'lola': 15137, '##hi': 4048, '[unused687]': 692, '##ifies': 14144, 'ħ': 1103, '314': 26257, 'jesuits': 20297, 'edges': 7926, '77': 6255, 'shetland': 25552, 'prima': 21111, 'ramirez': 15206, '[unused148]': 153, 'variant': 8349, '##olved': 16116, 'twitching': 25402, 'recipe': 17974, 'splashed': 22055, 'exchanged': 10573, 'hasty': 27151, 'filippo': 28669, '##ilde': 23891, 'arrondissement': 20522, 'authored': 8786, '[unused262]': 267, '[unused774]': 779, 'queue': 24240, 'socialist': 6102, 'melted': 12501, 'karachi': 15381, '[unused564]': 569, '[unused426]': 431, 'tugging': 17100, 'mummy': 22788, 'reykjavik': 28559, '[unused962]': 967, 'belonged': 6272, '##原': 30313, 'ʾ': 1147, 'rarely': 6524, 'pharmaceutical': 13859, 'shirts': 11344, 'owing': 11427, '##pass': 15194, '##uchi': 15217, '##vati': 20203, 'georgie': 20280, '##־': 29787, '1739': 25801, 'upbeat': 27999, '##quel': 22197, 'consistent': 8335, '##न': 29863, '[unused310]': 315, 'parallel': 5903, 'agents': 6074, 'nissan': 16509, '840': 28122, 'norse': 15342, '##dant': 28210, '[unused1]': 2, 'न': 1327, 'ragged': 14202, '##cliff': 27580, 'claims': 4447, 'financial': 3361, 'investigator': 14064, 'josephine': 16117, 'vu': 24728, '##法': 30427, '##acion': 21736, 'intimidating': 24439, 'lister': 27177, 'gough': 29124, 'simon': 4079, 'challenge': 4119, 'palermo': 18705, 'verify': 20410, 'likes': 7777, '[unused848]': 853, 'tex': 16060, 'differed': 19541, 'prisoner': 7267, 'consequence': 9509, '£2': 21853, '##aris': 23061, '##erted': 28728, 'inflation': 14200, '##mist': 23738, 'commemorates': 25530, 'ン': 1737, 'gmina': 7061, '##oir': 21165, '##aq': 20784, 'rounded': 8352, 'kincaid': 24510, 'gunners': 29000, 'arlington': 13929, 'false': 6270, 'jasmine': 14032, '[unused604]': 609, 'ے': 1310, 'intimacy': 20893, 'landfall': 21042, 'constabulary': 23791, 'defended': 8047, 'telegraph': 10013, '##gio': 11411, '##mas': 9335, 'monstrous': 21668, 'abel': 16768, '##imus': 19315, 'biennial': 20313, 'numbered': 8597, '##ɪ': 29685, 'blade': 6085, 'belarusian': 15626, 'alley': 8975, 'backseat': 19978, 'formulation': 20219, 'acts': 4490, 'tours': 7562, 'bacteria': 10327, '##mine': 11233, 'kristin': 25130, 'amman': 25703, 'vantage': 27274, 'ي': 1300, 'format': 4289, 'alexa': 24969, 'pitches': 19299, 'cause': 3426, 'valued': 11126, '231': 20304, 'australians': 15739, 'coupled': 11211, '##ylus': 18871, '[unused501]': 506, 'selective': 13228, 'nas': 17235, 'presiding': 18131, 'biology': 7366, '1956': 3838, 'bird': 4743, 'justin': 6796, 'hangul': 19051, 'cricket': 4533, 'protege': 28567, '##lina': 13786, 'spotted': 7282, 'contaminated': 19450, '[unused589]': 594, 'cricketer': 9490, 'islanders': 16422, '84': 6391, 'cooling': 11520, 'exchange': 3863, 'felt': 2371, 'anastasia': 19447, 'betray': 20895, 'lick': 15385, '##arus': 29133, 'restructuring': 18322, 'outrage': 19006, 'accompanied': 5642, '[unused326]': 331, 'manifold': 19726, 'orbital': 13943, '##ley': 3051, 'spreading': 9359, 'collaborate': 20880, 'tung': 27079, '##ina': 3981, 'purchases': 17402, '##書': 30397, 'nods': 11232, 'revolutionary': 6208, 'grief': 9940, 'routes': 5847, '##ille': 10484, 'weeks': 3134, 'salle': 18005, '##oda': 13390, '##mour': 20360, '##dran': 24914, '##amp': 16613, 'occasion': 6686, '##dya': 25838, 'exemplified': 28593, 'modernized': 27140, 'ridden': 15230, '##cu': 10841, 'salman': 28542, 'flank': 12205, '##wani': 29092, '##gic': 12863, 'flyers': 16217, 'governmental': 10605, 'controlled': 4758, '##வ': 29930, '##kow': 24144, 'picks': 11214, 'putnam': 20230, '##enity': 20693, '##rard': 25561, 'jerusalem': 6744, '281': 22955, 'ellison': 21513, 'pressed': 4508, 'doubted': 12979, 'timbers': 26238, '##{': 29640, 'mariano': 22695, 'somewhat': 5399, 'paying': 7079, 'macedonia': 11492, 'shareholder': 18668, '12th': 5940, 'reforms': 8818, 'susceptible': 18002, 'breaks': 7807, 'charters': 23010, '##}': 29642, 'plurality': 29018, 'menacing': 24060, 'researches': 27338, 'mascot': 13314, 'hindwings': 15998, 'broken': 3714, 'stove': 16247, '[unused664]': 669, '[unused403]': 408, 'travel': 3604, '##aia': 27131, 'garion': 12523, 'spearheaded': 27721, 'mobilized': 27526, 'roaming': 24430, 'consort': 13440, '##松': 30406, 'holmes': 9106, 'pulsed': 24107, 'reunification': 28044, '##fer': 7512, 'readers': 8141, 'writhing': 26185, 'symmetric': 19490, 'breeding': 8119, 'knesset': 27899, '##tors': 6591, '351': 28474, 'grades': 7022, 'randomly': 18154, 'hines': 25445, 'maritime': 7803, 'liege': 17766, 'appetite': 18923, 'monsters': 9219, '##ipes': 28108, 'dent': 21418, 'walker': 5232, 'personalities': 12857, 'lucha': 25390, 'evaluate': 16157, 'staircase': 10714, '##log': 21197, 'females': 3801, '##osh': 17369, 'medals': 6665, 'surpassed': 15602, '##ft': 6199, 'simplified': 11038, 'nonprofit': 14495, 'sumo': 28193, 'barcelona': 7623, 'pam': 14089, 'thriller': 10874, 'medley': 13863, 'lighted': 26390, 'races': 3837, 'shore': 5370, 'imperialism': 28087, 'beheaded': 28923, 'carousel': 27628, 'waved': 7147, 'upgrading': 25925, 'egg': 8288, '##ears': 26492, 'assassinated': 16370, 'naval': 3987, '##pled': 21132, 'graded': 21976, '##ards': 18117, 'separates': 18600, '##85': 27531, 'strange': 4326, '##kle': 19099, '##ᄊ': 29998, 'molecules': 10737, '##pot': 11008, 'strikes': 9326, 'compass': 16681, 'medicine': 4200, '##khar': 22510, 'janice': 20029, '##pile': 22090, 'exit': 6164, '##ᅦ': 30009, 'narrated': 17356, 'sakura': 23066, '##ind': 22254, 'weren': 4694, 'damages': 12394, 'not': 2025, 'partisans': 20762, 'bile': 23974, 'gracefully': 28266, '##phobic': 20200, 'priestess': 27677, 'lia': 22393, 'leafs': 21349, 'stucco': 27443, 'indicted': 21801, 'classical': 4556, 'camps': 7958, 'waldo': 28806, 'collin': 22180, 'attendant': 16742, 'romero': 18290, '##vier': 14356, 'suspicions': 17817, 'disclose': 26056, 'stryker': 25429, 'quad': 17718, 'rides': 12271, 'waiting': 3403, 'coating': 18898, 'debris': 11385, 'duties': 5704, 'um': 8529, 'cameo': 12081, 'insides': 19008, 'prior': 3188, 'illustrated': 7203, '##mun': 23041, '##ゆ': 30209, 'meaning': 3574, 'confederacy': 18179, 'dispose': 27764, 'outdated': 25963, 'trousers': 15292, 'consortium': 12360, '##uga': 16377, '##unes': 26639, 'wondered': 4999, 'concentrate': 10152, 'hawthorn': 21671, 'viewed': 7021, 'dupont': 22848, 'washed': 8871, 'irving': 12415, 'łodz': 17814, 'allegations': 9989, 'vanessa': 13226, 'builds': 16473, 'demos': 18267, 'coe': 24873, 'unlocked': 14058, 'efficient': 8114, 'immunity': 15403, 'lanterns': 26078, 'equations': 11380, 'magazines': 7298, 'gym': 9726, 'lieu': 22470, '[unused348]': 353, 'stepfather': 21481, '##ƒ': 29675, 'testosterone': 25937, '##42': 20958, 'ended': 3092, 'cabaret': 19685, 'parapet': 27372, 'ₗ': 1566, 'hid': 11041, 'breakup': 19010, 'photographers': 17008, 'sympathy': 11883, 'shares': 6661, '1996': 2727, 'invited': 4778, 'doctrine': 8998, 'relieve': 15804, '##field': 3790, 'algebraic': 17390, '##ept': 23606, 'dare': 8108, '##iya': 8717, '##व': 29871, 'ང': 1427, 'neumann': 22860, 'peeked': 18652, '##git': 23806, 'hirsch': 28127, 'treatments': 13441, 'coup': 8648, '##centric': 22461, 'weasel': 29268, 'common': 2691, '##佐': 30290, 'winnie': 27125, 'timetable': 23839, '##cuit': 28168, 'wa': 11333, 'shandong': 25768, 'close': 2485, 'patsy': 25382, 'yale': 7996, '[unused754]': 759, 'topology': 19587, 'clicks': 29225, 'tel': 10093, 'attend': 5463, 'tended': 11121, 'cache': 17053, 'townspeople': 27938, '##劉': 30303, 'sudan': 10411, 'contemporary': 3824, 'trilogy': 11544, 'fairly': 7199, 'parliamentary': 5768, '24': 2484, '1910': 4976, 'opposed': 4941, '##∘': 30126, 'churches': 5231, 'alligator': 28833, 'yet': 2664, '##rate': 11657, '[unused88]': 89, 'fibre': 20962, 'colossal': 29523, 'law': 2375, '[unused509]': 514, '##mmel': 29033, '1773': 19916, 'britten': 29429, 'anchor': 8133, 'payton': 28666, 'peking': 27057, '##rak': 16555, '##rop': 18981, 'nicola': 17388, 'picture': 3861, 'throughout': 2802, 'action': 2895, 'ds': 16233, 'debra': 28762, 'mikey': 25998, '##ny': 4890, 'escorts': 24877, 'shoved': 7468, 'greasy': 26484, 'rohan': 28605, '⋅': 1614, '##rill': 24714, 'italics': 19408, '##or': 2953, 'marshes': 19257, 'surprised': 4527, 'ace': 9078, 'suspicion': 10928, 'disturbed': 12491, 'aide': 14895, 'baked': 17776, 'naturalist': 19176, 'wraps': 19735, 'promising': 10015, 'diagnosis': 11616, 'quoted': 9339, 'op': 6728, 'glances': 13021, 'williamsburg': 26366, 'thirst': 21810, 'louisiana': 5773, 'distant': 6802, 'racism': 14398, 'vector': 9207, 'roth': 12211, '##13': 17134, 'bulls': 12065, 'supplement': 12448, 'croix': 18733, 'hezbollah': 25713, 'logs': 15664, '##des': 6155, 'kin': 12631, '##tering': 17989, 'planetary': 17700, 'citing': 8951, 'mythology': 11327, 'bangladesh': 7269, 'relate': 14396, 'signaling': 14828, '##firmed': 23141, 'hind': 17666, 'estrada': 26482, 'shoving': 15866, 'disguised': 17330, 'route': 2799, 'leith': 29011, '##sion': 10992, '##tum': 11667, 'lowry': 27281, 'অ': 1347, '章': 1932, 'codes': 9537, 'ব': 1368, '##liffe': 27931, 'emeritus': 12372, 'modelled': 23364, '[unused495]': 500, '[unused818]': 823, 'royalty': 16664, 'outbreak': 8293, 'د': 1278, '##թ': 29771, 'repair': 7192, 'inspect': 22459, 'trustee': 13209, 'reasoned': 24557, '##iring': 24771, 'believed': 3373, '##ck': 3600, 'swaying': 21826, 'dynasties': 23014, '[unused891]': 896, 'prisoners': 5895, 'council': 2473, '##ล': 29954, 'theoretical': 9373, 'paperback': 13611, 'whitney': 9809, 'production': 2537, 'post': 2695, 'ش': 1283, 'petty': 11612, 'gig': 15453, '1642': 24061, 'allow': 3499, 'podium': 14502, 'agent': 4005, 'siam': 25583, 'sony': 8412, '2012': 2262, 'valle': 20171, 'mendoza': 18021, 'jewels': 15565, '##cology': 19824, 'hand': 2192, 'arrives': 8480, 'malaya': 19979, 'taut': 21642, 'differing': 16965, 'klan': 26613, 'stretches': 14082, '1622': 28133, 'impatient': 17380, '##nl': 20554, 'curiously': 16484, 'barnett': 20073, 'bengali': 11267, 'hasan': 17000, 'swindon': 22350, 'chatting': 22331, 'johnstone': 24160, 'spawning': 27957, '##heim': 8049, 'abolition': 15766, 'clans': 16411, 'langley': 19094, 'shrimp': 20130, '－': 1990, 'repository': 22409, '##訁': 30475, 'verb': 12034, 'washing': 12699, 'battleships': 21327, '##gul': 24848, 'meg': 12669, 'france': 2605, 'whitehall': 28361, '##fm': 16715, 'carrying': 4755, 'contributing': 8020, 'dil': 29454, '##rin': 6657, 'curry': 15478, 'sockets': 27540, 'jiang': 20613, '##buck': 24204, 'supervisor': 12366, 'normal': 3671, 'revoked': 22837, 'citadel': 15364, 'mate': 6775, 'strand': 11226, '##gel': 12439, '##rued': 28551, 'burgundy': 18383, 'consent': 9619, '##quisite': 24871, '##ben': 10609, 'seeks': 11014, 'kensington': 17775, 'ordination': 18129, 'hem': 19610, 'corruption': 7897, 'flee': 10574, '##hul': 21886, '##anov': 25417, 'winkler': 20472, 'প': 1367, 'february': 2337, 'several': 2195, 'remembered': 4622, 'ways': 3971, 'valuable': 7070, '1713': 27016, 'geometric': 14965, '##onus': 24891, 'յ': 1230, 'drunk': 7144, '##տ': 29783, 'feeding': 8521, '##nce': 5897, 'digitally': 18397, 'article': 3720, 'consultants': 22283, 'belong': 7141, 'pit': 6770, '歌': 1886, 'bearer': 20905, 'virgil': 17270, 'november': 2281, 'vocals': 2955, 'hal': 11085, 'whirled': 17097, 'dawned': 27702, 'reviews': 4391, '##sant': 22341, 'commanded': 6311, '##turing': 16037, 'fantasia': 29203, 'geoffrey': 11023, 'howling': 22298, 'interests': 5426, 'ordinance': 16692, 'adams': 5922, 'lb': 6053, 'nagar': 16395, 'streaked': 21276, '##llum': 20845, 'traitor': 17328, 'seine': 16470, 'palatinate': 18990, 'eric': 4388, 'ate': 8823, 'citizenship': 9068, 'deeds': 15616, 'expenditures': 22697, '##recht': 28109, '##на': 19865, 'theorists': 28442, 'aside': 4998, 'sicily': 12071, 'danube': 16212, 'ache': 12336, 'っ': 1663, 'baptism': 18336, 'vomit': 23251, '##古': 30315, 'salford': 23001, 'entities': 11422, 'shelley': 15828, 'elders': 13376, 'outta': 24955, '##zos': 28370, 'burials': 23109, '[unused265]': 270, 'copa': 10613, 'granville': 24926, 'yellow': 3756, '##umb': 25438, 'integral': 9897, 'jews': 5181, 'replica': 15059, 'migration': 9230, 'stand': 3233, 'retreat': 7822, 'easily': 4089, 'open': 2330, '見': 1948, 'danes': 27476, 'lock': 5843, 'mad': 5506, 'pool': 4770, '1940s': 7675, 'isotope': 28846, 'bethany': 16559, 'productive': 13318, '地': 1802, 'mvp': 12041, 'joker': 19318, '##oran': 18842, '[unused340]': 345, 'pinnacle': 26007, 'conception': 13120, '##tung': 21847, 'rivalry': 10685, 'koppen': 20139, 'ibn': 7839, '1666': 27407, '##χ': 29737, '##mba': 11201, 'microscopic': 26396, 'settings': 10906, 'facades': 28708, 'finalized': 23575, 'dominant': 7444, 'dated': 6052, 'chemotherapy': 27144, 'uncertainty': 12503, '安': 1820, 'pancakes': 28470, 'pretended': 14688, '##lka': 26518, 'edward': 3487, 'susanna': 26681, '##yria': 20379, 'darryl': 22821, 'christie': 13144, 'trying': 2667, 'usage': 8192, 'weep': 27874, '##東': 30405, 'peasant': 14539, 'jamaican': 17851, 'embodied': 25405, 'requiring': 9034, 'professions': 22797, 'drowning': 14759, 'gladstone': 21765, 'energies': 19320, 'adhere': 25276, '##form': 14192, '##onte': 28040, 'belly': 7579, 'marcel': 13389, '##hei': 26036, 'rotary': 16933, 'artistic': 6018, '1723': 26621, '[unused323]': 328, 'formations': 13197, 'steep': 9561, 'dealer': 11033, '##illo': 10486, 'rivera': 14043, '##ack': 8684, 'joanne': 23459, 'bomber': 9472, 'consultant': 8930, 'estuary': 18056, '##カ': 30226, 'と': 1666, 'hernandez': 13688, 'guillaume': 20061, 'apical': 29197, '##cellular': 16882, 'frowning': 14587, 'stabbing': 21690, '##ツ': 30238, 'occupants': 18837, 'tuscany': 23322, '[unused896]': 901, '##worthy': 13966, '##cote': 19800, 'houghton': 21234, 'ز': 1281, '##oka': 12352, 'kilometer': 20595, 'grade': 3694, 'squeezed': 7757, 'himalayas': 26779, 'euro': 9944, 'affordable': 15184, 'ventilation': 19536, 'derivative': 13819, 'inhibit': 26402, 'meaningless': 25120, 'penetration': 20015, 'bobbie': 27731, 'che': 18178, 'indie': 10271, 'owls': 22388, 'regimental': 17604, 'canal': 5033, '##ss': 4757, 'relocating': 26811, '##saka': 29289, 'accommodate': 8752, '138': 15028, 'francis': 4557, 'pretty': 3492, 'peugeot': 26385, '[unused259]': 264, 'mercy': 8673, 'baba': 14208, 'renewable': 13918, 'april': 2258, 'depicted': 8212, 'reputed': 22353, 'mathews': 23287, 'apache': 15895, 'ף': 1260, '##ytic': 21252, 'depleted': 22595, 'vr': 27830, '[unused399]': 404, 'daniel': 3817, '##une': 9816, 'verde': 16184, '[unused872]': 877, 'journals': 9263, 'herd': 14906, 'neat': 15708, 'police': 2610, 'olga': 15585, 'downhill': 19448, 'talking': 3331, 'stereo': 12991, '##zcz': 29419, 'heel': 12073, '[unused102]': 107, 'respects': 17475, 'savings': 10995, 'basha': 26074, 'backing': 5150, 'madrid': 6921, 'willem': 18811, 'strife': 27865, 'sufficient': 7182, 'departing': 15971, '##war': 9028, 'admiring': 24588, 'nice': 3835, 'tour': 2778, '[unused602]': 607, 'peacock': 18931, 'fronts': 21430, 'terror': 7404, 'repeating': 15192, 'shivering': 19197, 'vocalists': 27478, 'backstroke': 28373, 'propellers': 23405, 'rao': 10546, 'pride': 6620, '[unused133]': 138, 'saddam': 24111, '北': 1781, 'resolution': 5813, 'wiped': 8342, 'supplied': 8127, 'dhabi': 23153, '##ire': 7442, 'swift': 9170, '##om': 5358, 'stool': 14708, '##itated': 15198, 'ow': 27593, 'engineering': 3330, 'persecution': 14522, 'taekwondo': 28529, '##bei': 19205, 'mp': 6131, 'restrictions': 9259, 'tr': 19817, 'objects': 5200, 'till': 6229, 'overlooking': 12549, '##buro': 23670, '##rus': 7946, 'courthouse': 10816, 'incidence': 18949, 'retained': 6025, '##eding': 17819, 'wo': 24185, 'projectile': 25921, 'turtles': 16489, 'parks': 6328, 'worn': 6247, 'redundant': 21707, 'size': 2946, 'advances': 9849, 'arrest': 6545, '##?': 29632, 'things': 2477, '##uze': 20395, '##aly': 20766, 'linnaeus': 21610, 'coupe': 15130, 'populated': 10357, '##word': 18351, 'braced': 15515, 'outfit': 11018, 'volume': 3872, 'defenses': 13345, 'original': 2434, '1932': 4673, 'total': 2561, 'circumstances': 6214, 'hauled': 13161, '##rien': 23144, 'ₘ': 1567, 'sausage': 24165, 'statement': 4861, '##ow': 5004, 'allison': 10786, '##rra': 11335, '##ehan': 24660, 'staging': 15308, 'isabelle': 13942, 'housekeeper': 22583, 'starboard': 25211, 'pink': 5061, '47th': 28243, 'harbor': 6496, 'carlton': 12989, '43rd': 25747, '##lation': 13490, 'competing': 6637, '##rry': 12244, 'persuaded': 11766, 'murdoch': 19954, 'useful': 6179, 'bratislava': 22992, 'ₑ': 1561, 'anyway': 4312, '##⁺': 12744, 'cured': 21391, '##bil': 14454, 'repealed': 21492, 'scale': 4094, 'starr': 14330, 'awaited': 19605, '##nb': 27698, '##lyn': 9644, '##gau': 20420, 'bra': 11655, 'accident': 4926, '20s': 27074, 'affluent': 22666, 'warns': 19428, 'artifacts': 10471, 'folks': 12455, 'kinda': 17704, '##ulio': 24825, 'burst': 6532, 'spencer': 7084, '##boro': 12691, '##graphic': 14773, '##bot': 18384, 'juris': 27551, 'louisville': 11577, 'twain': 24421, '##nting': 24360, '##hyllum': 27750, 'love': 2293, '##aker': 22626, 'uruguayan': 23464, 'rejoined': 14311, 'nrhp': 22424, 'capture': 5425, 'faction': 10233, 'fluffy': 27036, 'politicians': 8801, '92': 6227, '1763': 18432, 'treatment': 3949, 'headline': 17653, 'starbucks': 29500, 'canonical': 18562, 'rocket': 7596, '##rter': 19418, 'nolan': 13401, 'alternatively': 14084, 'phylogenetic': 23192, 'azure': 24296, 'inhibitors': 25456, '##mum': 27147, 'domed': 29208, '[unused761]': 766, '##ee': 4402, 'intentions': 11174, 'prefers': 19233, 'script': 5896, 'rufus': 18316, 'penis': 19085, 'belonging': 7495, 'lou': 10223, 'industrialist': 21691, '##ニ': 30242, '##oms': 22225, 'palestinians': 21524, 'hunts': 28526, 'hindu': 7560, '##ima': 9581, 'rotation': 9963, 'scheme': 5679, 'peered': 10757, 'affair': 6771, '##nch': 12680, '##ddin': 18277, 'consolidate': 24939, 'homage': 14822, 'generated': 7013, 'credits': 6495, 'hinges': 25484, 'pan': 6090, '1715': 22606, '##ritan': 25279, 'check': 4638, '301': 19123, 'infirmary': 23453, 'coyote': 20457, 'ram': 8223, 'swelled': 21851, 'prosperous': 18241, '##less': 3238, 'nbc': 6788, '##master': 8706, 'イ': 1695, '##bling': 9709, 'imposing': 16625, '[unused993]': 998, 'additionally': 5678, '##berry': 9766, 'dj': 6520, 'deported': 17929, 'unbroken': 29505, '##rce': 19170, 'lunged': 17755, 'crime': 4126, 'sights': 15925, '##media': 16969, 'assemblies': 17720, '[unused233]': 238, 'independently': 9174, 'declare': 13520, '##section': 29015, 'ki': 11382, 'preparatory': 13485, '##tura': 27431, 'fia': 19807, '##eson': 21421, 'grimsby': 24470, 'bedrooms': 18390, 'xx': 22038, 'cadence': 23620, 'revenue': 6599, 'governments': 6867, 'somehow': 5064, 'taliban': 16597, '##г': 29741, 'furnishings': 23127, 'tend': 7166, 'ɕ': 1113, 'ᆼ': 1489, '₩': 1573, 'federal': 2976, 'joanna': 15730, '[unused166]': 171, 'pba': 21563, 'exported': 15612, 'belongs': 7460, 'connectivity': 20831, 'battles': 7465, 'pastry': 27060, 'silvia': 27827, 'romania': 6339, 'assessment': 7667, 'emory': 25853, 'athlete': 8258, 'toy': 9121, 'investigates': 28062, '[unused517]': 522, 'breach': 12510, '##ষ': 29911, 'chiefs': 9058, 'scored': 3195, 'restraints': 28054, 'larry': 6554, 'sister': 2905, 'scanning': 13722, 'polite': 13205, 'buried': 3950, 'butterflies': 15023, 'penelope': 18641, 'sincere': 18006, '159': 18914, 'zoology': 22405, 'ngos': 22165, 'gems': 20296, 'international': 2248, 'pe': 21877, 'dimitri': 15953, 'bared': 23485, 'defence': 4721, 'special': 2569, 'decks': 19963, 'eurovision': 12714, 'dull': 10634, 'beaver': 13570, '1855': 8492, 'newsletter': 17178, '##28': 22407, 'hutchinson': 17165, 'intent': 7848, 'psychologists': 25428, '##hosh': 26643, 'token': 19204, '##ssi': 18719, 'surveyed': 12876, 'cluster': 9324, 'endeavors': 28809, '##etic': 16530, '187': 19446, '##inae': 16414, '##finger': 20349, '##ও': 29888, '##horse': 23024, 'ministers': 7767, 'lengthy': 12401, 'homes': 5014, 'forming': 5716, 'main': 2364, 'nevertheless': 6600, '##民': 30418, '##qvist': 28705, 'walks': 7365, '##ych': 17994, 'pixels': 27725, 'analyzed': 16578, 'wen': 19181, 'batch': 14108, '[unused870]': 875, 'clear': 3154, 'succeeds': 21645, '##onal': 16026, 'birch': 16421, 'broadcasting': 5062, 'wingspan': 9635, 'cube': 14291, '282': 26267, 'superliga': 25922, 'static': 10763, 'lovely': 8403, 're': 2128, '##υ': 29735, '水': 1893, '##tron': 15312, 'dancing': 5613, 'terrible': 6659, 'murderous': 25303, 'dodgers': 13391, 'comparison': 7831, 'passed': 2979, 'monitors': 15410, 'yusuf': 23495, 'mgm': 15418, '[unused988]': 993, 'unwanted': 18162, '##tious': 20771, 'shine': 12342, '##≤': 30135, 'ming': 11861, 'scriptures': 22481, 'bomb': 5968, 'tracing': 16907, 'instituto': 22596, '[unused448]': 453, 'own': 2219, 'universidad': 16501, 'dividing': 16023, 'ך': 1251, 'affect': 7461, 'morphological': 24012, '##icio': 27113, 'felicia': 27579, '##9th': 25660, 'tailed': 14578, 'az': 17207, 'peck': 18082, 'pieces': 4109, 'identifying': 12151, 'howell': 18473, '##tized': 23355, '##lok': 29027, 'bradshaw': 23762, '##pro': 21572, 'prototype': 8773, '[unused373]': 378, 'anthology': 9637, 'material': 3430, '##eis': 17580, 'jade': 12323, 'shepard': 22189, 'serials': 28172, 'erin': 11781, 'flinched': 19201, 'canterbury': 9976, 'khan': 4967, 'rocking': 14934, 'sul': 21396, 'carmichael': 23537, 'colleen': 28385, '##mot': 18938, 'turned': 2357, 'protective': 9474, 'pang': 20657, 'computation': 22334, 'devote': 23313, 'gust': 26903, 'gravely': 28070, '##shot': 19040, '##ita': 6590, 'voters': 7206, '##ur': 3126, 'sensation': 8742, '##nai': 26416, '##ances': 26755, '##∇': 30123, 'excavation': 16456, '210': 12875, 'rosewood': 29528, 'fortunate': 19590, 'go': 2175, 'switched': 7237, '##icum': 22167, 'ii': 2462, 'prix': 5431, 'sick': 5305, '##bes': 12681, 'quality': 3737, 'bottoms': 24196, 'ipad': 25249, 'stil': 25931, 'sachs': 22818, 'rails': 15168, 'marble': 7720, '##zi': 5831, 'coliseum': 18338, 'plumbing': 27902, 'retaining': 12823, 'talk': 2831, 'lebanese': 12592, '##ache': 15395, 'imminent': 17566, 'feed': 5438, 'streamed': 18498, 'vacation': 10885, 'sherry': 22268, 'lashes': 16008, 'banned': 7917, 'shooter': 13108, 'ʊ': 1132, 'grayson': 17556, 'fright': 25966, '##kk': 19658, 'two': 2048, 'millionaire': 19965, '[unused565]': 570, '299': 25926, 'chemist': 15535, 'sharing': 6631, 'valid': 9398, '326': 28188, '##ilis': 24411, 'mistaken': 13534, 'cochin': 28982, 'ballistic': 19630, 'mechanism': 7337, '@': 1030, 'conducting': 9283, '##ע': 29805, 'virus': 7865, 'methyl': 25003, 'wedding': 5030, 'sour': 14768, 'mechanically': 27846, 'headed': 3753, '[unused63]': 64, 'lakes': 6597, 'heavier': 11907, 'mae': 11530, 'ᅦ': 1473, 'assistant': 3353, 'logged': 26618, 'rage': 7385, 'thighs': 9222, 'val': 11748, '1742': 25704, 'registers': 18687, 'welch': 17939, '##tina': 13770, 'giacomo': 22873, 'spawn': 25645, 'leagues': 8121, 'plc': 15492, 'wilmington': 17025, 'pubs': 23598, 'nakamura': 23981, '##lass': 27102, 'clicking': 22042, 'recognition': 5038, 'romeo': 12390, '##rgan': 16998, 'molly': 9618, 'tramway': 17050, 'halle': 19769, 'chalk': 16833, 'spec': 28699, '##\\\\': 29635, 'sure': 2469, 'thermal': 9829, 'patterned': 25336, '##arina': 27943, 'deceased': 10181, 'comparable': 12435, 'inhibition': 23586, '##eia': 27958, 'much': 2172, '##ies': 3111, 'diocesan': 18680, 'ltd': 5183, 'surgeon': 9431, 'dashed': 18198, 'get': 2131, 'fake': 8275, '1759': 21667, '##ilised': 21758, '##ᵀ': 30031, '##den': 4181, '##ャ': 30254, 'cozy': 26931, '島': 1833, 'prose': 12388, 'corbin': 24003, 'plight': 24525, 'frenzy': 21517, '##tori': 29469, '##ल': 29870, '##ung': 5575, 'procedure': 7709, 'willingly': 18110, 'losers': 23160, 'wadi': 28380, 'campaigns': 8008, 'sells': 15187, 'semiconductor': 20681, '##iff': 13355, 'widowed': 22874, 'metro': 6005, '##yre': 16363, '1868': 7582, 'э': 1208, 'officer': 2961, 'festival': 2782, 'stacey': 19997, 'lizzie': 15860, 'supporting': 4637, 'გ': 1440, 'obscured': 23649, 'pea': 26034, 'courts': 5434, 'await': 26751, '##ʸ': 29710, '##w': 2860, 'yunnan': 21607, 'fig': 20965, '[unused507]': 512, '118': 12963, 'aggressively': 24663, 'italiana': 28059, 'solemnly': 26294, 'forks': 19896, '##zai': 25290, 'rr': 25269, 'notre': 10289, 'dye': 18554, 'wong': 11789, 'carriers': 11363, 'insist': 18292, 'hound': 19598, 'operating': 4082, 'bharatiya': 24243, '##ually': 28488, 'perkins': 13601, '##dly': 18718, 'immigrant': 11560, '163': 17867, '空': 1930, 'bee': 10506, 'review': 3319, '[unused487]': 492, 'whenever': 7188, 'beneath': 4218, '##ode': 10244, 'cinematographer': 19245, '##tral': 21493, '##ж': 29743, '##会': 30289, 'shell': 5806, 'polly': 17543, '[unused357]': 362, 'braid': 24148, 'magazine': 2932, '##ope': 17635, 'monarch': 11590, 'chartered': 12443, 'integer': 16109, '##ography': 9888, '##mi': 4328, '##av': 11431, 'marxist': 15511, 'brewing': 16005, 'cambridge': 4729, 'finest': 10418, 'stench': 21555, 'metadata': 27425, 'a': 1037, 'simultaneously': 7453, 'concerns': 5936, '[unused387]': 392, '##nr': 16118, 'identified': 4453, 'nashville': 8423, 'poker': 11662, 'encountered': 8567, 'spartans': 24293, 'smoothing': 27045, 'goalscorer': 28602, 'dung': 29328, 'sonar': 24609, 'merchant': 6432, 'prayer': 7083, 'screened': 12238, 'rugby': 4043, 'endelle': 29581, 'seems': 3849, 'compelled': 15055, 'haze': 16332, '##creen': 24410, 'subdivided': 15369, 'deficiency': 18888, 'founders': 8759, 'salesman': 18968, 'helicopters': 12400, 'synthetic': 12553, 'wigan': 15598, '##por': 17822, '##ש': 29812, 'britain': 3725, 'agitated': 21568, '##ryl': 23320, '##trust': 24669, 'but': 2021, 'decorative': 11584, 'hurdles': 18608, 'brooks': 8379, '##swell': 19228, '[unused508]': 513, 'adventure': 6172, 'bread': 7852, 'complexes': 15420, 'nope': 16780, 'tested': 7718, '##pop': 16340, 'halo': 17201, 'bear': 4562, 'waits': 18074, 'peek': 19043, 'emperors': 19655, 'deemed': 8357, '##nged': 21558, 'uncomfortable': 8796, 'conserved': 19995, '海': 1902, 'treasury': 9837, 'remaining': 3588, 'general': 2236, 'jealousy': 14225, 'helen': 6330, '₆': 1553, 'mallory': 20468, '##icative': 25184, '##erina': 26052, '##henko': 19767, 'dixon': 11357, 'firstly': 15847, 'upton': 26900, '##lita': 27606, 'monk': 8284, 'ufo': 24321, 'foolish': 13219, 'technically': 10892, 'reigning': 16323, 'mother': 2388, 'jubilee': 13078, 'acted': 6051, '##isk': 20573, 'proliferation': 20250, 'trough': 23389, 'les': 4649, 'emmanuel': 14459, 'strengthened': 13949, '##rran': 28327, 'accents': 24947, '##sei': 20240, '##koto': 24886, 'restrained': 19868, 'congenital': 27480, 'sims': 18135, 'crowe': 25657, 'body': 2303, '##5': 2629, '1909': 5556, '##nca': 20909, 'discs': 15303, 'raiders': 10642, 'cigarettes': 15001, 'mutant': 15527, 'unfinished': 14342, 'crude': 13587, '##tracted': 24301, 'regulate': 15176, 'ง': 1410, 'vaccine': 17404, '##light': 7138, 'texted': 24637, 'provided': 3024, 'nation': 3842, 'maturity': 16736, 'dyke': 22212, 'motel': 14901, 'muddy': 15405, 'concussion': 23159, 'sites': 4573, 'erratic': 24122, 'arya': 26140, 'drowned': 12805, 'buckled': 26368, '##itors': 27287, 'sidewalk': 11996, '36th': 21460, '\\\\': 1032, 'extensions': 14305, 'alex': 4074, 'aluminum': 13061, 'stunt': 15412, 'guarded': 13802, 'cosmos': 21182, 'fury': 8111, 'andy': 5557, 'ransom': 16540, 'beth': 7014, 'depths': 11143, 'liv': 22135, 'reopened': 11882, 'degree': 3014, 'plants': 4264, 'fellowship': 7881, 'warnings': 16234, 'yeshiva': 22142, '##ி': 29932, 'lazarus': 23623, 'situ': 26179, 'crossbow': 28692, 'shaw': 8233, 'preserve': 7969, '##rize': 25709, 'wielding': 26974, 'grave': 6542, 'grove': 7676, '[unused864]': 869, '##it': 4183, 'upstream': 13909, 'rhone': 22351, 'devotees': 22707, 'faith': 4752, 'ɑ': 1110, '″': 1532, 'wikipedia': 16948, '##ต': 29947, 'subsp': 24807, 'circus': 9661, '[unused970]': 975, 'sirens': 20675, 'conway': 14783, 'overtook': 28920, 'erik': 10240, 'expertise': 11532, '##ined': 21280, '劉': 1777, 'unfolded': 23959, '[PAD]': 0, 'operatives': 25631, 'allegedly': 9382, 'lyrics': 4581, 'souls': 9293, 'olympian': 25977, '##jured': 26949, 'cookie': 17387, 'classes': 4280, 'insulin': 22597, 'projected': 11310, 'actresses': 19910, '##omi': 20936, '##mes': 7834, '##ple': 10814, 'chapman': 11526, 'sided': 11536, '[unused610]': 615, 'power': 2373, '##dating': 16616, '1874': 7586, 'flattened': 16379, 'instruments': 5693, '1854': 8421, 'conqueror': 25466, 'barely': 4510, 'advocacy': 12288, 'ev': 23408, 'hopeful': 17772, 'hume': 20368, '1944': 3646, '##ɑ': 29677, 'sling': 27076, 'attending': 7052, 'morale': 19292, 'jing': 21536, 'outnumbered': 21943, 'enterprise': 6960, 'پ': 1302, 'frightened': 10363, 'fallout': 23902, 'periodicals': 21855, 'ghostly': 24407, '##nge': 15465, 'notebook': 14960, 'sw': 25430, 'idle': 18373, 'hearings': 19153, 'socially': 14286, 'maryland': 5374, '##rst': 12096, '##ates': 8520, 'health': 2740, 'calling': 4214, 'explode': 15044, 'spartak': 28560, '##đ': 29671, 'receptions': 16466, 'hiv': 9820, 'animals': 4176, 'likeness': 28275, 'muffled': 15783, '##ji': 4478, 'omitted': 16647, '##wire': 20357, 'easy': 3733, '##sing': 7741, '##arded': 28037, 'kinds': 7957, '##ropolis': 23248, '##outs': 12166, 'frenchman': 26529, 'penned': 17430, 'lars': 16357, 'chimney': 17321, 'ensured': 16316, '##ße': 17499, 'illustrating': 28252, 'wealthiest': 27809, 'hesitation': 13431, 'leaking': 24325, 'giro': 19226, 'sheet': 7123, 'eventual': 9523, 'icelandic': 14024, 'diagonal': 19754, 'rehabilitation': 11252, 'extending': 8402, '183': 18677, 'milne': 24377, 'betty': 9306, 'million': 2454, 'meadows': 13524, 'shortage': 15843, '##ς': 19579, 'sg': 22214, 'mob': 11240, '[unused847]': 852, '##lub': 27959, 'far': 2521, 'hands': 2398, 'chapter': 3127, 'arching': 27335, 'fitzpatrick': 26249, 'easiest': 25551, 'streamlined': 29445, '##scu': 28817, 'balcony': 11673, 'giant': 5016, '##du': 8566, 'complete': 3143, '##rma': 17830, 'paranormal': 20725, 'closed': 2701, 'rendition': 19187, 'bracelet': 19688, 'antibiotics': 24479, 'exhibits': 10637, 'kindly': 19045, 'jingle': 29262, '##ー': 30265, 'clocks': 20940, '1748': 24445, '##works': 9316, 'stupidity': 28072, 'helena': 10269, 'imaginative': 28575, 'harta': 26283, 'ヒ': 1719, 'arizona': 5334, 'podcast': 16110, 'sighing': 19381, '##orno': 26295, 'genera': 11416, 'temporal': 15850, 'provocative': 26422, 'nutrients': 20435, 'slight': 7263, 'unicorn': 21830, 'haunt': 24542, 'nobody': 6343, 'appropriations': 22713, '##cea': 21456, 'commits': 27791, '##rre': 14343, 'smooth': 5744, 'commenced': 8420, '##enbach': 21409, '##ndt': 26379, '##gling': 18483, '##omo': 19506, 'rushed': 6760, 'commanders': 11437, 'airline': 8582, 'deadline': 15117, 'arbor': 19679, 'ought': 11276, 'grandmaster': 27101, 'motors': 9693, '##ra': 2527, 'ballot': 10428, 'soy': 25176, 'bastards': 21123, 'bristol': 7067, '1948': 3882, 'bad': 2919, 'folklore': 13104, 'sensual': 18753, 'soc': 27084, 'bowling': 9116, 'tray': 11851, '##zd': 26494, 'quantity': 11712, 'fukuoka': 26998, 'partnering': 27001, 'coordinating': 19795, '##text': 18209, 'コ': 1704, 'comply': 14037, 'tuition': 15413, 'buccaneers': 21629, 'beverly': 12218, 'dl': 21469, '451': 28161, '##und': 8630, 'summoned': 11908, 'invention': 11028, 'feathers': 12261, '##vus': 27500, 'lacey': 16355, '##group': 17058, 'hardness': 23608, 'conferences': 9281, '[unused974]': 979, 'vertex': 19449, 'benevolent': 25786, 'homogeneous': 24854, '1865': 6725, '##fles': 28331, 'coverage': 6325, 'arrested': 4727, 'promotional': 10319, 'rockwell': 25235, '##jon': 14339, 'id': 8909, '##raine': 26456, '##ტ': 29989, 'watering': 25813, '##hos': 15006, 'snail': 10879, '##lle': 6216, '109': 11518, 'motown': 22654, 'dinner': 4596, 'prize': 3396, 'banners': 23562, 'folding': 12745, 'larvae': 9673, 'cyril': 16049, 'juliette': 24696, '##ator': 8844, 'correct': 6149, 'fitzgerald': 11864, '##care': 16302, 'lansing': 22304, 'mon': 12256, '##missible': 26770, 'omaha': 12864, 'ℝ': 1579, 'tackles': 10455, 'storey': 11676, 'tuba': 29242, 'occupying': 13992, 'advancement': 12607, '##ulu': 20391, 'arson': 24912, 'ゆ': 1683, 'ortega': 25859, 'undergone': 17215, 'disc': 5860, 'ridges': 16386, 'weakening': 22031, 'filtering': 22910, 'hides': 17382, 'takeoff': 19744, 'eight': 2809, 'merits': 22617, '##gol': 24141, 'mach': 24532, 'permit': 9146, '[unused286]': 291, 'twists': 21438, 'adopt': 11092, '##oom': 17650, 'archaeologist': 18821, '##gers': 15776, 'equally': 8053, '五': 1753, 'ordained': 9492, 'λ': 1165, 'uc': 15384, 'registrar': 24580, 'sailing': 8354, 'freestyle': 9817, 'helmet': 10412, 'accusing': 16723, 'º': 1089, 'napier': 21320, 'valor': 27314, 'likely': 3497, 'genome': 13458, '##sat': 16846, 'provost': 18523, 'trailing': 12542, 'exclude': 23329, '53': 5187, 'rehearsal': 17887, 'enclosed': 10837, 'smash': 15132, '00pm': 27995, '1611': 28769, '[unused56]': 57, 'д': 1184, 'anglia': 24217, 'clements': 28572, 'forgive': 9641, 'tense': 9049, 'curt': 20099, 'absolutely': 7078, '##wd': 21724, '##cute': 26869, '##後': 30372, '##oman': 20778, 'authorized': 9362, 'onwards': 9921, 'seen': 2464, 'sheriff': 6458, 'minded': 13128, 'collaborations': 17437, 'utility': 9710, '1560': 29185, 'conversation': 4512, 'boarding': 9405, 'discovery': 5456, '##resses': 25932, '##tablished': 28146, 'medallion': 22541, 'im': 10047, '##uder': 29190, 'establishments': 17228, '134': 15170, 'missy': 25019, 'ass': 4632, 'kellogg': 26129, 'recounts': 23412, 'fae': 17282, 'trump': 8398, 'fletcher': 10589, 'bothering': 17067, 'racks': 27259, 'decca': 21079, 'praised': 5868, 'sheila': 15062, 'asks': 5176, '[unused709]': 714, 'mace': 19382, 'medicines': 20233, 'cattle': 7125, 'toast': 15174, 'anger': 4963, '##awan': 25903, 'down': 2091, 'clenching': 23970, 'ask': 3198, 'bowen': 16208, 'forbes': 10822, 'enlarged': 11792, 'punches': 17957, 'rehab': 24497, 'civic': 8388, 'hesitantly': 24626, 'warmed': 17336, 'kay': 10905, 'underground': 5230, 'retrospective': 15354, 'milos': 26038, '##長': 30493, 'bot': 28516, 'concert': 4164, 'destructive': 15615, 'observers': 14009, '##mar': 7849, 'searched': 9022, '##yan': 7054, 'jedi': 27273, 'chair': 3242, '##ing': 2075, 'smartphone': 26381, '##lization': 22731, 'decreased': 10548, '##ovo': 16059, 'jenkins': 11098, 'lied': 9828, 'loans': 10940, 'bouquet': 26700, 'sp': 11867, 'impact': 4254, 'actor': 3364, 'clerical': 23106, 'cds': 14340, 'participated': 4194, '##mmon': 24521, '##գ': 29768, '245': 21005, '##hell': 18223, 'avid': 18568, 'battled': 19787, 'oregon': 5392, 'wal': 24547, 'groom': 18087, 'cafe': 7668, 'frequented': 24832, 'kendall': 14509, 'sino': 19432, 'mb': 16914, 'street': 2395, '[unused594]': 599, 'whatsoever': 18971, 'mirza': 18366, 'forefront': 22870, 'inequality': 16440, '亻': 1757, 'cromwell': 16759, 'therese': 25598, 'hays': 29051, 'nyu': 27935, 'regained': 11842, 'affairs': 3821, 'sloan': 19024, 'occupational': 16928, 'lt': 8318, '##ias': 7951, 'tin': 9543, '##iva': 11444, 'ก': 1409, 'educate': 16957, 'wonderful': 6919, 'keystone': 22271, 'heinz': 17655, 'immersed': 26275, 'jul': 21650, '##ati': 10450, 'meek': 28997, 'muller': 12304, 'minute': 3371, '##gies': 17252, 'thessaloniki': 23162, '##治': 30426, 'sped': 16887, 'adoptive': 24787, 'factions': 13815, '##aring': 22397, 'agrees': 10217, 'expects': 24273, 'myanmar': 12620, 'clip': 12528, 'patel': 20455, '##pur': 5311, 'donna': 10972, 'outlying': 25376, '##vies': 25929, 'պ': 1233, '##ち': 30188, '##hu': 6979, 'sails': 17553, 'dublin': 5772, 'build': 3857, '1992': 2826, 'platforms': 7248, '##vite': 25217, 'travis': 10001, 'harden': 28751, 'glaciers': 21193, 'augmented': 19335, 'slovene': 18326, 'haute': 18535, 'fremantle': 20358, '##ond': 15422, 'pneumonia': 18583, 'shaky': 15311, '1656': 28434, 'escape': 4019, 'obe': 15578, 'account': 4070, 'pearls': 21944, 'storyline': 9994, '##vez': 26132, 'communes': 16569, 'watford': 21740, 'however': 2174, '##ʃ': 29696, 'critic': 6232, 'sanctuary': 8493, 'vs': 5443, 'eagerly': 17858, '##nac': 18357, '[unused114]': 119, '##igo': 14031, '##inkles': 28562, '##giving': 23795, 'demetrius': 28367, 'night': 2305, 'hospice': 29277, '##gible': 18507, 'talmud': 24380, 'form': 2433, 'library': 3075, 'li': 5622, '##nington': 23155, 'translating': 22969, '##oz': 18153, '406': 27433, '##ulating': 10924, 'alleviate': 24251, 'baptist': 7550, '10': 2184, 'journalism': 8083, 'kicks': 14590, 'hiram': 27410, '真': 1921, 'fools': 18656, 'ambushed': 22168, 'hemisphere': 14130, 'chase': 5252, '##yal': 21095, 'truly': 5621, 'bell': 4330, 'barbed': 25007, 'cecil': 11978, 'fivb': 28423, 'engagement': 8147, 'shark': 11420, 'puzzled': 14909, 'situated': 4350, 'electoral': 6092, '##岡': 30358, '##ell': 5349, 'cowboys': 11666, 'slaughter': 14574, 'accessible': 7801, 'tay': 28117, 'benton': 18685, 'mitchell': 6395, 'potent': 16834, 'orlando': 10108, 'regions': 4655, 'fatalities': 20871, 'drives': 9297, '##pie': 14756, 'blasted': 18461, '##west': 19650, '[unused989]': 994, '246': 22376, 'ns': 24978, '##gnon': 26977, 'combine': 11506, 'tyne': 17742, '590': 25186, '##bble': 11362, 'inheritance': 12839, '##cam': 28727, 'swamp': 11963, '##ensburg': 28812, 'bf': 28939, 'deal': 3066, 'indicated': 5393, '[unused453]': 458, '竹': 1933, 'lesser': 8276, 'evangelical': 11295, 'calcium': 13853, 'hilary': 22744, '##ʳ': 29708, 'drones': 24633, 'aroma': 23958, 'soothing': 16684, 'averaging': 14985, '32': 3590, 'organ': 5812, '##umber': 29440, 'same': 2168, 'him': 2032, 'suggests': 6083, 'fortified': 13313, '##iste': 27870, 'shepherd': 11133, 'businessman': 6883, '##¶': 29660, 'compromise': 12014, 'ɪ': 1119, 'haryana': 23261, 'whispered': 3990, '198': 20003, 'reformation': 13708, '##ander': 12243, 'canteen': 26449, 'bizarre': 13576, '[unused87]': 88, 'facilitating': 25505, 'vowels': 15260, 'royce': 14789, 'interface': 8278, 'quinn': 8804, 'aqua': 28319, 'pleasantly': 27726, 'trace': 7637, '##ead': 13775, '##zman': 24340, 'blindly': 25734, 'hydro': 18479, 'sugarcane': 28910, '274': 25586, 'treaty': 5036, '##ware': 8059, '##fying': 14116, 'dressing': 11225, 'iced': 28248, 'receptors': 13833, '##と': 30192, '##宿': 30352, 'crisis': 5325, '##skie': 23955, 'bankruptcy': 10528, 'gunnar': 26138, 'ahmedabad': 27249, 'spaced': 19835, 'attempts': 4740, 'myriad': 25028, '##bber': 29325, '##扌': 30385, 'culturally': 20547, '##uis': 27020, '5th': 4833, 'archdeacon': 21742, 'ends': 4515, 'beauty': 5053, 'breaststroke': 28164, 'я': 1210, 'loch': 14941, '[unused288]': 293, 'demolished': 7002, 'implication': 25323, 'relational': 28771, 'dalton': 12413, '有': 1873, 'qatar': 12577, 'yuri': 14331, '##rya': 20444, '##hur': 24572, '##王': 30435, '86': 6564, 'bal': 28352, '##sts': 12837, 'ט': 1249, 'showed': 3662, 'formation': 4195, '[unused267]': 272, '〈': 1637, 'drained': 11055, 'storytelling': 20957, 'denton': 23906, '334': 29562, 'wife': 2564, 'distraction': 14836, 'cemeteries': 20973, 'province': 2874, 'mora': 26821, 'bart': 12075, 'shawn': 13218, 'auspices': 20153, 'mimi': 20705, '##nem': 25832, 'afterward': 9707, 'generating': 11717, 'zebra': 29145, 'karnataka': 12092, 'termination': 18287, 'analyzing': 20253, 'gambling': 12219, 'simulations': 24710, '##ser': 8043, 'defiance': 19674, '##ively': 14547, 'scar': 11228, '##√': 30127, 'conditions': 3785, '##sund': 25168, '##chen': 8661, 'ts': 24529, '[unused643]': 648, 'sometimes': 2823, 'jakarta': 14426, 'nair': 22525, '##神': 30451, 'gaza': 14474, 'titans': 13785, 'paw': 22195, 'civilizations': 24784, 'longevity': 26906, 'militias': 29271, 'martinique': 29365, 'orchestras': 19505, 'rpg': 22531, 'adventist': 25696, 'loren': 28779, '2019': 10476, 'rows': 10281, 'rue': 13413, 'nemesis': 21363, '##ns': 3619, 'potassium': 18044, 'absurd': 18691, '##ped': 5669, 'financially': 13732, 'manpower': 22039, 'manufactured': 7609, 'porn': 22555, '##ual': 8787, 'builders': 16472, 'bernardino': 21370, 'channel': 3149, '##ga': 3654, 'redwood': 27552, 'themes': 6991, '##rch': 11140, 'blows': 13783, 'reich': 14365, '##貝': 30478, 'swear': 8415, 'explodes': 27583, '##hini': 20535, 'proven': 10003, '##maid': 28478, 'camouflage': 21356, '##ugh': 8953, 'objected': 15959, '##uz': 17040, 'fronted': 23291, '258': 24398, 'forearms': 27323, 'reflection': 9185, 'auschwitz': 24363, 'illuminating': 27816, 'congresses': 24460, 'স': 1376, '48': 4466, 'topical': 25665, 'waterford': 17769, 'after': 2044, 'building': 2311, 'downtown': 5116, 'slammed': 7549, '##park': 14432, '##ppa': 13944, 'implicated': 20467, 'sibling': 22941, 'ʒ': 1138, 'sounds': 4165, 'were': 2020, 'educators': 19156, 'cyrillic': 15522, '##eme': 21382, 'vendor': 21431, 'transcription': 14193, 'extremely': 5186, 'weaknesses': 21775, 'mushrooms': 23827, 'northwest': 4514, 'romanian': 7056, 'wesleyan': 19858, 'supplementary': 26215, '##onis': 27296, 'walsall': 29054, 'fate': 6580, 'f1': 20069, 'paulo': 9094, 'disgusted': 17733, '##keeper': 13106, 'ा': 1340, '[unused819]': 824, '000': 2199, 'abuse': 6905, 'gaped': 23832, 'mrs': 3680, 'harsh': 8401, 'apocalypse': 16976, 'regulating': 21575, 'georgian': 9166, 'purchased': 4156, 'captained': 16041, '##tucket': 29315, 'lowering': 13845, 'speeches': 13867, 'crane': 11308, 'confused': 5457, 'northumberland': 16205, 'juice': 10869, 'boiled': 17020, '古': 1789, '།': 1425, 'mayo': 14415, 'charred': 29030, 'eastenders': 28936, 'terms': 3408, 'northwards': 27592, 'role': 2535, 'bt': 18411, '##ii': 6137, 'frances': 10360, 'baghdad': 13952, 'whispers': 11054, 'sonora': 26647, 'vanuatu': 27625, 'matching': 9844, 'roster': 9238, 'mets': 15253, '[unused385]': 390, 'fifteenth': 16249, 'organise': 22933, 'abbreviation': 22498, 'knowles': 22815, '##and': 5685, '##lak': 23451, 'swearing': 25082, 'crouched': 14275, '##ן': 29802, 'raft': 21298, 'enjoys': 15646, 'goodnight': 22708, 'exhibition': 4538, '##ha': 3270, 'pockets': 10306, 'resonance': 17011, 'eastward': 17318, 'face': 2227, 'db': 16962, 'caspian': 25893, '∧': 1602, 'worse': 4788, 'cornered': 25878, 'lodges': 26767, 'externally': 27223, 'repertory': 21099, '─': 1615, 'disk': 9785, 'manhattan': 7128, 'nodding': 11863, 'hurling': 10839, 'crystal': 6121, 'slant': 27474, 'interfering': 24324, '##ivo': 20984, 'ascension': 18071, '##nage': 27031, '##ore': 5686, 'grimace': 25898, 'interfere': 15115, 'zion': 19999, '##jing': 29518, 'policemen': 19809, '##zine': 21254, 'wicket': 12937, 'site': 2609, '202': 16798, '出': 1774, 'weekend': 5353, 'chevrolet': 14724, '##owing': 14138, '169': 18582, 'kicked': 6476, 'vamps': 24064, 'natalia': 21521, 'thanks': 4283, '人': 1756, '##2': 2475, 'robin': 5863, '##ook': 14659, 'barbados': 16893, '##breaker': 21204, '##辶': 30482, 'dietrich': 22567, 'planned': 3740, 'estonian': 12029, 'roe': 20944, 'cowboy': 11762, 'volcanoes': 23694, 'damned': 9636, 'resolve': 10663, 'ethan': 6066, 'suite': 7621, 'authentic': 14469, 'warmer': 16676, '101': 7886, 'worlds': 8484, 'practitioner': 18742, 'shane': 8683, '##ches': 8376, '##parts': 26950, 'tibet': 13319, 'antonio': 4980, '[unused445]': 450, '##dalen': 26414, '##ulent': 27581, '##lings': 11227, 'novak': 19580, 'ponytail': 18865, 'baden': 12189, 'internationals': 27340, 'rector': 10935, 'genetics': 14471, '##¨': 29651, '##を': 30216, 'global': 3795, 'vertigo': 28246, 'conservatory': 11879, 'indifference': 25920, 'emerging': 8361, 'gora': 26967, '##宮': 30350, '1840': 8905, '##nik': 8238, 'crashes': 19119, 'adorned': 19189, '##cytes': 27321, 'magnus': 10045, 'subsistence': 27667, 'posing': 20540, 'gunpowder': 22220, '##か': 30177, '##agne': 25440, '##aghan': 26685, 'rower': 21984, 'philanthropist': 15246, 'უ': 1454, '##ᆼ': 30025, 'sunk': 10417, 'explosive': 11355, 'psychiatry': 18420, 'highly': 3811, 'pavilion': 10531, '##pressing': 24128, 'quarter': 4284, 'budge': 24981, 'butterfly': 9112, 'september': 2244, '46th': 27990, '##в': 25529, 'une': 16655, '##chase': 26300, 'fourth': 2959, 'print': 6140, 'wien': 22782, '##xious': 25171, 'untitled': 24819, 'companion': 7452, '[unused613]': 618, 'attacked': 4457, 'cheaper': 16269, 'warned': 7420, 'sobbing': 20040, 'softball': 12585, 'pumped': 16486, 'footballers': 27784, 'hebrew': 6836, 'digging': 10443, 'investors': 9387, 'musicals': 20103, '##xa': 18684, '##rating': 15172, '[unused141]': 146, 'relieved': 7653, 'cairo': 11096, 'schizophrenia': 23683, 'facilitates': 27777, 'designs': 5617, '##nae': 17452, 'strangled': 21384, '[unused563]': 568, 'para': 11498, 'strides': 22215, 'stocks': 15768, 'youthful': 22446, 'ि': 1341, '[unused76]': 77, 'beautiful': 3376, 'kicking': 10209, 'remorse': 23124, 'locally': 7246, '##sel': 11246, 'lawful': 26410, 'searches': 17193, '##ance': 6651, 'peacekeeping': 28364, '[unused556]': 561, 'einstein': 15313, 'copyright': 9385, 'divided': 4055, 'conclude': 16519, 'resource': 7692, '[unused331]': 336, '##logy': 6483, '##ular': 7934, 'ava': 10927, 'freaking': 13847, '##sner': 20479, 'solomon': 9168, 'beale': 28371, 'deadly': 9252, 'grapes': 16575, '##+': 29622, 'disruption': 20461, 'archway': 28189, 'enjoyment': 20195, 'dwellings': 16707, '##yme': 25219, 'outdoor': 7254, '##m': 2213, 'consolidation': 17439, 'intermittent': 23852, '皇': 1917, 'stimulated': 25194, '[unused812]': 817, 'mastering': 11495, 'gr': 24665, 'comparisons': 18539, 'revealed': 3936, 'sun': 3103, 'richie': 14411, 'sunset': 10434, 'fiesta': 24050, 'cubs': 12469, 'assistants': 16838, 'linen': 17517, 'stalls': 19753, 'guineas': 22385, '##cast': 10526, 'suns': 19352, 'engineers': 6145, 'wives': 10403, 'corey': 18132, 'colour': 6120, 'differently': 11543, 'syrup': 23353, 'spirits': 8633, 'goa': 15244, '##utter': 26878, '##mmer': 15810, 'jaime': 14519, 'accept': 5138, '##wara': 11872, 'socialists': 21633, 'ʋ': 1133, 'thursday': 9432, 'another': 2178, 'roasted': 28115, 'stills': 26105, 'culture': 3226, 'webb': 10923, 'innings': 7202, 'diplomat': 11125, 'haas': 22996, 'torino': 24737, 'sits': 7719, '##tani': 17681, 'cynical': 26881, 'ferrer': 28390, 'townland': 23635, 'le': 3393, 'clary': 13145, '##lists': 27103, '##etti': 18319, '##bone': 14417, '##sl': 14540, 'wasp': 19411, 'ratio': 6463, '##oi': 10448, '##ₗ': 30095, 'landlord': 18196, '##ɹ': 29691, 'buzz': 12610, '##mu': 12274, 'faulty': 28927, 'curves': 10543, 'spear': 12341, 'esa': 28776, 'president': 2343, 'lucian': 18218, 'massif': 24875, 'lucy': 7004, 'buck': 10131, 'burn': 6402, 'beyond': 3458, 'ₚ': 1569, '[unused874]': 879, 'wi': 15536, 'flashbacks': 28945, 'writings': 7896, '##xia': 14787, 'was': 2001, '##hee': 21030, '##rence': 24413, 'competitors': 10159, 'glider': 18788, 'pepsi': 27237, '##tsa': 27110, 'divorce': 8179, 'fleming': 13779, 'minutes': 2781, '##vira': 24093, 'slot': 10453, 'progressively': 20519, '395': 24673, '[unused942]': 947, 'claim': 4366, 'poland': 3735, '##sation': 26652, 'toad': 21344, '[unused520]': 525, 'logan': 6307, '##ை': 29935, 'judo': 19083, 'drug': 4319, '—': 1517, 'anxious': 11480, '1984': 3118, 'richly': 26502, 'novelty': 21160, 'friedrich': 8896, '##ই': 29885, '[unused193]': 198, 'constitute': 12346, 'sniper': 17515, 'assassinate': 25683, 'rand': 14566, 'gottingen': 23607, '##ধ': 29901, '##山': 30357, '1778': 16331, 'different': 2367, 'capturing': 11847, 'mira': 18062, '##村': 30404, 'doors': 4303, 'supplier': 17024, 'щ': 1204, 'shaped': 5044, 'huang': 15469, '##sio': 20763, '##lr': 20974, 'pacing': 15732, '##iente': 25099, '##uous': 8918, 'gideon': 12137, '##plication': 21557, 'persian': 4723, 'midfielder': 8850, 'toyota': 11742, '##ᄌ': 30000, 'fan': 5470, 'moods': 27824, 'chain': 4677, '##区': 30308, 'aid': 4681, '##ष': 29873, 'induced': 10572, 'spp': 26924, '##arts': 20591, 'polk': 20819, 'contested': 7259, 'shanghai': 8344, 'bray': 19743, 'mcgraw': 24179, 'exams': 13869, 'hardwood': 23165, '##jee': 16963, '##lais': 28704, 'tilt': 17010, 'sparkled': 28092, 'performer': 9256, 'activated': 8878, 'reissued': 14805, '268': 25143, 'mystery': 6547, 'swollen': 13408, '##忠': 30378, 'syntax': 20231, 'window': 3332, 'persecuted': 27666, 'adamant': 29502, '##berg': 4059, 'rice': 5785, 'missed': 4771, '##་': 29960, 'arched': 9194, '[unused330]': 335, '##iso': 19565, 'fit': 4906, 'cub': 21987, 'campuses': 13696, 'expressed': 5228, '##nsis': 11745, 'gma': 20917, 'astor': 25159, 'contradictory': 27894, 'obscure': 14485, '##ant': 4630, 'knockout': 11369, 'sized': 7451, 'norton': 10770, '##lten': 29005, '##tones': 11115, 'fresco': 26991, 'mas': 16137, 'job': 3105, 'injury': 4544, 'isis': 18301, '##ʐ': 29702, 'colby': 18650, 'senators': 10153, '##stic': 10074, '64': 4185, '##ria': 4360, '312': 21036, 'fabio': 25616, '##ams': 13596, 'ata': 29533, '152': 15017, '##ch': 2818, 'buzzing': 20386, 'splendid': 21459, 'welding': 23604, 'dogg': 28844, 'cao': 12966, 'magna': 20201, 'barrel': 8460, 'organising': 21317, 'inscribed': 14551, '##lmer': 23398, 'mccall': 25790, 'residency': 14079, 'triassic': 29529, '##lz': 23858, 'peas': 26072, 'lids': 26122, 'jed': 24401, 'acknowledging': 21894, 'covered': 3139, 'conversations': 11450, '##avian': 21654, 'eminent': 14953, 'rolling': 5291, '##у': 29748, 'bulldogs': 15120, 'inevitably': 21268, 'victories': 9248, 'activism': 16841, 'kaufman': 23699, 'strategic': 6143, 'continuation': 13633, 'once': 2320, 'laguna': 18169, 'traveled': 6158, '##day': 10259, 'handcuffs': 28338, 'ж': 1186, 'helene': 20149, 'horribly': 27762, 'edmund': 9493, 'assent': 27195, '##urs': 9236, 'informally': 21858, 'impacted': 19209, '##wad': 26016, 'tottenham': 18127, '##gin': 11528, 'livingston': 14109, '##nx': 26807, 'cheekbones': 27181, '##ᄀ': 29991, 'length': 3091, 'sainte': 16947, 'clothes': 4253, 'branches': 5628, 'swami': 20087, 'uhf': 20131, 'kapoor': 17129, '##ט': 29795, '##gled': 11533, 'spa': 12403, 'preserved': 6560, 'wins': 5222, 'funded': 6787, 'malayalam': 12998, '1890s': 13678, '##umen': 27417, 'capability': 10673, 'ь': 1207, 'explosives': 14792, 'weak': 5410, 'intrigued': 18896, 'south': 2148, 'devil': 6548, 'uncertain': 9662, 'temps': 29023, 'hockey': 3873, 'swelling': 18348, '保': 1766, 'strongest': 10473, 'th': 16215, 'manly': 19385, 'サ': 1705, 'bounced': 13605, 'polls': 14592, '∗': 1598, '##name': 18442, 'mina': 19808, 'fare': 13258, 'essentially': 7687, 'prostitute': 19215, 'towards': 2875, 'comment': 7615, 'helpless': 13346, 'ه': 1297, 'gregor': 16973, 'circumstance': 25652, '##sky': 5874, 'driveway': 11202, 'gps': 14658, 'orson': 25026, 'sincerely': 25664, 'domestically': 27143, 'wee': 16776, 'busch': 15840, '日': 1864, 'dish': 9841, 'experimental': 6388, 'donor': 15009, '##sten': 16173, '##nti': 16778, 'modified': 6310, 'maccabi': 24055, 'clapping': 27104, '##ises': 13087, 'gee': 20277, 'addicted': 23042, 'directional': 20396, 'repeatedly': 8385, 'pretending': 12097, '##う': 30174, 'mirrors': 13536, 'craven': 21232, 'sacred': 6730, 'ib': 21307, 'archbishop': 6507, 'translucent': 22897, 'stealth': 22150, 'rebirth': 22785, 'long': 2146, 'publication': 4772, '##mage': 26860, 'ian': 4775, 'passages': 13768, 'stake': 8406, 'opinion': 5448, 'mixing': 6809, 'soprano': 10430, 'arrangements': 7565, '225': 14993, '##rky': 15952, 'lucan': 20764, 'tentacles': 24719, '##far': 14971, 'midlands': 13256, 'emptied': 21764, 'prey': 8336, 'lopez': 8685, 'sideways': 12579, 'saves': 13169, 'wasted': 13842, 'vedic': 25824, 'dubois': 26258, 'kent': 5982, 'receipt': 24306, 'tb': 26419, 'crises': 25332, 'bastion': 26829, 'separated': 5459, '1655': 28438, 'consisted': 5031, 'inconsistent': 20316, '[unused447]': 452, 'seekers': 24071, '##wang': 16600, 'devon': 7614, 'ო': 1450, '##rta': 13320, 'coaster': 16817, '##s': 2015, '##ivist': 21997, '640': 19714, 'mixer': 23228, '[unused895]': 900, 'unfortunately': 6854, 'splinter': 27546, '##nator': 27413, '名': 1795, 'hui': 17504, 'situations': 8146, '##iable': 19210, 'undergoing': 14996, 'pup': 26781, 'cloak': 11965, 'rejoin': 25261, 'joe': 3533, 'synagogue': 13067, 'garcia': 7439, 'inspirational': 28676, '##楊': 30409, 'readings': 15324, 'grand': 2882, '##istle': 24242, 'deprivation': 29516, 'detained': 14620, '##aster': 24268, 'sequence': 5537, '##bler': 16213, 'chopped': 24881, 'anxiously': 23403, 'ʰ': 1140, '[unused488]': 493, 'parentheses': 27393, 'masks': 15806, 'cobra': 16604, 'aired': 4836, 'swim': 9880, 'mobility': 12969, 'circular': 8206, 'terri': 26568, '##cini': 27085, 'latter': 3732, 'a2': 22441, 'griffiths': 21960, 'speaks': 8847, 'carleton': 22273, 'corner': 3420, 'spelled': 11479, 'fey': 23864, '##ente': 15781, '##bourne': 11634, 'london': 2414, '##iciencies': 28227, 'baltic': 11275, '##©': 29652, 'considering': 6195, '##hui': 20552, '##kh': 10023, 'evidenced': 21328, '##ennial': 22929, '##ci': 6895, 'pause': 8724, 'maison': 26420, 'х': 1200, '##puted': 29462, 'calmly': 12885, 'wax': 13844, 'hale': 13084, 'congress': 3519, 'presenter': 10044, 'terrain': 9291, 'shen': 21882, '##stand': 21515, '##hler': 13620, 'mindanao': 23669, '##taka': 28412, 'priory': 14284, '##ю': 29757, 'scheduling': 19940, 'expenditure': 20700, 'adolescents': 25947, 'revenge': 7195, 'maintenance': 6032, 'deserved': 10849, '##ght': 13900, '[unused707]': 712, '##ᅳ': 30017, '德': 1848, 'flirt': 27978, '[unused383]': 388, 'scarborough': 18603, 'originating': 14802, 'mongol': 17450, 'ezra': 16245, 'ウ': 1696, 'piled': 17835, 'lieutenant': 3812, 'embark': 28866, 'authorization': 20104, '##phile': 24862, 'alexander': 3656, 'immature': 26838, 'czech': 5569, 'excerpts': 24962, '##chia': 20881, '##zumi': 28114, '##laid': 24393, 'levin': 20206, 'dot': 11089, 'baseline': 26163, 'durga': 28746, 'yourselves': 25035, 'wireless': 9949, '##hay': 24180, '##phi': 21850, 'gallons': 18501, '##how': 14406, 'breaking': 4911, '[unused587]': 592, 'election': 2602, 'statutes': 18574, 'invitation': 8468, 'sudden': 5573, '博': 1786, 'maia': 23478, 'teasing': 12216, '##udi': 21041, 'rot': 18672, 'cheek': 5048, 'literally': 6719, 'taps': 25316, '##leton': 19263, 'impressive': 8052, 'identifiable': 27800, 'drops': 9010, 'ap': 9706, 'alsace': 24922, 'ranked': 4396, 'piano': 3682, 'ャ': 1728, 'experienced': 5281, '愛': 1853, 'coffee': 4157, 'anglo': 7819, 'ma': 5003, 'wichita': 18614, '##hend': 22342, 'dail': 26181, 'corrupt': 13593, 'gross': 7977, '##nsen': 29428, 'bop': 29432, '##nna': 9516, 'vocabulary': 16188, 'techniques': 5461, 'airfields': 25278, '[unused303]': 308, '##eller': 24038, 'pontiac': 24538, 'startled': 9696, '##mology': 20570, 'embroidery': 29507, 'charleston': 10907, '[unused391]': 396, 'boca': 22765, 'caressing': 25296, '[unused184]': 189, 'forced': 3140, 'kid': 4845, 'bats': 12236, '##elin': 18809, 'lyricist': 19489, 'aristocratic': 19774, 'reconnaissance': 8967, '[unused301]': 306, 'egypt': 5279, 'hurricane': 7064, 'piazza': 22463, 'urgent': 13661, '##hold': 12640, '##oper': 25918, 'pep': 27233, '##大': 30336, '[unused131]': 136, 'manfred': 19149, 'clap': 28618, 'linguistics': 15397, 'scenarios': 16820, 'dam': 5477, '##ucher': 22368, 'scans': 27404, 'sedan': 15134, '1720': 23535, 'terrorist': 9452, 'vague': 13727, 'elliot': 11759, 'akin': 17793, '##zquez': 22938, 'leaned': 4016, '##osed': 24768, 'celebrate': 8439, 'formulas': 25814, 'preceding': 11003, '[unused897]': 902, '[unused471]': 476, '##yev': 17240, 'notoriety': 23215, 'ajax': 18176, 'giants': 7230, 'kidding': 12489, 'di': 4487, 'cameron': 7232, 'poster': 13082, 'medicinal': 20632, '##ici': 28775, '##अ': 29847, '[unused771]': 776, 'openings': 16556, '##ग': 29853, '##⁻': 30079, 'wholly': 12590, 'gabe': 12900, '##ᅴ': 30018, 'jockey': 13989, 'talent': 5848, 'chesapeake': 20867, 'extensive': 4866, 'cooperative': 10791, '##orne': 23846, 'mari': 16266, 'pleasing': 24820, 'populations': 7080, 'protein': 5250, 'soundtrack': 6050, 'oilers': 19778, '435': 24125, 'betting': 19244, 'steaming': 19986, 'doctrines': 23252, 'dioceses': 26586, '##unk': 16814, 'ɴ': 1124, 'desperately': 9652, '##grin': 24860, '##hopper': 27330, 'attain': 18759, 'grammatical': 24402, 'armistice': 19125, '##dge': 11818, 'baby': 3336, '##tten': 25970, '##edly': 26207, 'clung': 14752, 'authoritative': 23949, 'metaphor': 19240, 'cock': 10338, '##gar': 6843, 'ɾ': 1126, 'sweatshirt': 28095, 'nbl': 28013, 'measurements': 11702, 'weir': 16658, 'neighborhood': 5101, 'lift': 6336, 'attendees': 19973, '[unused609]': 614, '##rica': 14735, 'hydrogen': 9732, 'honour': 6225, '##ues': 15808, 'carrier': 6839, '343': 27810, '##⅓': 30109, 'forehead': 6130, 'combines': 13585, 'metz': 24424, 'crete': 19111, '：': 1993, 'cancers': 25409, 'gamma': 13091, '##lter': 21928, 'harrow': 24560, '##ople': 27469, 'shoulders': 4065, 'adults': 6001, 'primaries': 27419, '##rring': 18807, 'jurist': 22757, 'flows': 6223, 'numerous': 3365, '##ao': 7113, '##mous': 27711, '##ア': 30219, '1774': 17593, 'researched': 18800, 'select': 7276, 'け': 1654, 'reynolds': 9579, 'moody': 14434, 'gods': 5932, 'revolt': 10073, 'slate': 12796, 'recycled': 22207, '##ssel': 21218, 'beaufort': 23622, 'shrugs': 23822, 'clover': 25133, 'kruger': 27823, 'existed': 5839, 'balanced': 12042, 'quiver': 29049, 'discussed': 6936, 'bloc': 15984, 'maitland': 25511, 'elimination': 9614, 'wholesale': 17264, 'salvage': 18340, '##rge': 20800, '400': 4278, 'ter': 28774, 'elaborate': 9603, 'paid': 3825, 'skilled': 10571, 'ी': 1342, '##connected': 24230, 'reason': 3114, 'valley': 3028, '##duction': 16256, '火': 1906, 'quartermaster': 24688, 'condo': 25805, 'types': 4127, '1000': 6694, 'gaze': 3657, '[unused302]': 307, 'ro': 20996, 'jens': 25093, 'conspiracy': 9714, '##jar': 16084, 'weighed': 12781, 'codex': 15763, 'keel': 19602, 'directing': 9855, 'listing': 10328, '##vio': 25500, 'glands': 23340, '203': 18540, 'past': 2627, 'anne': 4776, 'neck': 3300, 'drill': 12913, '##cton': 28312, 'easton': 21636, 'motivated': 12774, 'retorted': 24056, 'steps': 4084, 'frantically': 16460, '##monium': 26387, 'certified': 7378, 'averaged': 11398, 'weimar': 20695, '##plicity': 27293, 'grammy': 8922, 'recognizes': 14600, 'japan': 2900, '##₩': 30101, 'examine': 11628, 'botanist': 17098, 'wrath': 14532, 'jam': 9389, 'renaissance': 8028, 'ernst': 10728, 'spontaneous': 17630, 'bi': 12170, 'vulgar': 29364, 'elisabeth': 12877, '##lary': 28221, 'killing': 4288, 'busted': 23142, 'gained': 4227, 'button': 6462, 'kepler': 28219, 'prepared': 4810, 'duet': 11979, 'wheeler': 12819, 'stomped': 21918, 'georges': 10870, 'chance': 3382, 'friendship': 6860, '[unused975]': 980, 'lbs': 20702, '##path': 15069, 'pair': 3940, 'clouded': 26761, 'fines': 21892, 'fired': 5045, '1849': 9037, 'manson': 21440, '⁹': 1543, 'dean': 4670, '[unused839]': 844, 'katherine': 9477, 'natal': 17489, 'castillo': 19371, '##arding': 29154, 'proceeds': 10951, 'phyllis': 20328, '##ive': 3512, 'layered': 21323, 'fitness': 10516, 'afro': 17694, '##dman': 21804, '##tyle': 27983, 'newly': 4397, 'fighters': 7299, 'spray': 12509, 'clara': 10254, 'hunting': 5933, 'careers': 10922, '##ht': 11039, 'doctors': 7435, '##sdale': 15145, '♠': 1623, '##沢': 30424, 'inexperienced': 26252, 'recorded': 2680, 'grew': 3473, 'papua': 13049, '1843': 10075, 'draw': 4009, 'saliva': 26308, 'tension': 6980, 'battery': 6046, '##moral': 22049, '[unused877]': 882, 'crystalline': 24628, 'catalonia': 16711, 'moors': 24812, '##grants': 27444, '##gong': 17036, '##な': 30193, '##kur': 18569, 'baird': 20866, '[unused716]': 721, '##ব': 29904, 'thunder': 8505, '##we': 8545, 'linda': 8507, 'coroner': 22896, 'supervise': 28589, '##nais': 28020, 'monsoon': 19183, 'fooled': 25857, 'armagh': 24678, 'creativity': 14842, 'generator': 13103, 'snakes': 12971, 'sportsman': 27168, 'worthless': 22692, '##dic': 14808, 'come': 2272, 'jacob': 6213, 'conrad': 10931, 'finch': 16133, 'holm': 28925, '##ия': 23483, '##udeau': 27627, 'phenomena': 13352, '##ん': 30217, 'diffusion': 19241, 'colombian': 13598, 'examples': 4973, '##rad': 12173, 'elephant': 10777, 'blend': 12586, 'substances': 13978, '1900s': 16430, 'cornwall': 10387, 'perpetrators': 27675, 'tactics': 9887, 'sheer': 11591, 'pump': 10216, 'creative': 5541, 'denominations': 17384, 'happily': 11361, '##patient': 24343, 'algorithms': 13792, 'crow': 11465, 'barbara': 6437, 'fuzzy': 18001, 'opponents': 7892, 'roads': 4925, 'ren': 14916, 'jacqueline': 17551, 'rani': 21617, 'falsely': 23123, '##sight': 25807, '##bib': 28065, '##小': 30355, 'healed': 15027, 'fowler': 14990, '##エ': 30224, 'cool': 4658, '1926': 4881, 'blake': 6511, 'pollock': 25218, 'tracey': 25984, 'uncommon': 13191, 'marries': 19941, '255': 20637, '##ⁿ': 30080, 'gus': 12670, 'kingston': 9803, 'constituency': 5540, 'mantra': 25951, 'buildings': 3121, 'temperatures': 7715, 'drilling': 15827, 'foundry': 19853, '##ans': 6962, 'thumb': 7639, 'cheering': 24867, 'q': 1053, 'islamic': 5499, 'retail': 7027, 'warlord': 16579, 'senator': 5205, 'stroll': 27244, '[unused692]': 697, 'spherical': 18970, '##ounded': 26240, 'apologies': 25380, 'trail': 4446, 'dwell': 23120, 'ligue': 18374, 'gum': 16031, 'college': 2267, 'reality': 4507, 'undertook': 12543, '##ज': 29855, 'shaggy': 25741, 'minions': 28071, '##idium': 28742, 'rising': 4803, '##ignment': 24838, '##ction': 7542, 'printers': 23557, 'preferred': 6871, 'passion': 6896, 'irrigation': 12442, 'mariners': 18049, 'cylindrical': 18797, 'thus': 2947, 'dissolved': 8314, 'full': 2440, 'inversion': 28527, 'america': 2637, 'mer': 21442, 'gazes': 23514, 'camping': 13215, '680': 23944, '##θ': 29725, 'shamrock': 28782, 'evacuation': 13982, '##star': 14117, '学': 1817, 'switch': 6942, 'mantle': 16019, '##弘': 30369, '##rian': 6862, '##uj': 23049, '##nbc': 28957, 'robotic': 20478, 'michel': 8709, 'developmental': 13908, '##ove': 21818, 'seeking': 6224, 'spitting': 24307, 'shot': 2915, '##sian': 17043, '##37': 24434, 'assignment': 8775, '##ees': 10285, 'quicker': 19059, 'graph': 10629, 'bestseller': 24304, 'realizing': 9301, 'pasha': 14292, '[unused292]': 297, 'ling': 17002, 'manufacturers': 8712, '##uting': 20807, 'moment': 2617, 'paper': 3259, 'bois': 19651, 'pop': 3769, '##®': 29656, 'eugene': 8207, '##nad': 25389, 'thrust': 7400, 'ingredients': 12760, 'alliances': 21277, 'forsyth': 29434, 'griffith': 14135, 'columbia': 3996, 'counterpart': 13637, '2003': 2494, 'evil': 4763, 'desperate': 7143, 'display': 4653, '##cies': 9243, 'drip': 27304, 'philology': 29286, '##18': 15136, 'doc': 9986, 'layla': 19786, '[unused805]': 810, '##ken': 7520, '##rou': 22494, 'organizer': 19012, 'conclusions': 15306, '1803': 12651, 'storing': 23977, 'ы': 1206, 'status': 3570, 'gail': 18576, 'somme': 25158, 'drinking': 5948, 'reduction': 7312, '##lets': 13461, '##skaya': 23070, 'boutique': 24611, 'chancel': 16482, 'drum': 6943, 'steve': 3889, 'challenging': 10368, 'lina': 27022, '##chrome': 20366, 'anya': 21728, '[unused637]': 642, 'registered': 5068, '87': 6584, 'arbitration': 18010, 'forewings': 13211, '[unused533]': 538, 'surprises': 20096, 'tnt': 24048, 'intuitive': 29202, '##კ': 29982, 'comes': 3310, 'congressman': 12295, 'cambrian': 29228, 'dramas': 16547, 'charges': 5571, 'attained': 12754, '##ppet': 29519, 'havoc': 22156, 'zur': 17924, 'tender': 8616, 'potion': 26722, '##hum': 28600, 'dyer': 23494, 'scenes': 5019, 'ⱼ': 1631, 'skater': 18815, '[unused770]': 775, '##rip': 29443, 'ray': 4097, '##uan': 13860, '##ning': 5582, 'como': 18609, 'ᅮ': 1478, '##₃': 11622, 'archers': 23118, 'warp': 24136, 'region': 2555, 'gases': 15865, '##ano': 6761, 'iqbal': 28111, '##た': 30187, 'devlin': 24389, 'เ': 1423, '[unused968]': 973, 'weeping': 19750, '##8': 2620, 'fantasy': 5913, '##dev': 24844, '##ju': 9103, '294': 28135, 'faster': 5514, 'extinct': 8548, '##oso': 19137, '##cl': 20464, 'ambient': 17093, 'steak': 21475, '##aja': 22734, '##о': 14150, 'frown': 11330, 'lifespan': 26462, 'reyes': 12576, '##owski': 15249, 'vols': 18709, 'tissue': 8153, 'drove': 5225, 'no': 2053, 'electricity': 6451, '##jun': 19792, 'gaga': 23332, 'cavaliers': 25096, '##fra': 27843, '##安': 30346, '[unused906]': 911, 'ʻ': 1145, 'tanned': 25973, 'spiral': 12313, 'counterparts': 14562, 'entire': 2972, 'bombardment': 10400, '##san': 8791, 'domination': 17882, 'plaza': 8232, 'mighty': 10478, 'catches': 11269, 'disciplinary': 17972, '##tas': 10230, 'eyelids': 16544, 'catchment': 22865, '##ota': 17287, 'landau': 28570, 'liter': 23675, 'umpire': 20887, 'boost': 12992, '##dom': 9527, 'supplemented': 20585, 'few': 2261, 'straightened': 11168, 'distinctions': 25995, '##nent': 21576, 'raced': 8255, 'checkpoint': 26520, 'litre': 16812, 'archaeology': 12009, 'fledgling': 25954, '##শ': 29910, 'ratios': 21879, '##vna': 29207, 'counterattack': 27835, '##lla': 4571, 'cedar': 11354, '##|': 29641, '##erine': 24226, 'refreshing': 27150, 'secondly': 16378, 'uv': 23068, 'bio': 16012, 'bobo': 27418, '##wil': 29602, 'fitted': 7130, '213': 19883, 'aiding': 24791, '##itzer': 19253, '266': 25162, 'muriel': 27616, 'pasture': 20787, 'along': 2247, 'newcomer': 16866, 'randolph': 13031, '##xie': 16898, 'vigorous': 21813, 'subcontinent': 26125, '##band': 12733, 'minimize': 18478, '##rga': 28921, 'col': 8902, '2': 1016, 'roi': 25223, 'policeman': 14460, 'pilgrims': 17992, '[unused631]': 636, '##nu': 11231, 'meridian': 17984, 'growling': 22413, 'stiffly': 27499, 'telecommunications': 12108, 'directly': 3495, 'viewpoint': 21386, 'lithium': 22157, 'nick': 4172, '##llie': 23697, 'goodwin': 19928, 'capita': 8353, 'bangladeshi': 24267, 'planners': 27155, '##urgent': 27176, 'notice': 5060, '##bm': 25526, 'joined': 2587, '[unused18]': 19, 'lying': 4688, 'fields': 4249, '##minated': 26972, 'hackett': 28630, 'habits': 14243, 'mineral': 9754, '事': 1751, '##nikov': 22576, 'assignments': 14799, 'rv': 27634, 'katrina': 16864, 'groningen': 29070, 'bidding': 17534, 'google': 8224, 'turing': 28639, 'pose': 13382, 'detectives': 18145, 'smashed': 14368, 'devised': 14917, 'roar': 11950, '1892': 6527, '##ggs': 21314, 'meteor': 23879, 'australian': 2827, '##jected': 24455, 'failing': 7989, '172': 18253, 'embroidered': 23590, '[unused726]': 731, 'progress': 5082, '1689': 22685, 'xander': 29019, '##aldo': 27318, 'mammoth': 23714, '##bbling': 15343, 'greeting': 14806, 'motives': 17108, '17th': 5550, 'weaving': 15360, '##dora': 24562, 'dime': 27211, 'costello': 21015, 'buttons': 11287, '##ishing': 21837, 'cleared': 5985, ';': 1025, 'egyptians': 23437, '↦': 1588, 'none': 3904, 'nicholson': 16955, 'hike': 21857, 'margarita': 24570, '61': 6079, 'nursery': 13640, 'puget': 27879, '##lp': 14277, 'karlsruhe': 26660, 'experimenting': 23781, 'stephenson': 19789, 'protest': 6186, 'cupping': 25076, 'describing': 7851, 'reform': 5290, 'entertained': 21474, '〜': 1645, 'westfield': 26584, 'gazette': 11391, 'prefix': 17576, 'subcommittee': 18052, '¹': 1088, 'curl': 15390, 'discourse': 15152, '##鈴': 30491, 'shouts': 15701, 'develops': 11791, 'atrium': 26204, '256': 17273, 'kansas': 5111, 'missiles': 10815, 'の': 1671, 'remained': 2815, 'pietro': 15541, 'well': 2092, 'ice': 3256, 'caledonia': 19305, 'monroe': 9747, 'nuremberg': 19346, 'sprawled': 21212, '##lkirk': 23913, '##タ': 30235, 'likened': 28834, 'mexican': 4916, 'intensely': 20531, 'della': 8611, '##ˤ': 29719, 'kashmir': 13329, 'brig': 16908, '##auer': 21126, 'crosby': 14282, 'hari': 21291, 'parental': 18643, 'inexpensive': 23766, 'magdalene': 26890, 'tidy': 29369, 'across': 2408, 'chewed': 18362, 'spectroscopy': 25458, 'links': 6971, 'left': 2187, '##bry': 25731, 'bmw': 13154, '1672': 27253, 'jurisdiction': 7360, 'teaching': 4252, 'stated': 3090, 'draft': 4433, 'racecourse': 15948, 'generation': 4245, 'electrification': 23679, '[unused217]': 222, 'you': 2017, 'of': 1997, '##ble': 3468, 'throat': 3759, '##ettes': 26592, 'six': 2416, 'employing': 15440, 'imprint': 15738, '##sau': 23823, 'survive': 5788, '##metric': 12589, '##サ': 30231, 'backed': 6153, 'hundred': 3634, 'historically': 7145, '##eres': 18702, 'youtube': 7858, 'marketed': 11625, '1869': 7845, '##27': 22907, 'endorsement': 20380, 'lodging': 26859, 'classics': 10002, '##ph': 8458, 'reelection': 17648, '##rance': 21621, '##メ': 30252, '##agi': 22974, '##cum': 24894, '##tenberg': 21806, 'beverley': 29057, '##থ': 29899, 'symbolism': 22050, 'lung': 11192, 'shea': 16994, 'brad': 8226, 'cookies': 16324, 'conquer': 16152, 'testing': 5604, 'project': 2622, '奈': 1814, 'stafford': 15064, '##rz': 15378, 'occur': 5258, 'ն': 1231, 'malone': 16321, 'principals': 27928, 'gets': 4152, 'pius': 14363, 'authorities': 4614, 'compatibility': 21778, 'delicate': 10059, 'vehicles': 4683, 'seymour': 13254, 'wiener': 25072, '##ophone': 25232, 'darby': 25844, '##kha': 15256, 'notified': 19488, 'stiffened': 16090, 'paperwork': 17397, 'hill': 2940, 'mali': 16007, 'marko': 28003, 'except': 3272, '##llon': 22179, 'midsummer': 28171, 'passports': 19494, '##vet': 19510, 'actual': 5025, 'evacuated': 13377, 'all': 2035, 'adrenaline': 14963, 'artifact': 20785, '##tness': 27401, '##sir': 29481, 'terminus': 7342, 'bernstein': 18862, 'jd': 26219, '##ization': 3989, 'wrapped': 5058, 'planets': 11358, 'recognizing': 14622, '[unused578]': 583, 'resulted': 4504, 'flesh': 5771, 'ulysses': 22784, 'moroccan': 17494, 'shallow': 8467, 'truth': 3606, 'cheeks': 6029, '##mina': 22311, 'synthesized': 23572, 'accumulate': 27598, 'ollie': 25208, '##out': 5833, 'sevens': 19463, 'supermarkets': 26676, '1651': 29077, '##bu': 8569, '[unused248]': 253, 'queens': 8603, '##founded': 21001, '##gill': 19791, 'vincenzo': 24712, '##oun': 23709, '##lock': 7878, 'turn': 2735, '209': 19348, 'simpson': 9304, 'casualties': 8664, 'invariant': 23915, 'eyebrows': 8407, 'pascal': 17878, 'industrial': 3919, 'colon': 16844, 'irvine': 16272, 'exemption': 19621, '##dles': 27822, 'functioned': 20903, 'abby': 9460, '##nessy': 25441, 'neoclassical': 22657, 'progressing': 27673, '##lander': 22691, 'cory': 18342, 'architecture': 4294, '[unused661]': 666, 'pandora': 19066, 'des': 4078, 'writers': 4898, 'con': 9530, 'selects': 27034, 'fold': 10671, 'stomach': 4308, 'country': 2406, 'being': 2108, 'updated': 7172, 'dissipated': 22572, '上': 1742, 'tian': 23401, 'smarter': 25670, 'meters': 5563, 'unionist': 17104, 'somerset': 9198, 'greenhouse': 16635, 'ljubljana': 21588, '##kala': 26907, 't': 1056, 'crushing': 14527, 'lays': 19764, 'filling': 8110, 'demonstrators': 28337, '[unused6]': 7, 'bedrock': 28272, 'heidi': 21372, '∘': 1599, '山': 1831, '##ure': 5397, 'enlist': 28845, 'urban': 3923, 'optional': 11887, 'avoiding': 9992, 'pleistocene': 25080, 'lake': 2697, '##wal': 13476, 'rd': 16428, '4th': 4343, 'unbearable': 24257, 'loses': 12386, 'battered': 17548, 'covent': 29456, '##typic': 23186, 'clever': 12266, 'nrl': 20686, 'צ': 1263, '##ulated': 8898, '1653': 29309, 'stalker': 23883, 'hurt': 3480, 'habsburg': 17486, 'bourgeois': 22846, '##mania': 27010, 'manga': 8952, 'kind': 2785, 'relaxed': 8363, 'emperor': 3750, 'jon': 6285, 'translated': 5421, 'apple': 6207, 'tod': 28681, 'ant': 14405, '##wc': 16526, '##的': 30442, 'wisconsin': 5273, 'equipment': 3941, 'reflects': 11138, 'proclaimed': 10116, 'skull': 7412, 'dusty': 12727, 'pointer': 20884, 'abigail': 15983, 'crawley': 28854, 'jackets': 17764, 'disorder': 8761, '##vao': 24682, 'origins': 7321, '##pina': 26787, 'boxers': 18508, 'sovereignty': 12601, '##icular': 21412, 'heavyweight': 8366, 'evacuate': 22811, 'alistair': 23752, 'releasing': 8287, '##·': 29661, 'blended': 19803, 'combinations': 14930, 'emitted': 22627, '##qua': 16211, 'governors': 11141, 'boer': 19945, 'elgin': 23792, 'hudson': 6842, '[unused946]': 951, 'bathed': 24614, '1804': 13140, 'aka': 9875, 'rigorous': 20001, '##bt': 19279, '[unused585]': 590, 'twelfth': 11313, '##feld': 8151, 'simulation': 12504, '##mma': 14760, 'insurgents': 20541, 'limbs': 10726, '144': 14748, 'delighted': 15936, 'iaaf': 21259, 'masonry': 18103, 'fallon': 16443, '朝': 1874, 'stable': 6540, 'wire': 7318, '##kor': 21815, '##ʾ': 29713, '##ete': 12870, 'collaborated': 8678, '##pm': 9737, 'progression': 14967, 'reply': 7514, '##alia': 22786, 'undisclosed': 18206, 'lime': 14123, 'methane': 24481, '11th': 6252, 'membership': 5779, 'virtue': 11870, 'also': 2036, 'apostolic': 11815, 'both': 2119, 'neville': 14871, 'bananas': 26191, 'relegation': 9591, 'coordinator': 10669, 'prayed': 14283, '##hism': 23108, '##rke': 25074, '##gny': 19393, 'transfer': 4651, '[unused820]': 825, 'penal': 18476, 'predatory': 21659, 'waking': 12447, 'professorship': 22661, '##mur': 20136, 'unitarian': 25477, 'ʳ': 1142, 'terminated': 12527, 'methods': 4725, 'legion': 8009, '1831': 10937, 'her': 2014, 'flashlight': 15257, 'distinctly': 19517, 'introduces': 13999, 'triangle': 9546, 'guests': 6368, 'dictionary': 9206, 'sen': 12411, 'summarized': 22539, '##ably': 8231, 'delta': 7160, 'poznan': 18320, '³': 1083, '##hea': 20192, 'here': 2182, 'census': 2883, 'install': 16500, 'bobby': 6173, 'ひ': 1673, 'staggered': 14648, 'coral': 11034, 'functions': 4972, 'maxwell': 10691, '8th': 5893, 'repay': 24565, 'essendon': 25149, 'qualities': 11647, '₊': 1557, 'violin': 6710, 'outlawed': 29131, 'ascending': 22316, 'grasses': 21620, '=': 1027, '郎': 1958, 'silence': 4223, 'withdrawal': 10534, '##comb': 18274, '##yad': 25152, 'canucks': 26177, '##amps': 25167, 'aircraft': 2948, 'persisted': 19035, '##tech': 15007, 'ny': 6396, 'alain': 15654, 'differentiation': 20582, 'nitrogen': 14114, 'settled': 3876, '##rsa': 22381, 'regis': 20588, 'summary': 12654, 'department': 2533, 'detachments': 29568, '[unused3]': 4, '737': 22061, 'prompted': 9469, 'runes': 29161, '050': 28714, '##hora': 16977, 'feng': 19004, 'excellence': 8012, 'pardon': 14933, 'diamonds': 11719, 'theologians': 29013, 'mythical': 19336, 'until': 2127, 'concealed': 14091, 'offspring': 13195, 'த': 1385, 'থ': 1363, '##wife': 19993, 'rattle': 23114, '##feng': 24383, '##iving': 14966, 'shrewsbury': 18174, 'posse': 25751, 'chichester': 23406, 'khyber': 28416, '##ose': 9232, 'trouble': 4390, 'tons': 6197, 'colleges': 6667, '[unused802]': 807, 'jenna': 13504, 'patio': 19404, 'cupped': 13740, 'ভ': 1369, 'sue': 9790, 'abu': 8273, 'headmaster': 16296, 'vito': 25550, '##houses': 15666, 'programmer': 20273, '25': 2423, '##burgh': 15496, 'proposition': 14848, '##atus': 15590, '238': 22030, '##ე': 29978, 'caution': 14046, '##հ': 29775, 'contributions': 5857, 'moshe': 27123, 'shouting': 11273, 'cruisers': 16722, '[unused92]': 93, 'fortifications': 14507, 'townsville': 27492, 'comprising': 9605, 'terrified': 10215, '##in': 2378, 'kari': 27356, '##kee': 20553, 'towing': 28379, 'inevitable': 13418, 'indonesia': 6239, 'squares': 14320, 'packaging': 14793, 'trivial': 20610, '1711': 28455, 'flexed': 24244, 'slowing': 18068, 'rudd': 25298, '##7th': 27506, '##kulam': 28871, '1853': 8933, 'rama': 14115, 'alejandro': 16810, 'seed': 6534, 'feeds': 14172, 'শ': 1374, '[unused977]': 982, 'arab': 5424, 'av': 20704, 'bobbed': 29579, 'florida': 3516, '正': 1888, 'traveler': 20174, '##ovic': 9142, '##sor': 21748, 'resembling': 15525, 'mechanisms': 10595, 'friends': 2814, 'hermann': 12224, 'interfered': 28976, 'geared': 23636, 'tempest': 22553, 'spawned': 18379, 'appointment': 6098, 'dempsey': 28165, 'wound': 6357, 'french': 2413, 'flames': 7311, 'allowing': 4352, 'render': 17552, 'nightstand': 23135, 'thanking': 28638, 'mounted': 5614, 'swore': 12860, 'bikini': 20345, 'calhoun': 22982, 'neuroscience': 23700, 'govt': 22410, '##lett': 20897, '[UNK]': 100, 'metacritic': 14476, 'detected': 11156, 'astros': 22058, 'epa': 19044, '##nare': 26148, 'austrian': 6161, 'loads': 15665, '##llation': 20382, 'guyana': 18786, 'hopped': 17230, 'maxi': 21510, 'begin': 4088, 'carnegie': 11298, '##cta': 25572, '##eh': 11106, 'cliff': 7656, 'particle': 10811, '[unused257]': 262, '[unused314]': 319, '##lie': 8751, 'flashes': 16121, 'blouse': 18149, 'emigration': 20387, 'sane': 22856, 'noodles': 27130, '##para': 28689, 'clears': 28837, 'histories': 15215, 'arteries': 28915, '##uth': 14317, 'outskirts': 12730, '>': 1028, 'zhou': 14367, '##torium': 24390, 'ந': 1386, 'judiciary': 14814, '##ছ': 29893, 'father': 2269, 'lengths': 10742, '##cross': 16458, 'memories': 5758, 'cumulative': 23260, 'hacker': 23307, 'dictator': 21237, '##32': 16703, 'simeon': 24371, 'faint': 8143, 'dance': 3153, 'earls': 27371, '##hop': 18471, 'dramatic': 6918, 'donald': 6221, '##ching': 8450, '##sop': 28793, '[unused459]': 464, '##6': 2575, 'percival': 27832, 'coefficient': 19064, 'renewal': 14524, 'enjoy': 5959, 'start': 2707, 'ld': 25510, '1913': 5124, 'highest': 3284, '[unused591]': 596, 'newbury': 29457, '##strom': 15687, '##jou': 23099, '##lated': 13776, 'keating': 25865, 'ł': 1105, 'remixes': 15193, 'sacked': 14159, 'mustafa': 19683, 'stamford': 22469, '##cao': 20808, '##dice': 24598, '[unused992]': 997, 'koln': 28306, 'somber': 28587, 'leaf': 7053, '28th': 15538, 'polynomials': 28175, 'barony': 19365, '##iard': 14619, 'rockies': 22366, '##z': 2480, '45': 3429, 'needy': 23927, 'reese': 15883, 'jonathan': 5655, '[unused429]': 434, '##ы': 29113, '322': 23768, 'kitchens': 26051, '800': 5385, 'cement': 11297, 'captains': 15755, 'avoid': 4468, '##cous': 27199, '97': 5989, 'ᵏ': 1501, 'glad': 5580, '[unused119]': 124, 'informal': 11900, 'corinne': 26879, '338': 27908, '##iness': 9961, 'proteins': 8171, 'cathy': 18305, 'crowded': 10789, 'aggregator': 24089, 'learns': 10229, 'sticks': 12668, '[unused862]': 867, 'ranks': 6938, 'disco': 12532, 'intellectuals': 17412, 'stockport': 26284, 'в': 1182, 'shootout': 18297, 'contentious': 29308, 'kota': 23856, 'recommend': 16755, 'throne': 6106, 'contradiction': 26917, 'bind': 14187, 'renamed': 4096, 'guerrero': 16938, 'ய': 1390, 'afghanistan': 7041, '##tain': 18249, 'ramsay': 19460, 'mammals': 11993, 'jew': 16522, 'stakeholders': 22859, 'nepali': 23418, '夏': 1808, 'profile': 6337, 'fragmented': 26872, 'tasmanian': 21394, 'sixth': 4369, 'staring': 4582, 'bark': 11286, 'description': 6412, '[unused759]': 764, 'reward': 10377, 'coinage': 25396, 'rider': 7945, 'et': 3802, 'harmonic': 19452, '##十': 30309, 'nurse': 6821, '##shaw': 17980, 'unemployed': 18787, '##carriage': 21539, '##ishment': 21808, 'gibbons': 22194, '##mith': 19864, 'bays': 13933, '##dek': 24463, 'diameter': 6705, 'whitley': 27007, 'freshman': 10452, 'absorption': 16326, '##cht': 10143, 'newmarket': 22489, 'tim': 5199, 'τ': 1174, '##lar': 8017, 'bride': 8959, 'kaladin': 22588, '##bbed': 15499, 'parochial': 28773, 'shown': 3491, 'mood': 6888, '[unused491]': 496, 'awkward': 9596, 'toxic': 11704, '##lene': 11474, 'talon': 19048, 'units': 3197, 'asserts': 19514, '##rse': 22573, 'sidewalks': 28386, 'goo': 27571, 'jailed': 21278, 'figures': 4481, '##yala': 28617, 'python': 18750, 'castle': 3317, 'wan': 14071, 'lamp': 10437, 'fife': 20537, 'vines': 16702, 'dancer': 8033, 'balkans': 19733, '秋': 1929, 'gustavo': 24801, 'required': 3223, '##mart': 22345, 'sao': 7509, 'awards': 2982, 'obstruction': 27208, 'sessions': 6521, 'ᅢ': 1471, '│': 1616, 'currie': 20667, 'currency': 9598, '##ioned': 19798, 'jennie': 27557, '[unused264]': 269, '[unused635]': 640, '語': 1950, 'ep': 4958, 'unpredictable': 21446, '##ganj': 22738, 'articulated': 20742, 'deploy': 21296, 'markings': 13967, 'ら': 1685, 'marketplace': 18086, 'gasps': 23813, 'loomed': 24358, 'ut': 21183, 'member': 2266, 'vested': 26003, 'amend': 27950, '[unused328]': 333, '##ciency': 29125, 'transmitted': 11860, 'part': 2112, 'frame': 4853, 'ם': 1254, 'extravagant': 27856, '##tino': 25690, 'interpret': 17841, '##rc': 11890, 'fatima': 27596, '##高': 30507, 'jackie': 9901, 'europe': 2885, 'troubled': 11587, '##rization': 26910, '1812': 9842, 'prc': 26141, 'congressional': 7740, 'raise': 5333, '##osomal': 27642, 'convert': 10463, 'notions': 21951, 'fluent': 19376, 'antarctica': 12615, 'processors': 18017, 'continual': 27222, 'rely': 11160, 'redevelopment': 15582, '##ek': 5937, '##chester': 25322, 'described': 2649, '##see': 19763, 'visions': 12018, '##ion': 3258, '##真': 30447, 'stylistic': 24828, 'newfoundland': 11944, 'reza': 26323, '[unused358]': 363, 'classmate': 23175, '##dson': 25112, 'science': 2671, 'indianapolis': 9506, 'compensated': 29258, 'disease': 4295, 'gunslinger': 29274, 'orb': 19607, 'ledger': 27106, 'hogan': 14851, 'c2': 29248, '[unused245]': 250, 'bunny': 16291, 'cell': 3526, 'nt': 23961, '##lined': 18194, 'gills': 29468, 'ordeal': 23304, '##pkins': 29531, 'jacques': 7445, 'answering': 10739, 'ounce': 19471, '##之': 30275, '##zoo': 23221, '##asurable': 28329, 'defect': 21262, '##shu': 14235, 'andersson': 28643, '##евич': 22646, 'miles': 2661, '[unused698]': 703, 'installations': 14111, 'becca': 22140, 'baker': 6243, 'barbarians': 28056, '犬': 1908, 'surfaces': 9972, '42nd': 21373, 'jaws': 16113, 'hastily': 15789, 'amiga': 24126, 'choreography': 16967, 'り': 1686, '##hd': 14945, 'detail': 6987, 'civil': 2942, 'amazement': 21606, '##sed': 6924, 'marlene': 26921, 'luisa': 25412, 'extracted': 15901, 'ʂ': 1129, 'emphasizes': 20618, 'example': 2742, 'aspen': 18567, 'ba': 8670, '[unused32]': 33, '710': 27671, 'rubbing': 10137, 'kristina': 28802, 'rhapsody': 29395, 'growled': 8881, 'scott': 3660, 'aba': 19557, 'gardens': 5822, '[unused266]': 271, 'legacy': 8027, 'cracking': 15729, 'guts': 18453, 'built': 2328, '##oy': 6977, 'abrams': 23063, 'legislation': 6094, 'residues': 22644, 'retailer': 20196, 'bottom': 3953, 'juno': 20788, 'saturday': 5095, 'pioneers': 13200, 'caldwell': 16589, 'finishes': 12321, 'fridge': 16716, 'clergyman': 22232, 'palestine': 8976, 'nationalities': 24904, '##mble': 19661, 'employment': 6107, '[unused163]': 168, 'ء': 1269, '[unused927]': 932, 'ん': 1691, '世': 1745, 'mediation': 26435, 'augustine': 14060, 'ふ': 1674, 'pont': 21179, 'malacca': 28481, '##ood': 17139, 'secretariat': 15128, 'dusseldorf': 18160, 'payments': 10504, 'documenting': 23138, '##nar': 11802, 'froze': 10619, 'vitro': 25714, '##adia': 25205, '广': 1842, 'elk': 18995, '##pin': 8091, 'ml': 19875, 'federer': 28294, '##⽥': 30160, '##vil': 14762, 'noble': 7015, 'contractual': 27948, '1938': 4260, 'using': 2478, 'flick': 17312, 'stains': 27094, 'wheelbase': 29141, '##uti': 21823, 'liner': 11197, '##keeping': 18321, 'fredrik': 25454, '[unused734]': 739, 'stunned': 9860, 'stops': 6762, '##hat': 12707, '##dial': 27184, 'scrolls': 23074, 'grandma': 13055, '##ole': 9890, '##tle': 9286, 'exemplary': 27792, 'fires': 8769, 'mccartney': 15320, 'feels': 5683, 'crawl': 13529, '##tler': 25091, 'uncles': 27328, 'alleged': 6884, 'southern': 2670, 'bsc': 23533, '##air': 11215, 'racers': 25791, 'yells': 22114, 'diva': 25992, '[unused332]': 337, 'cooled': 12981, 'dimensions': 9646, '[unused899]': 904, '##mack': 26945, 'phelps': 20475, 'founder': 3910, 'elise': 14251, 'carriages': 17744, '##row': 10524, 'stirred': 13551, 'mughal': 17877, 'invade': 18445, '##gart': 27378, '85': 5594, 'southport': 27494, 'innovations': 15463, '1762': 20827, '外': 1809, '##otto': 23052, 'rooms': 4734, 'author': 3166, 'rooney': 24246, '##ary': 5649, 'considered': 2641, 'conductor': 7589, 'treasurer': 10211, 'criterion': 19229, 'charismatic': 23916, 'bordering': 18299, 'assured': 8916, '[unused685]': 690, 'grange': 18203, '李': 1877, 'macy': 20914, '##worm': 22769, 'stacks': 20829, 'lifts': 13695, 'intensified': 15767, 'abbott': 14455, 'upbringing': 24615, 'stimulation': 20858, 'nectar': 24816, '##raus': 25965, 'beijing': 7211, '##riated': 25475, '##bright': 26614, '##モ': 30253, 'crusader': 25237, 'cad': 28353, '##ড': 29896, 'munoz': 23685, 'uninhabited': 24749, 'awaiting': 15497, 'hers': 5106, 'guido': 20239, '##य': 29868, 'commentators': 15957, 'mike': 3505, 'cultivated': 13237, 'finger': 4344, 'documents': 5491, 'instincts': 16160, 'leverage': 21155, 'gorman': 25693, 'pasta': 24857, 'robotics': 21331, 'trumpet': 9368, 'hiking': 13039, '##ometric': 28993, 'instrumental': 6150, 'হ': 1377, 'nervous': 6091, 'unlimited': 14668, '##lizer': 28863, 'malik': 14360, 'amateur': 5515, 'ic': 24582, 'teatro': 16038, 'bodyguard': 16174, 'hesse': 16399, 'fingers': 3093, 'politics': 4331, '##lo': 4135, 'exciting': 10990, 'bolivia': 11645, 'mixtape': 18713, 'unesco': 12239, '≤': 1608, 'kills': 8563, 'grants': 8624, 'filing': 15242, 'eco': 17338, 'callum': 15229, 'twentieth': 9086, '##week': 28075, 'underside': 17313, 'foster': 6469, 'annette': 22521, 'rectory': 24606, 'ほ': 1676, 'preparation': 7547, 'kyrgyzstan': 23209, '##li': 3669, 'kaiser': 15676, 'veracruz': 23741, '##ב': 29789, 'subscription': 15002, 'read': 3191, 'contribution': 6691, '##´': 29658, 'measures': 5761, 'kobayashi': 28930, 'under': 2104, '[unused957]': 962, 'т': 1197, 'awakened': 20256, 'च': 1318, 'prologue': 18877, '##ery': 7301, '治': 1900, '##los': 10483, 'confidential': 18777, 'accelerate': 23306, 'forged': 16158, 'bach': 10384, 'lines': 3210, 'big': 2502, 'middleton': 17756, 'edna': 21051, '##dit': 23194, 'drama': 3689, 'notion': 9366, 'berkshire': 15570, 'teachings': 12209, '##utive': 28546, 'mathematicians': 29374, '##stov': 29473, 'runners': 7190, 'dub': 12931, 'consoles': 22659, 'guest': 4113, 'enzymes': 16285, 'ottoman': 6188, '##zano': 24147, 'mane': 23055, 'rosary': 28291, '##hila': 26415, 'kevin': 4901, 'accurate': 8321, 'peterborough': 17587, '##phobia': 24920, 'nixon': 11296, 'apt': 26794, 'teaming': 27025, 'kaye': 23686, 'elections': 3864, 'chronic': 11888, 'louisa': 15731, 'corvette': 22687, '##tist': 16774, 'coupling': 19780, 'appliances': 22449, 'luz': 26214, '##н': 18947, 'optical': 9380, 'on': 2006, '23rd': 13928, '153': 16710, 'sept': 17419, 'ached': 15043, '樹': 1884, '##ვ': 29979, 'cord': 11601, 'lexie': 24123, '1930s': 5687, 'bending': 14457, 'cappella': 23013, 'budapest': 10926, '##iot': 25185, '##ᵒ': 30039, 'garments': 21902, 'bridges': 7346, '##raße': 27807, 'ecosystem': 16927, 'info': 18558, 'kenji': 25894, '##rf': 12881, '##iger': 17071, 'conflicting': 19326, 'customary': 16120, 'gerais': 29220, '##uria': 27703, 'saskatchewan': 10068, 'ville': 20184, 'taken': 2579, 'jumped': 5598, '##get': 18150, 'authentication': 27280, 'uruguay': 11724, 'written': 2517, '##routed': 25849, 'realised': 11323, '##uw': 25974, '##उ': 29849, 'ukraine': 5924, 'flourished': 17893, 'leland': 27134, 'theorist': 24241, '##ern': 11795, '##ily': 6588, 'tamara': 21772, '##ris': 6935, '##ksha': 28132, 'selfish': 14337, '##mbre': 19908, 'mab': 26661, 'scent': 6518, 'anders': 15387, 'vfl': 13480, 'still': 2145, 'neglect': 19046, 'citations': 22921, '177': 18118, 'afternoons': 24738, 'survival': 7691, '[unused69]': 70, 'prevents': 16263, '##sey': 7952, 'direct': 3622, '##rk': 8024, 'seduce': 23199, '##李': 30403, 'elegant': 11552, 'excursion': 26144, 'rafael': 10999, 'tricks': 12225, 'merrill': 16239, 'greenish': 23753, 'b': 1038, 'microscope': 24635, 'primal': 22289, '##gne': 10177, 'matched': 10349, 'hats': 16717, 'collectively': 13643, '[unused192]': 197, 'vicki': 25424, 'ussr': 10331, 'shocking': 16880, '##uy': 26230, '[unused492]': 497, 'woody': 13703, 'ร': 1417, '##lan': 5802, 'twelve': 4376, 'cart': 11122, 'モ': 1727, 'ranch': 8086, '196': 20035, 'gave': 2435, 'unions': 9209, 'noctuidae': 24893, 'milling': 22491, 'leash': 26834, '##systems': 29390, '##xide': 19491, 'glistening': 25203, 'cong': 26478, 'overs': 15849, 'inform': 12367, '1905': 5497, 'absorbed': 9063, 'ops': 23092, 'interned': 26345, 'wrap': 10236, '62': 5786, '##⊗': 30140, '[unused442]': 447, 'cracked': 9630, 'killer': 6359, 'trademark': 11749, 'kobe': 24113, 'spielberg': 28740, 'track': 2650, 'bloomington': 26372, 'secretary': 3187, 'ken': 6358, '610': 19827, '##mined': 25089, 'novelist': 9974, 'coordination': 12016, 'slack': 19840, '##dd': 14141, 'swirled': 19171, 'bn': 24869, '1540': 27536, 'capacities': 21157, '##unciation': 24101, 'filipino': 10275, 'notable': 3862, 'bourne': 15803, '##ish': 4509, 'address': 4769, '1400': 20652, 'glitter': 27566, 'judge': 3648, 'work': 2147, '##ential': 24271, 'bat': 7151, 'volga': 26459, 'provincial': 4992, '##rity': 15780, '[unused456]': 461, 'rbi': 16929, 'patty': 17798, 'matteo': 24713, '##bags': 26813, 'certain': 3056, '##ese': 6810, 'spend': 5247, '##‐': 30048, '##oc': 10085, 'kung': 18577, 'economically': 15318, 'repeal': 21825, 'robertson': 9923, 'giles': 13287, 'expeditionary': 15372, '##ce': 3401, 'congregation': 7769, 'erika': 24900, 'postdoctoral': 29272, 'closet': 9346, 'canada': 2710, 'embarked': 11299, 'bombs': 9767, 'substantive': 27737, 'boris': 11235, '##mora': 22122, 'rifles': 9494, 'tierney': 26198, 'vernacular': 18485, '###': 29614, 'ᆨ': 1484, 'density': 4304, 'nephew': 7833, 'either': 2593, '##ᴰ': 30028, '##point': 8400, 'forested': 15205, 'germain': 19192, 'pathetic': 17203, '+': 1009, 'lima': 12967, '[unused778]': 783, '##wan': 7447, 'disability': 11980, 'scattering': 17501, '##dents': 28986, 'vo': 29536, 'home': 2188, 'heresy': 28354, 'fascism': 23779, 'stabbed': 13263, 'petra': 20953, 'protesting': 21248, '##bedo': 28759, 'brewers': 20007, '##raj': 14220, '##ᵃ': 30032, '##cke': 19869, '##chaft': 29043, 'turns': 4332, '##zell': 24085, 'ferris': 23202, '##frid': 27439, 'hotspur': 25985, 'holiday': 6209, '[unused171]': 176, 'directorate': 13634, 'strait': 11195, 'troll': 18792, 'spots': 7516, 'parody': 12354, '27': 2676, 'criticized': 6367, 'strategically': 23972, 'boat': 4049, 'シ': 1706, 'chao': 22455, 'enduring': 16762, 'cemetery': 4528, 'campbell': 6063, 'picket': 28972, '##up': 6279, 'playback': 18245, 'chill': 10720, '和': 1796, 'got': 2288, 'sigma': 13201, 'flexibility': 16991, 'theologian': 17200, 'discipline': 9009, '[unused915]': 920, 'jem': 24193, 'χ': 1177, 'centres': 8941, 'orphans': 21478, 'salvador': 10582, 'heraldic': 28867, 'aidan': 12643, '##ffer': 12494, 'pertaining': 20246, '##athy': 17308, 'silly': 10021, 'ල': 1406, 'adaptive': 19293, 'queer': 19483, '##evsky': 27015, 'shenzhen': 26555, 'petitioned': 22527, 'intricate': 17796, '##door': 23835, 'fairfield': 18986, 'isolated': 7275, '##ض': 29827, 'bury': 11010, '##oise': 23565, 'questionable': 21068, '##ei': 7416, 'spinal': 16492, '[unused528]': 533, '##acker': 25418, 'stairs': 5108, '##য': 29907, 'punta': 27377, 'arch': 7905, 'june': 2238, 'reverend': 10547, '##copic': 26461, '##bia': 11607, 'manufacturing': 5814, '02': 6185, 'recommendations': 11433, 'tearing': 13311, 'cynthia': 15809, 'exercised': 17747, 'fuji': 20933, '##›': 30068, 'annexed': 13291, '228': 22238, 'sep': 19802, 'succeeding': 13034, '##iac': 20469, 'bangalore': 14022, 'anterior': 15099, 'introduction': 4955, 'hugging': 17662, 'wreckage': 21056, 'decade': 5476, 'terry': 6609, '##rley': 12866, 'demanding': 9694, '[unused319]': 324, 'surveying': 19654, 'fact': 2755, 'raising': 6274, 'sanskrit': 11353, 'sermon': 18408, '##]': 29636, '##haven': 14650, 'decreasing': 16922, 'patti': 22732, 'drawing': 5059, 'middleweight': 18741, '##ductive': 26638, '##thal': 24090, 'tesla': 26060, 'linking': 11383, '##介': 30285, 'favourable': 18731, 'padua': 24941, 'aix': 28443, 'recalled': 7383, 'station': 2276, 'journalists': 8845, 'mechanical': 6228, '##ław': 19704, 'cliffs': 13333, 'silhouette': 21776, 'replay': 15712, '仁': 1758, 'uae': 17641, '##b': 2497, 'potter': 10693, 'pu': 16405, '117': 12567, 'recruits': 15024, 'tee': 17170, 'murmur': 20227, 'mcdonald': 9383, 'del': 3972, 'tc': 22975, '##64': 21084, '105': 8746, '##icz': 18682, 'thinkers': 24762, 'massimo': 28080, '##є': 29759, 'lau': 21360, 'portraying': 17274, 'refurbishment': 24478, 'ryan': 4575, 'named': 2315, 'medical': 2966, '##ostal': 29381, 'sultanate': 21308, 'accord': 15802, 'galaxy': 9088, 'nickelodeon': 20814, '##chner': 28254, 'slamming': 15209, 'trends': 12878, 'presentations': 18216, '##dc': 16409, '930': 29359, 'modifying': 29226, '##พ': 29950, 'permission': 6656, 'publishers': 8544, 'え': 1649, 'enjoying': 9107, 'gossip': 13761, 'periodic': 15861, 'shed': 8328, 'judges': 6794, 'folder': 19622, '##tem': 18532, 'jolly': 22193, 'compression': 13379, 'unreliable': 23579, '£1': 14534, 'restrictive': 25986, '##α': 14608, 'arnold': 7779, 'galicia': 19017, '##33': 22394, 'snooker': 26448, 'coveted': 28821, 'pottery': 11378, 'someday': 13834, 'taxpayer': 26980, 'waitress': 13877, 'contingent': 15445, 'complimented': 27175, 'ramos': 18882, 'oracle': 14721, '##bay': 15907, 'course': 2607, 'buyer': 17634, 'torture': 8639, '470': 21064, 'overall': 3452, '##chment': 22729, 'juniper': 29425, 'population': 2313, '##bal': 10264, 'maud': 21696, 'marin': 16400, 'mayor': 3664, 'recreation': 8640, 'or': 2030, 'fujian': 29551, 'liar': 16374, '##eration': 16754, '##hale': 15238, 'eliot': 16292, 'wally': 18202, 'slain': 19668, 'derry': 17455, 'drivers': 6853, 'saloon': 17078, 'fashionable': 19964, 'kat': 10645, '##₇': 30083, 'activation': 13791, 'appear': 3711, 'stare': 6237, 'madame': 10602, '##aya': 12186, 'vocal': 5554, 'nationalists': 17934, 'rocker': 24779, 'internacional': 27607, 'hazy': 26710, 'floods': 14295, '不': 1744, 'voivodeship': 7225, '[unused46]': 47, 'joy': 6569, 'booming': 24716, 'brazilian': 6142, 'gulf': 6084, 'clifford': 13894, 'competitive': 6975, 'fishermen': 16532, 'ghanaian': 27202, 'ф': 1199, 'ezio': 23043, 'peshawar': 28777, 'economists': 22171, 'currents': 14731, 'offense': 10048, '##rane': 18053, 'prentice': 23429, 'safety': 3808, 'prof': 11268, 'strengthen': 12919, 'residing': 7154, '##ła': 22972, '##itte': 27100, 'latvia': 12429, '##բ': 29767, 'television': 2547, 'bahamas': 17094, '##ller': 10820, 'curling': 11599, 'revered': 23886, 'study': 2817, 'mala': 28935, '770': 29065, 'dwarves': 29281, '[unused465]': 470, '青': 1975, 'shudder': 18261, 'relates': 14623, 'scheduled': 5115, 'norma': 20692, 'demand': 5157, 'dimension': 9812, 'pedestal': 24496, 'want': 2215, '##pad': 15455, '[unused494]': 499, 'everest': 23914, 'educator': 11490, 'adapt': 15581, '##↦': 30116, 'overlap': 17702, 'voluntarily': 17912, '##hering': 22658, 'survives': 13655, 'hellenic': 23122, 'conversion': 7584, 'domestic': 4968, 'healer': 19783, 'seal': 7744, 'tolerate': 19242, 'accountant': 17907, 'airbus': 20901, '[unused480]': 485, 'careless': 23358, 'eye': 3239, 'jungle': 8894, 'gong': 17242, 'influenza': 24442, '##shore': 19208, '##rden': 18246, 'reinforcements': 14214, 'companies': 3316, 'gunther': 19384, 'icons': 18407, 'hornet': 26795, 'laughs': 11680, 'moreno': 17921, 'investigative': 15025, 'distal': 29333, 'mozambique': 16274, 'superficial': 23105, '##cite': 17847, 'ᅥ': 1472, 'figured': 6618, 'postponed': 14475, '田': 1911, '##ida': 8524, 'missionaries': 11743, 'model': 2944, '1799': 13839, 'assembled': 9240, '##zard': 26154, 'adds': 9909, 'servers': 14903, 'clarke': 8359, '[unused513]': 518, 'pulses': 23894, '##laus': 28128, 'meant': 3214, 'connected': 4198, 'phone': 3042, 'thyroid': 29610, '##ta': 2696, 'han': 7658, 'seats': 4272, '##چ': 29840, 'footballer': 4362, 'partially': 6822, '##ings': 8613, 'boxes': 8378, '##een': 12129, '##gor': 20255, 'lotus': 13030, '##ture': 11244, 'hereditary': 14800, '##sden': 27903, '##breakers': 27623, 'pissed': 9421, '‑': 1514, 'antiquities': 21387, '##pf': 14376, 'idea': 2801, 'eldest': 7310, 'muscles': 6650, 'browns': 13240, 'font': 15489, 'grasslands': 26183, 'bisexual': 22437, 'marvel': 8348, 'thinking': 3241, 'bender': 25386, 'portrayed': 6791, 'continuum': 22961, 'view': 3193, 'forest': 3224, 'heading': 5825, 'preschool': 23655, 'bayern': 21350, 'expansive': 25145, 'kinetic': 20504, '##tropical': 25528, 'brennan': 13962, '[unused361]': 366, 'czechoslovak': 21282, '##つ': 30190, '##icles': 20921, 'blossoms': 28766, 'huey': 29503, '##iana': 11410, '4': 1018, '##wo': 12155, 'lighthouse': 10171, 'bauer': 17838, 'operated': 3498, 'up': 2039, '##і': 29760, 'scrapped': 14553, 'hymns': 17777, 'traumatic': 19686, 'excess': 9987, 'dreaming': 12802, 'curated': 17940, 'induction': 15946, '##ination': 12758, 'documentation': 12653, 'mango': 24792, '##chee': 25923, '##points': 26521, 'laws': 4277, 'guild': 9054, 'loyal': 8884, 'bows': 21207, '##lized': 28931, 'insults': 23862, 'chunk': 20000, 'arcadia': 25178, 'bloom': 13426, '##rier': 16252, '##mx': 22984, 'tendency': 11765, 'muhammad': 7187, 'aero': 18440, 'kinship': 27866, 'constructed': 3833, '##ditional': 27064, 'σ': 1173, 'asserted': 13647, 'charge': 3715, '1847': 9176, 'wilcox': 23926, '[unused215]': 220, 'journey': 4990, 'surrounded': 5129, '##keepers': 24764, '250': 5539, '##te': 2618, 'afterwards': 5728, 'try': 3046, 'honduras': 14373, '1890': 6193, '[unused628]': 633, 'habitats': 10746, '##ided': 14097, 'editing': 9260, 'albert': 4789, 'supposedly': 10743, 'shipments': 24636, '##dh': 16425, 'heard': 2657, '##pes': 10374, 'brilliance': 28850, 'actually': 2941, 'harvesting': 21534, 'adolescent': 20274, '##36': 21619, 'mixture': 8150, '1802': 13515, 'wine': 4511, 'interval': 13483, 'canine': 28735, '##aks': 29243, '##pon': 26029, 'pinched': 18521, 'diplomacy': 17610, 'interventions': 19388, 'certification': 10618, 'pegasus': 26606, 'ghz': 29066, '##used': 13901, 'obligatory': 26471, '[unused221]': 226, 'hitchcock': 19625, 'listings': 26213, 'installed': 5361, 'inappropriate': 15884, 'habit': 10427, 'spade': 23288, 'removing': 9268, 'trench': 14185, 'grover': 25242, '¥': 1071, 'pebbles': 28962, 'wilson': 4267, 'serve': 3710, '##peration': 29487, 'sanchez': 10568, 'tori': 23413, 'because': 2138, 'murdered': 7129, 'pga': 14198, 'html': 16129, 'seized': 8243, 'peaceful': 9379, '##otic': 20214, 'championed': 28683, '##dad': 14697, 'մ': 1229, 'constant': 5377, 'faculties': 18658, 'plumage': 28764, '##kill': 15872, 'finished': 2736, 'dynamic': 8790, '##beck': 12750, 'harlem': 14864, 'collector': 10018, '##∧': 30129, 'bestowed': 17398, 'ipod': 26322, 'invitations': 29492, 'limiting': 14879, 'frescoes': 23360, 'roberta': 23455, '##hill': 7100, '329': 29567, '1999': 2639, 'salute': 17664, '##sser': 18116, '##bruck': 28985, 'dismounted': 29580, '[unused722]': 727, 'downing': 22501, '##esian': 25253, 'racer': 14878, 'postal': 10690, 'barriers': 13500, '##uen': 24997, 'slowly': 3254, 'sharply': 9249, '[unused950]': 955, 'contact': 3967, 'chad': 9796, '##lin': 4115, 'daughters': 5727, 'accelerated': 14613, '[unused708]': 713, 'realism': 15650, 'speak': 3713, 'hr': 17850, 'naturalized': 27558, 'colton': 21000, 'hero': 5394, '##minster': 24431, '45th': 24634, 'kata': 29354, 'roam': 25728, '##wr': 13088, '##щ': 29754, 'nouvelle': 25207, 'ceramics': 17314, 'baking': 21522, 'monuments': 10490, '##tical': 14656, 'metres': 3620, 'bb': 22861, '##naud': 26160, 'poorer': 27196, 'tying': 15233, '##eke': 23941, '##ے': 29846, '##ciful': 26336, 'र': 1333, 'lisa': 7059, 'sonya': 27926, '[unused130]': 135, 'purple': 6379, 'hallways': 28274, 'player': 2447, 'cultures': 8578, 'orgasm': 13892, 'hendricks': 28895, 'survey': 5002, 'collaborative': 12317, 'detroit': 5626, '##lia': 6632, 'subconscious': 27952, 'superstar': 18795, 'trinity': 7124, 'hooker': 17074, 'purposes': 5682, 'fear': 3571, 'definite': 15298, 'crunch': 24514, 'nano': 28991, '##yck': 28377, 'brody': 19631, 'ล': 1418, 'execution': 7781, 'perrin': 28106, 'lillian': 19344, 'goes': 3632, 'twenty20': 22240, 'scents': 29254, 'revolution': 4329, 'attack': 2886, 'charm': 11084, 'blessings': 24618, 'fielder': 25000, '[unused237]': 242, 'swords': 10689, 'agreements': 10540, 'ventures': 13252, 'bolts': 19947, '##ape': 24065, 'fender': 19028, 'witty': 25591, '##isse': 23491, '[unused240]': 245, 'cure': 9526, 'probable': 15596, '120': 6036, 'rbis': 23583, 'harrisburg': 24569, 'manner': 5450, 'disciplines': 12736, 'ottomans': 17774, 'grab': 6723, '##city': 12972, '##elia': 13902, 'responded': 5838, 'taxonomy': 25274, 'נ': 1257, 'van': 3158, 'teacher': 3836, 'jewel': 13713, '##iere': 21253, 'karel': 28021, '##ella': 8411, 'fleeing': 14070, 'outline': 12685, 'liturgical': 19246, '##rera': 24068, 'servicing': 26804, '40th': 16541, 'rita': 11620, 'executing': 23448, 'ᆷ': 1487, 'mainly': 3701, '125': 8732, '##stown': 13731, 'draining': 19689, 'obligation': 14987, 'ham': 10654, 'petals': 15829, '##het': 27065, 'blitz': 22312, 'expected': 3517, 'munich': 7469, 'ska': 24053, 'russian': 2845, '##zin': 17168, '##zation': 9276, 'late': 2397, 'scholar': 6288, '##wyn': 11761, 'sharp': 4629, 'renault': 14605, 'concacaf': 22169, 'yu': 9805, 'album': 2201, 'internment': 29041, 'vaguely': 15221, 'softly': 5238, 'childbirth': 27162, '##iol': 20282, '##hammer': 19742, '##zzle': 17644, 'crystals': 14438, 'nme': 23770, 'stuffed': 11812, 'gloucestershire': 15905, 'drought': 14734, 'supermarket': 17006, 'avon': 16131, '##gration': 29397, 'boarded': 17383, '[unused677]': 682, '##五': 30279, 'doesn': 2987, 'bruised': 18618, 'jammed': 21601, 'tensor': 23435, '##yson': 25385, 'guangzhou': 18693, 'moldova': 16772, 'recurring': 10694, '##marks': 27373, 'ᄉ': 1461, 'byzantine': 8734, 'version': 2544, 'bash': 24234, 'realms': 18814, '##am': 3286, 'iowa': 5947, '227': 21489, 'joshua': 9122, '##owa': 21293, 'morphology': 19476, 'cabin': 6644, 'commandant': 15254, '##orus': 21694, 'mcgregor': 23023, 'wheels': 7787, 'cruiser': 10844, 'liechtenstein': 26500, '##avs': 29553, 'testify': 19919, '##ox': 11636, 'noteworthy': 19144, '##post': 19894, '##ust': 19966, 'mammal': 25476, '##bf': 29292, 'autobiography': 10828, 'urge': 9075, 'qualifier': 10981, 'misleading': 22369, '##mium': 27759, 'mixes': 21109, 'pokemon': 20421, 'erotic': 14253, 'mortality': 13356, 'shots': 7171, 'initiation': 17890, 'wild': 3748, '‘': 1520, 'medici': 22605, 'andover': 29463, '[unused724]': 729, 'categories': 7236, 'facilities': 4128, '1838': 9931, '[unused632]': 637, 'lori': 18669, '##eak': 25508, '##bson': 27355, 'socket': 22278, 'drenched': 25265, 'derrick': 18928, 'doctorate': 8972, 'shadow': 5192, '##gent': 11461, 'sans': 20344, 'complaints': 10821, 'phenomenon': 9575, '##listic': 27348, 'fundamentally': 24670, 'pajamas': 27621, 'disagree': 21090, 'reilly': 13875, 'sleek': 21185, '##krishna': 23017, '332': 29327, '##ק': 29810, 'goblin': 22639, '##urian': 29091, 'advisory': 7319, 'slab': 17584, 'reporter': 6398, 'expect': 5987, 'clouds': 8044, 'decline': 6689, 'causeway': 23336, '##世': 30271, '[unused753]': 758, 'composing': 16572, 'buena': 27493, 'replied': 3880, 'commando': 15054, 'aged': 4793, '1945': 3386, 'gi': 21025, 'violate': 23640, 'sometime': 8811, 'durable': 25634, '##zong': 13150, 'hayward': 21506, '##landa': 24448, 'ₜ': 1571, 'adoption': 9886, '##dos': 12269, '[unused776]': 781, 'declining': 13993, 'exceeding': 17003, 'hadn': 2910, 'reminiscent': 14563, 'laps': 10876, 'uploaded': 21345, 'pitt': 15091, '##vial': 18660, '##hard': 11783, '##tted': 16190, 'blackmail': 25044, 'exam': 11360, 'favor': 5684, 'emotions': 6699, 'och': 28166, '##anger': 25121, 'comprise': 15821, 'scramble': 25740, '##լ': 29773, 'windshield': 19521, 'bk': 23923, '##ding': 4667, '[unused7]': 8, '##⅔': 30110, 'beating': 6012, '##iter': 21646, 'cartwright': 27513, 'defences': 16828, '##wich': 12414, 'link': 4957, 'horrified': 14603, 'mechanized': 23387, 'colonel': 4327, 'pbs': 13683, 'marian': 14042, 'straits': 18849, '1623': 29056, '[unused938]': 943, 'illness': 7355, 'cobb': 17176, 'headlines': 19377, '##ر': 17149, 'vet': 29525, 'ъ': 1205, 'incurred': 22667, 'in': 1999, '[unused175]': 180, '##hyl': 29598, 'improved': 5301, 'dinners': 29236, 'ulster': 11059, 'caribbean': 7139, 'heaviest': 26858, 'una': 14477, '[unused104]': 109, '1702': 26776, '[unused814]': 819, 'forces': 2749, 'irrelevant': 22537, 'rules': 3513, 'hussein': 16543, 'collisions': 28820, '##trom': 13887, '##kaya': 20718, 'survivors': 8643, 'intends': 18754, 'upgraded': 9725, 'lots': 7167, 'lungs': 8948, 'msc': 23794, 'quintet': 18109, 'venezuela': 8326, 'mla': 18619, 'superhero': 16251, 'xl': 28712, 'seating': 10747, '##our': 8162, 'tower': 3578, 'declaration': 8170, 'honored': 8686, 'whisky': 21265, '##bella': 21700, 'sponsorship': 12026, 'justified': 15123, 'blackpool': 17444, 'api': 17928, 'sgt': 17001, 'delicately': 28814, 'characteristics': 6459, 'finalists': 13527, 'processing': 6364, 'primera': 14837, 'taxon': 28521, 'thirty': 4228, '##kal': 12902, 'effects': 3896, 'erase': 22505, 'adriatic': 23400, '##itating': 16518, '##ngen': 25997, '[unused823]': 828, 'iv': 4921, '[unused140]': 145, 'severed': 16574, 'yarmouth': 28792, '[unused252]': 257, 'critically': 11321, 'bwv': 23860, 'myra': 23020, 'narrator': 11185, '[unused343]': 348, 'stab': 17079, '##נ': 29803, 'everyday': 10126, '1959': 3851, 'hoarse': 21221, 'charitable': 11128, 'filming': 7467, 'sequencing': 24558, 'involuntary': 26097, 'glint': 25263, 'yoon': 24863, 'incredible': 9788, 'shreveport': 23740, 'bulging': 27569, '##rds': 17811, '##bank': 9299, 'eras': 28500, 'albion': 13392, 'spacecraft': 12076, '##mento': 23065, '##sler': 20590, 'gui': 26458, 'schooling': 14118, 'europa': 12124, 'tail': 5725, 'dreamed': 13830, 'eternity': 12715, 'advising': 23875, 'floppy': 28491, 'slick': 13554, '##lu': 7630, 'reactor': 13308, 'љ': 1215, '##森': 30408, 'fingertips': 12206, '##rians': 23543, 'intelligence': 4454, 'collegiate': 9234, 'caracas': 21675, 'julien': 19290, '##rr': 12171, '##bc': 9818, 'although': 2348, 'claire': 6249, '1775': 14276, 'better': 2488, 'necks': 26082, 'hung': 5112, '##rba': 28483, 'ahead': 3805, '##cola': 26289, 'adventurer': 29506, 'fishing': 5645, 'styled': 13650, 'harriet': 14207, 'mammalian': 26524, 'premiered': 5885, 'cry': 5390, 'fried': 13017, 'margins': 17034, 'incomplete': 12958, '##ability': 8010, 'brunette': 27261, 'affinity': 16730, '[unused914]': 919, 'emile': 17127, 'bequeathed': 27180, 'chatham': 16727, 'migrate': 22806, 'howled': 27489, 'rudolf': 12794, 'smith': 3044, '##ivision': 24607, 'quota': 20563, 'outstanding': 5151, 'nude': 15287, '##nical': 20913, 'fail': 8246, '##ride': 15637, 'drifting': 15013, 'favorable': 11119, 'incentives': 21134, 'substrate': 16305, '[unused284]': 289, 'singapore': 5264, '##vas': 12044, '##ya': 3148, 'accomplishments': 17571, '##ated': 4383, 'francaise': 18427, 'destroyed': 3908, 'specify': 20648, 'blizzard': 21689, 'warships': 15964, 'shang': 29382, 'hurley': 25124, 'philosophical': 9569, 'ascot': 29036, 'congestion': 20176, '1632': 28216, 'lamb': 12559, '##rol': 13153, 'ligament': 25641, '##ental': 21050, 'raid': 8118, 'beatrice': 14807, 'tilted': 9939, 'choke': 16769, 'starring': 4626, 'dire': 18704, 'punishment': 7750, '201': 16345, 'bells': 10118, 'guidelines': 11594, 'oriental': 11481, 'vietnam': 5148, '2009': 2268, 'agricultural': 4910, 'expert': 6739, 'reflecting': 10842, 'tile': 14090, 'characterised': 17253, '##tin': 7629, 'charley': 20430, '##ently': 28198, '##rise': 29346, 'shifting': 9564, '##eft': 29218, '[unused983]': 988, '##cturing': 19159, 'surprising': 11341, '##gonal': 20028, 'groundwater': 22761, '[unused678]': 683, '##layer': 24314, '##acle': 18630, '##\"': 29613, 'rest': 2717, 'thor': 15321, 'request': 5227, 'cracks': 15288, 'releases': 7085, 'ا': 1270, 'handkerchief': 23975, 'remarked': 10783, '##making': 12614, 'henan': 27837, 'odisha': 21874, 'bolton': 12118, 'fringe': 13548, '1680': 24621, 'fission': 27521, '##ˈ': 29715, 'totaling': 21798, '侍': 1765, 'fallen': 5357, 'averages': 20185, '##onale': 22823, 'accounted': 14729, 'sandstone': 11694, 'burgess': 17754, 'verses': 11086, 'sbs': 21342, '##nius': 20447, 'clinch': 29119, 'estimation': 24155, 'mcbride': 18958, 'wool': 12121, 'proposing': 21991, '[unused967]': 972, 'rippled': 26862, 'stout': 18890, 'protesters': 13337, 'explains': 7607, '##phonic': 27385, 'rustic': 27471, '##dan': 7847, 'belgium': 5706, 'agenda': 11376, 'fabian': 21174, 'scaling': 25169, 'more': 2062, '[unused898]': 903, '##ros': 7352, 'princeton': 9173, '##coming': 18935, '##yen': 20684, 'preoccupied': 23962, '##enter': 29110, '##lwyn': 29287, '##口': 30314, 'dollar': 7922, 'tennis': 5093, 'nov': 13292, 'wavelengths': 29263, '[unused608]': 613, '##lley': 25105, 'aria': 9342, '##morphism': 19539, 'latin': 3763, 'eyelashes': 25150, '##リ': 30258, 'buick': 28865, 'eligibility': 11395, 'affiliate': 8727, 'pork': 15960, 'doe': 18629, '980': 25195, '##skin': 29334, 'blame': 7499, '##tment': 21181, 'consulting': 10552, 'stamp': 11359, 'right': 2157, 'laughing': 5870, 'peninsula': 6000, 'beau': 17935, 'parole': 17393, '30': 2382, '##osa': 8820, 'mating': 15100, 'regretted': 18991, '##ology': 6779, '##ifice': 23664, '##hrer': 17875, 'wounding': 27285, 'withdrew': 6780, 'proclamation': 16413, 'guadalajara': 22887, 'ideals': 15084, 'quakers': 28301, 'elvis': 12280, '33rd': 20883, 'countdown': 18144, 'meanings': 15383, 'cannot': 3685, 'robot': 8957, 'mandatory': 10915, 'paintings': 5265, 'listened': 7791, 'taunting': 29442, '##ʌ': 29700, '##nal': 12032, 'damien': 12587, 'cardinals': 9310, 'iihf': 26904, 'subject': 3395, 'parties': 4243, 'suspected': 6878, 'colours': 8604, 'modules': 14184, 'courses': 5352, 'chile': 7029, 'bernardo': 21175, 'bohemian': 18063, '##sson': 7092, 'bathurst': 21897, 'transient': 25354, '[unused959]': 964, 'wildcats': 19495, 'icon': 12696, 'montenegro': 13018, '##zak': 20685, 'odor': 19255, 'll': 2222, 'cages': 27157, 'detached': 12230, 'mustache': 28786, 'ল': 1373, '##asia': 15396, 'goldsmith': 22389, '[unused668]': 673, 'ives': 23945, 'set': 2275, 'quarterfinals': 9237, '##feit': 21156, 'faced': 4320, 'dauphin': 29102, '##anne': 20147, 'pioneering': 13280, 'slung': 20078, 'regina': 12512, '67': 6163, 'valiant': 24329, 'involvement': 6624, 'wishes': 8996, 'patricia': 10717, 'potatoes': 14629, 'afghan': 12632, 'displaying': 14962, 'support': 2490, '192': 17613, 'newcastle': 8142, 'β': 1156, '##hita': 23935, 'suites': 19796, 'antagonist': 17379, 'aging': 12520, 'seater': 23392, 'restoration': 6418, 'templar': 27850, 'batavia': 28917, 'pearce': 19560, '1856': 8708, '##kt': 25509, '##nty': 29405, '##ר': 29811, 'sharpe': 22147, '##亻': 30283, 'complexity': 11619, 'tha': 22794, '##cking': 23177, 'differentiate': 21032, '##leigh': 13615, 'bankers': 25375, '##ා': 29944, 'puzzle': 11989, '##oney': 17791, 'mount': 4057, '##nberg': 11144, 'extended': 3668, 'penultimate': 25512, '194': 19955, 'ξ': 1168, '##athlon': 17563, '##atz': 20501, 'quilt': 27565, 'brisbane': 7717, 'bangs': 28490, 'reproduce': 21376, '##lights': 15733, 'leung': 26037, '1827': 12309, 'dignity': 13372, 'ruse': 26307, 'worth': 4276, 'met': 2777, 'prick': 24858, 'activity': 4023, '[unused401]': 406, 'colin': 6972, '##ration': 8156, '##lford': 23211, '##uary': 24384, 'owning': 19273, '##ffled': 28579, 'awe': 15180, '##lez': 28060, 'licensing': 13202, 'painted': 4993, 'unitary': 22127, 'sir': 2909, '##mite': 23419, 'newfound': 27608, '##ե': 29770, 'def': 13366, 'representative': 4387, 'weights': 15871, 'herbert': 7253, 'enthusiastic': 14727, '##ulla': 22187, 'dwelling': 13160, 'fell': 3062, 'manuscripts': 10485, 'rabbi': 7907, 'anthem': 11971, 'primate': 25662, 'spence': 22186, '##kling': 20260, 'discover': 7523, '[unused139]': 144, '₁': 1548, '##ome': 8462, 'inquisition': 24638, 'emphasize': 17902, 'ta': 11937, '##ssion': 28231, 'intellect': 24823, 'brands': 9639, 'supervision': 10429, 'hoax': 28520, 'parts': 3033, 'smell': 5437, 'kerry': 11260, 'float': 14257, 'rapid': 5915, 'information': 2592, 'engaging': 11973, 'with': 2007, 'fbi': 8495, '##inate': 14776, 'fairchild': 24258, 'firth': 25138, '##র': 29908, '##card': 11522, 'transparency': 16987, 'sunni': 18883, 'byu': 23471, 'arrivals': 25470, 'squeezing': 15328, '##ologists': 16886, '##ication': 21261, 'spacious': 22445, '##amina': 27651, 'diploma': 9827, 'vaulted': 24616, 'butter': 12136, 'severely': 8949, 'baja': 19497, 'siege': 6859, 'depiction': 15921, 'beige': 28799, 'eastbound': 24773, 'domains': 13100, 'allocation': 16169, 'planes': 9738, 'jointly': 10776, 'flawless': 27503, 'whipping': 23016, 'centenary': 17705, 'british': 2329, 'clarified': 20485, 'brewer': 18710, 'ud': 20904, 'mustard': 23187, '[unused549]': 554, '##car': 10010, 'focal': 15918, 'fixtures': 17407, 'curb': 13730, 'roc': 21326, 'dee': 9266, 'rana': 22175, 'borough': 5538, 'abandon': 10824, 'vast': 6565, 'resurrection': 15218, 'supported': 3569, 'maroon': 22222, 'atop': 10234, 'donkey': 20325, 'riot': 11421, 'peterson': 12001, 'ᅪ': 1476, 'bravo': 17562, 'auditory': 28042, 'kant': 26044, 'orleans': 5979, 'etudes': 25041, '##‡': 30062, 'cypriot': 18543, 'tunis': 25317, 'forget': 5293, 'boiler': 15635, 'meritorious': 22053, '[unused362]': 367, 'walter': 4787, 'fee': 7408, '##ail': 12502, 'neighbor': 11429, 'how': 2129, 'asking': 4851, '155': 14168, 'mongols': 22235, 'state': 2110, 'ˤ': 1154, 'clad': 13681, '31st': 17089, '##izer': 17629, '##case': 18382, 'rammed': 27653, 'plan': 2933, 'richest': 18429, '##ter': 3334, '##nging': 22373, '[unused475]': 480, 'acquainted': 19056, '[unused790]': 795, 'percy': 11312, 'otherwise': 4728, '##idad': 27893, 'constitutional': 6543, 'losing': 3974, 'anthropology': 12795, 'kern': 22762, 'prominent': 4069, 'economical': 21791, '##power': 11452, 'rumor': 19075, 'saber': 25653, '##tium': 16398, '##tt': 4779, 'memory': 3638, 'technique': 6028, 'guidance': 8606, 'cheerful': 18350, 'eternal': 10721, 'ants': 16111, '330': 14210, 'removes': 20362, 'significant': 3278, 'volcanic': 10942, 'believing': 8929, 'dia': 22939, 'partial': 7704, 'nominees': 17853, 'need': 2342, 'magnet': 16853, 'hedge': 17834, 'literature': 3906, 'oscar': 7436, 'refinery': 21034, '[unused684]': 689, 'idiots': 28781, 'ᵐ': 1502, '##sol': 19454, 'surprisingly': 10889, '##ulf': 21007, 'metabolic': 21453, 'cambodia': 12899, 'disadvantage': 20502, 'puerto': 5984, '##ible': 7028, 'corporal': 14265, '##xi': 9048, 'transporting': 18276, 'head': 2132, '##jack': 17364, 'transferred': 4015, 'accolades': 27447, 'measured': 7594, '##ン': 30263, 'gap': 6578, 'emblem': 16199, 'concluding': 16228, '##eaux': 25639, '18th': 4985, 'plaintiff': 20579, '[unused85]': 86, 'lounge': 11549, '##thi': 15222, 'strolled': 20354, 'antigen': 28873, 'berth': 17064, 'haunted': 11171, 'disasters': 18665, 'glamour': 22439, '十': 1783, '##jin': 14642, 'nomadic': 21702, 'interruption': 24191, 'water': 2300, '[unused806]': 811, 'tame': 24763, '##wed': 15557, 'sociologist': 25106, 'interchange': 8989, '##vern': 23062, 'doubled': 11515, 'chateau': 11874, 'windsor': 10064, 'betrayal': 14583, 'auditions': 21732, 'al': 2632, 'rate': 3446, 'locate': 12453, 'ked': 16135, '河': 1899, 'cheating': 16789, 'inputs': 20407, 'barr': 19820, 'unarmed': 23206, '##行': 30471, 'yoga': 13272, '##bau': 27773, 'animation': 7284, 'ari': 10488, 'remedy': 19519, 'convoys': 23083, 'gentle': 7132, 'canned': 27141, 'computing': 9798, 'shit': 4485, 'folded': 6999, 'invite': 13260, 'cherokee': 13796, 'transfers': 15210, 'deserves': 17210, 'dq': 25410, 'frankenstein': 22478, 'marylebone': 28537, 'vaults': 28658, '##orted': 15613, 'tatar': 29241, 'coaching': 7748, 'intelligent': 9414, 'opens': 7480, 'huntington': 16364, 'mcmahon': 17741, 'basis': 3978, 'intact': 10109, 'refusing': 11193, 'nichols': 15746, 'attraction': 8432, '##evich': 16277, '##zel': 12638, '##cine': 16567, 'liang': 16982, 'dans': 18033, 'rodrigo': 18943, 'mig': 19117, '##nsky': 25655, 'component': 6922, 'depressed': 14777, 'neue': 27713, 'deserts': 28858, 'werewolves': 18306, '##γ': 29721, 'mm': 3461, 'aware': 5204, 'schneider': 15159, '##nivorous': 29193, 'ministry': 3757, 'three': 2093, 'loud': 5189, 'solution': 5576, 'laurie': 16450, 'relation': 7189, 'nguyen': 16577, '「': 1641, 'yen': 18371, '##ignon': 24796, 'acacia': 24766, 'knives': 13227, 'inline': 23881, 'sophia': 9665, '##yard': 14132, 'hitter': 18694, 'ink': 10710, 'scoop': 23348, 'touches': 12817, 'note': 3602, 'murmurs': 22888, 'cues': 23391, 'pastoral': 13645, 'exaggerated': 16903, 'beads': 17530, 'boeing': 10321, 'viewing': 10523, 'behave': 16582, 'sal': 16183, 'bowie': 14598, '##tly': 14626, 'piles': 18526, 'jail': 7173, 'madeleine': 19324, '960': 26637, 'bellevue': 26756, 'bacterium': 24024, 'optics': 21026, 'iec': 24940, 'reflected': 7686, 'cent': 9358, '[unused733]': 738, 'lacrosse': 13488, 'spicy': 25482, 'electrode': 28688, '##ᆷ': 30023, '560': 21267, '##ante': 12956, 'florence': 7701, 'safe': 3647, 'sunrise': 13932, 'fay': 23201, 'bon': 14753, 'page': 3931, 'nomenclature': 23718, 'realises': 25924, 'messy': 18307, 'ashok': 28916, 'speech': 4613, '##iii': 28954, 'dolphin': 17801, '##ი': 29981, 'golfer': 20601, 'magistrates': 23007, 'nun': 16634, 'carson': 9806, 'bugs': 12883, 'thailand': 6504, 'cyrus': 16123, 'who': 2040, 'expanding': 9186, 'ted': 6945, 'janata': 20308, 'sentiment': 15792, 'darling': 9548, 'requiem': 21199, '神': 1925, 'aqueduct': 24016, 'quan': 24110, '##ŋ': 29673, 'reserved': 9235, '##囗': 30323, '[unused856]': 861, 'anchorage': 21086, 'televised': 13762, 'alteration': 26014, 'virginity': 26970, '##tland': 19270, 'sharks': 12004, 'adult': 4639, '##ogical': 20734, '##三': 30267, '口': 1788, 'alicia': 15935, '##omba': 24991, 'clutched': 13514, 'ci': 25022, 'fore': 18921, 'devices': 5733, 'tightening': 18711, '##wl': 13668, 'snaps': 20057, 'envelope': 11255, 'homework': 19453, '##zic': 27586, '[unused226]': 231, 'leaves': 3727, 'wear': 4929, 'mama': 9588, '##nsed': 27730, 'retro': 22307, 'hardin': 27522, 'lax': 27327, 'knit': 22404, 'civilized': 25825, '##litz': 24725, '##│': 30143, 'adobe': 18106, 'hesitate': 16390, 'decree': 10037, '##フ': 30246, '78': 6275, 'white': 2317, 'take': 2202, 'ban': 7221, 'honorable': 13556, '##cratic': 17510, '##ellant': 24178, 'ze': 27838, '##£': 29646, '／': 1992, 'cared': 8725, 'detailing': 17555, 'glided': 26936, 'frail': 25737, 'driving': 4439, 'fireworks': 16080, '##aha': 23278, 'tajikistan': 23538, 'bonnie': 12220, 'chung': 15972, '[unused843]': 848, 'banda': 24112, 'micro': 12702, 'recommended': 6749, 'ᅵ': 1483, '[unused543]': 548, 'now': 2085, 'turks': 12896, 'calculating': 20177, 'revue': 17480, '##washed': 27283, 'obeyed': 22665, 'chief': 2708, 'muted': 22124, 'arrange': 13621, 'buchanan': 14349, '[unused785]': 790, 'gesellschaft': 27482, 'hear': 2963, 'surgeons': 16804, 'oyster': 21480, 'dangerous': 4795, 'earlier': 3041, 'deterioration': 26118, '[unused840]': 845, '[unused663]': 668, 'emily': 6253, 'bleed': 19501, 'recited': 24843, '1876': 7326, '##guide': 28582, '##par': 19362, 'bengals': 23227, '##÷': 29669, '##ural': 11137, '##tica': 22723, '1767': 21502, '[unused789]': 794, 'ს': 1452, 'suits': 11072, 'discharge': 11889, '[unused737]': 742, 'ing': 13749, 'hermes': 24127, 'martyrs': 18945, 'courtship': 28592, '##κ': 29726, 'sank': 7569, '[unused54]': 55, 'painful': 9145, 'intending': 16533, '##а': 10260, '[unused746]': 751, 'passageway': 27336, 'lissa': 20244, '##asa': 16782, 'vertically': 20018, 'hepatitis': 28389, '##wei': 19845, '##hett': 28499, 'unofficial': 11982, '##ity': 3012, 'wasps': 23146, 'butte': 25024, 'gold': 2751, '381': 29335, 'operatic': 22534, 'paralysis': 26287, '##↔': 30115, 'accompaniment': 22205, '##tc': 13535, '##ctric': 22601, 'postgraduate': 15438, 'fulbright': 26695, 'circuits': 13782, 'pivotal': 20369, '##rik': 15564, '[unused777]': 782, 'ˡ': 1151, 'shards': 23327, 'half': 2431, '335': 24426, 'jensen': 14857, '##roi': 26692, 'initially': 3322, '1016': 28707, '##chua': 26200, 'humiliation': 21171, 'specialists': 15744, 'য': 1371, 'ecole': 12431, 'practices': 6078, '1857': 8204, 'wade': 10653, '##rgos': 22280, '##free': 23301, 'ladies': 6456, 'hostel': 21071, '[unused630]': 635, 'brothel': 27308, '##acio': 21361, 'darkened': 13755, '##ᆸ': 30024, 'illinois': 4307, 'jewelry': 11912, 'stores': 5324, 'lone': 10459, 'creamy': 24519, 'suzuki': 14278, 'ecology': 13517, 'astronomical': 13674, 'gov': 18079, '##urities': 29366, 'coyotes': 26880, 'dubbed': 9188, 'uzbekistan': 17065, 'c': 1039, 'coiled': 24599, '##rift': 16338, 'refugees': 8711, 'taller': 12283, '##itt': 12474, 'crore': 21665, 'chairs': 8397, 'dioxide': 14384, 'subordinate': 15144, '##lding': 24573, 'cannons': 17138, 'headquartered': 9403, 'rolf': 23381, '##pton': 15857, 'workout': 27090, 'organisation': 5502, 'assumptions': 17568, 'phillips': 8109, 'fixing': 15887, 'dmitri': 28316, '##7': 2581, '##vey': 12417, 'terminating': 23552, 'concurrency': 24154, 'principal': 4054, 'eileen': 20495, '1721': 27689, 'steady': 6706, 'rotterdam': 15632, 'ewing': 24023, 'dollars': 6363, '##now': 19779, '##†': 30061, '##chule': 20269, 'termed': 12061, 'neurological': 23130, 'dash': 11454, '##*': 29621, 'brave': 9191, '龍': 1982, 'differentiated': 24374, 'ferry': 7115, '##emia': 17577, '##ffen': 18032, 'provoke': 27895, 'explain': 4863, 'poet': 4802, 'swimmer': 13361, 'jury': 6467, '##rford': 26430, 'francisco': 3799, 'copper': 6967, 'rigged': 25216, 'clint': 16235, '##own': 12384, '279': 25745, 'side': 2217, 'filters': 17736, 'faded': 8105, 'army': 2390, 'determined': 4340, 'ned': 12311, 'promptly': 13364, 'kill': 3102, 'bribes': 29117, 'spontaneously': 27491, 'restless': 15035, '##sman': 11512, 'community': 2451, '##logists': 18738, 'mohamed': 14467, 'interviews': 7636, 'hanged': 17818, '[unused760]': 765, '[unused552]': 557, 'crown': 4410, 'beatty': 27305, 'brawl': 23244, 'ama': 25933, 'stephen': 4459, 'ո': 1232, 'bite': 6805, 'flip': 11238, 'skylar': 27970, '##an': 2319, 'represent': 5050, 'blackberry': 25935, 'perch': 21836, 'devi': 14386, 'min': 8117, 'ca': 6187, 'anton': 9865, '##ira': 7895, 'forcefully': 23097, 'samples': 8168, 'loyalist': 23414, 'ა': 1438, 'moisture': 14098, 'stroked': 11699, '340': 16029, '##eni': 18595, 'examines': 20798, '##hiff': 25798, 'mere': 8210, 'beetle': 7813, 'glanced': 3887, 'keyboard': 9019, 'talbot': 14838, 'cleanup': 27686, 'stabilize': 27790, '##eld': 14273, 'remove': 6366, '₀': 1547, 'phosphate': 17344, 'slavic': 13838, 'compiler': 21624, 'lovers': 10205, '●': 1619, 'mat': 13523, 'xiang': 27735, 'withdraw': 10632, 'sis': 24761, 'lets': 11082, 'lew': 24992, 'larger': 3469, 'victims': 5694, 'bono': 23648, 'flicker': 17909, 'stationed': 8895, 'vivian': 13801, 'crambidae': 21585, 'margin': 7785, 'humble': 15716, 'zac': 29250, 'teamed': 12597, 'fates': 26417, 'crocodile': 21843, 'spur': 12996, 'laundry': 14533, '##rth': 15265, '##factory': 21450, 'nikolai': 13870, '##ws': 9333, 'shouldered': 29383, 'raaf': 22709, 'lodged': 20451, 'senses': 9456, 'hapoel': 24208, 'spat': 14690, 'assertion': 23617, 'melbourne': 4940, '##owed': 15096, 'drawings': 9254, '##ulate': 9869, 'pathway': 12732, 'snuck': 24492, 'malay': 12605, '##ovsky': 21983, 'flaming': 19091, 'southbound': 19751, 'boyd': 12164, 'bud': 13007, '##camp': 26468, '[unused679]': 684, 'beef': 12486, 'wishing': 10261, '##sters': 15608, 'movie': 3185, 'transformed': 8590, '##地': 30328, '##way': 4576, '##車': 30480, 'freighter': 27945, 'stretching': 10917, 'goethe': 23173, 'consult': 23363, 'hunan': 27374, 'natasha': 17399, 'miller': 4679, 'hint': 9374, 'lyric': 13677, 'yukon': 19898, '##ifiers': 28295, 'lowell': 15521, 'pairs': 7689, 'divert': 27345, 'expressway': 11720, 'logo': 8154, 'coats': 15695, '##saurus': 22244, 'published': 2405, 'notices': 14444, 'crank': 27987, '##num': 19172, '##ics': 6558, '[unused935]': 940, 'peppers': 23582, 'refrigerator': 18097, 'cancer': 4456, '[unused984]': 989, 'brings': 7545, 'premiership': 11264, 'center': 2415, 'gracie': 19005, '##aceous': 25560, 'optimism': 27451, 'behalf': 6852, 'commemoration': 22377, 'holders': 13304, '##dis': 10521, '04': 5840, 'barbarian': 25075, '##oof': 21511, '1929': 4612, 'updates': 14409, 'driven': 5533, 'condemning': 28525, 'split': 3975, '##udged': 27066, '##stein': 8602, 'jerry': 6128, '##♭': 24102, 'swirl': 28693, '##tern': 16451, '##ree': 9910, 'submarine': 6982, 'lanka': 7252, 'moon': 4231, '##wat': 24281, 'kelvin': 24810, 'subjected': 13532, 'wilkins': 22745, 'israel': 3956, 'aquatics': 26649, 'to': 2000, '##jk': 15992, 'ioc': 25941, 'criticism': 6256, 'textures': 29343, 'output': 6434, 'footsteps': 9717, 'magnesium': 24983, 'explosions': 18217, '##ving': 6455, '##cured': 19405, 'annals': 16945, '##dded': 19520, 'kitten': 18401, '##เ': 29959, 'brighton': 10309, 'vladimir': 8748, 'pursued': 9505, 'americas': 10925, '##urai': 24804, 'studios': 4835, 'striking': 8478, 'pie': 11345, 'mao': 15158, 'sv': 17917, 'humorous': 14742, 'countries': 3032, 'curator': 13023, '##duced': 28901, '##ile': 9463, 'manchu': 26650, 'interrupting': 22602, 'gymnastics': 14002, 'ensued': 18942, '##iaceae': 23357, 'ज': 1319, 'sin': 8254, 'ashland': 24346, '新': 1862, '##urable': 23086, 'trojan': 23445, 'oldies': 26826, '##⁄': 30070, 'resemblance': 14062, 'tempting': 23421, 'creed': 16438, 'sink': 7752, '##child': 19339, 'faerie': 27498, 'milestone': 19199, 'exited': 15284, 'collected': 5067, 'medallist': 28595, '##onia': 12488, 'campus': 3721, 'volcano': 12779, '##vian': 18073, 'bosnia': 9562, 'lunch': 6265, '##fl': 10258, 'schumacher': 22253, 'votes': 4494, '##tream': 25379, '##cier': 19562, 'ethics': 9615, 'ц': 1201, 'sweets': 26844, 'iran': 4238, 'reservoirs': 22535, 'pepper': 11565, 'owen': 7291, 'ju': 18414, 'warfare': 8309, 'begging': 12858, 'paints': 23262, 'caressed': 21473, 'jacket': 6598, 'inwardly': 28710, 'taunton': 28181, 'eta': 27859, 'http': 8299, 'isaiah': 18850, 'collections': 6407, 'karate': 16894, 'isbn': 3175, 'within': 2306, '##lip': 15000, '1821': 11723, 'bouts': 23666, '##crat': 23185, 'amour': 21518, '##laze': 24472, '##р': 16856, 'cable': 5830, '##bol': 14956, '##roy': 13238, 'anthologies': 25675, '910': 27743, '[unused894]': 899, '##zle': 29247, 'protestants': 19592, 'wta': 21925, 'tangent': 27250, 'lose': 4558, 'competitions': 6479, '##su': 6342, '##ama': 8067, 'toll': 9565, 'diane': 12082, 'uber': 19169, 'intel': 13420, '[unused540]': 545, 'comments': 7928, 'calf': 19134, '##gawa': 21188, '##10': 10790, 'graphs': 19287, '##ad': 4215, 'misunderstood': 28947, 'hello': 7592, 'beit': 28236, 'jocelyn': 23786, 'phrases': 15672, 'ว': 1419, 'press': 2811, '[unused811]': 816, 'jobs': 5841, '##ales': 23266, 'mitsubishi': 19695, '1947': 4006, 'driver': 4062, 'bohemia': 16799, 'i': 1045, '##dle': 10362, 'liking': 16663, 'resort': 7001, 'ruiz': 18773, '##amy': 24079, 'breathe': 7200, '42': 4413, '##suka': 26544, '##enia': 19825, '##rogen': 22991, 'compulsory': 14770, 'patriots': 11579, 'marquess': 17391, '##ality': 23732, 'chet': 25157, 'searing': 25764, '167': 16785, 'wings': 4777, 'caledonian': 27449, 'administered': 8564, '##akes': 20060, 'glacial': 17381, '##ส': 29956, 'increased': 3445, 'should': 2323, 'cnn': 13229, 'suffolk': 12291, 'breathless': 16701, '##nium': 14907, 'rebellion': 7417, 'ى': 1299, '##gman': 25494, 'walls': 3681, 'lured': 26673, 'inhibitor': 24054, 'kendrick': 25341, 'scientists': 6529, '##sher': 19603, 'repeats': 17993, '##rm': 10867, 'latest': 6745, 'bikes': 18105, 'კ': 1446, '⁸': 1542, 'ʀ': 1127, 'pleas': 22512, '##ட': 29920, 'produced': 2550, 'commence': 22825, 'լ': 1226, 'dade': 27647, 'seeded': 13916, '##iz': 10993, '[unused425]': 430, 'butt': 10007, 'analyses': 16478, 'sears': 18493, 'cereal': 20943, '##hedral': 27310, 'succeeded': 4594, 'hilbert': 27434, '[unused909]': 914, 'bypass': 11826, '##定': 30348, 'ponds': 16879, 'walsh': 11019, '##pheus': 22809, 'genre': 6907, '182': 17691, 'siemens': 22108, 'ida': 16096, 'screenplay': 9000, '##lit': 15909, '##drik': 28730, 'freaks': 29526, '[unused300]': 305, 'madness': 12013, 'subdivisions': 22095, 'gazed': 11114, 'solidarity': 14657, 'augsburg': 24362, '##aid': 14326, 'shirley': 11280, 'paxton': 27765, '##ries': 5134, '[unused738]': 743, 'clock': 5119, '##bert': 8296, 'weightlifting': 29305, 'seventeen': 9171, 'turin': 13667, '##ference': 25523, 'indices': 29299, 'realization': 12393, '##gg': 13871, '##bell': 17327, '##yra': 19563, 'engraving': 23501, 'colorful': 14231, 'shall': 4618, '##四': 30324, 'ever': 2412, '##ines': 10586, 'twisting': 12814, 'antibody': 27781, 'indigo': 22472, 'blooded': 26064, 'way': 2126, 'bautista': 27845, '##thorpe': 22398, 'usc': 15529, 'eduardo': 14846, '##rdes': 26371, 'unopposed': 17123, 'suggested': 4081, '##aries': 12086, 'passage': 6019, 'triangular': 14023, '##llins': 26655, 'celaena': 29252, 'burial': 8940, 'straw': 13137, '会': 1763, '##ved': 7178, 'hop': 6154, 'youngest': 6587, 'investigated': 10847, 'escapes': 12976, 'usd': 13751, 'ala': 21862, '##uating': 24133, 'resembled': 15881, 'poets': 9736, 'gamble': 18503, 'lasts': 16180, 'stored': 8250, 'que': 10861, '##ces': 9623, 'rihanna': 25439, '[unused418]': 423, 'symposium': 17899, 'conventional': 7511, 'orchestrated': 23339, 'wonders': 16278, 'props': 24387, '##hart': 10686, '##น': 29949, 'merging': 16468, 'incorporating': 13543, 'curtains': 14694, 'lankan': 16159, 'telangana': 23764, '##acing': 26217, 'cursed': 10678, 'confidently': 28415, '1777': 16144, 'sweat': 7518, 'backgrounds': 15406, 'populace': 22508, '##lang': 25023, 'tingle': 25792, 'adjust': 14171, '278': 24709, 'hawke': 24882, 'salad': 16521, 'tomatoes': 12851, 'commonwealth': 5663, 'greece': 5483, 'enhancing': 20226, '##nin': 11483, 'regaining': 28657, 'gil': 13097, 'leinster': 15684, 'weathered': 28445, '##kken': 24192, 'precision': 11718, '##via': 9035, 'hadley': 21681, 'debut': 2834, 'budgets': 26178, '##ந': 29922, 'wildlife': 6870, '##ル': 30259, 'elector': 20374, 'alternating': 15122, 'shoes': 6007, 'age': 2287, '[unused939]': 944, 'submission': 12339, 'dune': 21643, 'stefan': 8852, 'cantor': 26519, '[unused949]': 954, 'unfortunate': 15140, '##gis': 17701, 'separate': 3584, 'pole': 6536, 'opposing': 10078, 'rey': 12569, 'visually': 17453, 'tad': 18819, 'nielsen': 13188, 'divorced': 9196, 'reforming': 29455, 'masovian': 23703, 'offers': 4107, 'vh1': 26365, 'rebellious': 22614, 'pub': 9047, 'colts': 14390, '##life': 15509, '##ffs': 21807, 'apparent': 6835, 'christianity': 7988, 'voyager': 23493, 'roommate': 18328, 'load': 7170, 'aforementioned': 17289, 'ی': 1309, 'analyst': 12941, '##wala': 23877, '475': 28245, '##ρ': 29732, 'armenian': 7508, 'cooler': 14976, '॥': 1345, 'champion': 3410, '1970s': 3955, '##ffe': 16020, 'indoors': 24274, '1888': 6690, 'ashamed': 14984, 'litres': 25783, '##oids': 17086, '##з': 29744, 'wes': 14008, '##mberg': 29084, '##ව': 29943, '##yi': 10139, 'roadside': 25131, 'thick': 4317, 'beams': 13110, '##rong': 17583, 'jake': 5180, '[unused484]': 489, '##ep': 13699, '186': 19609, 'mccain': 19186, 'hartman': 26766, '##テ': 30239, '49th': 25726, 'rhineland': 21640, 'faltered': 26015, 'creek': 3636, 'soft': 3730, '540': 20263, 'snowy': 20981, '[unused571]': 576, '##berman': 23991, 'sequences': 10071, '##rton': 11715, 'absorb': 16888, '##cts': 16649, 'kyiv': 25669, 'staggering': 26233, '##¿': 29666, '##lat': 20051, 'wickets': 10370, '##hil': 19466, '##ร': 29953, '##〈': 30163, 'backpack': 13383, 'proponent': 22488, 'born': 2141, 'reinforcement': 23895, '##nk': 8950, 'ι': 1163, 'ownership': 6095, 'pregnant': 6875, 'ch': 10381, 'ak': 17712, 'brought': 2716, 'robes': 17925, 'registry': 15584, 'attorneys': 16214, 'greeks': 13176, '##est': 4355, 'funky': 24151, 'doorstep': 26581, 'rio': 5673, 'nightmare': 10103, '[unused603]': 608, '##ring': 4892, 'kazakhstan': 11769, 'teaser': 27071, 'exercising': 28428, 'deserted': 12768, 'alarms': 29034, 'hampton': 11615, 'reid': 9027, 'explorer': 10566, '620': 23612, 'gilmore': 22999, 'technological': 10660, 'initiative': 6349, 'arias': 25905, 'tudor': 15588, 'website': 4037, 'griffin': 9258, 'feldman': 26908, 'benin': 21164, '##rgeon': 28242, 'ui': 21318, '10th': 6049, 'hoped': 5113, 'telephone': 7026, 'confirms': 23283, '[unused590]': 595, 'today': 2651, '##ul': 5313, 'confession': 12633, 'causal': 28102, 'consonant': 18265, 'reasons': 4436, '##yon': 14001, 'bombers': 10544, 'haji': 28174, 'pioneered': 16193, 'dismissing': 28913, '##ھ': 29844, '後': 1846, 'sediments': 20476, 'wiping': 14612, 'suspiciously': 21501, '##ane': 7231, '##thing': 20744, 'toilets': 21674, 'cleaning': 9344, '##bilities': 14680, 'panzer': 16946, 'csi': 22174, 'partridge': 26079, '##rom': 21716, 'illegitimate': 18102, 'ɐ': 1109, 'johnson': 3779, '##ш': 29753, 'sawmill': 24744, 'duke': 3804, 'alumni': 9441, 'communicate': 10639, '[unused875]': 880, '”': 1524, 'blinded': 20641, 'inferno': 21848, 'newton': 8446, '##tituted': 29139, '##ნ': 29985, 'suppressed': 13712, 'alcohol': 6544, 'upside': 14961, '304': 23859, 'airlines': 7608, 'eki': 23174, 'ru': 21766, 'goaltender': 21437, 'veterinary': 15651, 'bihar': 16178, '##stones': 29423, 'inc': 4297, 'smiled': 3281, 'skeptical': 18386, 'shrines': 22562, 'sim': 21934, 'waste': 5949, 'loop': 7077, '[unused743]': 748, '[unused28]': 29, '##ogen': 23924, 'coordinated': 14206, 'assuming': 10262, '##cchi': 25955, '##sburg': 9695, 'hara': 18820, '##pp': 9397, 'sleeping': 5777, 'robb': 26211, 'pcs': 27019, 'walkover': 17495, 'riverside': 12497, 'chattanooga': 23351, 'electro': 16175, 'maximum': 4555, 'francoise': 28979, 'exchequer': 28889, 'blackburn': 13934, 'vibrated': 27801, 'escorted': 13127, '1692': 28622, 'qualifiers': 18956, '##ان': 18511, 'k': 1047, 'chasing': 11777, 'skill': 8066, 'emails': 22028, '##llas': 25816, '[unused979]': 984, '##₍': 30087, '##彳': 30371, 'suggestions': 15690, '##dog': 16168, '##zzling': 20838, 'anticipated': 11436, 'directs': 23303, 'confessions': 21444, 'oldest': 4587, 'arrogance': 24416, 'shotgun': 13305, 'cardiovascular': 22935, 'chandler': 13814, 'tubes': 10868, 'mic': 23025, '[unused30]': 31, 'beak': 23525, 'calvert': 24800, 'voyage': 8774, '490': 22288, 'hawks': 12505, 'nur': 27617, 'wwf': 16779, 'jill': 10454, 'queensland': 5322, 'lowe': 14086, 'transit': 6671, 'stormy': 24166, 'angie': 14835, 'groans': 27778, '[unused77]': 78, 'torque': 15894, 'minds': 9273, '##ffey': 24513, 'forthcoming': 16875, 'sounding': 9391, '_': 1035, 'lee': 3389, 'ke': 17710, '##iad': 28665, '##ied': 6340, 'reuters': 26665, 'hugged': 10308, '##eus': 10600, 'human': 2529, 'stuff': 4933, 'lobster': 27940, 'bet': 6655, 'bubbles': 17255, '[unused965]': 970, 'gangster': 20067, 'kala': 26209, 'constraint': 27142, 'ɲ': 1123, 'boulder': 13264, 'rehearsals': 24760, '##pps': 28281, 'boredom': 29556, 'solid': 5024, 'conflicts': 9755, 'middlesbrough': 21655, 'earned': 3687, '##tower': 23196, 'gmbh': 18289, 'syndication': 26973, 'bournemouth': 22882, 'marsden': 29558, 'intervention': 8830, 'ں': 1306, 'lottery': 15213, 'artists': 3324, 'struggles': 11785, '##quil': 26147, 'asean': 27974, 'raceway': 23018, '##。': 30162, 'witnessed': 9741, 'prefer': 9544, 'caliphate': 28034, 'mandal': 24373, 'kiev': 12100, 'finding': 4531, 'defense': 3639, 'hurdle': 27630, '##may': 27871, 'closeness': 28398, 'underneath': 7650, 'princely': 22771, 'stresses': 23253, 'duel': 14216, 'bernard': 6795, 'hellenistic': 27464, 'discussing': 10537, 'coulter': 28293, 'lists': 7201, 'ship': 2911, 'attendance': 5270, 'ranges': 8483, 'fr': 10424, 'kidnap': 22590, 'sasha': 14673, 'watched': 3427, '##jure': 25243, '[unused275]': 280, 'eleanor': 10508, 'targeted': 9416, '##ل': 23673, '##chenko': 25872, '[unused892]': 897, 'chico': 22136, 'louis': 3434, 'evenings': 16241, '##ester': 20367, 'banished': 21319, 'choirs': 24743, 'waived': 16301, 'parasitic': 26045, 'doom': 12677, 'naia': 29511, 'lion': 7006, 'jaguar': 16490, 'rhythms': 17900, '[unused120]': 125, 'brake': 13428, 'priced': 21125, 'artwork': 8266, '##gged': 15567, '##rino': 17815, '##aged': 18655, 'violations': 13302, '[unused825]': 830, 'routed': 19578, 'swiftly': 12128, 'pantheon': 24152, '##…': 30064, 'homemade': 25628, '1974': 3326, '##rade': 13662, 'chilly': 24222, '##lysis': 26394, 'maximilian': 18838, 'presided': 15506, 'impending': 17945, 'iteration': 27758, 'recognised': 7843, '##ף': 29806, '##ท': 29948, '##保': 30292, 'atheist': 23503, 'irene': 12855, 'appropriated': 29223, 'underway': 14128, 'rotational': 25254, 'flickering': 20046, '##val': 10175, 'yvonne': 21684, '##½': 13714, '##ingdon': 29138, '##anus': 20849, 'war': 2162, 'meath': 26856, 'shaking': 5513, 'eating': 5983, 'writes': 7009, 'gladys': 22386, '##hips': 19801, '##心': 30375, 'rainer': 28035, '##lda': 15150, '##tara': 27115, 'sponge': 25742, 'point': 2391, 'couples': 6062, 'geometry': 10988, 'catalytic': 26244, '1952': 3999, 'vienna': 6004, 'contention': 18974, '##nham': 20465, '##fed': 25031, 'kitty': 14433, 'partition': 13571, 'expired': 13735, 'celebrations': 12035, 'rei': 24964, 'villa': 6992, '1758': 16832, 'unlawful': 22300, 'io': 22834, '##п': 29746, 'insisted': 7278, 'stone': 2962, 'handful': 9210, 'digits': 16648, 'mcgee': 22034, 'cyclones': 26069, 'potato': 14557, 'colombia': 7379, 'dismissal': 15322, 'sandy': 7525, 'bacterial': 17341, 'expanded': 4423, 'denies': 23439, 'flutes': 28453, '##wa': 4213, 'lakers': 18264, '[unused470]': 475, 'corresponds': 14788, '##fr': 19699, 'highs': 26836, '金': 1964, 'bullshit': 14636, 'hyde': 11804, 'reliance': 17975, 'р': 1195, 'destination': 7688, 'sonia': 16244, '##cp': 21906, 'amplified': 26986, 'twigs': 27964, 'heating': 10808, 'counties': 5721, 'aerodynamic': 28033, 'silver': 3165, '##ike': 17339, 'thanksgiving': 15060, '##cure': 23887, '##tsk': 29064, 'loading': 10578, 'told': 2409, 'tossing': 15021, 'presence': 3739, '[unused555]': 560, 'sorry': 3374, 'ion': 10163, 'formed': 2719, '##biology': 21685, 'monarchy': 12078, '2002': 2526, '##ₖ': 30094, 'greensboro': 27905, '1894': 6539, 'monmouth': 20433, '##lman': 12624, 'footprints': 24629, 'sparsely': 24961, 'intimidated': 28028, 'stumbled': 9845, '##ा': 29876, 'soviet': 3354, 'awoke': 19179, 'huge': 4121, 'festivals': 7519, 'cutter': 16343, 'relativity': 20805, 'columnist': 13317, 'overnight': 11585, '##pres': 28994, 'perceptions': 23271, 'ebony': 27680, 'caleb': 10185, 'mister': 12525, 'term': 2744, 'transport': 3665, 'confidence': 7023, '[unused647]': 652, 'hampshire': 7035, '880': 26839, 'language': 2653, 'tipping': 25486, 'hyundai': 25983, 'chopra': 28826, 'glittering': 20332, 'eliza': 13234, '##dion': 29573, 'genie': 22519, 'zimbabwe': 11399, 'enables': 12939, 'unity': 8499, 'tales': 7122, '[unused151]': 156, 'gram': 13250, 'requested': 7303, 'tiffany': 14381, 'mitochondrial': 23079, 'sneak': 13583, 'horrors': 22812, 'poisoning': 16149, 'brandenburg': 16426, 'amin': 24432, 'ecstasy': 19069, 'blank': 8744, 'dexter': 14375, '[unused732]': 737, 'newt': 25597, '##grate': 22780, 'inventor': 12235, 'southeastern': 8252, '##nation': 9323, 'momentum': 11071, 'copenhagen': 9664, 'salzburg': 18369, 'pauses': 19623, '[unused269]': 274, 'tv': 2694, 'perimeter': 13443, '[unused416]': 421, 'ex': 4654, 'assess': 14358, 'catalina': 22326, 'spatial': 13589, 'blasts': 25829, 'clit': 17962, 'completes': 28123, 'securing': 12329, 'stems': 12402, '321': 24030, 'terrace': 11134, 'bc': 4647, 'hi': 7632, '##rini': 22612, 'manifestation': 24491, 'warrior': 6750, 'baron': 5797, '##lls': 12718, 'simplest': 21304, 'awakening': 16936, 'our': 2256, 'served': 2366, 'ে': 1381, 'norwich': 12634, '##isto': 20483, '[unused27]': 28, 'shows': 3065, 'nobility': 11760, '##ricted': 20623, 'athletics': 6482, 'recognizable': 20123, 'shah': 7890, 'disappearing': 14489, 'scandinavia': 20612, '##rries': 26801, '##清': 30429, 'requirement': 9095, 'roman': 3142, 'arabic': 5640, '##inas': 15227, 'destinations': 14345, 'baronet': 8693, '##iman': 18505, 'trailer': 9117, 'nos': 16839, '##lik': 18393, 'beethoven': 15461, '##cave': 27454, 'ignacio': 22988, 'v': 1058, '##ific': 18513, 'chester': 8812, 'founding': 4889, 'farewell': 13407, 'bbc': 4035, '##仮': 30287, '[unused952]': 957, 'unharmed': 28150, 'stern': 8665, 'smyth': 28103, '##sibility': 28255, 'looming': 23430, 'fluid': 8331, 'cab': 9298, 'sudbury': 26487, 'sql': 29296, 'scoring': 4577, 'putting': 5128, 'fl': 13109, '##nell': 9091, 'bartlett': 20081, 'л': 1190, '##eka': 19025, 'wight': 20945, '##」': 30168, 'blanco': 20036, 'voiced': 6126, 'assigns': 24022, 'performs': 10438, 'acclaim': 10761, '##ᅯ': 30015, '##ffin': 15379, 'izzy': 16699, '»': 1090, 'greenberg': 24190, '##ceae': 9071, 'blossom': 20593, '##hog': 25852, 'knees': 5042, 'wary': 15705, '##ino': 5740, '##lite': 22779, 'matthew': 5487, 'apprentice': 13357, 'macro': 26632, 'oils': 20631, 'plum': 22088, 'meeting': 3116, 'drury': 25663, 'stratford': 17723, 'stitch': 26035, 'davies': 9082, '##80': 17914, '##rrier': 27051, 'profitable': 15282, 'administrators': 15631, 'earnings': 16565, 'gray': 3897, 'winchester': 12841, 'entertaining': 14036, '[unused541]': 546, 'refuse': 10214, 'docks': 15093, 'club': 2252, 'experiments': 7885, 'acquaintance': 18363, '##liche': 27412, 'embraced': 14218, 'cognition': 26497, '##レ': 30260, 'administrator': 8911, 'cid': 28744, '1b': 26314, 'constellation': 15300, 'bipolar': 29398, 'captain': 2952, 'extinguished': 27705, 'conversions': 25834, 'finance': 5446, 'attracted': 6296, 'escorting': 23831, 'leasing': 26707, 'worked': 2499, 'parlor': 18746, 'comedians': 25119, 'transcript': 24051, 'josh': 6498, '##sam': 21559, 'vain': 15784, 'dammit': 20878, 'unmarked': 25779, 'where': 2073, 'platform': 4132, 'amplifiers': 28633, '##ded': 5732, 'disqualified': 14209, 'lc': 29215, 'incorporates': 12374, '志': 1851, 'mfa': 26913, 'yeomanry': 26883, 'singers': 8453, 'shaken': 16697, '##ap': 9331, 'foundation': 3192, '##eur': 11236, '##uance': 26620, 'room': 2282, 'fisted': 28273, 'concern': 5142, 'subway': 10798, 'rounding': 26939, 'thames': 11076, 'squeak': 29552, '貝': 1952, '##nian': 11148, 'file': 5371, 'arterial': 25543, 'rankin': 25772, '##セ': 30234, 'stuttgart': 13022, '##ailed': 17440, 'dea': 26709, 'thriving': 20319, 'spends': 15970, 'marc': 7871, '##ando': 28574, 'durham': 9296, 'chimneys': 28885, '##selle': 19358, 'ogden': 23203, '##幸': 30367, 'fest': 17037, '[unused182]': 187, 'murals': 19016, 'refrain': 20703, 'nazareth': 27192, 'upscale': 28276, '166': 18610, 'colombo': 16224, 'gemini': 21424, 'maine': 7081, '##₂': 8229, 'merry': 12831, 'か': 1651, 'battlefield': 11686, '127': 13029, 'snout': 17291, '##pire': 20781, '##cius': 25393, 'jo': 8183, '##八': 30297, 'dillon': 14602, 'ʌ': 1134, 'relax': 9483, 'lama': 18832, 'apprenticeship': 20448, 'daze': 28918, 'intimidation': 28973, 'insight': 12369, '##cano': 23803, '272': 24231, 'nwa': 15737, 'walk': 3328, 'セ': 1708, 'reorganization': 17118, 'yielded': 17544, '[unused227]': 232, 'akbar': 20730, 'specialized': 7772, '##fat': 27753, 'commented': 7034, 'expresses': 16783, 'ナ': 1715, '##outh': 17167, '##阿': 30497, 'steele': 12872, 'bandits': 19088, 'necessarily': 9352, 'enterprises': 9926, 'forum': 7057, 'tag': 6415, '##mons': 16563, '##olic': 23518, '##islaus': 25678, 'detrimental': 29172, 'don': 2123, 'councillor': 10674, 'procedural': 24508, 'separating': 14443, 'ceilings': 21098, 'outburst': 27719, 'midst': 12930, 'stamped': 20834, '##dell': 12662, 'defiant': 27836, 'rates': 6165, '##dot': 27364, 'ivan': 7332, '##eon': 10242, '##cor': 27108, 'authoritarian': 27246, 'unbelievable': 23653, '##verance': 21998, 'showcases': 27397, 'popcorn': 24593, 'empires': 23560, 'presidential': 4883, 'ـ': 1290, 'tyre': 21904, '370': 16444, 'lash': 25210, 'panicked': 16035, '##elles': 22869, 'offensive': 5805, 'reunited': 11653, '##gum': 22850, 'letterman': 26593, 'generations': 8213, 'mountainous': 14897, '236': 23593, 'rubbish': 29132, '[unused126]': 131, 'resting': 8345, 'describe': 6235, 'wanted': 2359, 'distributions': 20611, '20': 2322, '##pal': 12952, '##dham': 17661, '##genesis': 23737, 'lifting': 8783, 'winced': 15574, 'nicholls': 27043, 'enforcing': 27455, 'ellen': 9155, 'regulations': 7040, 'yiddish': 20112, '##tial': 20925, '##chers': 21844, 'anwar': 28372, 'flaw': 28450, '[unused271]': 276, 'shiva': 12535, 'magnitude': 10194, '19': 2539, 'blonde': 9081, 'rasped': 28459, '##ivity': 7730, 'wills': 17234, 'commencing': 25819, 'revealing': 8669, 'peyton': 17931, 'tens': 15295, '##ナ': 30241, 'housed': 7431, '[unused659]': 664, '##imum': 28591, 'accountable': 26771, 'haiti': 12867, 'peaking': 13015, 'staunch': 26355, '##oulos': 28262, 'surrender': 7806, 'turkey': 4977, 'morals': 25288, '70s': 17549, 'approach': 3921, 'examined': 8920, 'abolished': 8961, '1907': 5528, '##boat': 11975, 'suspects': 13172, 'knocking': 10591, 'bitterness': 22364, 'southampton': 11833, 'ε': 1159, 'panorama': 23652, 'rooftop': 23308, 'ipswich': 15435, 'accreditation': 14893, '[unused309]': 314, 'cesare': 26708, '##be': 4783, '##das': 8883, 'stew': 20717, 'goofy': 27243, 'assistance': 5375, '7th': 5504, '##sos': 17063, 'event': 2724, '265': 20549, 'vigorously': 22400, 'bombay': 11831, '##pher': 27921, 'meyrick': 15228, 'motive': 15793, 'noisy': 20810, '##月': 30398, 'collaborator': 18843, '[unused973]': 978, 'compounds': 10099, 'nuclear': 4517, '##hma': 22444, '##=': 29630, 'boycott': 17757, 'jars': 25067, 'harbour': 7440, 'tribute': 7050, '##མ': 29968, 'aye': 13442, 'horrific': 23512, 'miss': 3335, 'aims': 8704, 'prematurely': 28179, 'warring': 28509, 'skirts': 18184, 'additional': 3176, 'dreams': 5544, '[unused312]': 317, 'recalls': 17722, '440': 17422, '1957': 3890, '##runner': 23195, '[unused904]': 909, '##rine': 11467, 'area': 2181, 'oxygen': 7722, 'river': 2314, 'ashley': 9321, 'availability': 11343, 'credited': 5827, '##core': 17345, '##chuk': 26516, 'tentatively': 19325, '##iques': 19516, 'israelis': 28363, 'greenfield': 26713, 'ix': 11814, 'groaned': 9655, '[unused279]': 284, '[unused529]': 534, '[unused381]': 386, 'distortion': 20870, '##ს': 29988, 'doing': 2725, 'featured': 2956, 'seduction': 26962, 'hysteria': 29004, '##hip': 5605, '##sable': 19150, 'evelyn': 12903, 'া': 1378, 'agencies': 6736, '261': 24441, 'mayfield': 27224, '##ghi': 28891, 'disperse': 28736, '##fb': 26337, 'sulfate': 26754, 'poppy': 19745, 'trainers': 21992, 'floral': 18686, '##ult': 11314, 'monaco': 14497, 'succeed': 9510, 'despite': 2750, 'vlad': 19163, 'pantry': 27796, 'biking': 28899, 'callahan': 25668, 'santa': 4203, 'madam': 21658, 'exponential': 27258, 'exist': 4839, 'indeed': 5262, 'meyer': 11527, 'disturbing': 14888, '##ules': 16308, '204': 19627, 'find': 2424, 'nationals': 10342, 'biker': 28988, '##rist': 15061, '1942': 3758, 'palace': 4186, 'cullen': 19925, 'serbia': 7238, 'indicators': 20390, 'informed': 6727, '[unused61]': 62, 'shook': 3184, 'leans': 12671, 'cbe': 15852, 'defeating': 6324, '##41': 23632, 'grounded': 16764, '[unused794]': 799, '##hr': 8093, 'deposed': 18298, 'documentary': 4516, 'warship': 21905, 'argues': 9251, 'comeback': 12845, '##osing': 18606, 'box': 3482, '08': 5511, '[unused717]': 722, '[unused532]': 537, '##ool': 13669, '1933': 4537, 'crab': 18081, '##戸': 30383, 'executed': 6472, '##tails': 22081, 'elusive': 26475, 'ff': 21461, '500': 3156, 'satisfy': 13225, 'bun': 21122, '##eli': 20806, '1783': 15331, 'stacked': 16934, 'city': 2103, 'mask': 7308, 'rn': 29300, 'deployed': 7333, 'sewing': 22746, 'mania': 29310, '##海': 30428, 'substitution': 20885, '##version': 27774, 'obsolete': 15832, 'fei': 24664, 'elderly': 9750, '##ত': 29898, '##信': 30293, '₎': 1559, 'vin': 19354, '##fest': 14081, 'eleven': 5408, 'chang': 11132, 'caden': 23600, 'cold': 3147, '##ℝ': 30107, 'hormones': 20752, 'chewing': 17492, '##ott': 14517, 'scissors': 25806, 'proper': 5372, 'fibers': 16662, '##ulously': 21227, 'pedestrians': 24946, 'functionality': 15380, 'andrea': 8657, 'anti': 3424, 'flat': 4257, 'eerie': 18823, 'opt': 23569, 'franks': 21310, 'brandy': 17951, 'ni': 9152, 'contexts': 18046, 'systemic': 22575, 'autopsy': 24534, '##bin': 8428, '##osphere': 25444, 'turbulent': 22609, 'pronounced': 8793, 'constituents': 24355, 'resentment': 20234, '信': 1767, 'horseback': 19926, 'sees': 5927, 'sigismund': 26748, 'marginal': 14785, '##tag': 15900, 'john': 2198, 'moore': 5405, 'ल': 1334, 'correspond': 17254, 'bloomfield': 25584, '[unused165]': 170, '##pati': 24952, 'everywhere': 7249, 'episode': 2792, 'elbows': 13690, 'weekly': 4882, '月': 1872, '##fold': 10371, 'donovan': 12729, '1755': 21417, 'decidedly': 27873, 'cale': 21854, 'п': 1194, 'scribe': 27789, 'investing': 19920, 'ethnic': 5636, '##yin': 25811, 'gel': 21500, '##kel': 11705, '##eous': 14769, 'ah': 6289, 'sampled': 18925, 'photographs': 7008, 'publishing': 4640, 'harassment': 16011, '##shan': 9688, 'bred': 13680, '##yse': 23274, 'resurrected': 23053, '##ttal': 28200, '##erland': 22492, '##mac': 22911, 'referenced': 14964, 'fastest': 7915, 'taking': 2635, 'memorial': 3986, 'straightforward': 19647, 'manipulated': 20063, 'front': 2392, 'alessandro': 17956, 'blindness': 26290, '[unused379]': 384, 'sham': 25850, 'stairway': 21952, 'challenger': 12932, 'krakow': 14576, 'brodie': 27379, 'pushes': 13956, 'shitty': 28543, 'triggered': 13330, 'coincide': 19680, 'contestants': 10584, 'relaunched': 26391, 'stephens': 15037, '##ink': 19839, 'puck': 22900, 'impatience': 28011, '##`': 29639, 'questioning': 11242, 'densely': 19441, '03': 6021, 'inland': 9514, 'energy': 2943, '##tops': 25181, 'dialects': 11976, '##ogy': 15707, '##birds': 12887, 'demographic': 15982, 'shuddered': 13937, 'machine': 3698, 'premier': 4239, '##tub': 28251, 'unearthed': 27422, 'claws': 10702, 'bwf': 21215, 'livelihood': 24585, 'paramount': 11734, 'furnace': 17533, 'surrey': 9948, 'islands': 3470, 'restriction': 16840, 'delivering': 12771, 'berber': 27514, 'guiana': 23568, '[SEP]': 102, '[unused858]': 863, '##so': 6499, 'raoul': 22063, 'ob': 27885, 'debated': 15268, 'annie': 8194, '##井': 30280, 'berger': 16758, 'к': 1189, 'paso': 17161, 'tessa': 13167, 'drake': 7867, '[unused434]': 439, '##mos': 15530, 'selection': 4989, 'metallic': 12392, 'rosario': 17496, '[unused551]': 556, '##vius': 26055, 'ambassadors': 20986, '##partisan': 26053, 'zone': 4224, 'journal': 3485, 'intake': 13822, 'dissent': 24116, 'olympia': 17096, '##gra': 17643, 'manufacturer': 7751, 'barbie': 22635, '##yles': 26274, 'contain': 5383, '##ctus': 22675, 'packs': 15173, 'arena': 5196, 'yields': 16189, 'rogers': 7369, '##כ': 29798, '460': 17267, 'bounded': 10351, 'divinity': 16968, 'cycling': 9670, '##lassified': 24938, 'mel': 11463, 'coward': 16592, 'discrimination': 9147, 'gladly': 24986, 'retribution': 25928, '##ario': 16843, '293': 26953, 'entrepreneur': 10670, 'ø': 1100, 'benefactor': 27398, 'effectiveness': 12353, 'remixed': 17574, '##ens': 6132, 'kettle': 22421, '##khan': 26370, 'highlanders': 22672, '157': 17403, '曲': 1870, 'privileged': 21598, 'fernando': 9158, '[unused293]': 298, 'linux': 11603, 'firing': 7493, 'カ': 1700, '勝': 1780, 'synthesis': 10752, 'staple': 18785, 'parkinson': 20310, '5': 1019, '##brick': 25646, '##ogo': 22844, 'ceremonial': 12961, ':': 1024, 'cassette': 13903, '##iza': 21335, '98': 5818, 'legions': 23269, '##ᆨ': 30020, 'winter': 3467, 'thump': 21324, 'showdown': 24419, 'bertram': 27515, 'sly': 18230, 'granddaughter': 12787, 'emi': 12495, '##州': 30362, '##taken': 25310, '##nberger': 28825, 'rodriguez': 9172, 'passenger': 4628, '##tal': 9080, 'goldstein': 25507, 'flying': 3909, 'churchill': 10888, 'toppled': 27251, 'arjun': 26024, 'jj': 29017, 'temperament': 26270, 'tao': 20216, '##ם': 29800, '[unused601]': 606, '[unused701]': 706, 'eds': 11985, 'sprang': 15627, '##win': 10105, '##₄': 17004, 'condition': 4650, 'gregory': 7296, 'successive': 11165, 'hills': 4564, 'conference': 3034, 'yield': 10750, 'anywhere': 5973, 'ruthless': 18101, 'privilege': 14293, 'partner': 4256, 'ms': 5796, '##100': 18613, '##rative': 18514, 'realise': 19148, 'wetland': 22217, '##yu': 10513, 'mystical': 17529, '##lston': 21540, 'kane': 8472, 'stopped': 3030, '##ip': 11514, '[unused526]': 531, 'raw': 6315, '##ب': 29816, 'seemingly': 9428, 'ر': 1280, 'sloane': 17558, 'baseman': 18038, 'dakota': 7734, 'laced': 17958, 'eat': 4521, '310': 17196, '550': 13274, 'renovations': 15576, 'carries': 7883, 'surpassing': 27097, 'youths': 19634, 'xv': 15566, 'meta': 18804, 'messina': 27270, 'ッ': 1711, 'fontana': 27182, '##უ': 29990, 'posthumously': 12770, 'narrowly': 11866, 'whole': 2878, '##berries': 20968, 'qi': 18816, 'converse': 23705, 'combining': 11566, '##charged': 22620, 'philippines': 5137, '##utz': 20267, 'dna': 6064, 'battalions': 10157, 'efficiency': 8122, 'gives': 3957, 'iceland': 10399, 'odd': 5976, 'sales': 4341, 'plotting': 20699, 'rome': 4199, 'naked': 6248, '[unused559]': 564, 'cia': 9915, 'wide': 2898, '##zzi': 13793, 'jays': 18930, '2008': 2263, 'wrenched': 26971, 'certificate': 8196, 'altogether': 10462, 'dunn': 14145, 'cones': 23825, 'subunit': 24312, 'deliberate': 15063, 'lancashire': 9638, 'foundations': 10100, 'photographic': 12416, 'hammering': 27883, 'moe': 22078, 'leisure': 12257, 'cola': 15270, 'trunks': 20509, '##wt': 26677, 'intended': 3832, '[unused795]': 800, 'mil': 23689, 'aristocracy': 22706, 'emergence': 14053, '[unused723]': 728, '##aria': 10980, '##tated': 16238, '93': 6109, 'andrei': 18125, 'tread': 29449, 'grasped': 15517, 'stars': 3340, 'prevent': 4652, 'integration': 8346, 'surveys': 12265, 'pigeons': 27567, 'squeeze': 11025, 'hilltop': 25566, '##inton': 27028, 'forgiven': 24280, '[unused43]': 44, 'ware': 16283, '##lena': 20844, 'momma': 23603, '##gano': 29451, 'abducted': 20361, 'became': 2150, '##dian': 11692, 'towns': 4865, 'dominance': 13811, 'purposely': 24680, 'kamen': 22099, 'markham': 26197, '石': 1922, 'strokes': 13692, 'expands': 24545, '##ceptive': 28687, 'insertion': 23851, 'cary': 20533, 'aerodrome': 23843, 'dr': 2852, '%': 1003, 'silently': 8601, 'custody': 9968, 'golf': 5439, 'decreases': 17913, '春': 1867, '##inations': 22045, '##trick': 22881, 'bern': 16595, 'chi': 9610, 'xiao': 19523, 'parisian': 24262, 'marriages': 12743, 'www': 7479, 'huh': 9616, '##brate': 22008, 'य': 1332, 'commitment': 8426, 'awareness': 7073, 'allowance': 21447, 'battle': 2645, 'viper': 17947, '[unused376]': 381, 'क': 1315, 'nigeria': 7387, 'biologist': 21477, 'crested': 25413, 'carts': 25568, 'regional': 3164, '下': 1743, '1705': 29326, 'bony': 22678, '670': 25535, '291': 27173, 'hide': 5342, '##db': 18939, 'shouldn': 5807, 'heap': 16721, 'polka': 29499, 'granted': 4379, '##gon': 7446, 'webster': 11635, '##lish': 13602, '##nni': 23500, '##nesia': 21509, '[unused444]': 449, 'elect': 11322, '##arth': 22425, 'exhibited': 8176, '##gur': 27390, 'robbed': 20114, 'took': 2165, 'confined': 12302, 'fiance': 19154, '[unused402]': 407, 'marshal': 8610, 'ominous': 23504, '##ctuated': 25638, '##art': 8445, 'hugo': 9395, 'catch': 4608, 'pixel': 22138, 'belfast': 10330, 'onslaught': 28644, '##eter': 15141, '[unused22]': 23, 'honesty': 16718, '[unused916]': 921, 'nominated': 4222, '[unused278]': 283, 'novice': 23131, '##dicated': 26022, 'collier': 20466, 'horne': 24084, 'ƒ': 1108, 'puff': 23893, 'dimly': 25361, 'locking': 14889, 'alla': 25699, '##hetic': 20086, '##ddling': 21814, '[unused258]': 263, 'yes': 2748, '##cher': 7474, '##cated': 12921, '##force': 14821, '##bria': 21556, 'upcoming': 9046, 'admission': 9634, '[unused522]': 527, '面': 1976, 'penetrate': 19136, 'soup': 11350, 'originates': 16896, 'aligned': 13115, 'unsigned': 27121, 'embryo': 28086, 'franchise': 6329, 'albanian': 9408, 'alright': 10303, 'enigma': 26757, 'ripped': 9157, 'afrikaans': 28673, 'syndicated': 13889, '##rigue': 27611, 'mathematics': 5597, 'standardized': 16367, 'acquisitions': 19530, 'girls': 3057, 'bertie': 20743, '##jia': 26541, 'education': 2495, 'm': 1049, 'barn': 8659, 'marlborough': 22830, 'dancers': 10487, 'stop': 2644, 'dock': 8946, 'dozens': 9877, 'supportive': 16408, '##bate': 20179, '[unused539]': 544, 'liquid': 6381, 'rubbed': 7503, 'mill': 4971, 'mein': 24182, 'uniting': 26112, 'triple': 6420, 'scarf': 18982, '##gam': 22864, '[unused253]': 258, 'continuing': 5719, 'coincidentally': 27542, 'beer': 5404, 'tell': 2425, '##थ': 29860, 'ɒ': 1111, 'expectations': 10908, 'tackle': 11147, '##rka': 22379, 'ulrich': 19619, 'smithsonian': 15720, '1928': 4662, 'hypothesis': 10744, 'infancy': 22813, '##bara': 20709, 'berlin': 4068, 'damage': 4053, '##cino': 21081, '戦': 1856, 'mosaic': 16061, 'gardening': 21529, 'composition': 5512, 'handsome': 8502, 'distrust': 29245, 'ultimatum': 29227, 'rode': 8469, 'altitudes': 21973, 'mahal': 27913, 'mk': 12395, 'thing': 2518, '1807': 13206, 'owners': 5608, '##sque': 17729, 'ambiguity': 27637, 'positive': 3893, 'works': 2573, 'arthritis': 27641, 'quest': 8795, 'outdoors': 19350, 'permanent': 4568, 'smuggling': 19535, 'contrasting': 22133, 'marking': 10060, 'nietzsche': 28898, 'rus': 22949, 'tried': 2699, 'gravel': 11127, 'chores': 27091, '1914': 4554, '[unused745]': 750, 'dec': 11703, 'reviewing': 15252, 'stale': 26729, 'cops': 10558, '##hiro': 18334, 'santos': 11053, 'sum': 7680, 'extend': 7949, 'trevor': 8672, 'atomic': 9593, 'platoon': 13799, '##uron': 21017, '##fighting': 22158, 'ribs': 10335, '##com': 9006, 'sired': 26940, '##vron': 28588, 'gifford': 29360, '##able': 3085, 'heritage': 4348, '##л': 29436, 'rhode': 9763, 'ן': 1256, 'ahl': 18347, 'murderers': 28882, '&': 1004, 'scholarship': 6566, 'political': 2576, 'begun': 5625, 'yemen': 13968, 'axial': 26819, 'newman': 10625, 'increase': 3623, '##¬': 29655, 'mitch': 11857, 'broader': 12368, 'ย': 1416, 'correspondent': 11370, '1746': 24507, '⽥': 1634, 'diversified': 24908, 'planet': 4774, 'viz': 26619, '##tis': 7315, '[unused640]': 645, '[unused121]': 126, 'letter': 3661, 'diagram': 16403, 'assure': 14306, 'sera': 26358, '[unused780]': 785, 'finland': 6435, 'teach': 6570, '##ppe': 21512, 'hollis': 27477, 'authorised': 19256, '##eland': 25689, 'renegade': 28463, 'lineage': 13321, 'styles': 6782, '[unused420]': 425, 'could': 2071, 'jurisprudence': 24697, 'saving': 7494, '°': 1080, 'prison': 3827, 'こ': 1655, '##ij': 28418, '##/': 29626, '##screen': 18182, 'cbs': 6568, 'ᅡ': 1470, '##iano': 15668, '##¼': 29664, 'portraits': 9668, 'angered': 18748, '[unused380]': 385, '##ier': 3771, 'frowns': 29303, 'navarro': 23524, 'craftsmen': 25736, 'authority': 3691, 'justification': 19777, 'reactors': 22223, 'sac': 17266, '##nsk': 25564, 'implies': 12748, 'basal': 15191, 'weather': 4633, 'constitution': 4552, 'slam': 9555, '##sław': 23305, 'mutter': 23457, '[unused621]': 626, 'sai': 18952, 'carmen': 11425, 'histoire': 20088, 'systematic': 11778, 'believer': 24591, '[unused944]': 949, 'problems': 3471, 'speedway': 10688, 'bathroom': 5723, '##rno': 19139, '[unused86]': 87, 'kilometres': 3717, 'hewitt': 19482, 'document': 6254, 'firearm': 23646, 'weed': 17901, 'screamed': 7210, 'maltese': 18563, 'cities': 3655, '##rite': 17625, 'residences': 14094, 'reminds': 15537, 'tae': 22297, 'quote': 14686, 'neighbours': 14754, 'penthouse': 22855, 'smashwords': 25151, '##vent': 15338, 'reunite': 25372, '[unused234]': 239, 'ع': 1288, 'க': 1382, 'decor': 25545, 'med': 19960, 'able': 2583, '##non': 8540, 'stunts': 28465, 'curiosity': 10628, 'interceptions': 18387, 'touchdown': 7921, 'difficulties': 8190, 'zagreb': 12974, '##clave': 23650, '##vsky': 15904, 'scars': 13521, 'multiplication': 24856, 'troop': 10123, 'chiba': 27368, 'sebastien': 28328, 'joint': 4101, 'lennon': 14294, 'sabbath': 19546, 'gal': 14891, 'certainty': 15855, 'italia': 13052, 'transforms': 21743, 'canons': 21975, '[unused797]': 802, 'lyrical': 16376, '##উ': 29886, 'ivanov': 26333, 'heather': 9533, 'paddy': 16063, '[unused20]': 21, '##cian': 14483, 'invasive': 17503, 'galen': 21062, 'harper': 8500, '[unused784]': 789, 'velvet': 10966, 'formerly': 3839, 'rec': 28667, 'springfield': 10493, 'cars': 3765, 'eu': 7327, 'digest': 17886, '##church': 22743, 'my': 2026, 'girlfriends': 27408, 'hovering': 16349, '[unused855]': 860, 'alonso': 17649, 'shaping': 20300, 'width': 9381, 'sucker': 26476, 'southeast': 4643, 'grin': 5861, '##vc': 25465, '[unused168]': 173, 'reveal': 7487, 'voyages': 21993, 'combined': 4117, '##stered': 24167, 'raiding': 23530, 'villagers': 11904, '1997': 2722, '##mo': 5302, '##23': 21926, '##rud': 28121, '##dez': 24601, '##made': 21565, 'edo': 18314, 'opted': 12132, 'coli': 27441, 'unlike': 4406, 'arranged': 5412, 'excuse': 8016, 'forearm': 16148, 'lantern': 12856, 'westchester': 25489, 'hmm': 17012, 'flyer': 23821, 'eps': 20383, 'ashford': 26545, 'objections': 17304, 'blackness': 19573, 'timeless': 27768, '##ev': 6777, 'does': 2515, '6': 1020, 'deployment': 10813, 'chuckle': 15375, 'romance': 7472, 'composite': 12490, 'conspicuous': 19194, 'getting': 2893, 'bergman': 24544, '##asi': 21369, 'betrayed': 12056, 'skeletal': 20415, 'avenues': 20859, 'ravine': 23188, 'spurs': 18205, 'marta': 18950, '[unused198]': 203, '¤': 1070, 'liverpool': 6220, 'olsen': 18997, '##stadt': 21543, 'eugen': 29273, 'ᄊ': 1462, 'emergencies': 26360, 'disapproval': 21406, 'fish': 3869, '[unused107]': 112, 'غ': 1289, 'arturo': 20520, '[unused26]': 27, 'java': 9262, 'mentions': 9704, 'become': 2468, 'arose': 10375, '##ouse': 15441, '##rial': 14482, '##edging': 28002, 'genealogical': 29606, '##陳': 30498, '292': 25797, 'cho': 16480, 'trondheim': 26331, 'humming': 20364, 'crowd': 4306, 'exhaustion': 15575, '37': 4261, '253': 23254, 'periods': 6993, '##jian': 27685, '##orin': 28741, '1923': 4927, 'almost': 2471, 'mazda': 25286, 'resided': 12427, 'outlined': 14801, 'drawers': 22497, 'beech': 21760, 'hannah': 8410, 'miserable': 13736, 'pledge': 16393, 'drive': 3298, 'compensation': 9430, 'revolver': 17863, '1979': 3245, 'mathias': 20494, 'faces': 5344, 'tow': 15805, '城': 1804, 'grazing': 15400, 'defeat': 4154, 'replication': 21647, 'eighty': 12021, 'musik': 28076, 'rodney': 13898, 'he': 2002, 'mana': 24951, '##quist': 18331, 'bouncing': 16361, 'revolutions': 25239, '##cat': 11266, 'sergio': 13983, 'picasso': 22457, 'strains': 18859, 'vamp': 20279, '1916': 4947, 'satellites': 14549, 'boomed': 28908, '[unused557]': 562, 'calvin': 11130, 'pagan': 14318, 'abrupt': 18772, 'buying': 9343, '‡': 1527, 'membranes': 24972, 'above': 2682, 'stupid': 5236, 'gift': 5592, '[unused683]': 688, '##man': 2386, 'botswana': 19414, 'scarce': 18782, 'roche': 20162, 'proxy': 24540, '[unused879]': 884, 'approx': 22480, '##ora': 6525, 'felipe': 17095, '[unused238]': 243, 'foraging': 26912, '1703': 28366, '##yuki': 19663, 'meditation': 13804, 'horns': 11569, 'lac': 18749, 'stanisław': 26133, 'guided': 8546, 'cincinnati': 7797, 'snatch': 23365, 'means': 2965, '[unused421]': 426, 'mode': 5549, 'gaining': 8550, 'dominic': 11282, 'aires': 9149, 'philosophers': 17586, '##oton': 25862, '##kota': 27380, 'honours': 8762, '[unused514]': 519, 'slits': 29199, '##ash': 11823, 'cooperation': 6792, 'washington': 2899, 'physics': 5584, '##ful': 3993, 'donated': 6955, '##sell': 23836, 'sprung': 22057, 'veterans': 8244, 'motorized': 26172, 'maids': 29229, 'nawab': 26543, 'enchanted': 22454, 'aurora': 13158, '[unused432]': 437, 'briefs': 28760, 'facial': 13268, 'doi': 9193, '##ked': 8126, 'nerves': 10627, 'old': 2214, '##ᵍ': 30036, 'gnu': 27004, 'flats': 14201, 'honor': 3932, '[unused741]': 746, 'legend': 5722, 'employs': 13495, '##48': 18139, 'hamlets': 21631, '##く': 30179, 'studied': 3273, 'wii': 16568, 'tainted': 26392, 'gogh': 26603, 'christine': 10941, 'int': 20014, '##pping': 14853, 'gregorian': 25847, 'based': 2241, '##ལ': 29971, '[unused702]': 707, '##culture': 14561, 'lithuanian': 10333, 'musee': 18070, 'preliminary': 8824, '##kou': 24861, 'fridays': 26587, '##stick': 21354, 'august': 2257, 'ball': 3608, '##dition': 20562, 'everton': 18022, 'parishes': 11600, '[unused324]': 329, '##vances': 26711, 'genevieve': 20245, 'addressing': 12786, 'raided': 18784, 'specimens': 9908, 'temptation': 17232, 'program': 2565, 'honourable': 17164, 'balance': 5703, 'bull': 7087, '##面': 30502, 'harlan': 24276, 'crescent': 13152, '[unused593]': 598, 'evangelist': 24036, 'five': 2274, '[unused83]': 84, 'dart': 14957, 'pornography': 19378, 'liberty': 7044, '##nded': 25848, '##犬': 30434, '[unused638]': 643, 'passing': 4458, '##eca': 19281, '„': 1525, 'us': 2149, 'reviewers': 15814, '##gie': 11239, 'restraining': 28285, 'remarkable': 9487, 'witches': 12566, 'winged': 14462, 'picked': 3856, 'supreme': 4259, 'rites': 17105, 'pressing': 7827, '##dai': 21351, 'coleman': 11608, 'applicant': 23761, 'walkway': 19467, 'tram': 12517, '##il': 4014, 'strip': 6167, 'smiley': 27420, '##iv': 12848, 'confessed': 14312, 'kernel': 16293, 'cavity': 17790, 'terrorism': 10130, 'hazel': 14015, '##aea': 21996, '##shin': 17426, 'secluded': 27044, 'planck': 26486, '##tick': 26348, '##ᵐ': 30038, 'fumbled': 20054, 'westminster': 9434, 'ニ': 1716, 'clandestine': 24450, '」': 1642, '1628': 28627, 'cuts': 7659, '##wig': 16279, 'awkwardly': 18822, 'liquids': 26820, 'brooklyn': 6613, 'modes': 11583, 'reece': 24227, '##sley': 8002, '##utnant': 24620, 'calabria': 29430, 'welfare': 7574, '[unused986]': 991, 'threshold': 11207, 'moaning': 22653, 'inception': 12149, 'pains': 20398, 'directorial': 21635, 'waller': 23550, 'raining': 24057, '##cera': 19357, 'wellesley': 25658, '##る': 30213, 'pick': 4060, 'alter': 11477, '##anian': 26032, '[unused158]': 163, 'patents': 13979, '##⁰': 30071, 'carbonate': 26427, 'coach': 2873, 'volumes': 6702, '##hmi': 26837, 'pension': 11550, 'cuisine': 12846, 'rt': 19387, '##eem': 21564, 'decreed': 28447, 'rebounds': 11049, 'thorne': 14296, 'stimulus': 19220, 'extends': 8908, 'certificates': 17987, 'went': 2253, 'sage': 10878, 'takes': 3138, '##ola': 6030, 'displayed': 6913, 'equator': 26640, 'wrongly': 29116, '##felt': 26675, '[unused558]': 563, '##chan': 14856, 'seated': 8901, 'thou': 15223, 'outgoing': 22011, '##wled': 28365, '##み': 30204, 'ア': 1693, 'pretoria': 25366, 'portable': 12109, 'lily': 7094, 'accurately': 14125, '##華': 30468, 'assign': 23911, 'discharged': 14374, 'voted': 5444, '##ro': 3217, 'spectator': 21027, 'pine': 7222, '[unused588]': 593, 'nazi': 6394, 'belgian': 6995, 'braden': 27232, 'taped': 19374, '##otti': 21325, 'rumored': 22710, '[unused235]': 240, '##han': 4819, '##liner': 20660, '##mism': 26725, 'infinitely': 25773, '05': 5709, 'screams': 11652, 'par': 11968, 'householder': 7536, 'mayhem': 26865, 'entrances': 18084, 'gurney': 25061, 'encounter': 8087, 'intensive': 11806, 'technicians': 20202, 'holding': 3173, 'inherit': 22490, '1612': 29186, 'westwood': 21730, '##eros': 27360, 'chicken': 7975, 'womb': 26578, 'hosted': 4354, 'andrews': 9261, 'oval': 9242, 'waiter': 15610, 'migratory': 22262, 'brenda': 15507, '##ania': 13241, 'interception': 17385, 'implant': 27159, 'hierarchy': 12571, 'disregard': 27770, 'pennsylvania': 3552, 'desert': 5532, '##isen': 28992, '##ised': 5084, 'discussion': 6594, 'seriousness': 27994, 'outstretched': 21059, 'programming': 4730, 'midnight': 7090, 'conscious': 9715, 'distinguishing': 20852, '33': 3943, '##sun': 19729, 'panted': 28149, 'henry': 2888, 'flirting': 20661, '2011': 2249, '##don': 5280, 'keith': 6766, 'mates': 14711, 'ю': 1209, 'tuberculosis': 15877, '##vic': 7903, 'lux': 28359, '##house': 4580, 'el': 3449, 'mack': 11349, 'suitcase': 15940, '##⇄': 30117, '##neer': 19755, 'shelf': 11142, 'live': 2444, '244': 24194, 'bran': 24905, 'tensions': 13136, 'northern': 2642, 'century': 2301, '##taff': 22542, 'vichy': 29177, 'rousseau': 26868, '296': 27200, 'spans': 14798, 'tormented': 29026, 'descriptive': 22726, 'sentiments': 23541, '[unused798]': 803, 'devin': 23601, '##ators': 18926, 'greg': 6754, 'agreeing': 16191, 'tulane': 27437, 'transactions': 11817, 'examiner': 19684, 'nec': 26785, 'perfume': 17013, 'chilean': 12091, 'sheridan': 13243, 'gershwin': 25600, 'alam': 26234, 'entrepreneurship': 20213, '##馬': 30506, 'prospects': 16746, 'jules': 11044, '##ik': 5480, 'dunbar': 23034, 'brothers': 3428, 'crawling': 15927, '[unused372]': 377, 'raja': 10164, 'bertrand': 20586, 'moves': 5829, 'paradigm': 20680, '##u': 2226, 'warren': 6031, 'remnant': 19614, 'abdominal': 21419, 'timed': 22313, 'languages': 4155, 'isabella': 10323, 'kirk': 11332, 'happened': 3047, '##ori': 10050, 'temperance': 23372, 'bison': 22285, 'jong': 18528, '##con': 8663, 'ђ': 1211, 'dried': 9550, 'fragile': 13072, 'rubber': 8903, '##sities': 24279, '##ɛ': 29275, 'redistribution': 25707, 'வ': 1394, 'vogue': 17734, '##ouk': 27967, '##rner': 18703, 'insect': 14211, 'kraft': 26680, 'sarcastic': 22473, 'emerged': 6003, '1718': 26995, 'ry': 29431, '1697': 28690, 'circa': 12800, '76': 6146, '##ʰ': 29706, '[unused247]': 252, '##ivation': 25761, '[unused263]': 268, 'swallow': 10577, 'biblical': 10213, 'owns': 8617, 'armies': 8749, '楊': 1883, 'maui': 28566, 'gaddafi': 28924, 'bands': 4996, 'indictment': 24265, 'comfort': 7216, 'mann': 10856, 'treacherous': 26648, '##水': 30419, 'dirk': 17594, 'cartridge': 15110, 'encyclopedia': 12204, '##ossa': 21842, 'trio': 7146, 'signals': 7755, 'axel': 18586, 'holster': 29447, 'unmanned': 24075, 'tied': 5079, 'athletic': 5188, '##ors': 5668, 'subsequently': 3525, 'greyhound': 21220, 'germanic': 15139, '##30': 14142, 'claudio': 19569, '##kshi': 27488, 'the': 1996, '##樹': 30410, '[unused832]': 837, 'gaming': 10355, 'sykes': 23531, '##ment': 3672, 'drinks': 8974, 'bryn': 19904, 'quezon': 26564, 'signage': 29404, 'dvds': 22477, '##ities': 6447, 'lan': 17595, 'moths': 14885, 'invitational': 20129, '[unused863]': 868, 'eisenhower': 16551, 'scowl': 19981, '南': 1785, '##ede': 14728, 'explorers': 19264, 'radical': 7490, 'fantasies': 21233, 'frog': 10729, 'reluctance': 21662, 'seton': 28796, 'snoop': 29044, 'smart': 6047, 'constituent': 13794, 'twins': 8178, '##nosis': 27109, 'attractive': 8702, 'slide': 7358, 'participation': 6577, '##wine': 21924, 'upper': 3356, 'answer': 3437, 'ji': 10147, 'locals': 10575, 'ambush': 15283, 'environment': 4044, 'holstein': 18864, 'spotting': 27963, 'libertadores': 27968, '##ties': 7368, '##slow': 26338, 'pomeranian': 17717, 'debts': 13930, 'reads': 9631, 'frs': 25188, 'buenos': 9204, 'ョ': 1730, 'ー': 1739, 'accounting': 9529, 'crept': 13147, 'hiroshima': 20168, 'delegation': 10656, 'replaces': 20736, 'cup': 2452, 'progressive': 6555, '##roving': 22046, 'premium': 12882, 'collateral': 24172, '##ヘ': 30247, 'viewers': 7193, '₱': 1575, '##thor': 27844, '##joy': 24793, 'trait': 18275, 'kite': 20497, 'raked': 22438, 'る': 1687, 'sichuan': 20980, '##rca': 18992, '1717': 27525, 'consulate': 19972, 'winters': 12214, 'recruiting': 14357, 'generates': 19421, '##phate': 24556, 'eliminated': 5892, 'qc': 25196, 'airmen': 29437, 'upstate': 29530, 'chicago': 3190, '[unused511]': 516, '¾': 1093, '##ami': 10631, 'plural': 13994, 'adler': 17969, 'forty': 5659, 'humiliating': 28284, '06': 5757, 'tho': 27793, 'perez': 10730, 'blown': 10676, 'pacific': 3534, 'moscow': 4924, '286': 24921, '示': 1923, 'jr': 3781, 'eddie': 5752, 'discarded': 15105, '##侍': 30291, 'extra': 4469, 'xiv': 15939, 'web': 4773, 'scenario': 11967, 'arches': 13540, 'mortals': 23844, 'ignored': 6439, 'bid': 7226, 'catholicism': 16138, 'thirties': 26313, 'ally': 9698, 'prevention': 9740, '##piration': 16781, 'guadalupe': 23529, 'venerable': 24541, 'vickers': 18571, 'reasonably': 16286, 'nell': 20970, 'retaliation': 18695, 'thin': 4857, 'byron': 12234, '##₎': 30088, 'crash': 5823, 'upon': 2588, '∂': 1592, 'wakefield': 16045, 'humboldt': 19249}\n",
      "Size of Vocab :  30522\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# De la même manière qu'avec Glove, on peut observer le vocabulaire : \n",
    "print(tokenizer.vocab)\n",
    "\n",
    "vocab_size=len(tokenizer.vocab)\n",
    "print(\"Size of Vocab : \",vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3f3994",
   "metadata": {},
   "source": [
    "Pour pouvoir utiliser ce tokenizer dans nos modèles, il faut recréer le datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "636f1d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 18/18 [00:00<00:00, 3597.52 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 18\n",
      "})\n",
      "[[101, 1996, 2346, 2003, 3442, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1996, 2304, 4937, 3248, 2007, 1037, 3608, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1037, 2502, 3899, 2007, 1037, 3608, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 3899, 1998, 4937, 2024, 2362, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 4026, 9389, 2006, 1996, 5351, 2346, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2317, 4743, 2006, 1037, 2502, 3392, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1037, 2502, 4744, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2048, 3765, 8007, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2048, 8448, 2015, 1999, 1037, 2492, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1045, 2066, 9436, 4667, 2026, 7997, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1037, 7006, 1999, 1996, 28350, 2638, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1037, 9055, 12271, 2006, 1996, 2346, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2009, 2003, 1037, 7997, 1010, 2009, 2003, 2025, 1037, 19091, 2080, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2009, 2003, 2025, 1037, 7997, 1010, 2009, 2003, 1037, 19091, 2080, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1037, 8000, 19026, 2011, 1037, 4937, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2048, 14695, 1999, 1996, 6888, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2202, 1037, 4946, 2003, 2823, 12430, 2084, 2635, 3345, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2202, 1996, 3307, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 13/13 [00:00<00:00, 2629.15 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 13\n",
      "})\n",
      "[[101, 1996, 7997, 9297, 2006, 1996, 2346, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1037, 7006, 1998, 1037, 4937, 1999, 1037, 3392, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2048, 3765, 8007, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1045, 2467, 2175, 2000, 2147, 2011, 7997, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1045, 2031, 2053, 4111, 2012, 2188, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 6077, 2066, 8808, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1037, 5061, 19091, 2080, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 9322, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 4744, 5705, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 4744, 4168, 20697, 29314, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1037, 3793, 2055, 9322, 1010, 2025, 4176, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1037, 3793, 2055, 4176, 1010, 2025, 9322, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 28844, 2015, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# On recrée nos datasets initiaux que l'on va traiter différemment : \n",
    "train_dataset = Dataset.from_pandas(tdf)\n",
    "validation_dataset = Dataset.from_pandas(vdf)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # Utiliser le tokenizer pour encoder les textes\n",
    "    inputs = tokenizer(examples[\"text\"], truncation=True, padding='max_length', max_length=256, return_tensors='pt')\n",
    "    inputs[\"labels\"] = torch.tensor([1 if label == 'Y' else 0 for label in examples[\"label\"]], dtype=torch.long)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True, remove_columns=train_dataset.features.keys(), load_from_cache_file=True)\n",
    "print(train_dataset)\n",
    "print(train_dataset[\"input_ids\"])\n",
    "validation_dataset = validation_dataset.map(preprocess_function, batched=True, remove_columns=validation_dataset.features.keys(), load_from_cache_file=True)\n",
    "print(validation_dataset)\n",
    "print(validation_dataset[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c3b04198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': tensor([0, 1, 0, 0]), 'input_ids': tensor([[ 101, 2009, 2003,  ...,    0,    0,    0],\n",
      "        [ 101, 2009, 2003,  ...,    0,    0,    0],\n",
      "        [ 101, 1037, 7006,  ...,    0,    0,    0],\n",
      "        [ 101, 3899, 1998,  ...,    0,    0,    0]])}\n",
      "['[CLS] it is not a bike, it is a flamingo [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]', '[CLS] it is a bike, it is not a flamingo [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]', '[CLS] a lion in the savane [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]', '[CLS] dog and cat are together [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]']\n",
      "{'labels': tensor([1, 0, 1, 1]), 'input_ids': tensor([[ 101, 1045, 2066,  ...,    0,    0,    0],\n",
      "        [ 101, 1037, 8000,  ...,    0,    0,    0],\n",
      "        [ 101, 1037, 2502,  ...,    0,    0,    0],\n",
      "        [ 101, 1037, 9055,  ...,    0,    0,    0]])}\n",
      "['[CLS] i like ridding my bike [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]', '[CLS] a mouse bitten by a cat [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]', '[CLS] a big truck [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]', '[CLS] a motorcycle rides on the road [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]']\n",
      "{'labels': tensor([0, 0, 1, 0]), 'input_ids': tensor([[  101,  2048, 14695,  ...,     0,     0,     0],\n",
      "        [  101,  2048,  8448,  ...,     0,     0,     0],\n",
      "        [  101,  1996,  2346,  ...,     0,     0,     0],\n",
      "        [  101,  1037,  2502,  ...,     0,     0,     0]])}\n",
      "['[CLS] two pigs in the mood [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]', '[CLS] two deers in a field [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]', '[CLS] the road is straight [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]', '[CLS] a big dog with a ball [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]']\n",
      "{'labels': tensor([1, 0, 0, 1]), 'input_ids': tensor([[ 101, 2202, 1037,  ...,    0,    0,    0],\n",
      "        [ 101, 1996, 2304,  ...,    0,    0,    0],\n",
      "        [ 101, 2317, 4743,  ...,    0,    0,    0],\n",
      "        [ 101, 2048, 3765,  ...,    0,    0,    0]])}\n",
      "['[CLS] take a plane is sometimes slower than taking train [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]', '[CLS] the black cat plays with a ball [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]', '[CLS] white bird on a big tree [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]', '[CLS] two cars crashed [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]']\n",
      "{'labels': tensor([1, 1]), 'input_ids': tensor([[ 101, 4026, 9389, 2006, 1996, 5351, 2346,  102,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0],\n",
      "        [ 101, 2202, 1996, 3307,  102,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0]])}\n",
      "['[CLS] traffic jam on the 6th road [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]', '[CLS] take the highway [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]']\n",
      "{'labels': tensor([1, 1, 0, 0]), 'input_ids': tensor([[  101,  9322,   102,  ...,     0,     0,     0],\n",
      "        [  101,  4744,  4168,  ...,     0,     0,     0],\n",
      "        [  101,  1037,  3793,  ...,     0,     0,     0],\n",
      "        [  101, 28844,  2015,  ...,     0,     0,     0]])}\n",
      "['[CLS] trucks [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]', '[CLS] truckmegatruck [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]', '[CLS] a text about animals, not trucks [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]', '[CLS] doggs [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]']\n",
      "{'labels': tensor([0, 0, 1, 1]), 'input_ids': tensor([[ 101, 1045, 2031,  ...,    0,    0,    0],\n",
      "        [ 101, 6077, 2066,  ...,    0,    0,    0],\n",
      "        [ 101, 2048, 3765,  ...,    0,    0,    0],\n",
      "        [ 101, 1037, 3793,  ...,    0,    0,    0]])}\n",
      "['[CLS] i have no animal at home [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]', '[CLS] dogs like cheese [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]', '[CLS] two cars crashed [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]', '[CLS] a text about trucks, not animals [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]']\n",
      "{'labels': tensor([1, 0, 1, 0]), 'input_ids': tensor([[ 101, 1996, 7997,  ...,    0,    0,    0],\n",
      "        [ 101, 1037, 7006,  ...,    0,    0,    0],\n",
      "        [ 101, 4744, 5705,  ...,    0,    0,    0],\n",
      "        [ 101, 1037, 5061,  ...,    0,    0,    0]])}\n",
      "['[CLS] the bike drives on the road [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]', '[CLS] a lion and a cat in a tree [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]', '[CLS] truckks [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]', '[CLS] a pink flamingo [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]']\n",
      "{'labels': tensor([1]), 'input_ids': tensor([[ 101, 1045, 2467, 2175, 2000, 2147, 2011, 7997,  102,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0]])}\n",
      "['[CLS] i always go to work by bike [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]']\n"
     ]
    }
   ],
   "source": [
    "pad_idx=tokenizer.pad_token_id  # On met à jour l'index de padding et on recrée les dataloaders\n",
    "\n",
    "train_sampler = LengthGroupedSampler(batchsize, train_dataset, megabatch_mul)\n",
    "\n",
    "train_loader=DataLoader(train_dataset,batchsize,sampler=train_sampler,collate_fn=data_collator,pin_memory=True, shuffle=False, num_workers=0)\n",
    "\n",
    "for data in train_loader:\n",
    "    print(data)\n",
    "    print(tokenizer.batch_decode(data[\"input_ids\"]))\n",
    "\n",
    "test_sampler = LengthGroupedSampler(batchsize, validation_dataset, megabatch_mul)    \n",
    "    \n",
    "test_loader=DataLoader(validation_dataset,batchsize,sampler=test_sampler,collate_fn=data_collator,pin_memory=True, shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "for data in test_loader:\n",
    "    print(data)\n",
    "    print(tokenizer.batch_decode(data[\"input_ids\"]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71eca84",
   "metadata": {},
   "source": [
    "On note que truckmegatruck n'est plus complètement ignoré mais découpé en tokens (on retrouve par exemple le token truck d'id 4744 au début du mot). \n",
    "\n",
    "On a donc un nouvel encodage de notre corpus, mais nous ne possédons pas d'embeddings pour les tokens correspondants. On peut essayer d'en récupérer en chargeant un modèle transformer pre-entraîné : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2e37a6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|██████████| 268M/268M [00:05<00:00, 51.5MB/s] \n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "distil_bert = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "print(distil_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3329584a",
   "metadata": {},
   "source": [
    "On se propose d'extraire les embeddings de ce modèle pour les utiliser directement dans notre modèle simple MyModel défini plus haut. Procédure pour charger ces embeddings et lancer l'entraînement du modèle :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f0c2fac8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss on epoch 0 : 0.695\n",
      "train accuracy on epoch 0: 0.389\n",
      "test loss on epoch 0: 0.679\n",
      "test accuracy on epoch 0: 0.615\n",
      "train loss on epoch 1 : 0.663\n",
      "train accuracy on epoch 1: 0.667\n",
      "test loss on epoch 1: 0.672\n",
      "test accuracy on epoch 1: 0.692\n",
      "train loss on epoch 2 : 0.648\n",
      "train accuracy on epoch 2: 0.833\n",
      "test loss on epoch 2: 0.663\n",
      "test accuracy on epoch 2: 0.615\n",
      "train loss on epoch 3 : 0.622\n",
      "train accuracy on epoch 3: 0.944\n",
      "test loss on epoch 3: 0.654\n",
      "test accuracy on epoch 3: 0.692\n",
      "train loss on epoch 4 : 0.594\n",
      "train accuracy on epoch 4: 1.000\n",
      "test loss on epoch 4: 0.645\n",
      "test accuracy on epoch 4: 0.692\n",
      "train loss on epoch 5 : 0.579\n",
      "train accuracy on epoch 5: 1.000\n",
      "test loss on epoch 5: 0.636\n",
      "test accuracy on epoch 5: 0.692\n",
      "train loss on epoch 6 : 0.572\n",
      "train accuracy on epoch 6: 0.889\n",
      "test loss on epoch 6: 0.631\n",
      "test accuracy on epoch 6: 0.769\n",
      "train loss on epoch 7 : 0.555\n",
      "train accuracy on epoch 7: 0.889\n",
      "test loss on epoch 7: 0.624\n",
      "test accuracy on epoch 7: 0.769\n",
      "train loss on epoch 8 : 0.525\n",
      "train accuracy on epoch 8: 0.889\n",
      "test loss on epoch 8: 0.617\n",
      "test accuracy on epoch 8: 0.769\n",
      "train loss on epoch 9 : 0.522\n",
      "train accuracy on epoch 9: 0.944\n",
      "test loss on epoch 9: 0.611\n",
      "test accuracy on epoch 9: 0.769\n",
      "train loss on epoch 10 : 0.508\n",
      "train accuracy on epoch 10: 0.889\n",
      "test loss on epoch 10: 0.604\n",
      "test accuracy on epoch 10: 0.769\n",
      "train loss on epoch 11 : 0.512\n",
      "train accuracy on epoch 11: 0.889\n",
      "test loss on epoch 11: 0.598\n",
      "test accuracy on epoch 11: 0.769\n",
      "train loss on epoch 12 : 0.473\n",
      "train accuracy on epoch 12: 0.889\n",
      "test loss on epoch 12: 0.592\n",
      "test accuracy on epoch 12: 0.769\n",
      "train loss on epoch 13 : 0.479\n",
      "train accuracy on epoch 13: 0.889\n",
      "test loss on epoch 13: 0.586\n",
      "test accuracy on epoch 13: 0.692\n",
      "train loss on epoch 14 : 0.468\n",
      "train accuracy on epoch 14: 0.833\n",
      "test loss on epoch 14: 0.582\n",
      "test accuracy on epoch 14: 0.692\n",
      "train loss on epoch 15 : 0.442\n",
      "train accuracy on epoch 15: 0.889\n",
      "test loss on epoch 15: 0.578\n",
      "test accuracy on epoch 15: 0.692\n",
      "train loss on epoch 16 : 0.425\n",
      "train accuracy on epoch 16: 0.944\n",
      "test loss on epoch 16: 0.574\n",
      "test accuracy on epoch 16: 0.692\n",
      "train loss on epoch 17 : 0.405\n",
      "train accuracy on epoch 17: 0.944\n",
      "test loss on epoch 17: 0.570\n",
      "test accuracy on epoch 17: 0.769\n",
      "train loss on epoch 18 : 0.404\n",
      "train accuracy on epoch 18: 0.944\n",
      "test loss on epoch 18: 0.566\n",
      "test accuracy on epoch 18: 0.692\n",
      "train loss on epoch 19 : 0.411\n",
      "train accuracy on epoch 19: 0.833\n",
      "test loss on epoch 19: 0.562\n",
      "test accuracy on epoch 19: 0.692\n",
      "train loss on epoch 20 : 0.377\n",
      "train accuracy on epoch 20: 1.000\n",
      "test loss on epoch 20: 0.559\n",
      "test accuracy on epoch 20: 0.692\n",
      "train loss on epoch 21 : 0.392\n",
      "train accuracy on epoch 21: 0.944\n",
      "test loss on epoch 21: 0.556\n",
      "test accuracy on epoch 21: 0.692\n",
      "train loss on epoch 22 : 0.388\n",
      "train accuracy on epoch 22: 0.889\n",
      "test loss on epoch 22: 0.552\n",
      "test accuracy on epoch 22: 0.692\n",
      "train loss on epoch 23 : 0.374\n",
      "train accuracy on epoch 23: 0.944\n",
      "test loss on epoch 23: 0.549\n",
      "test accuracy on epoch 23: 0.692\n",
      "train loss on epoch 24 : 0.375\n",
      "train accuracy on epoch 24: 0.889\n",
      "test loss on epoch 24: 0.547\n",
      "test accuracy on epoch 24: 0.692\n",
      "train loss on epoch 25 : 0.369\n",
      "train accuracy on epoch 25: 0.944\n",
      "test loss on epoch 25: 0.545\n",
      "test accuracy on epoch 25: 0.692\n",
      "train loss on epoch 26 : 0.339\n",
      "train accuracy on epoch 26: 0.944\n",
      "test loss on epoch 26: 0.543\n",
      "test accuracy on epoch 26: 0.692\n",
      "train loss on epoch 27 : 0.357\n",
      "train accuracy on epoch 27: 0.944\n",
      "test loss on epoch 27: 0.541\n",
      "test accuracy on epoch 27: 0.692\n",
      "train loss on epoch 28 : 0.353\n",
      "train accuracy on epoch 28: 0.889\n",
      "test loss on epoch 28: 0.540\n",
      "test accuracy on epoch 28: 0.692\n",
      "train loss on epoch 29 : 0.318\n",
      "train accuracy on epoch 29: 1.000\n",
      "test loss on epoch 29: 0.537\n",
      "test accuracy on epoch 29: 0.692\n",
      "train loss on epoch 30 : 0.311\n",
      "train accuracy on epoch 30: 1.000\n",
      "test loss on epoch 30: 0.535\n",
      "test accuracy on epoch 30: 0.692\n",
      "train loss on epoch 31 : 0.322\n",
      "train accuracy on epoch 31: 0.889\n",
      "test loss on epoch 31: 0.532\n",
      "test accuracy on epoch 31: 0.692\n",
      "train loss on epoch 32 : 0.320\n",
      "train accuracy on epoch 32: 0.944\n",
      "test loss on epoch 32: 0.530\n",
      "test accuracy on epoch 32: 0.692\n",
      "train loss on epoch 33 : 0.344\n",
      "train accuracy on epoch 33: 0.889\n",
      "test loss on epoch 33: 0.529\n",
      "test accuracy on epoch 33: 0.692\n",
      "train loss on epoch 34 : 0.307\n",
      "train accuracy on epoch 34: 0.944\n",
      "test loss on epoch 34: 0.527\n",
      "test accuracy on epoch 34: 0.692\n",
      "train loss on epoch 35 : 0.314\n",
      "train accuracy on epoch 35: 0.944\n",
      "test loss on epoch 35: 0.526\n",
      "test accuracy on epoch 35: 0.692\n",
      "train loss on epoch 36 : 0.307\n",
      "train accuracy on epoch 36: 0.944\n",
      "test loss on epoch 36: 0.525\n",
      "test accuracy on epoch 36: 0.692\n",
      "train loss on epoch 37 : 0.303\n",
      "train accuracy on epoch 37: 0.889\n",
      "test loss on epoch 37: 0.523\n",
      "test accuracy on epoch 37: 0.692\n",
      "train loss on epoch 38 : 0.275\n",
      "train accuracy on epoch 38: 0.944\n",
      "test loss on epoch 38: 0.522\n",
      "test accuracy on epoch 38: 0.692\n",
      "train loss on epoch 39 : 0.288\n",
      "train accuracy on epoch 39: 0.944\n",
      "test loss on epoch 39: 0.520\n",
      "test accuracy on epoch 39: 0.692\n",
      "train loss on epoch 40 : 0.279\n",
      "train accuracy on epoch 40: 0.889\n",
      "test loss on epoch 40: 0.518\n",
      "test accuracy on epoch 40: 0.692\n",
      "train loss on epoch 41 : 0.266\n",
      "train accuracy on epoch 41: 0.944\n",
      "test loss on epoch 41: 0.517\n",
      "test accuracy on epoch 41: 0.692\n",
      "train loss on epoch 42 : 0.297\n",
      "train accuracy on epoch 42: 0.889\n",
      "test loss on epoch 42: 0.516\n",
      "test accuracy on epoch 42: 0.692\n",
      "train loss on epoch 43 : 0.267\n",
      "train accuracy on epoch 43: 0.944\n",
      "test loss on epoch 43: 0.515\n",
      "test accuracy on epoch 43: 0.692\n",
      "train loss on epoch 44 : 0.269\n",
      "train accuracy on epoch 44: 0.944\n",
      "test loss on epoch 44: 0.514\n",
      "test accuracy on epoch 44: 0.692\n",
      "train loss on epoch 45 : 0.252\n",
      "train accuracy on epoch 45: 1.000\n",
      "test loss on epoch 45: 0.514\n",
      "test accuracy on epoch 45: 0.692\n",
      "train loss on epoch 46 : 0.314\n",
      "train accuracy on epoch 46: 0.889\n",
      "test loss on epoch 46: 0.511\n",
      "test accuracy on epoch 46: 0.692\n",
      "train loss on epoch 47 : 0.237\n",
      "train accuracy on epoch 47: 0.944\n",
      "test loss on epoch 47: 0.510\n",
      "test accuracy on epoch 47: 0.692\n",
      "train loss on epoch 48 : 0.245\n",
      "train accuracy on epoch 48: 0.944\n",
      "test loss on epoch 48: 0.509\n",
      "test accuracy on epoch 48: 0.692\n",
      "train loss on epoch 49 : 0.237\n",
      "train accuracy on epoch 49: 1.000\n",
      "test loss on epoch 49: 0.508\n",
      "test accuracy on epoch 49: 0.692\n",
      "train loss on epoch 50 : 0.263\n",
      "train accuracy on epoch 50: 0.889\n",
      "test loss on epoch 50: 0.507\n",
      "test accuracy on epoch 50: 0.692\n",
      "train loss on epoch 51 : 0.235\n",
      "train accuracy on epoch 51: 1.000\n",
      "test loss on epoch 51: 0.506\n",
      "test accuracy on epoch 51: 0.692\n",
      "train loss on epoch 52 : 0.251\n",
      "train accuracy on epoch 52: 0.889\n",
      "test loss on epoch 52: 0.505\n",
      "test accuracy on epoch 52: 0.692\n",
      "train loss on epoch 53 : 0.265\n",
      "train accuracy on epoch 53: 0.889\n",
      "test loss on epoch 53: 0.503\n",
      "test accuracy on epoch 53: 0.692\n",
      "train loss on epoch 54 : 0.216\n",
      "train accuracy on epoch 54: 1.000\n",
      "test loss on epoch 54: 0.502\n",
      "test accuracy on epoch 54: 0.692\n",
      "train loss on epoch 55 : 0.254\n",
      "train accuracy on epoch 55: 0.889\n",
      "test loss on epoch 55: 0.503\n",
      "test accuracy on epoch 55: 0.692\n",
      "train loss on epoch 56 : 0.235\n",
      "train accuracy on epoch 56: 0.944\n",
      "test loss on epoch 56: 0.504\n",
      "test accuracy on epoch 56: 0.692\n",
      "train loss on epoch 57 : 0.250\n",
      "train accuracy on epoch 57: 0.944\n",
      "test loss on epoch 57: 0.505\n",
      "test accuracy on epoch 57: 0.692\n",
      "train loss on epoch 58 : 0.233\n",
      "train accuracy on epoch 58: 0.889\n",
      "test loss on epoch 58: 0.504\n",
      "test accuracy on epoch 58: 0.692\n",
      "train loss on epoch 59 : 0.266\n",
      "train accuracy on epoch 59: 0.889\n",
      "test loss on epoch 59: 0.504\n",
      "test accuracy on epoch 59: 0.692\n",
      "train loss on epoch 60 : 0.236\n",
      "train accuracy on epoch 60: 0.889\n",
      "test loss on epoch 60: 0.505\n",
      "test accuracy on epoch 60: 0.692\n",
      "train loss on epoch 61 : 0.212\n",
      "train accuracy on epoch 61: 0.944\n",
      "test loss on epoch 61: 0.505\n",
      "test accuracy on epoch 61: 0.692\n",
      "train loss on epoch 62 : 0.235\n",
      "train accuracy on epoch 62: 0.889\n",
      "test loss on epoch 62: 0.503\n",
      "test accuracy on epoch 62: 0.692\n",
      "train loss on epoch 63 : 0.209\n",
      "train accuracy on epoch 63: 0.944\n",
      "test loss on epoch 63: 0.503\n",
      "test accuracy on epoch 63: 0.692\n",
      "train loss on epoch 64 : 0.200\n",
      "train accuracy on epoch 64: 1.000\n",
      "test loss on epoch 64: 0.503\n",
      "test accuracy on epoch 64: 0.692\n",
      "train loss on epoch 65 : 0.215\n",
      "train accuracy on epoch 65: 0.944\n",
      "test loss on epoch 65: 0.503\n",
      "test accuracy on epoch 65: 0.692\n",
      "train loss on epoch 66 : 0.192\n",
      "train accuracy on epoch 66: 1.000\n",
      "test loss on epoch 66: 0.502\n",
      "test accuracy on epoch 66: 0.692\n",
      "train loss on epoch 67 : 0.215\n",
      "train accuracy on epoch 67: 0.944\n",
      "test loss on epoch 67: 0.502\n",
      "test accuracy on epoch 67: 0.692\n",
      "train loss on epoch 68 : 0.202\n",
      "train accuracy on epoch 68: 0.944\n",
      "test loss on epoch 68: 0.503\n",
      "test accuracy on epoch 68: 0.692\n",
      "train loss on epoch 69 : 0.229\n",
      "train accuracy on epoch 69: 0.889\n",
      "test loss on epoch 69: 0.502\n",
      "test accuracy on epoch 69: 0.692\n",
      "train loss on epoch 70 : 0.216\n",
      "train accuracy on epoch 70: 0.889\n",
      "test loss on epoch 70: 0.501\n",
      "test accuracy on epoch 70: 0.692\n",
      "train loss on epoch 71 : 0.220\n",
      "train accuracy on epoch 71: 0.889\n",
      "test loss on epoch 71: 0.502\n",
      "test accuracy on epoch 71: 0.692\n",
      "train loss on epoch 72 : 0.200\n",
      "train accuracy on epoch 72: 0.944\n",
      "test loss on epoch 72: 0.505\n",
      "test accuracy on epoch 72: 0.692\n",
      "train loss on epoch 73 : 0.184\n",
      "train accuracy on epoch 73: 0.944\n",
      "test loss on epoch 73: 0.506\n",
      "test accuracy on epoch 73: 0.692\n",
      "train loss on epoch 74 : 0.236\n",
      "train accuracy on epoch 74: 0.944\n",
      "test loss on epoch 74: 0.507\n",
      "test accuracy on epoch 74: 0.692\n",
      "train loss on epoch 75 : 0.174\n",
      "train accuracy on epoch 75: 0.944\n",
      "test loss on epoch 75: 0.505\n",
      "test accuracy on epoch 75: 0.692\n",
      "train loss on epoch 76 : 0.272\n",
      "train accuracy on epoch 76: 0.889\n",
      "test loss on epoch 76: 0.503\n",
      "test accuracy on epoch 76: 0.692\n",
      "train loss on epoch 77 : 0.197\n",
      "train accuracy on epoch 77: 0.889\n",
      "test loss on epoch 77: 0.505\n",
      "test accuracy on epoch 77: 0.692\n",
      "train loss on epoch 78 : 0.158\n",
      "train accuracy on epoch 78: 1.000\n",
      "test loss on epoch 78: 0.505\n",
      "test accuracy on epoch 78: 0.692\n",
      "train loss on epoch 79 : 0.183\n",
      "train accuracy on epoch 79: 0.944\n",
      "test loss on epoch 79: 0.505\n",
      "test accuracy on epoch 79: 0.692\n",
      "train loss on epoch 80 : 0.206\n",
      "train accuracy on epoch 80: 0.944\n",
      "test loss on epoch 80: 0.504\n",
      "test accuracy on epoch 80: 0.692\n",
      "train loss on epoch 81 : 0.208\n",
      "train accuracy on epoch 81: 0.889\n",
      "test loss on epoch 81: 0.502\n",
      "test accuracy on epoch 81: 0.692\n",
      "train loss on epoch 82 : 0.205\n",
      "train accuracy on epoch 82: 0.889\n",
      "test loss on epoch 82: 0.501\n",
      "test accuracy on epoch 82: 0.692\n",
      "train loss on epoch 83 : 0.198\n",
      "train accuracy on epoch 83: 0.944\n",
      "test loss on epoch 83: 0.498\n",
      "test accuracy on epoch 83: 0.692\n",
      "train loss on epoch 84 : 0.192\n",
      "train accuracy on epoch 84: 0.944\n",
      "test loss on epoch 84: 0.493\n",
      "test accuracy on epoch 84: 0.692\n",
      "train loss on epoch 85 : 0.205\n",
      "train accuracy on epoch 85: 0.944\n",
      "test loss on epoch 85: 0.491\n",
      "test accuracy on epoch 85: 0.692\n",
      "train loss on epoch 86 : 0.205\n",
      "train accuracy on epoch 86: 0.944\n",
      "test loss on epoch 86: 0.493\n",
      "test accuracy on epoch 86: 0.692\n",
      "train loss on epoch 87 : 0.208\n",
      "train accuracy on epoch 87: 0.944\n",
      "test loss on epoch 87: 0.494\n",
      "test accuracy on epoch 87: 0.692\n",
      "train loss on epoch 88 : 0.215\n",
      "train accuracy on epoch 88: 0.944\n",
      "test loss on epoch 88: 0.496\n",
      "test accuracy on epoch 88: 0.692\n",
      "train loss on epoch 89 : 0.168\n",
      "train accuracy on epoch 89: 1.000\n",
      "test loss on epoch 89: 0.498\n",
      "test accuracy on epoch 89: 0.692\n",
      "train loss on epoch 90 : 0.161\n",
      "train accuracy on epoch 90: 1.000\n",
      "test loss on epoch 90: 0.498\n",
      "test accuracy on epoch 90: 0.692\n",
      "train loss on epoch 91 : 0.133\n",
      "train accuracy on epoch 91: 1.000\n",
      "test loss on epoch 91: 0.501\n",
      "test accuracy on epoch 91: 0.692\n",
      "train loss on epoch 92 : 0.159\n",
      "train accuracy on epoch 92: 1.000\n",
      "test loss on epoch 92: 0.503\n",
      "test accuracy on epoch 92: 0.692\n",
      "train loss on epoch 93 : 0.181\n",
      "train accuracy on epoch 93: 0.944\n",
      "test loss on epoch 93: 0.502\n",
      "test accuracy on epoch 93: 0.692\n",
      "train loss on epoch 94 : 0.214\n",
      "train accuracy on epoch 94: 0.944\n",
      "test loss on epoch 94: 0.499\n",
      "test accuracy on epoch 94: 0.692\n",
      "train loss on epoch 95 : 0.190\n",
      "train accuracy on epoch 95: 0.889\n",
      "test loss on epoch 95: 0.500\n",
      "test accuracy on epoch 95: 0.692\n",
      "train loss on epoch 96 : 0.197\n",
      "train accuracy on epoch 96: 0.944\n",
      "test loss on epoch 96: 0.501\n",
      "test accuracy on epoch 96: 0.692\n",
      "train loss on epoch 97 : 0.194\n",
      "train accuracy on epoch 97: 0.889\n",
      "test loss on epoch 97: 0.499\n",
      "test accuracy on epoch 97: 0.692\n",
      "train loss on epoch 98 : 0.199\n",
      "train accuracy on epoch 98: 0.944\n",
      "test loss on epoch 98: 0.499\n",
      "test accuracy on epoch 98: 0.692\n",
      "train loss on epoch 99 : 0.169\n",
      "train accuracy on epoch 99: 0.944\n",
      "test loss on epoch 99: 0.503\n",
      "test accuracy on epoch 99: 0.692\n",
      "train loss on epoch 100 : 0.164\n",
      "train accuracy on epoch 100: 0.944\n",
      "test loss on epoch 100: 0.507\n",
      "test accuracy on epoch 100: 0.692\n",
      "train loss on epoch 101 : 0.169\n",
      "train accuracy on epoch 101: 0.944\n",
      "test loss on epoch 101: 0.508\n",
      "test accuracy on epoch 101: 0.692\n",
      "train loss on epoch 102 : 0.168\n",
      "train accuracy on epoch 102: 0.944\n",
      "test loss on epoch 102: 0.508\n",
      "test accuracy on epoch 102: 0.692\n",
      "train loss on epoch 103 : 0.160\n",
      "train accuracy on epoch 103: 0.944\n",
      "test loss on epoch 103: 0.512\n",
      "test accuracy on epoch 103: 0.692\n",
      "train loss on epoch 104 : 0.158\n",
      "train accuracy on epoch 104: 0.944\n",
      "test loss on epoch 104: 0.515\n",
      "test accuracy on epoch 104: 0.692\n",
      "train loss on epoch 105 : 0.159\n",
      "train accuracy on epoch 105: 0.944\n",
      "test loss on epoch 105: 0.517\n",
      "test accuracy on epoch 105: 0.692\n",
      "train loss on epoch 106 : 0.185\n",
      "train accuracy on epoch 106: 0.944\n",
      "test loss on epoch 106: 0.515\n",
      "test accuracy on epoch 106: 0.692\n",
      "train loss on epoch 107 : 0.171\n",
      "train accuracy on epoch 107: 0.944\n",
      "test loss on epoch 107: 0.511\n",
      "test accuracy on epoch 107: 0.692\n",
      "train loss on epoch 108 : 0.135\n",
      "train accuracy on epoch 108: 1.000\n",
      "test loss on epoch 108: 0.510\n",
      "test accuracy on epoch 108: 0.692\n",
      "train loss on epoch 109 : 0.123\n",
      "train accuracy on epoch 109: 1.000\n",
      "test loss on epoch 109: 0.508\n",
      "test accuracy on epoch 109: 0.692\n",
      "train loss on epoch 110 : 0.126\n",
      "train accuracy on epoch 110: 1.000\n",
      "test loss on epoch 110: 0.509\n",
      "test accuracy on epoch 110: 0.692\n",
      "train loss on epoch 111 : 0.136\n",
      "train accuracy on epoch 111: 0.944\n",
      "test loss on epoch 111: 0.508\n",
      "test accuracy on epoch 111: 0.692\n",
      "train loss on epoch 112 : 0.184\n",
      "train accuracy on epoch 112: 0.944\n",
      "test loss on epoch 112: 0.507\n",
      "test accuracy on epoch 112: 0.692\n",
      "train loss on epoch 113 : 0.178\n",
      "train accuracy on epoch 113: 0.889\n",
      "test loss on epoch 113: 0.505\n",
      "test accuracy on epoch 113: 0.692\n",
      "train loss on epoch 114 : 0.182\n",
      "train accuracy on epoch 114: 0.944\n",
      "test loss on epoch 114: 0.508\n",
      "test accuracy on epoch 114: 0.692\n",
      "train loss on epoch 115 : 0.184\n",
      "train accuracy on epoch 115: 0.944\n",
      "test loss on epoch 115: 0.506\n",
      "test accuracy on epoch 115: 0.692\n",
      "train loss on epoch 116 : 0.176\n",
      "train accuracy on epoch 116: 1.000\n",
      "test loss on epoch 116: 0.501\n",
      "test accuracy on epoch 116: 0.692\n",
      "train loss on epoch 117 : 0.184\n",
      "train accuracy on epoch 117: 0.889\n",
      "test loss on epoch 117: 0.499\n",
      "test accuracy on epoch 117: 0.692\n",
      "train loss on epoch 118 : 0.166\n",
      "train accuracy on epoch 118: 0.944\n",
      "test loss on epoch 118: 0.500\n",
      "test accuracy on epoch 118: 0.692\n",
      "train loss on epoch 119 : 0.136\n",
      "train accuracy on epoch 119: 0.944\n",
      "test loss on epoch 119: 0.501\n",
      "test accuracy on epoch 119: 0.692\n",
      "train loss on epoch 120 : 0.175\n",
      "train accuracy on epoch 120: 0.944\n",
      "test loss on epoch 120: 0.502\n",
      "test accuracy on epoch 120: 0.692\n",
      "train loss on epoch 121 : 0.141\n",
      "train accuracy on epoch 121: 0.944\n",
      "test loss on epoch 121: 0.504\n",
      "test accuracy on epoch 121: 0.692\n",
      "train loss on epoch 122 : 0.192\n",
      "train accuracy on epoch 122: 0.944\n",
      "test loss on epoch 122: 0.507\n",
      "test accuracy on epoch 122: 0.692\n",
      "train loss on epoch 123 : 0.152\n",
      "train accuracy on epoch 123: 0.889\n",
      "test loss on epoch 123: 0.506\n",
      "test accuracy on epoch 123: 0.692\n",
      "train loss on epoch 124 : 0.166\n",
      "train accuracy on epoch 124: 0.944\n",
      "test loss on epoch 124: 0.507\n",
      "test accuracy on epoch 124: 0.692\n",
      "train loss on epoch 125 : 0.215\n",
      "train accuracy on epoch 125: 0.889\n",
      "test loss on epoch 125: 0.510\n",
      "test accuracy on epoch 125: 0.692\n",
      "train loss on epoch 126 : 0.164\n",
      "train accuracy on epoch 126: 0.944\n",
      "test loss on epoch 126: 0.510\n",
      "test accuracy on epoch 126: 0.692\n",
      "train loss on epoch 127 : 0.137\n",
      "train accuracy on epoch 127: 1.000\n",
      "test loss on epoch 127: 0.509\n",
      "test accuracy on epoch 127: 0.692\n",
      "train loss on epoch 128 : 0.173\n",
      "train accuracy on epoch 128: 0.944\n",
      "test loss on epoch 128: 0.506\n",
      "test accuracy on epoch 128: 0.692\n",
      "train loss on epoch 129 : 0.157\n",
      "train accuracy on epoch 129: 0.944\n",
      "test loss on epoch 129: 0.507\n",
      "test accuracy on epoch 129: 0.692\n",
      "train loss on epoch 130 : 0.175\n",
      "train accuracy on epoch 130: 0.889\n",
      "test loss on epoch 130: 0.506\n",
      "test accuracy on epoch 130: 0.692\n",
      "train loss on epoch 131 : 0.198\n",
      "train accuracy on epoch 131: 0.889\n",
      "test loss on epoch 131: 0.509\n",
      "test accuracy on epoch 131: 0.692\n",
      "train loss on epoch 132 : 0.133\n",
      "train accuracy on epoch 132: 1.000\n",
      "test loss on epoch 132: 0.511\n",
      "test accuracy on epoch 132: 0.692\n",
      "train loss on epoch 133 : 0.127\n",
      "train accuracy on epoch 133: 0.944\n",
      "test loss on epoch 133: 0.510\n",
      "test accuracy on epoch 133: 0.692\n",
      "train loss on epoch 134 : 0.157\n",
      "train accuracy on epoch 134: 0.944\n",
      "test loss on epoch 134: 0.510\n",
      "test accuracy on epoch 134: 0.692\n",
      "train loss on epoch 135 : 0.168\n",
      "train accuracy on epoch 135: 0.944\n",
      "test loss on epoch 135: 0.512\n",
      "test accuracy on epoch 135: 0.692\n",
      "train loss on epoch 136 : 0.127\n",
      "train accuracy on epoch 136: 1.000\n",
      "test loss on epoch 136: 0.511\n",
      "test accuracy on epoch 136: 0.692\n",
      "train loss on epoch 137 : 0.170\n",
      "train accuracy on epoch 137: 0.889\n",
      "test loss on epoch 137: 0.512\n",
      "test accuracy on epoch 137: 0.692\n",
      "train loss on epoch 138 : 0.149\n",
      "train accuracy on epoch 138: 0.944\n",
      "test loss on epoch 138: 0.517\n",
      "test accuracy on epoch 138: 0.692\n",
      "train loss on epoch 139 : 0.198\n",
      "train accuracy on epoch 139: 0.889\n",
      "test loss on epoch 139: 0.522\n",
      "test accuracy on epoch 139: 0.692\n",
      "train loss on epoch 140 : 0.125\n",
      "train accuracy on epoch 140: 1.000\n",
      "test loss on epoch 140: 0.527\n",
      "test accuracy on epoch 140: 0.692\n",
      "train loss on epoch 141 : 0.151\n",
      "train accuracy on epoch 141: 0.944\n",
      "test loss on epoch 141: 0.532\n",
      "test accuracy on epoch 141: 0.692\n",
      "train loss on epoch 142 : 0.135\n",
      "train accuracy on epoch 142: 0.944\n",
      "test loss on epoch 142: 0.533\n",
      "test accuracy on epoch 142: 0.692\n",
      "train loss on epoch 143 : 0.192\n",
      "train accuracy on epoch 143: 0.889\n",
      "test loss on epoch 143: 0.536\n",
      "test accuracy on epoch 143: 0.692\n",
      "train loss on epoch 144 : 0.139\n",
      "train accuracy on epoch 144: 0.944\n",
      "test loss on epoch 144: 0.534\n",
      "test accuracy on epoch 144: 0.692\n",
      "train loss on epoch 145 : 0.152\n",
      "train accuracy on epoch 145: 0.889\n",
      "test loss on epoch 145: 0.532\n",
      "test accuracy on epoch 145: 0.692\n",
      "train loss on epoch 146 : 0.167\n",
      "train accuracy on epoch 146: 0.944\n",
      "test loss on epoch 146: 0.533\n",
      "test accuracy on epoch 146: 0.692\n",
      "train loss on epoch 147 : 0.149\n",
      "train accuracy on epoch 147: 0.944\n",
      "test loss on epoch 147: 0.535\n",
      "test accuracy on epoch 147: 0.692\n",
      "train loss on epoch 148 : 0.140\n",
      "train accuracy on epoch 148: 0.944\n",
      "test loss on epoch 148: 0.532\n",
      "test accuracy on epoch 148: 0.692\n",
      "train loss on epoch 149 : 0.114\n",
      "train accuracy on epoch 149: 0.944\n",
      "test loss on epoch 149: 0.526\n",
      "test accuracy on epoch 149: 0.692\n",
      "train loss on epoch 150 : 0.169\n",
      "train accuracy on epoch 150: 0.889\n",
      "test loss on epoch 150: 0.521\n",
      "test accuracy on epoch 150: 0.692\n",
      "train loss on epoch 151 : 0.152\n",
      "train accuracy on epoch 151: 0.889\n",
      "test loss on epoch 151: 0.523\n",
      "test accuracy on epoch 151: 0.692\n",
      "train loss on epoch 152 : 0.138\n",
      "train accuracy on epoch 152: 0.944\n",
      "test loss on epoch 152: 0.522\n",
      "test accuracy on epoch 152: 0.692\n",
      "train loss on epoch 153 : 0.128\n",
      "train accuracy on epoch 153: 1.000\n",
      "test loss on epoch 153: 0.521\n",
      "test accuracy on epoch 153: 0.692\n",
      "train loss on epoch 154 : 0.128\n",
      "train accuracy on epoch 154: 1.000\n",
      "test loss on epoch 154: 0.516\n",
      "test accuracy on epoch 154: 0.692\n",
      "train loss on epoch 155 : 0.127\n",
      "train accuracy on epoch 155: 0.944\n",
      "test loss on epoch 155: 0.514\n",
      "test accuracy on epoch 155: 0.692\n",
      "train loss on epoch 156 : 0.086\n",
      "train accuracy on epoch 156: 1.000\n",
      "test loss on epoch 156: 0.514\n",
      "test accuracy on epoch 156: 0.692\n",
      "train loss on epoch 157 : 0.179\n",
      "train accuracy on epoch 157: 0.889\n",
      "test loss on epoch 157: 0.514\n",
      "test accuracy on epoch 157: 0.692\n",
      "train loss on epoch 158 : 0.130\n",
      "train accuracy on epoch 158: 0.944\n",
      "test loss on epoch 158: 0.516\n",
      "test accuracy on epoch 158: 0.692\n",
      "train loss on epoch 159 : 0.169\n",
      "train accuracy on epoch 159: 0.889\n",
      "test loss on epoch 159: 0.518\n",
      "test accuracy on epoch 159: 0.692\n",
      "train loss on epoch 160 : 0.155\n",
      "train accuracy on epoch 160: 0.944\n",
      "test loss on epoch 160: 0.517\n",
      "test accuracy on epoch 160: 0.692\n",
      "train loss on epoch 161 : 0.115\n",
      "train accuracy on epoch 161: 0.944\n",
      "test loss on epoch 161: 0.519\n",
      "test accuracy on epoch 161: 0.692\n",
      "train loss on epoch 162 : 0.156\n",
      "train accuracy on epoch 162: 0.944\n",
      "test loss on epoch 162: 0.520\n",
      "test accuracy on epoch 162: 0.692\n",
      "train loss on epoch 163 : 0.154\n",
      "train accuracy on epoch 163: 0.889\n",
      "test loss on epoch 163: 0.522\n",
      "test accuracy on epoch 163: 0.692\n",
      "train loss on epoch 164 : 0.116\n",
      "train accuracy on epoch 164: 0.944\n",
      "test loss on epoch 164: 0.523\n",
      "test accuracy on epoch 164: 0.692\n",
      "train loss on epoch 165 : 0.121\n",
      "train accuracy on epoch 165: 1.000\n",
      "test loss on epoch 165: 0.526\n",
      "test accuracy on epoch 165: 0.692\n",
      "train loss on epoch 166 : 0.108\n",
      "train accuracy on epoch 166: 1.000\n",
      "test loss on epoch 166: 0.525\n",
      "test accuracy on epoch 166: 0.692\n",
      "train loss on epoch 167 : 0.133\n",
      "train accuracy on epoch 167: 0.944\n",
      "test loss on epoch 167: 0.528\n",
      "test accuracy on epoch 167: 0.692\n",
      "train loss on epoch 168 : 0.154\n",
      "train accuracy on epoch 168: 0.944\n",
      "test loss on epoch 168: 0.530\n",
      "test accuracy on epoch 168: 0.692\n",
      "train loss on epoch 169 : 0.124\n",
      "train accuracy on epoch 169: 0.944\n",
      "test loss on epoch 169: 0.532\n",
      "test accuracy on epoch 169: 0.692\n",
      "train loss on epoch 170 : 0.158\n",
      "train accuracy on epoch 170: 0.944\n",
      "test loss on epoch 170: 0.535\n",
      "test accuracy on epoch 170: 0.692\n",
      "train loss on epoch 171 : 0.165\n",
      "train accuracy on epoch 171: 0.889\n",
      "test loss on epoch 171: 0.541\n",
      "test accuracy on epoch 171: 0.615\n",
      "train loss on epoch 172 : 0.162\n",
      "train accuracy on epoch 172: 0.889\n",
      "test loss on epoch 172: 0.543\n",
      "test accuracy on epoch 172: 0.615\n",
      "train loss on epoch 173 : 0.136\n",
      "train accuracy on epoch 173: 0.944\n",
      "test loss on epoch 173: 0.544\n",
      "test accuracy on epoch 173: 0.615\n",
      "train loss on epoch 174 : 0.171\n",
      "train accuracy on epoch 174: 0.944\n",
      "test loss on epoch 174: 0.547\n",
      "test accuracy on epoch 174: 0.615\n",
      "train loss on epoch 175 : 0.127\n",
      "train accuracy on epoch 175: 0.944\n",
      "test loss on epoch 175: 0.547\n",
      "test accuracy on epoch 175: 0.615\n",
      "train loss on epoch 176 : 0.155\n",
      "train accuracy on epoch 176: 0.889\n",
      "test loss on epoch 176: 0.544\n",
      "test accuracy on epoch 176: 0.615\n",
      "train loss on epoch 177 : 0.110\n",
      "train accuracy on epoch 177: 1.000\n",
      "test loss on epoch 177: 0.542\n",
      "test accuracy on epoch 177: 0.615\n",
      "train loss on epoch 178 : 0.110\n",
      "train accuracy on epoch 178: 0.944\n",
      "test loss on epoch 178: 0.540\n",
      "test accuracy on epoch 178: 0.692\n",
      "train loss on epoch 179 : 0.121\n",
      "train accuracy on epoch 179: 0.944\n",
      "test loss on epoch 179: 0.531\n",
      "test accuracy on epoch 179: 0.692\n",
      "train loss on epoch 180 : 0.120\n",
      "train accuracy on epoch 180: 1.000\n",
      "test loss on epoch 180: 0.529\n",
      "test accuracy on epoch 180: 0.692\n",
      "train loss on epoch 181 : 0.138\n",
      "train accuracy on epoch 181: 0.944\n",
      "test loss on epoch 181: 0.527\n",
      "test accuracy on epoch 181: 0.692\n",
      "train loss on epoch 182 : 0.145\n",
      "train accuracy on epoch 182: 0.944\n",
      "test loss on epoch 182: 0.527\n",
      "test accuracy on epoch 182: 0.692\n",
      "train loss on epoch 183 : 0.115\n",
      "train accuracy on epoch 183: 1.000\n",
      "test loss on epoch 183: 0.529\n",
      "test accuracy on epoch 183: 0.692\n",
      "train loss on epoch 184 : 0.103\n",
      "train accuracy on epoch 184: 1.000\n",
      "test loss on epoch 184: 0.531\n",
      "test accuracy on epoch 184: 0.692\n",
      "train loss on epoch 185 : 0.176\n",
      "train accuracy on epoch 185: 0.889\n",
      "test loss on epoch 185: 0.536\n",
      "test accuracy on epoch 185: 0.692\n",
      "train loss on epoch 186 : 0.145\n",
      "train accuracy on epoch 186: 0.889\n",
      "test loss on epoch 186: 0.542\n",
      "test accuracy on epoch 186: 0.692\n",
      "train loss on epoch 187 : 0.157\n",
      "train accuracy on epoch 187: 0.889\n",
      "test loss on epoch 187: 0.542\n",
      "test accuracy on epoch 187: 0.692\n",
      "train loss on epoch 188 : 0.090\n",
      "train accuracy on epoch 188: 1.000\n",
      "test loss on epoch 188: 0.544\n",
      "test accuracy on epoch 188: 0.692\n",
      "train loss on epoch 189 : 0.081\n",
      "train accuracy on epoch 189: 1.000\n",
      "test loss on epoch 189: 0.546\n",
      "test accuracy on epoch 189: 0.615\n",
      "train loss on epoch 190 : 0.100\n",
      "train accuracy on epoch 190: 1.000\n",
      "test loss on epoch 190: 0.544\n",
      "test accuracy on epoch 190: 0.692\n",
      "train loss on epoch 191 : 0.157\n",
      "train accuracy on epoch 191: 0.889\n",
      "test loss on epoch 191: 0.542\n",
      "test accuracy on epoch 191: 0.692\n",
      "train loss on epoch 192 : 0.113\n",
      "train accuracy on epoch 192: 0.944\n",
      "test loss on epoch 192: 0.541\n",
      "test accuracy on epoch 192: 0.692\n",
      "train loss on epoch 193 : 0.080\n",
      "train accuracy on epoch 193: 1.000\n",
      "test loss on epoch 193: 0.538\n",
      "test accuracy on epoch 193: 0.692\n",
      "train loss on epoch 194 : 0.177\n",
      "train accuracy on epoch 194: 0.889\n",
      "test loss on epoch 194: 0.534\n",
      "test accuracy on epoch 194: 0.692\n",
      "train loss on epoch 195 : 0.169\n",
      "train accuracy on epoch 195: 0.889\n",
      "test loss on epoch 195: 0.531\n",
      "test accuracy on epoch 195: 0.692\n",
      "train loss on epoch 196 : 0.139\n",
      "train accuracy on epoch 196: 0.889\n",
      "test loss on epoch 196: 0.535\n",
      "test accuracy on epoch 196: 0.692\n",
      "train loss on epoch 197 : 0.153\n",
      "train accuracy on epoch 197: 0.944\n",
      "test loss on epoch 197: 0.536\n",
      "test accuracy on epoch 197: 0.692\n",
      "train loss on epoch 198 : 0.139\n",
      "train accuracy on epoch 198: 0.889\n",
      "test loss on epoch 198: 0.538\n",
      "test accuracy on epoch 198: 0.692\n",
      "train loss on epoch 199 : 0.156\n",
      "train accuracy on epoch 199: 0.889\n",
      "test loss on epoch 199: 0.534\n",
      "test accuracy on epoch 199: 0.692\n",
      "train loss on epoch 200 : 0.095\n",
      "train accuracy on epoch 200: 0.944\n",
      "test loss on epoch 200: 0.530\n",
      "test accuracy on epoch 200: 0.692\n",
      "train loss on epoch 201 : 0.117\n",
      "train accuracy on epoch 201: 0.944\n",
      "test loss on epoch 201: 0.531\n",
      "test accuracy on epoch 201: 0.692\n",
      "train loss on epoch 202 : 0.168\n",
      "train accuracy on epoch 202: 0.889\n",
      "test loss on epoch 202: 0.529\n",
      "test accuracy on epoch 202: 0.692\n",
      "train loss on epoch 203 : 0.152\n",
      "train accuracy on epoch 203: 0.944\n",
      "test loss on epoch 203: 0.532\n",
      "test accuracy on epoch 203: 0.692\n",
      "train loss on epoch 204 : 0.095\n",
      "train accuracy on epoch 204: 1.000\n",
      "test loss on epoch 204: 0.532\n",
      "test accuracy on epoch 204: 0.692\n",
      "train loss on epoch 205 : 0.138\n",
      "train accuracy on epoch 205: 0.944\n",
      "test loss on epoch 205: 0.534\n",
      "test accuracy on epoch 205: 0.692\n",
      "train loss on epoch 206 : 0.092\n",
      "train accuracy on epoch 206: 1.000\n",
      "test loss on epoch 206: 0.539\n",
      "test accuracy on epoch 206: 0.692\n",
      "train loss on epoch 207 : 0.114\n",
      "train accuracy on epoch 207: 0.944\n",
      "test loss on epoch 207: 0.536\n",
      "test accuracy on epoch 207: 0.692\n",
      "train loss on epoch 208 : 0.113\n",
      "train accuracy on epoch 208: 1.000\n",
      "test loss on epoch 208: 0.535\n",
      "test accuracy on epoch 208: 0.692\n",
      "train loss on epoch 209 : 0.139\n",
      "train accuracy on epoch 209: 0.889\n",
      "test loss on epoch 209: 0.535\n",
      "test accuracy on epoch 209: 0.692\n",
      "train loss on epoch 210 : 0.114\n",
      "train accuracy on epoch 210: 0.944\n",
      "test loss on epoch 210: 0.535\n",
      "test accuracy on epoch 210: 0.692\n",
      "train loss on epoch 211 : 0.176\n",
      "train accuracy on epoch 211: 0.889\n",
      "test loss on epoch 211: 0.535\n",
      "test accuracy on epoch 211: 0.692\n",
      "train loss on epoch 212 : 0.121\n",
      "train accuracy on epoch 212: 0.889\n",
      "test loss on epoch 212: 0.536\n",
      "test accuracy on epoch 212: 0.692\n",
      "train loss on epoch 213 : 0.125\n",
      "train accuracy on epoch 213: 0.944\n",
      "test loss on epoch 213: 0.539\n",
      "test accuracy on epoch 213: 0.692\n",
      "train loss on epoch 214 : 0.121\n",
      "train accuracy on epoch 214: 0.944\n",
      "test loss on epoch 214: 0.543\n",
      "test accuracy on epoch 214: 0.692\n",
      "train loss on epoch 215 : 0.113\n",
      "train accuracy on epoch 215: 0.944\n",
      "test loss on epoch 215: 0.545\n",
      "test accuracy on epoch 215: 0.692\n",
      "train loss on epoch 216 : 0.179\n",
      "train accuracy on epoch 216: 0.889\n",
      "test loss on epoch 216: 0.547\n",
      "test accuracy on epoch 216: 0.692\n",
      "train loss on epoch 217 : 0.120\n",
      "train accuracy on epoch 217: 0.944\n",
      "test loss on epoch 217: 0.558\n",
      "test accuracy on epoch 217: 0.615\n",
      "train loss on epoch 218 : 0.129\n",
      "train accuracy on epoch 218: 1.000\n",
      "test loss on epoch 218: 0.567\n",
      "test accuracy on epoch 218: 0.615\n",
      "train loss on epoch 219 : 0.114\n",
      "train accuracy on epoch 219: 0.944\n",
      "test loss on epoch 219: 0.571\n",
      "test accuracy on epoch 219: 0.615\n",
      "train loss on epoch 220 : 0.112\n",
      "train accuracy on epoch 220: 0.944\n",
      "test loss on epoch 220: 0.571\n",
      "test accuracy on epoch 220: 0.615\n",
      "train loss on epoch 221 : 0.103\n",
      "train accuracy on epoch 221: 1.000\n",
      "test loss on epoch 221: 0.572\n",
      "test accuracy on epoch 221: 0.615\n",
      "train loss on epoch 222 : 0.246\n",
      "train accuracy on epoch 222: 0.889\n",
      "test loss on epoch 222: 0.569\n",
      "test accuracy on epoch 222: 0.615\n",
      "train loss on epoch 223 : 0.143\n",
      "train accuracy on epoch 223: 0.889\n",
      "test loss on epoch 223: 0.564\n",
      "test accuracy on epoch 223: 0.615\n",
      "train loss on epoch 224 : 0.117\n",
      "train accuracy on epoch 224: 0.889\n",
      "test loss on epoch 224: 0.562\n",
      "test accuracy on epoch 224: 0.692\n",
      "train loss on epoch 225 : 0.109\n",
      "train accuracy on epoch 225: 0.944\n",
      "test loss on epoch 225: 0.561\n",
      "test accuracy on epoch 225: 0.692\n",
      "train loss on epoch 226 : 0.150\n",
      "train accuracy on epoch 226: 0.889\n",
      "test loss on epoch 226: 0.561\n",
      "test accuracy on epoch 226: 0.692\n",
      "train loss on epoch 227 : 0.093\n",
      "train accuracy on epoch 227: 1.000\n",
      "test loss on epoch 227: 0.559\n",
      "test accuracy on epoch 227: 0.692\n",
      "train loss on epoch 228 : 0.124\n",
      "train accuracy on epoch 228: 0.944\n",
      "test loss on epoch 228: 0.559\n",
      "test accuracy on epoch 228: 0.692\n",
      "train loss on epoch 229 : 0.095\n",
      "train accuracy on epoch 229: 0.944\n",
      "test loss on epoch 229: 0.554\n",
      "test accuracy on epoch 229: 0.692\n",
      "train loss on epoch 230 : 0.082\n",
      "train accuracy on epoch 230: 1.000\n",
      "test loss on epoch 230: 0.554\n",
      "test accuracy on epoch 230: 0.692\n",
      "train loss on epoch 231 : 0.105\n",
      "train accuracy on epoch 231: 0.944\n",
      "test loss on epoch 231: 0.558\n",
      "test accuracy on epoch 231: 0.692\n",
      "train loss on epoch 232 : 0.095\n",
      "train accuracy on epoch 232: 0.944\n",
      "test loss on epoch 232: 0.564\n",
      "test accuracy on epoch 232: 0.692\n",
      "train loss on epoch 233 : 0.092\n",
      "train accuracy on epoch 233: 1.000\n",
      "test loss on epoch 233: 0.568\n",
      "test accuracy on epoch 233: 0.692\n",
      "train loss on epoch 234 : 0.142\n",
      "train accuracy on epoch 234: 0.944\n",
      "test loss on epoch 234: 0.578\n",
      "test accuracy on epoch 234: 0.615\n",
      "train loss on epoch 235 : 0.095\n",
      "train accuracy on epoch 235: 1.000\n",
      "test loss on epoch 235: 0.584\n",
      "test accuracy on epoch 235: 0.615\n",
      "train loss on epoch 236 : 0.186\n",
      "train accuracy on epoch 236: 0.889\n",
      "test loss on epoch 236: 0.591\n",
      "test accuracy on epoch 236: 0.615\n",
      "train loss on epoch 237 : 0.069\n",
      "train accuracy on epoch 237: 1.000\n",
      "test loss on epoch 237: 0.588\n",
      "test accuracy on epoch 237: 0.615\n",
      "train loss on epoch 238 : 0.125\n",
      "train accuracy on epoch 238: 0.889\n",
      "test loss on epoch 238: 0.585\n",
      "test accuracy on epoch 238: 0.615\n",
      "train loss on epoch 239 : 0.111\n",
      "train accuracy on epoch 239: 0.944\n",
      "test loss on epoch 239: 0.583\n",
      "test accuracy on epoch 239: 0.615\n",
      "train loss on epoch 240 : 0.105\n",
      "train accuracy on epoch 240: 0.944\n",
      "test loss on epoch 240: 0.580\n",
      "test accuracy on epoch 240: 0.615\n",
      "train loss on epoch 241 : 0.075\n",
      "train accuracy on epoch 241: 1.000\n",
      "test loss on epoch 241: 0.583\n",
      "test accuracy on epoch 241: 0.615\n",
      "train loss on epoch 242 : 0.100\n",
      "train accuracy on epoch 242: 0.944\n",
      "test loss on epoch 242: 0.583\n",
      "test accuracy on epoch 242: 0.615\n",
      "train loss on epoch 243 : 0.086\n",
      "train accuracy on epoch 243: 1.000\n",
      "test loss on epoch 243: 0.580\n",
      "test accuracy on epoch 243: 0.615\n",
      "train loss on epoch 244 : 0.131\n",
      "train accuracy on epoch 244: 0.944\n",
      "test loss on epoch 244: 0.572\n",
      "test accuracy on epoch 244: 0.692\n",
      "train loss on epoch 245 : 0.077\n",
      "train accuracy on epoch 245: 1.000\n",
      "test loss on epoch 245: 0.572\n",
      "test accuracy on epoch 245: 0.692\n",
      "train loss on epoch 246 : 0.124\n",
      "train accuracy on epoch 246: 0.944\n",
      "test loss on epoch 246: 0.572\n",
      "test accuracy on epoch 246: 0.692\n",
      "train loss on epoch 247 : 0.160\n",
      "train accuracy on epoch 247: 0.889\n",
      "test loss on epoch 247: 0.576\n",
      "test accuracy on epoch 247: 0.692\n",
      "train loss on epoch 248 : 0.094\n",
      "train accuracy on epoch 248: 0.944\n",
      "test loss on epoch 248: 0.575\n",
      "test accuracy on epoch 248: 0.692\n",
      "train loss on epoch 249 : 0.147\n",
      "train accuracy on epoch 249: 0.889\n",
      "test loss on epoch 249: 0.578\n",
      "test accuracy on epoch 249: 0.615\n",
      "train loss on epoch 250 : 0.169\n",
      "train accuracy on epoch 250: 0.889\n",
      "test loss on epoch 250: 0.577\n",
      "test accuracy on epoch 250: 0.692\n",
      "train loss on epoch 251 : 0.159\n",
      "train accuracy on epoch 251: 0.889\n",
      "test loss on epoch 251: 0.568\n",
      "test accuracy on epoch 251: 0.692\n",
      "train loss on epoch 252 : 0.099\n",
      "train accuracy on epoch 252: 0.944\n",
      "test loss on epoch 252: 0.559\n",
      "test accuracy on epoch 252: 0.692\n",
      "train loss on epoch 253 : 0.098\n",
      "train accuracy on epoch 253: 0.944\n",
      "test loss on epoch 253: 0.557\n",
      "test accuracy on epoch 253: 0.692\n",
      "train loss on epoch 254 : 0.148\n",
      "train accuracy on epoch 254: 0.889\n",
      "test loss on epoch 254: 0.554\n",
      "test accuracy on epoch 254: 0.692\n",
      "train loss on epoch 255 : 0.139\n",
      "train accuracy on epoch 255: 0.889\n",
      "test loss on epoch 255: 0.556\n",
      "test accuracy on epoch 255: 0.692\n",
      "train loss on epoch 256 : 0.138\n",
      "train accuracy on epoch 256: 0.889\n",
      "test loss on epoch 256: 0.558\n",
      "test accuracy on epoch 256: 0.692\n",
      "train loss on epoch 257 : 0.101\n",
      "train accuracy on epoch 257: 0.944\n",
      "test loss on epoch 257: 0.561\n",
      "test accuracy on epoch 257: 0.692\n",
      "train loss on epoch 258 : 0.098\n",
      "train accuracy on epoch 258: 0.944\n",
      "test loss on epoch 258: 0.563\n",
      "test accuracy on epoch 258: 0.692\n",
      "train loss on epoch 259 : 0.079\n",
      "train accuracy on epoch 259: 1.000\n",
      "test loss on epoch 259: 0.565\n",
      "test accuracy on epoch 259: 0.692\n",
      "train loss on epoch 260 : 0.069\n",
      "train accuracy on epoch 260: 1.000\n",
      "test loss on epoch 260: 0.567\n",
      "test accuracy on epoch 260: 0.692\n",
      "train loss on epoch 261 : 0.127\n",
      "train accuracy on epoch 261: 0.944\n",
      "test loss on epoch 261: 0.574\n",
      "test accuracy on epoch 261: 0.692\n",
      "train loss on epoch 262 : 0.125\n",
      "train accuracy on epoch 262: 0.889\n",
      "test loss on epoch 262: 0.579\n",
      "test accuracy on epoch 262: 0.692\n",
      "train loss on epoch 263 : 0.179\n",
      "train accuracy on epoch 263: 0.889\n",
      "test loss on epoch 263: 0.581\n",
      "test accuracy on epoch 263: 0.615\n",
      "train loss on epoch 264 : 0.123\n",
      "train accuracy on epoch 264: 0.944\n",
      "test loss on epoch 264: 0.581\n",
      "test accuracy on epoch 264: 0.692\n",
      "train loss on epoch 265 : 0.095\n",
      "train accuracy on epoch 265: 1.000\n",
      "test loss on epoch 265: 0.585\n",
      "test accuracy on epoch 265: 0.615\n",
      "train loss on epoch 266 : 0.109\n",
      "train accuracy on epoch 266: 0.944\n",
      "test loss on epoch 266: 0.586\n",
      "test accuracy on epoch 266: 0.615\n",
      "train loss on epoch 267 : 0.137\n",
      "train accuracy on epoch 267: 0.889\n",
      "test loss on epoch 267: 0.580\n",
      "test accuracy on epoch 267: 0.692\n",
      "train loss on epoch 268 : 0.082\n",
      "train accuracy on epoch 268: 1.000\n",
      "test loss on epoch 268: 0.573\n",
      "test accuracy on epoch 268: 0.692\n",
      "train loss on epoch 269 : 0.097\n",
      "train accuracy on epoch 269: 0.944\n",
      "test loss on epoch 269: 0.568\n",
      "test accuracy on epoch 269: 0.692\n",
      "train loss on epoch 270 : 0.129\n",
      "train accuracy on epoch 270: 0.944\n",
      "test loss on epoch 270: 0.569\n",
      "test accuracy on epoch 270: 0.692\n",
      "train loss on epoch 271 : 0.133\n",
      "train accuracy on epoch 271: 0.944\n",
      "test loss on epoch 271: 0.569\n",
      "test accuracy on epoch 271: 0.692\n",
      "train loss on epoch 272 : 0.081\n",
      "train accuracy on epoch 272: 1.000\n",
      "test loss on epoch 272: 0.569\n",
      "test accuracy on epoch 272: 0.692\n",
      "train loss on epoch 273 : 0.135\n",
      "train accuracy on epoch 273: 0.889\n",
      "test loss on epoch 273: 0.571\n",
      "test accuracy on epoch 273: 0.692\n",
      "train loss on epoch 274 : 0.130\n",
      "train accuracy on epoch 274: 0.889\n",
      "test loss on epoch 274: 0.568\n",
      "test accuracy on epoch 274: 0.692\n",
      "train loss on epoch 275 : 0.096\n",
      "train accuracy on epoch 275: 0.944\n",
      "test loss on epoch 275: 0.569\n",
      "test accuracy on epoch 275: 0.692\n",
      "train loss on epoch 276 : 0.144\n",
      "train accuracy on epoch 276: 0.889\n",
      "test loss on epoch 276: 0.574\n",
      "test accuracy on epoch 276: 0.692\n",
      "train loss on epoch 277 : 0.145\n",
      "train accuracy on epoch 277: 0.944\n",
      "test loss on epoch 277: 0.576\n",
      "test accuracy on epoch 277: 0.692\n",
      "train loss on epoch 278 : 0.132\n",
      "train accuracy on epoch 278: 0.889\n",
      "test loss on epoch 278: 0.584\n",
      "test accuracy on epoch 278: 0.615\n",
      "train loss on epoch 279 : 0.070\n",
      "train accuracy on epoch 279: 1.000\n",
      "test loss on epoch 279: 0.588\n",
      "test accuracy on epoch 279: 0.615\n",
      "train loss on epoch 280 : 0.157\n",
      "train accuracy on epoch 280: 0.889\n",
      "test loss on epoch 280: 0.593\n",
      "test accuracy on epoch 280: 0.615\n",
      "train loss on epoch 281 : 0.076\n",
      "train accuracy on epoch 281: 1.000\n",
      "test loss on epoch 281: 0.594\n",
      "test accuracy on epoch 281: 0.615\n",
      "train loss on epoch 282 : 0.099\n",
      "train accuracy on epoch 282: 0.944\n",
      "test loss on epoch 282: 0.600\n",
      "test accuracy on epoch 282: 0.615\n",
      "train loss on epoch 283 : 0.076\n",
      "train accuracy on epoch 283: 1.000\n",
      "test loss on epoch 283: 0.601\n",
      "test accuracy on epoch 283: 0.615\n",
      "train loss on epoch 284 : 0.169\n",
      "train accuracy on epoch 284: 0.889\n",
      "test loss on epoch 284: 0.602\n",
      "test accuracy on epoch 284: 0.615\n",
      "train loss on epoch 285 : 0.115\n",
      "train accuracy on epoch 285: 0.944\n",
      "test loss on epoch 285: 0.600\n",
      "test accuracy on epoch 285: 0.615\n",
      "train loss on epoch 286 : 0.106\n",
      "train accuracy on epoch 286: 0.944\n",
      "test loss on epoch 286: 0.599\n",
      "test accuracy on epoch 286: 0.615\n",
      "train loss on epoch 287 : 0.119\n",
      "train accuracy on epoch 287: 0.889\n",
      "test loss on epoch 287: 0.598\n",
      "test accuracy on epoch 287: 0.615\n",
      "train loss on epoch 288 : 0.089\n",
      "train accuracy on epoch 288: 1.000\n",
      "test loss on epoch 288: 0.595\n",
      "test accuracy on epoch 288: 0.615\n",
      "train loss on epoch 289 : 0.137\n",
      "train accuracy on epoch 289: 0.889\n",
      "test loss on epoch 289: 0.595\n",
      "test accuracy on epoch 289: 0.615\n",
      "train loss on epoch 290 : 0.118\n",
      "train accuracy on epoch 290: 0.889\n",
      "test loss on epoch 290: 0.592\n",
      "test accuracy on epoch 290: 0.615\n",
      "train loss on epoch 291 : 0.109\n",
      "train accuracy on epoch 291: 0.944\n",
      "test loss on epoch 291: 0.595\n",
      "test accuracy on epoch 291: 0.615\n",
      "train loss on epoch 292 : 0.165\n",
      "train accuracy on epoch 292: 0.889\n",
      "test loss on epoch 292: 0.600\n",
      "test accuracy on epoch 292: 0.615\n",
      "train loss on epoch 293 : 0.097\n",
      "train accuracy on epoch 293: 1.000\n",
      "test loss on epoch 293: 0.603\n",
      "test accuracy on epoch 293: 0.615\n",
      "train loss on epoch 294 : 0.093\n",
      "train accuracy on epoch 294: 0.944\n",
      "test loss on epoch 294: 0.596\n",
      "test accuracy on epoch 294: 0.615\n",
      "train loss on epoch 295 : 0.074\n",
      "train accuracy on epoch 295: 0.944\n",
      "test loss on epoch 295: 0.598\n",
      "test accuracy on epoch 295: 0.615\n",
      "train loss on epoch 296 : 0.124\n",
      "train accuracy on epoch 296: 0.944\n",
      "test loss on epoch 296: 0.599\n",
      "test accuracy on epoch 296: 0.615\n",
      "train loss on epoch 297 : 0.076\n",
      "train accuracy on epoch 297: 1.000\n",
      "test loss on epoch 297: 0.601\n",
      "test accuracy on epoch 297: 0.615\n",
      "train loss on epoch 298 : 0.103\n",
      "train accuracy on epoch 298: 0.944\n",
      "test loss on epoch 298: 0.601\n",
      "test accuracy on epoch 298: 0.615\n",
      "train loss on epoch 299 : 0.084\n",
      "train accuracy on epoch 299: 0.944\n",
      "test loss on epoch 299: 0.602\n",
      "test accuracy on epoch 299: 0.615\n",
      "train loss on epoch 300 : 0.079\n",
      "train accuracy on epoch 300: 1.000\n",
      "test loss on epoch 300: 0.602\n",
      "test accuracy on epoch 300: 0.615\n",
      "train loss on epoch 301 : 0.160\n",
      "train accuracy on epoch 301: 0.889\n",
      "test loss on epoch 301: 0.605\n",
      "test accuracy on epoch 301: 0.615\n",
      "train loss on epoch 302 : 0.089\n",
      "train accuracy on epoch 302: 1.000\n",
      "test loss on epoch 302: 0.594\n",
      "test accuracy on epoch 302: 0.615\n",
      "train loss on epoch 303 : 0.165\n",
      "train accuracy on epoch 303: 0.889\n",
      "test loss on epoch 303: 0.589\n",
      "test accuracy on epoch 303: 0.692\n",
      "train loss on epoch 304 : 0.093\n",
      "train accuracy on epoch 304: 0.944\n",
      "test loss on epoch 304: 0.577\n",
      "test accuracy on epoch 304: 0.692\n",
      "train loss on epoch 305 : 0.085\n",
      "train accuracy on epoch 305: 1.000\n",
      "test loss on epoch 305: 0.572\n",
      "test accuracy on epoch 305: 0.692\n",
      "train loss on epoch 306 : 0.074\n",
      "train accuracy on epoch 306: 1.000\n",
      "test loss on epoch 306: 0.574\n",
      "test accuracy on epoch 306: 0.692\n",
      "train loss on epoch 307 : 0.129\n",
      "train accuracy on epoch 307: 0.944\n",
      "test loss on epoch 307: 0.579\n",
      "test accuracy on epoch 307: 0.692\n",
      "train loss on epoch 308 : 0.079\n",
      "train accuracy on epoch 308: 1.000\n",
      "test loss on epoch 308: 0.582\n",
      "test accuracy on epoch 308: 0.692\n",
      "train loss on epoch 309 : 0.100\n",
      "train accuracy on epoch 309: 0.944\n",
      "test loss on epoch 309: 0.587\n",
      "test accuracy on epoch 309: 0.692\n",
      "train loss on epoch 310 : 0.105\n",
      "train accuracy on epoch 310: 0.889\n",
      "test loss on epoch 310: 0.593\n",
      "test accuracy on epoch 310: 0.692\n",
      "train loss on epoch 311 : 0.068\n",
      "train accuracy on epoch 311: 1.000\n",
      "test loss on epoch 311: 0.598\n",
      "test accuracy on epoch 311: 0.692\n",
      "train loss on epoch 312 : 0.103\n",
      "train accuracy on epoch 312: 0.889\n",
      "test loss on epoch 312: 0.602\n",
      "test accuracy on epoch 312: 0.692\n",
      "train loss on epoch 313 : 0.086\n",
      "train accuracy on epoch 313: 0.944\n",
      "test loss on epoch 313: 0.604\n",
      "test accuracy on epoch 313: 0.692\n",
      "train loss on epoch 314 : 0.119\n",
      "train accuracy on epoch 314: 0.889\n",
      "test loss on epoch 314: 0.610\n",
      "test accuracy on epoch 314: 0.615\n",
      "train loss on epoch 315 : 0.134\n",
      "train accuracy on epoch 315: 0.889\n",
      "test loss on epoch 315: 0.615\n",
      "test accuracy on epoch 315: 0.615\n",
      "train loss on epoch 316 : 0.065\n",
      "train accuracy on epoch 316: 1.000\n",
      "test loss on epoch 316: 0.619\n",
      "test accuracy on epoch 316: 0.615\n",
      "train loss on epoch 317 : 0.087\n",
      "train accuracy on epoch 317: 0.944\n",
      "test loss on epoch 317: 0.619\n",
      "test accuracy on epoch 317: 0.615\n",
      "train loss on epoch 318 : 0.131\n",
      "train accuracy on epoch 318: 0.889\n",
      "test loss on epoch 318: 0.619\n",
      "test accuracy on epoch 318: 0.615\n",
      "train loss on epoch 319 : 0.085\n",
      "train accuracy on epoch 319: 0.944\n",
      "test loss on epoch 319: 0.625\n",
      "test accuracy on epoch 319: 0.615\n",
      "train loss on epoch 320 : 0.128\n",
      "train accuracy on epoch 320: 0.889\n",
      "test loss on epoch 320: 0.635\n",
      "test accuracy on epoch 320: 0.615\n",
      "train loss on epoch 321 : 0.141\n",
      "train accuracy on epoch 321: 0.944\n",
      "test loss on epoch 321: 0.639\n",
      "test accuracy on epoch 321: 0.615\n",
      "train loss on epoch 322 : 0.146\n",
      "train accuracy on epoch 322: 0.944\n",
      "test loss on epoch 322: 0.636\n",
      "test accuracy on epoch 322: 0.615\n",
      "train loss on epoch 323 : 0.095\n",
      "train accuracy on epoch 323: 0.944\n",
      "test loss on epoch 323: 0.631\n",
      "test accuracy on epoch 323: 0.615\n",
      "train loss on epoch 324 : 0.124\n",
      "train accuracy on epoch 324: 0.889\n",
      "test loss on epoch 324: 0.628\n",
      "test accuracy on epoch 324: 0.615\n",
      "train loss on epoch 325 : 0.157\n",
      "train accuracy on epoch 325: 0.889\n",
      "test loss on epoch 325: 0.627\n",
      "test accuracy on epoch 325: 0.615\n",
      "train loss on epoch 326 : 0.100\n",
      "train accuracy on epoch 326: 0.944\n",
      "test loss on epoch 326: 0.621\n",
      "test accuracy on epoch 326: 0.615\n",
      "train loss on epoch 327 : 0.118\n",
      "train accuracy on epoch 327: 0.944\n",
      "test loss on epoch 327: 0.619\n",
      "test accuracy on epoch 327: 0.615\n",
      "train loss on epoch 328 : 0.135\n",
      "train accuracy on epoch 328: 0.889\n",
      "test loss on epoch 328: 0.621\n",
      "test accuracy on epoch 328: 0.615\n",
      "train loss on epoch 329 : 0.104\n",
      "train accuracy on epoch 329: 0.944\n",
      "test loss on epoch 329: 0.616\n",
      "test accuracy on epoch 329: 0.615\n",
      "train loss on epoch 330 : 0.079\n",
      "train accuracy on epoch 330: 1.000\n",
      "test loss on epoch 330: 0.617\n",
      "test accuracy on epoch 330: 0.615\n",
      "train loss on epoch 331 : 0.105\n",
      "train accuracy on epoch 331: 0.944\n",
      "test loss on epoch 331: 0.618\n",
      "test accuracy on epoch 331: 0.615\n",
      "train loss on epoch 332 : 0.073\n",
      "train accuracy on epoch 332: 1.000\n",
      "test loss on epoch 332: 0.618\n",
      "test accuracy on epoch 332: 0.615\n",
      "train loss on epoch 333 : 0.170\n",
      "train accuracy on epoch 333: 0.889\n",
      "test loss on epoch 333: 0.617\n",
      "test accuracy on epoch 333: 0.615\n",
      "train loss on epoch 334 : 0.068\n",
      "train accuracy on epoch 334: 1.000\n",
      "test loss on epoch 334: 0.614\n",
      "test accuracy on epoch 334: 0.615\n",
      "train loss on epoch 335 : 0.140\n",
      "train accuracy on epoch 335: 0.944\n",
      "test loss on epoch 335: 0.610\n",
      "test accuracy on epoch 335: 0.615\n",
      "train loss on epoch 336 : 0.086\n",
      "train accuracy on epoch 336: 0.944\n",
      "test loss on epoch 336: 0.614\n",
      "test accuracy on epoch 336: 0.615\n",
      "train loss on epoch 337 : 0.061\n",
      "train accuracy on epoch 337: 1.000\n",
      "test loss on epoch 337: 0.616\n",
      "test accuracy on epoch 337: 0.615\n",
      "train loss on epoch 338 : 0.092\n",
      "train accuracy on epoch 338: 1.000\n",
      "test loss on epoch 338: 0.613\n",
      "test accuracy on epoch 338: 0.615\n",
      "train loss on epoch 339 : 0.179\n",
      "train accuracy on epoch 339: 0.889\n",
      "test loss on epoch 339: 0.613\n",
      "test accuracy on epoch 339: 0.615\n",
      "train loss on epoch 340 : 0.117\n",
      "train accuracy on epoch 340: 0.944\n",
      "test loss on epoch 340: 0.615\n",
      "test accuracy on epoch 340: 0.615\n",
      "train loss on epoch 341 : 0.069\n",
      "train accuracy on epoch 341: 1.000\n",
      "test loss on epoch 341: 0.612\n",
      "test accuracy on epoch 341: 0.615\n",
      "train loss on epoch 342 : 0.230\n",
      "train accuracy on epoch 342: 0.889\n",
      "test loss on epoch 342: 0.609\n",
      "test accuracy on epoch 342: 0.692\n",
      "train loss on epoch 343 : 0.151\n",
      "train accuracy on epoch 343: 0.889\n",
      "test loss on epoch 343: 0.606\n",
      "test accuracy on epoch 343: 0.692\n",
      "train loss on epoch 344 : 0.084\n",
      "train accuracy on epoch 344: 0.944\n",
      "test loss on epoch 344: 0.605\n",
      "test accuracy on epoch 344: 0.692\n",
      "train loss on epoch 345 : 0.081\n",
      "train accuracy on epoch 345: 0.944\n",
      "test loss on epoch 345: 0.609\n",
      "test accuracy on epoch 345: 0.692\n",
      "train loss on epoch 346 : 0.115\n",
      "train accuracy on epoch 346: 0.889\n",
      "test loss on epoch 346: 0.615\n",
      "test accuracy on epoch 346: 0.615\n",
      "train loss on epoch 347 : 0.156\n",
      "train accuracy on epoch 347: 0.889\n",
      "test loss on epoch 347: 0.619\n",
      "test accuracy on epoch 347: 0.615\n",
      "train loss on epoch 348 : 0.072\n",
      "train accuracy on epoch 348: 1.000\n",
      "test loss on epoch 348: 0.620\n",
      "test accuracy on epoch 348: 0.615\n",
      "train loss on epoch 349 : 0.151\n",
      "train accuracy on epoch 349: 0.889\n",
      "test loss on epoch 349: 0.627\n",
      "test accuracy on epoch 349: 0.615\n",
      "train loss on epoch 350 : 0.056\n",
      "train accuracy on epoch 350: 1.000\n",
      "test loss on epoch 350: 0.630\n",
      "test accuracy on epoch 350: 0.615\n",
      "train loss on epoch 351 : 0.087\n",
      "train accuracy on epoch 351: 0.944\n",
      "test loss on epoch 351: 0.640\n",
      "test accuracy on epoch 351: 0.615\n",
      "train loss on epoch 352 : 0.087\n",
      "train accuracy on epoch 352: 1.000\n",
      "test loss on epoch 352: 0.646\n",
      "test accuracy on epoch 352: 0.615\n",
      "train loss on epoch 353 : 0.055\n",
      "train accuracy on epoch 353: 1.000\n",
      "test loss on epoch 353: 0.645\n",
      "test accuracy on epoch 353: 0.615\n",
      "train loss on epoch 354 : 0.061\n",
      "train accuracy on epoch 354: 1.000\n",
      "test loss on epoch 354: 0.645\n",
      "test accuracy on epoch 354: 0.615\n",
      "train loss on epoch 355 : 0.118\n",
      "train accuracy on epoch 355: 0.944\n",
      "test loss on epoch 355: 0.643\n",
      "test accuracy on epoch 355: 0.615\n",
      "train loss on epoch 356 : 0.164\n",
      "train accuracy on epoch 356: 0.889\n",
      "test loss on epoch 356: 0.643\n",
      "test accuracy on epoch 356: 0.615\n",
      "train loss on epoch 357 : 0.122\n",
      "train accuracy on epoch 357: 0.944\n",
      "test loss on epoch 357: 0.635\n",
      "test accuracy on epoch 357: 0.615\n",
      "train loss on epoch 358 : 0.122\n",
      "train accuracy on epoch 358: 0.944\n",
      "test loss on epoch 358: 0.637\n",
      "test accuracy on epoch 358: 0.615\n",
      "train loss on epoch 359 : 0.101\n",
      "train accuracy on epoch 359: 0.944\n",
      "test loss on epoch 359: 0.637\n",
      "test accuracy on epoch 359: 0.615\n",
      "train loss on epoch 360 : 0.088\n",
      "train accuracy on epoch 360: 0.944\n",
      "test loss on epoch 360: 0.636\n",
      "test accuracy on epoch 360: 0.615\n",
      "train loss on epoch 361 : 0.102\n",
      "train accuracy on epoch 361: 0.944\n",
      "test loss on epoch 361: 0.637\n",
      "test accuracy on epoch 361: 0.615\n",
      "train loss on epoch 362 : 0.155\n",
      "train accuracy on epoch 362: 0.889\n",
      "test loss on epoch 362: 0.635\n",
      "test accuracy on epoch 362: 0.615\n",
      "train loss on epoch 363 : 0.038\n",
      "train accuracy on epoch 363: 1.000\n",
      "test loss on epoch 363: 0.635\n",
      "test accuracy on epoch 363: 0.615\n",
      "train loss on epoch 364 : 0.100\n",
      "train accuracy on epoch 364: 0.944\n",
      "test loss on epoch 364: 0.638\n",
      "test accuracy on epoch 364: 0.615\n",
      "train loss on epoch 365 : 0.111\n",
      "train accuracy on epoch 365: 0.944\n",
      "test loss on epoch 365: 0.639\n",
      "test accuracy on epoch 365: 0.615\n",
      "train loss on epoch 366 : 0.083\n",
      "train accuracy on epoch 366: 1.000\n",
      "test loss on epoch 366: 0.640\n",
      "test accuracy on epoch 366: 0.615\n",
      "train loss on epoch 367 : 0.105\n",
      "train accuracy on epoch 367: 0.944\n",
      "test loss on epoch 367: 0.634\n",
      "test accuracy on epoch 367: 0.615\n",
      "train loss on epoch 368 : 0.125\n",
      "train accuracy on epoch 368: 0.944\n",
      "test loss on epoch 368: 0.631\n",
      "test accuracy on epoch 368: 0.615\n",
      "train loss on epoch 369 : 0.050\n",
      "train accuracy on epoch 369: 1.000\n",
      "test loss on epoch 369: 0.630\n",
      "test accuracy on epoch 369: 0.615\n",
      "train loss on epoch 370 : 0.096\n",
      "train accuracy on epoch 370: 0.944\n",
      "test loss on epoch 370: 0.630\n",
      "test accuracy on epoch 370: 0.615\n",
      "train loss on epoch 371 : 0.082\n",
      "train accuracy on epoch 371: 0.944\n",
      "test loss on epoch 371: 0.627\n",
      "test accuracy on epoch 371: 0.615\n",
      "train loss on epoch 372 : 0.131\n",
      "train accuracy on epoch 372: 0.944\n",
      "test loss on epoch 372: 0.628\n",
      "test accuracy on epoch 372: 0.615\n",
      "train loss on epoch 373 : 0.123\n",
      "train accuracy on epoch 373: 0.889\n",
      "test loss on epoch 373: 0.629\n",
      "test accuracy on epoch 373: 0.615\n",
      "train loss on epoch 374 : 0.119\n",
      "train accuracy on epoch 374: 0.889\n",
      "test loss on epoch 374: 0.632\n",
      "test accuracy on epoch 374: 0.615\n",
      "train loss on epoch 375 : 0.139\n",
      "train accuracy on epoch 375: 0.889\n",
      "test loss on epoch 375: 0.635\n",
      "test accuracy on epoch 375: 0.615\n",
      "train loss on epoch 376 : 0.116\n",
      "train accuracy on epoch 376: 0.889\n",
      "test loss on epoch 376: 0.625\n",
      "test accuracy on epoch 376: 0.615\n",
      "train loss on epoch 377 : 0.158\n",
      "train accuracy on epoch 377: 0.889\n",
      "test loss on epoch 377: 0.623\n",
      "test accuracy on epoch 377: 0.615\n",
      "train loss on epoch 378 : 0.185\n",
      "train accuracy on epoch 378: 0.889\n",
      "test loss on epoch 378: 0.617\n",
      "test accuracy on epoch 378: 0.692\n",
      "train loss on epoch 379 : 0.111\n",
      "train accuracy on epoch 379: 0.944\n",
      "test loss on epoch 379: 0.609\n",
      "test accuracy on epoch 379: 0.692\n",
      "train loss on epoch 380 : 0.071\n",
      "train accuracy on epoch 380: 1.000\n",
      "test loss on epoch 380: 0.608\n",
      "test accuracy on epoch 380: 0.692\n",
      "train loss on epoch 381 : 0.133\n",
      "train accuracy on epoch 381: 0.944\n",
      "test loss on epoch 381: 0.609\n",
      "test accuracy on epoch 381: 0.692\n",
      "train loss on epoch 382 : 0.089\n",
      "train accuracy on epoch 382: 0.944\n",
      "test loss on epoch 382: 0.618\n",
      "test accuracy on epoch 382: 0.692\n",
      "train loss on epoch 383 : 0.088\n",
      "train accuracy on epoch 383: 0.944\n",
      "test loss on epoch 383: 0.619\n",
      "test accuracy on epoch 383: 0.692\n",
      "train loss on epoch 384 : 0.129\n",
      "train accuracy on epoch 384: 0.889\n",
      "test loss on epoch 384: 0.620\n",
      "test accuracy on epoch 384: 0.692\n",
      "train loss on epoch 385 : 0.129\n",
      "train accuracy on epoch 385: 0.889\n",
      "test loss on epoch 385: 0.610\n",
      "test accuracy on epoch 385: 0.692\n",
      "train loss on epoch 386 : 0.171\n",
      "train accuracy on epoch 386: 0.889\n",
      "test loss on epoch 386: 0.606\n",
      "test accuracy on epoch 386: 0.692\n",
      "train loss on epoch 387 : 0.114\n",
      "train accuracy on epoch 387: 0.944\n",
      "test loss on epoch 387: 0.606\n",
      "test accuracy on epoch 387: 0.692\n",
      "train loss on epoch 388 : 0.134\n",
      "train accuracy on epoch 388: 0.889\n",
      "test loss on epoch 388: 0.610\n",
      "test accuracy on epoch 388: 0.692\n",
      "train loss on epoch 389 : 0.092\n",
      "train accuracy on epoch 389: 0.944\n",
      "test loss on epoch 389: 0.606\n",
      "test accuracy on epoch 389: 0.692\n",
      "train loss on epoch 390 : 0.124\n",
      "train accuracy on epoch 390: 0.889\n",
      "test loss on epoch 390: 0.604\n",
      "test accuracy on epoch 390: 0.692\n",
      "train loss on epoch 391 : 0.103\n",
      "train accuracy on epoch 391: 0.944\n",
      "test loss on epoch 391: 0.601\n",
      "test accuracy on epoch 391: 0.692\n",
      "train loss on epoch 392 : 0.105\n",
      "train accuracy on epoch 392: 0.889\n",
      "test loss on epoch 392: 0.607\n",
      "test accuracy on epoch 392: 0.692\n",
      "train loss on epoch 393 : 0.078\n",
      "train accuracy on epoch 393: 0.944\n",
      "test loss on epoch 393: 0.610\n",
      "test accuracy on epoch 393: 0.692\n",
      "train loss on epoch 394 : 0.078\n",
      "train accuracy on epoch 394: 0.944\n",
      "test loss on epoch 394: 0.614\n",
      "test accuracy on epoch 394: 0.692\n",
      "train loss on epoch 395 : 0.139\n",
      "train accuracy on epoch 395: 0.889\n",
      "test loss on epoch 395: 0.617\n",
      "test accuracy on epoch 395: 0.692\n",
      "train loss on epoch 396 : 0.088\n",
      "train accuracy on epoch 396: 0.944\n",
      "test loss on epoch 396: 0.620\n",
      "test accuracy on epoch 396: 0.692\n",
      "train loss on epoch 397 : 0.143\n",
      "train accuracy on epoch 397: 0.944\n",
      "test loss on epoch 397: 0.624\n",
      "test accuracy on epoch 397: 0.692\n",
      "train loss on epoch 398 : 0.121\n",
      "train accuracy on epoch 398: 0.889\n",
      "test loss on epoch 398: 0.624\n",
      "test accuracy on epoch 398: 0.692\n",
      "train loss on epoch 399 : 0.120\n",
      "train accuracy on epoch 399: 0.944\n",
      "test loss on epoch 399: 0.627\n",
      "test accuracy on epoch 399: 0.692\n",
      "train loss on epoch 400 : 0.193\n",
      "train accuracy on epoch 400: 0.889\n",
      "test loss on epoch 400: 0.627\n",
      "test accuracy on epoch 400: 0.692\n",
      "train loss on epoch 401 : 0.090\n",
      "train accuracy on epoch 401: 0.944\n",
      "test loss on epoch 401: 0.630\n",
      "test accuracy on epoch 401: 0.615\n",
      "train loss on epoch 402 : 0.094\n",
      "train accuracy on epoch 402: 0.944\n",
      "test loss on epoch 402: 0.633\n",
      "test accuracy on epoch 402: 0.615\n",
      "train loss on epoch 403 : 0.066\n",
      "train accuracy on epoch 403: 1.000\n",
      "test loss on epoch 403: 0.627\n",
      "test accuracy on epoch 403: 0.692\n",
      "train loss on epoch 404 : 0.163\n",
      "train accuracy on epoch 404: 0.889\n",
      "test loss on epoch 404: 0.624\n",
      "test accuracy on epoch 404: 0.692\n",
      "train loss on epoch 405 : 0.178\n",
      "train accuracy on epoch 405: 0.889\n",
      "test loss on epoch 405: 0.613\n",
      "test accuracy on epoch 405: 0.692\n",
      "train loss on epoch 406 : 0.077\n",
      "train accuracy on epoch 406: 0.944\n",
      "test loss on epoch 406: 0.610\n",
      "test accuracy on epoch 406: 0.692\n",
      "train loss on epoch 407 : 0.151\n",
      "train accuracy on epoch 407: 0.889\n",
      "test loss on epoch 407: 0.613\n",
      "test accuracy on epoch 407: 0.692\n",
      "train loss on epoch 408 : 0.108\n",
      "train accuracy on epoch 408: 0.889\n",
      "test loss on epoch 408: 0.617\n",
      "test accuracy on epoch 408: 0.692\n",
      "train loss on epoch 409 : 0.228\n",
      "train accuracy on epoch 409: 0.889\n",
      "test loss on epoch 409: 0.620\n",
      "test accuracy on epoch 409: 0.692\n",
      "train loss on epoch 410 : 0.106\n",
      "train accuracy on epoch 410: 0.944\n",
      "test loss on epoch 410: 0.622\n",
      "test accuracy on epoch 410: 0.692\n",
      "train loss on epoch 411 : 0.063\n",
      "train accuracy on epoch 411: 1.000\n",
      "test loss on epoch 411: 0.625\n",
      "test accuracy on epoch 411: 0.692\n",
      "train loss on epoch 412 : 0.082\n",
      "train accuracy on epoch 412: 0.944\n",
      "test loss on epoch 412: 0.623\n",
      "test accuracy on epoch 412: 0.692\n",
      "train loss on epoch 413 : 0.081\n",
      "train accuracy on epoch 413: 1.000\n",
      "test loss on epoch 413: 0.626\n",
      "test accuracy on epoch 413: 0.692\n",
      "train loss on epoch 414 : 0.056\n",
      "train accuracy on epoch 414: 1.000\n",
      "test loss on epoch 414: 0.630\n",
      "test accuracy on epoch 414: 0.692\n",
      "train loss on epoch 415 : 0.068\n",
      "train accuracy on epoch 415: 1.000\n",
      "test loss on epoch 415: 0.631\n",
      "test accuracy on epoch 415: 0.692\n",
      "train loss on epoch 416 : 0.117\n",
      "train accuracy on epoch 416: 0.889\n",
      "test loss on epoch 416: 0.631\n",
      "test accuracy on epoch 416: 0.692\n",
      "train loss on epoch 417 : 0.195\n",
      "train accuracy on epoch 417: 0.889\n",
      "test loss on epoch 417: 0.636\n",
      "test accuracy on epoch 417: 0.692\n",
      "train loss on epoch 418 : 0.107\n",
      "train accuracy on epoch 418: 0.889\n",
      "test loss on epoch 418: 0.642\n",
      "test accuracy on epoch 418: 0.615\n",
      "train loss on epoch 419 : 0.101\n",
      "train accuracy on epoch 419: 0.944\n",
      "test loss on epoch 419: 0.646\n",
      "test accuracy on epoch 419: 0.615\n",
      "train loss on epoch 420 : 0.118\n",
      "train accuracy on epoch 420: 0.889\n",
      "test loss on epoch 420: 0.649\n",
      "test accuracy on epoch 420: 0.615\n",
      "train loss on epoch 421 : 0.078\n",
      "train accuracy on epoch 421: 0.944\n",
      "test loss on epoch 421: 0.648\n",
      "test accuracy on epoch 421: 0.615\n",
      "train loss on epoch 422 : 0.085\n",
      "train accuracy on epoch 422: 1.000\n",
      "test loss on epoch 422: 0.646\n",
      "test accuracy on epoch 422: 0.615\n",
      "train loss on epoch 423 : 0.098\n",
      "train accuracy on epoch 423: 0.944\n",
      "test loss on epoch 423: 0.634\n",
      "test accuracy on epoch 423: 0.692\n",
      "train loss on epoch 424 : 0.132\n",
      "train accuracy on epoch 424: 0.889\n",
      "test loss on epoch 424: 0.632\n",
      "test accuracy on epoch 424: 0.692\n",
      "train loss on epoch 425 : 0.146\n",
      "train accuracy on epoch 425: 0.889\n",
      "test loss on epoch 425: 0.622\n",
      "test accuracy on epoch 425: 0.692\n",
      "train loss on epoch 426 : 0.177\n",
      "train accuracy on epoch 426: 0.889\n",
      "test loss on epoch 426: 0.617\n",
      "test accuracy on epoch 426: 0.692\n",
      "train loss on epoch 427 : 0.113\n",
      "train accuracy on epoch 427: 0.889\n",
      "test loss on epoch 427: 0.620\n",
      "test accuracy on epoch 427: 0.692\n",
      "train loss on epoch 428 : 0.095\n",
      "train accuracy on epoch 428: 0.944\n",
      "test loss on epoch 428: 0.626\n",
      "test accuracy on epoch 428: 0.692\n",
      "train loss on epoch 429 : 0.073\n",
      "train accuracy on epoch 429: 0.944\n",
      "test loss on epoch 429: 0.634\n",
      "test accuracy on epoch 429: 0.692\n",
      "train loss on epoch 430 : 0.086\n",
      "train accuracy on epoch 430: 0.944\n",
      "test loss on epoch 430: 0.637\n",
      "test accuracy on epoch 430: 0.615\n",
      "train loss on epoch 431 : 0.126\n",
      "train accuracy on epoch 431: 0.944\n",
      "test loss on epoch 431: 0.639\n",
      "test accuracy on epoch 431: 0.615\n",
      "train loss on epoch 432 : 0.069\n",
      "train accuracy on epoch 432: 1.000\n",
      "test loss on epoch 432: 0.640\n",
      "test accuracy on epoch 432: 0.615\n",
      "train loss on epoch 433 : 0.107\n",
      "train accuracy on epoch 433: 0.889\n",
      "test loss on epoch 433: 0.639\n",
      "test accuracy on epoch 433: 0.615\n",
      "train loss on epoch 434 : 0.134\n",
      "train accuracy on epoch 434: 0.944\n",
      "test loss on epoch 434: 0.640\n",
      "test accuracy on epoch 434: 0.615\n",
      "train loss on epoch 435 : 0.090\n",
      "train accuracy on epoch 435: 0.944\n",
      "test loss on epoch 435: 0.632\n",
      "test accuracy on epoch 435: 0.692\n",
      "train loss on epoch 436 : 0.127\n",
      "train accuracy on epoch 436: 0.944\n",
      "test loss on epoch 436: 0.633\n",
      "test accuracy on epoch 436: 0.692\n",
      "train loss on epoch 437 : 0.106\n",
      "train accuracy on epoch 437: 0.944\n",
      "test loss on epoch 437: 0.624\n",
      "test accuracy on epoch 437: 0.692\n",
      "train loss on epoch 438 : 0.066\n",
      "train accuracy on epoch 438: 1.000\n",
      "test loss on epoch 438: 0.620\n",
      "test accuracy on epoch 438: 0.692\n",
      "train loss on epoch 439 : 0.105\n",
      "train accuracy on epoch 439: 0.944\n",
      "test loss on epoch 439: 0.617\n",
      "test accuracy on epoch 439: 0.692\n",
      "train loss on epoch 440 : 0.045\n",
      "train accuracy on epoch 440: 1.000\n",
      "test loss on epoch 440: 0.608\n",
      "test accuracy on epoch 440: 0.692\n",
      "train loss on epoch 441 : 0.081\n",
      "train accuracy on epoch 441: 1.000\n",
      "test loss on epoch 441: 0.607\n",
      "test accuracy on epoch 441: 0.692\n",
      "train loss on epoch 442 : 0.142\n",
      "train accuracy on epoch 442: 0.944\n",
      "test loss on epoch 442: 0.607\n",
      "test accuracy on epoch 442: 0.692\n",
      "train loss on epoch 443 : 0.150\n",
      "train accuracy on epoch 443: 0.889\n",
      "test loss on epoch 443: 0.610\n",
      "test accuracy on epoch 443: 0.692\n",
      "train loss on epoch 444 : 0.088\n",
      "train accuracy on epoch 444: 0.944\n",
      "test loss on epoch 444: 0.614\n",
      "test accuracy on epoch 444: 0.692\n",
      "train loss on epoch 445 : 0.100\n",
      "train accuracy on epoch 445: 0.944\n",
      "test loss on epoch 445: 0.609\n",
      "test accuracy on epoch 445: 0.692\n",
      "train loss on epoch 446 : 0.115\n",
      "train accuracy on epoch 446: 0.889\n",
      "test loss on epoch 446: 0.614\n",
      "test accuracy on epoch 446: 0.692\n",
      "train loss on epoch 447 : 0.132\n",
      "train accuracy on epoch 447: 0.944\n",
      "test loss on epoch 447: 0.619\n",
      "test accuracy on epoch 447: 0.692\n",
      "train loss on epoch 448 : 0.111\n",
      "train accuracy on epoch 448: 0.944\n",
      "test loss on epoch 448: 0.637\n",
      "test accuracy on epoch 448: 0.692\n",
      "train loss on epoch 449 : 0.049\n",
      "train accuracy on epoch 449: 1.000\n",
      "test loss on epoch 449: 0.649\n",
      "test accuracy on epoch 449: 0.692\n",
      "train loss on epoch 450 : 0.086\n",
      "train accuracy on epoch 450: 0.944\n",
      "test loss on epoch 450: 0.653\n",
      "test accuracy on epoch 450: 0.615\n",
      "train loss on epoch 451 : 0.091\n",
      "train accuracy on epoch 451: 0.944\n",
      "test loss on epoch 451: 0.651\n",
      "test accuracy on epoch 451: 0.615\n",
      "train loss on epoch 452 : 0.103\n",
      "train accuracy on epoch 452: 0.944\n",
      "test loss on epoch 452: 0.650\n",
      "test accuracy on epoch 452: 0.615\n",
      "train loss on epoch 453 : 0.125\n",
      "train accuracy on epoch 453: 0.889\n",
      "test loss on epoch 453: 0.647\n",
      "test accuracy on epoch 453: 0.692\n",
      "train loss on epoch 454 : 0.113\n",
      "train accuracy on epoch 454: 0.889\n",
      "test loss on epoch 454: 0.643\n",
      "test accuracy on epoch 454: 0.692\n",
      "train loss on epoch 455 : 0.130\n",
      "train accuracy on epoch 455: 0.944\n",
      "test loss on epoch 455: 0.642\n",
      "test accuracy on epoch 455: 0.692\n",
      "train loss on epoch 456 : 0.136\n",
      "train accuracy on epoch 456: 0.889\n",
      "test loss on epoch 456: 0.647\n",
      "test accuracy on epoch 456: 0.692\n",
      "train loss on epoch 457 : 0.077\n",
      "train accuracy on epoch 457: 0.944\n",
      "test loss on epoch 457: 0.651\n",
      "test accuracy on epoch 457: 0.615\n",
      "train loss on epoch 458 : 0.137\n",
      "train accuracy on epoch 458: 0.889\n",
      "test loss on epoch 458: 0.651\n",
      "test accuracy on epoch 458: 0.615\n",
      "train loss on epoch 459 : 0.133\n",
      "train accuracy on epoch 459: 0.944\n",
      "test loss on epoch 459: 0.655\n",
      "test accuracy on epoch 459: 0.615\n",
      "train loss on epoch 460 : 0.109\n",
      "train accuracy on epoch 460: 0.944\n",
      "test loss on epoch 460: 0.655\n",
      "test accuracy on epoch 460: 0.615\n",
      "train loss on epoch 461 : 0.233\n",
      "train accuracy on epoch 461: 0.889\n",
      "test loss on epoch 461: 0.647\n",
      "test accuracy on epoch 461: 0.692\n",
      "train loss on epoch 462 : 0.107\n",
      "train accuracy on epoch 462: 0.944\n",
      "test loss on epoch 462: 0.644\n",
      "test accuracy on epoch 462: 0.692\n",
      "train loss on epoch 463 : 0.085\n",
      "train accuracy on epoch 463: 0.944\n",
      "test loss on epoch 463: 0.650\n",
      "test accuracy on epoch 463: 0.615\n",
      "train loss on epoch 464 : 0.053\n",
      "train accuracy on epoch 464: 1.000\n",
      "test loss on epoch 464: 0.661\n",
      "test accuracy on epoch 464: 0.615\n",
      "train loss on epoch 465 : 0.125\n",
      "train accuracy on epoch 465: 0.889\n",
      "test loss on epoch 465: 0.668\n",
      "test accuracy on epoch 465: 0.615\n",
      "train loss on epoch 466 : 0.074\n",
      "train accuracy on epoch 466: 0.944\n",
      "test loss on epoch 466: 0.671\n",
      "test accuracy on epoch 466: 0.615\n",
      "train loss on epoch 467 : 0.117\n",
      "train accuracy on epoch 467: 0.944\n",
      "test loss on epoch 467: 0.669\n",
      "test accuracy on epoch 467: 0.615\n",
      "train loss on epoch 468 : 0.067\n",
      "train accuracy on epoch 468: 1.000\n",
      "test loss on epoch 468: 0.667\n",
      "test accuracy on epoch 468: 0.615\n",
      "train loss on epoch 469 : 0.074\n",
      "train accuracy on epoch 469: 1.000\n",
      "test loss on epoch 469: 0.664\n",
      "test accuracy on epoch 469: 0.615\n",
      "train loss on epoch 470 : 0.138\n",
      "train accuracy on epoch 470: 0.889\n",
      "test loss on epoch 470: 0.660\n",
      "test accuracy on epoch 470: 0.615\n",
      "train loss on epoch 471 : 0.066\n",
      "train accuracy on epoch 471: 1.000\n",
      "test loss on epoch 471: 0.663\n",
      "test accuracy on epoch 471: 0.615\n",
      "train loss on epoch 472 : 0.123\n",
      "train accuracy on epoch 472: 0.944\n",
      "test loss on epoch 472: 0.657\n",
      "test accuracy on epoch 472: 0.615\n",
      "train loss on epoch 473 : 0.080\n",
      "train accuracy on epoch 473: 0.944\n",
      "test loss on epoch 473: 0.653\n",
      "test accuracy on epoch 473: 0.615\n",
      "train loss on epoch 474 : 0.129\n",
      "train accuracy on epoch 474: 0.944\n",
      "test loss on epoch 474: 0.645\n",
      "test accuracy on epoch 474: 0.692\n",
      "train loss on epoch 475 : 0.108\n",
      "train accuracy on epoch 475: 0.944\n",
      "test loss on epoch 475: 0.649\n",
      "test accuracy on epoch 475: 0.692\n",
      "train loss on epoch 476 : 0.130\n",
      "train accuracy on epoch 476: 0.944\n",
      "test loss on epoch 476: 0.641\n",
      "test accuracy on epoch 476: 0.692\n",
      "train loss on epoch 477 : 0.105\n",
      "train accuracy on epoch 477: 0.944\n",
      "test loss on epoch 477: 0.627\n",
      "test accuracy on epoch 477: 0.692\n",
      "train loss on epoch 478 : 0.079\n",
      "train accuracy on epoch 478: 0.944\n",
      "test loss on epoch 478: 0.622\n",
      "test accuracy on epoch 478: 0.692\n",
      "train loss on epoch 479 : 0.064\n",
      "train accuracy on epoch 479: 1.000\n",
      "test loss on epoch 479: 0.629\n",
      "test accuracy on epoch 479: 0.692\n",
      "train loss on epoch 480 : 0.110\n",
      "train accuracy on epoch 480: 0.944\n",
      "test loss on epoch 480: 0.636\n",
      "test accuracy on epoch 480: 0.692\n",
      "train loss on epoch 481 : 0.113\n",
      "train accuracy on epoch 481: 0.944\n",
      "test loss on epoch 481: 0.643\n",
      "test accuracy on epoch 481: 0.692\n",
      "train loss on epoch 482 : 0.124\n",
      "train accuracy on epoch 482: 0.889\n",
      "test loss on epoch 482: 0.650\n",
      "test accuracy on epoch 482: 0.692\n",
      "train loss on epoch 483 : 0.129\n",
      "train accuracy on epoch 483: 0.889\n",
      "test loss on epoch 483: 0.654\n",
      "test accuracy on epoch 483: 0.692\n",
      "train loss on epoch 484 : 0.092\n",
      "train accuracy on epoch 484: 0.944\n",
      "test loss on epoch 484: 0.658\n",
      "test accuracy on epoch 484: 0.615\n",
      "train loss on epoch 485 : 0.087\n",
      "train accuracy on epoch 485: 0.944\n",
      "test loss on epoch 485: 0.665\n",
      "test accuracy on epoch 485: 0.615\n",
      "train loss on epoch 486 : 0.059\n",
      "train accuracy on epoch 486: 1.000\n",
      "test loss on epoch 486: 0.675\n",
      "test accuracy on epoch 486: 0.615\n",
      "train loss on epoch 487 : 0.172\n",
      "train accuracy on epoch 487: 0.889\n",
      "test loss on epoch 487: 0.678\n",
      "test accuracy on epoch 487: 0.615\n",
      "train loss on epoch 488 : 0.104\n",
      "train accuracy on epoch 488: 0.944\n",
      "test loss on epoch 488: 0.683\n",
      "test accuracy on epoch 488: 0.615\n",
      "train loss on epoch 489 : 0.097\n",
      "train accuracy on epoch 489: 0.944\n",
      "test loss on epoch 489: 0.673\n",
      "test accuracy on epoch 489: 0.615\n",
      "train loss on epoch 490 : 0.119\n",
      "train accuracy on epoch 490: 0.889\n",
      "test loss on epoch 490: 0.660\n",
      "test accuracy on epoch 490: 0.615\n",
      "train loss on epoch 491 : 0.128\n",
      "train accuracy on epoch 491: 0.944\n",
      "test loss on epoch 491: 0.651\n",
      "test accuracy on epoch 491: 0.692\n",
      "train loss on epoch 492 : 0.190\n",
      "train accuracy on epoch 492: 0.944\n",
      "test loss on epoch 492: 0.648\n",
      "test accuracy on epoch 492: 0.692\n",
      "train loss on epoch 493 : 0.087\n",
      "train accuracy on epoch 493: 0.944\n",
      "test loss on epoch 493: 0.636\n",
      "test accuracy on epoch 493: 0.692\n",
      "train loss on epoch 494 : 0.087\n",
      "train accuracy on epoch 494: 1.000\n",
      "test loss on epoch 494: 0.631\n",
      "test accuracy on epoch 494: 0.692\n",
      "train loss on epoch 495 : 0.072\n",
      "train accuracy on epoch 495: 0.944\n",
      "test loss on epoch 495: 0.634\n",
      "test accuracy on epoch 495: 0.692\n",
      "train loss on epoch 496 : 0.080\n",
      "train accuracy on epoch 496: 0.944\n",
      "test loss on epoch 496: 0.638\n",
      "test accuracy on epoch 496: 0.692\n",
      "train loss on epoch 497 : 0.095\n",
      "train accuracy on epoch 497: 0.944\n",
      "test loss on epoch 497: 0.643\n",
      "test accuracy on epoch 497: 0.692\n",
      "train loss on epoch 498 : 0.085\n",
      "train accuracy on epoch 498: 0.944\n",
      "test loss on epoch 498: 0.639\n",
      "test accuracy on epoch 498: 0.692\n",
      "train loss on epoch 499 : 0.176\n",
      "train accuracy on epoch 499: 0.889\n",
      "test loss on epoch 499: 0.640\n",
      "test accuracy on epoch 499: 0.692\n",
      "train loss on epoch 500 : 0.078\n",
      "train accuracy on epoch 500: 0.944\n",
      "test loss on epoch 500: 0.644\n",
      "test accuracy on epoch 500: 0.692\n",
      "train loss on epoch 501 : 0.112\n",
      "train accuracy on epoch 501: 0.889\n",
      "test loss on epoch 501: 0.648\n",
      "test accuracy on epoch 501: 0.692\n",
      "train loss on epoch 502 : 0.041\n",
      "train accuracy on epoch 502: 1.000\n",
      "test loss on epoch 502: 0.651\n",
      "test accuracy on epoch 502: 0.615\n",
      "train loss on epoch 503 : 0.120\n",
      "train accuracy on epoch 503: 0.944\n",
      "test loss on epoch 503: 0.648\n",
      "test accuracy on epoch 503: 0.692\n",
      "train loss on epoch 504 : 0.090\n",
      "train accuracy on epoch 504: 0.944\n",
      "test loss on epoch 504: 0.644\n",
      "test accuracy on epoch 504: 0.692\n",
      "train loss on epoch 505 : 0.085\n",
      "train accuracy on epoch 505: 0.944\n",
      "test loss on epoch 505: 0.631\n",
      "test accuracy on epoch 505: 0.692\n",
      "train loss on epoch 506 : 0.093\n",
      "train accuracy on epoch 506: 0.944\n",
      "test loss on epoch 506: 0.624\n",
      "test accuracy on epoch 506: 0.692\n",
      "train loss on epoch 507 : 0.066\n",
      "train accuracy on epoch 507: 1.000\n",
      "test loss on epoch 507: 0.621\n",
      "test accuracy on epoch 507: 0.692\n",
      "train loss on epoch 508 : 0.109\n",
      "train accuracy on epoch 508: 0.889\n",
      "test loss on epoch 508: 0.617\n",
      "test accuracy on epoch 508: 0.692\n",
      "train loss on epoch 509 : 0.130\n",
      "train accuracy on epoch 509: 0.889\n",
      "test loss on epoch 509: 0.615\n",
      "test accuracy on epoch 509: 0.692\n",
      "train loss on epoch 510 : 0.053\n",
      "train accuracy on epoch 510: 1.000\n",
      "test loss on epoch 510: 0.625\n",
      "test accuracy on epoch 510: 0.692\n",
      "train loss on epoch 511 : 0.074\n",
      "train accuracy on epoch 511: 1.000\n",
      "test loss on epoch 511: 0.632\n",
      "test accuracy on epoch 511: 0.692\n",
      "train loss on epoch 512 : 0.077\n",
      "train accuracy on epoch 512: 0.944\n",
      "test loss on epoch 512: 0.640\n",
      "test accuracy on epoch 512: 0.692\n",
      "train loss on epoch 513 : 0.095\n",
      "train accuracy on epoch 513: 0.944\n",
      "test loss on epoch 513: 0.645\n",
      "test accuracy on epoch 513: 0.692\n",
      "train loss on epoch 514 : 0.157\n",
      "train accuracy on epoch 514: 0.944\n",
      "test loss on epoch 514: 0.652\n",
      "test accuracy on epoch 514: 0.692\n",
      "train loss on epoch 515 : 0.074\n",
      "train accuracy on epoch 515: 0.944\n",
      "test loss on epoch 515: 0.641\n",
      "test accuracy on epoch 515: 0.692\n",
      "train loss on epoch 516 : 0.040\n",
      "train accuracy on epoch 516: 1.000\n",
      "test loss on epoch 516: 0.639\n",
      "test accuracy on epoch 516: 0.692\n",
      "train loss on epoch 517 : 0.060\n",
      "train accuracy on epoch 517: 1.000\n",
      "test loss on epoch 517: 0.639\n",
      "test accuracy on epoch 517: 0.692\n",
      "train loss on epoch 518 : 0.145\n",
      "train accuracy on epoch 518: 0.889\n",
      "test loss on epoch 518: 0.641\n",
      "test accuracy on epoch 518: 0.692\n",
      "train loss on epoch 519 : 0.069\n",
      "train accuracy on epoch 519: 1.000\n",
      "test loss on epoch 519: 0.639\n",
      "test accuracy on epoch 519: 0.692\n",
      "train loss on epoch 520 : 0.139\n",
      "train accuracy on epoch 520: 0.944\n",
      "test loss on epoch 520: 0.636\n",
      "test accuracy on epoch 520: 0.692\n",
      "train loss on epoch 521 : 0.054\n",
      "train accuracy on epoch 521: 1.000\n",
      "test loss on epoch 521: 0.649\n",
      "test accuracy on epoch 521: 0.692\n",
      "train loss on epoch 522 : 0.156\n",
      "train accuracy on epoch 522: 0.889\n",
      "test loss on epoch 522: 0.656\n",
      "test accuracy on epoch 522: 0.692\n",
      "train loss on epoch 523 : 0.117\n",
      "train accuracy on epoch 523: 0.889\n",
      "test loss on epoch 523: 0.662\n",
      "test accuracy on epoch 523: 0.615\n",
      "train loss on epoch 524 : 0.104\n",
      "train accuracy on epoch 524: 0.944\n",
      "test loss on epoch 524: 0.676\n",
      "test accuracy on epoch 524: 0.615\n",
      "train loss on epoch 525 : 0.118\n",
      "train accuracy on epoch 525: 0.944\n",
      "test loss on epoch 525: 0.686\n",
      "test accuracy on epoch 525: 0.615\n",
      "train loss on epoch 526 : 0.093\n",
      "train accuracy on epoch 526: 0.944\n",
      "test loss on epoch 526: 0.692\n",
      "test accuracy on epoch 526: 0.615\n",
      "train loss on epoch 527 : 0.050\n",
      "train accuracy on epoch 527: 1.000\n",
      "test loss on epoch 527: 0.689\n",
      "test accuracy on epoch 527: 0.615\n",
      "train loss on epoch 528 : 0.101\n",
      "train accuracy on epoch 528: 0.944\n",
      "test loss on epoch 528: 0.677\n",
      "test accuracy on epoch 528: 0.615\n",
      "train loss on epoch 529 : 0.231\n",
      "train accuracy on epoch 529: 0.889\n",
      "test loss on epoch 529: 0.671\n",
      "test accuracy on epoch 529: 0.615\n",
      "train loss on epoch 530 : 0.171\n",
      "train accuracy on epoch 530: 0.889\n",
      "test loss on epoch 530: 0.661\n",
      "test accuracy on epoch 530: 0.615\n",
      "train loss on epoch 531 : 0.121\n",
      "train accuracy on epoch 531: 0.889\n",
      "test loss on epoch 531: 0.660\n",
      "test accuracy on epoch 531: 0.615\n",
      "train loss on epoch 532 : 0.059\n",
      "train accuracy on epoch 532: 1.000\n",
      "test loss on epoch 532: 0.652\n",
      "test accuracy on epoch 532: 0.692\n",
      "train loss on epoch 533 : 0.155\n",
      "train accuracy on epoch 533: 0.889\n",
      "test loss on epoch 533: 0.647\n",
      "test accuracy on epoch 533: 0.692\n",
      "train loss on epoch 534 : 0.075\n",
      "train accuracy on epoch 534: 0.944\n",
      "test loss on epoch 534: 0.644\n",
      "test accuracy on epoch 534: 0.692\n",
      "train loss on epoch 535 : 0.074\n",
      "train accuracy on epoch 535: 1.000\n",
      "test loss on epoch 535: 0.636\n",
      "test accuracy on epoch 535: 0.692\n",
      "train loss on epoch 536 : 0.095\n",
      "train accuracy on epoch 536: 0.944\n",
      "test loss on epoch 536: 0.632\n",
      "test accuracy on epoch 536: 0.692\n",
      "train loss on epoch 537 : 0.063\n",
      "train accuracy on epoch 537: 1.000\n",
      "test loss on epoch 537: 0.627\n",
      "test accuracy on epoch 537: 0.692\n",
      "train loss on epoch 538 : 0.051\n",
      "train accuracy on epoch 538: 1.000\n",
      "test loss on epoch 538: 0.626\n",
      "test accuracy on epoch 538: 0.692\n",
      "train loss on epoch 539 : 0.092\n",
      "train accuracy on epoch 539: 0.944\n",
      "test loss on epoch 539: 0.628\n",
      "test accuracy on epoch 539: 0.692\n",
      "train loss on epoch 540 : 0.084\n",
      "train accuracy on epoch 540: 0.944\n",
      "test loss on epoch 540: 0.629\n",
      "test accuracy on epoch 540: 0.692\n",
      "train loss on epoch 541 : 0.105\n",
      "train accuracy on epoch 541: 0.889\n",
      "test loss on epoch 541: 0.634\n",
      "test accuracy on epoch 541: 0.692\n",
      "train loss on epoch 542 : 0.085\n",
      "train accuracy on epoch 542: 0.944\n",
      "test loss on epoch 542: 0.641\n",
      "test accuracy on epoch 542: 0.692\n",
      "train loss on epoch 543 : 0.076\n",
      "train accuracy on epoch 543: 1.000\n",
      "test loss on epoch 543: 0.646\n",
      "test accuracy on epoch 543: 0.692\n",
      "train loss on epoch 544 : 0.221\n",
      "train accuracy on epoch 544: 0.889\n",
      "test loss on epoch 544: 0.654\n",
      "test accuracy on epoch 544: 0.692\n",
      "train loss on epoch 545 : 0.137\n",
      "train accuracy on epoch 545: 0.889\n",
      "test loss on epoch 545: 0.661\n",
      "test accuracy on epoch 545: 0.692\n",
      "train loss on epoch 546 : 0.102\n",
      "train accuracy on epoch 546: 0.944\n",
      "test loss on epoch 546: 0.667\n",
      "test accuracy on epoch 546: 0.615\n",
      "train loss on epoch 547 : 0.083\n",
      "train accuracy on epoch 547: 0.944\n",
      "test loss on epoch 547: 0.676\n",
      "test accuracy on epoch 547: 0.615\n",
      "train loss on epoch 548 : 0.128\n",
      "train accuracy on epoch 548: 0.889\n",
      "test loss on epoch 548: 0.680\n",
      "test accuracy on epoch 548: 0.615\n",
      "train loss on epoch 549 : 0.112\n",
      "train accuracy on epoch 549: 0.944\n",
      "test loss on epoch 549: 0.678\n",
      "test accuracy on epoch 549: 0.615\n",
      "train loss on epoch 550 : 0.079\n",
      "train accuracy on epoch 550: 0.944\n",
      "test loss on epoch 550: 0.678\n",
      "test accuracy on epoch 550: 0.615\n",
      "train loss on epoch 551 : 0.089\n",
      "train accuracy on epoch 551: 0.944\n",
      "test loss on epoch 551: 0.671\n",
      "test accuracy on epoch 551: 0.615\n",
      "train loss on epoch 552 : 0.067\n",
      "train accuracy on epoch 552: 1.000\n",
      "test loss on epoch 552: 0.669\n",
      "test accuracy on epoch 552: 0.615\n",
      "train loss on epoch 553 : 0.098\n",
      "train accuracy on epoch 553: 0.944\n",
      "test loss on epoch 553: 0.671\n",
      "test accuracy on epoch 553: 0.615\n",
      "train loss on epoch 554 : 0.053\n",
      "train accuracy on epoch 554: 1.000\n",
      "test loss on epoch 554: 0.679\n",
      "test accuracy on epoch 554: 0.615\n",
      "train loss on epoch 555 : 0.087\n",
      "train accuracy on epoch 555: 0.944\n",
      "test loss on epoch 555: 0.677\n",
      "test accuracy on epoch 555: 0.615\n",
      "train loss on epoch 556 : 0.103\n",
      "train accuracy on epoch 556: 0.889\n",
      "test loss on epoch 556: 0.673\n",
      "test accuracy on epoch 556: 0.615\n",
      "train loss on epoch 557 : 0.152\n",
      "train accuracy on epoch 557: 0.889\n",
      "test loss on epoch 557: 0.666\n",
      "test accuracy on epoch 557: 0.692\n",
      "train loss on epoch 558 : 0.097\n",
      "train accuracy on epoch 558: 0.944\n",
      "test loss on epoch 558: 0.663\n",
      "test accuracy on epoch 558: 0.692\n",
      "train loss on epoch 559 : 0.093\n",
      "train accuracy on epoch 559: 0.944\n",
      "test loss on epoch 559: 0.656\n",
      "test accuracy on epoch 559: 0.692\n",
      "train loss on epoch 560 : 0.222\n",
      "train accuracy on epoch 560: 0.889\n",
      "test loss on epoch 560: 0.649\n",
      "test accuracy on epoch 560: 0.692\n",
      "train loss on epoch 561 : 0.081\n",
      "train accuracy on epoch 561: 0.944\n",
      "test loss on epoch 561: 0.659\n",
      "test accuracy on epoch 561: 0.692\n",
      "train loss on epoch 562 : 0.128\n",
      "train accuracy on epoch 562: 0.889\n",
      "test loss on epoch 562: 0.670\n",
      "test accuracy on epoch 562: 0.692\n",
      "train loss on epoch 563 : 0.126\n",
      "train accuracy on epoch 563: 0.889\n",
      "test loss on epoch 563: 0.672\n",
      "test accuracy on epoch 563: 0.615\n",
      "train loss on epoch 564 : 0.066\n",
      "train accuracy on epoch 564: 0.944\n",
      "test loss on epoch 564: 0.684\n",
      "test accuracy on epoch 564: 0.615\n",
      "train loss on epoch 565 : 0.097\n",
      "train accuracy on epoch 565: 0.944\n",
      "test loss on epoch 565: 0.689\n",
      "test accuracy on epoch 565: 0.615\n",
      "train loss on epoch 566 : 0.077\n",
      "train accuracy on epoch 566: 1.000\n",
      "test loss on epoch 566: 0.691\n",
      "test accuracy on epoch 566: 0.615\n",
      "train loss on epoch 567 : 0.190\n",
      "train accuracy on epoch 567: 0.944\n",
      "test loss on epoch 567: 0.693\n",
      "test accuracy on epoch 567: 0.615\n",
      "train loss on epoch 568 : 0.062\n",
      "train accuracy on epoch 568: 1.000\n",
      "test loss on epoch 568: 0.682\n",
      "test accuracy on epoch 568: 0.615\n",
      "train loss on epoch 569 : 0.064\n",
      "train accuracy on epoch 569: 1.000\n",
      "test loss on epoch 569: 0.672\n",
      "test accuracy on epoch 569: 0.615\n",
      "train loss on epoch 570 : 0.115\n",
      "train accuracy on epoch 570: 0.889\n",
      "test loss on epoch 570: 0.661\n",
      "test accuracy on epoch 570: 0.692\n",
      "train loss on epoch 571 : 0.070\n",
      "train accuracy on epoch 571: 1.000\n",
      "test loss on epoch 571: 0.658\n",
      "test accuracy on epoch 571: 0.692\n",
      "train loss on epoch 572 : 0.075\n",
      "train accuracy on epoch 572: 1.000\n",
      "test loss on epoch 572: 0.668\n",
      "test accuracy on epoch 572: 0.615\n",
      "train loss on epoch 573 : 0.104\n",
      "train accuracy on epoch 573: 0.944\n",
      "test loss on epoch 573: 0.669\n",
      "test accuracy on epoch 573: 0.615\n",
      "train loss on epoch 574 : 0.183\n",
      "train accuracy on epoch 574: 0.889\n",
      "test loss on epoch 574: 0.675\n",
      "test accuracy on epoch 574: 0.615\n",
      "train loss on epoch 575 : 0.079\n",
      "train accuracy on epoch 575: 1.000\n",
      "test loss on epoch 575: 0.677\n",
      "test accuracy on epoch 575: 0.615\n",
      "train loss on epoch 576 : 0.149\n",
      "train accuracy on epoch 576: 0.889\n",
      "test loss on epoch 576: 0.680\n",
      "test accuracy on epoch 576: 0.615\n",
      "train loss on epoch 577 : 0.099\n",
      "train accuracy on epoch 577: 0.889\n",
      "test loss on epoch 577: 0.680\n",
      "test accuracy on epoch 577: 0.615\n",
      "train loss on epoch 578 : 0.039\n",
      "train accuracy on epoch 578: 1.000\n",
      "test loss on epoch 578: 0.686\n",
      "test accuracy on epoch 578: 0.615\n",
      "train loss on epoch 579 : 0.094\n",
      "train accuracy on epoch 579: 0.944\n",
      "test loss on epoch 579: 0.690\n",
      "test accuracy on epoch 579: 0.615\n",
      "train loss on epoch 580 : 0.192\n",
      "train accuracy on epoch 580: 0.889\n",
      "test loss on epoch 580: 0.691\n",
      "test accuracy on epoch 580: 0.615\n",
      "train loss on epoch 581 : 0.079\n",
      "train accuracy on epoch 581: 0.944\n",
      "test loss on epoch 581: 0.696\n",
      "test accuracy on epoch 581: 0.615\n",
      "train loss on epoch 582 : 0.137\n",
      "train accuracy on epoch 582: 0.944\n",
      "test loss on epoch 582: 0.693\n",
      "test accuracy on epoch 582: 0.615\n",
      "train loss on epoch 583 : 0.057\n",
      "train accuracy on epoch 583: 1.000\n",
      "test loss on epoch 583: 0.690\n",
      "test accuracy on epoch 583: 0.615\n",
      "train loss on epoch 584 : 0.077\n",
      "train accuracy on epoch 584: 0.944\n",
      "test loss on epoch 584: 0.681\n",
      "test accuracy on epoch 584: 0.615\n",
      "train loss on epoch 585 : 0.076\n",
      "train accuracy on epoch 585: 0.944\n",
      "test loss on epoch 585: 0.680\n",
      "test accuracy on epoch 585: 0.615\n",
      "train loss on epoch 586 : 0.162\n",
      "train accuracy on epoch 586: 0.889\n",
      "test loss on epoch 586: 0.675\n",
      "test accuracy on epoch 586: 0.692\n",
      "train loss on epoch 587 : 0.110\n",
      "train accuracy on epoch 587: 0.944\n",
      "test loss on epoch 587: 0.676\n",
      "test accuracy on epoch 587: 0.692\n",
      "train loss on epoch 588 : 0.111\n",
      "train accuracy on epoch 588: 0.889\n",
      "test loss on epoch 588: 0.673\n",
      "test accuracy on epoch 588: 0.692\n",
      "train loss on epoch 589 : 0.135\n",
      "train accuracy on epoch 589: 0.889\n",
      "test loss on epoch 589: 0.672\n",
      "test accuracy on epoch 589: 0.692\n",
      "train loss on epoch 590 : 0.120\n",
      "train accuracy on epoch 590: 0.944\n",
      "test loss on epoch 590: 0.678\n",
      "test accuracy on epoch 590: 0.692\n",
      "train loss on epoch 591 : 0.076\n",
      "train accuracy on epoch 591: 1.000\n",
      "test loss on epoch 591: 0.680\n",
      "test accuracy on epoch 591: 0.615\n",
      "train loss on epoch 592 : 0.145\n",
      "train accuracy on epoch 592: 0.944\n",
      "test loss on epoch 592: 0.677\n",
      "test accuracy on epoch 592: 0.692\n",
      "train loss on epoch 593 : 0.203\n",
      "train accuracy on epoch 593: 0.889\n",
      "test loss on epoch 593: 0.678\n",
      "test accuracy on epoch 593: 0.692\n",
      "train loss on epoch 594 : 0.065\n",
      "train accuracy on epoch 594: 1.000\n",
      "test loss on epoch 594: 0.681\n",
      "test accuracy on epoch 594: 0.692\n",
      "train loss on epoch 595 : 0.096\n",
      "train accuracy on epoch 595: 0.944\n",
      "test loss on epoch 595: 0.685\n",
      "test accuracy on epoch 595: 0.615\n",
      "train loss on epoch 596 : 0.130\n",
      "train accuracy on epoch 596: 0.889\n",
      "test loss on epoch 596: 0.689\n",
      "test accuracy on epoch 596: 0.615\n",
      "train loss on epoch 597 : 0.167\n",
      "train accuracy on epoch 597: 0.889\n",
      "test loss on epoch 597: 0.695\n",
      "test accuracy on epoch 597: 0.615\n",
      "train loss on epoch 598 : 0.044\n",
      "train accuracy on epoch 598: 1.000\n",
      "test loss on epoch 598: 0.695\n",
      "test accuracy on epoch 598: 0.615\n",
      "train loss on epoch 599 : 0.057\n",
      "train accuracy on epoch 599: 1.000\n",
      "test loss on epoch 599: 0.688\n",
      "test accuracy on epoch 599: 0.615\n",
      "train loss on epoch 600 : 0.055\n",
      "train accuracy on epoch 600: 1.000\n",
      "test loss on epoch 600: 0.685\n",
      "test accuracy on epoch 600: 0.615\n",
      "train loss on epoch 601 : 0.122\n",
      "train accuracy on epoch 601: 0.889\n",
      "test loss on epoch 601: 0.680\n",
      "test accuracy on epoch 601: 0.692\n",
      "train loss on epoch 602 : 0.102\n",
      "train accuracy on epoch 602: 0.944\n",
      "test loss on epoch 602: 0.680\n",
      "test accuracy on epoch 602: 0.692\n",
      "train loss on epoch 603 : 0.058\n",
      "train accuracy on epoch 603: 1.000\n",
      "test loss on epoch 603: 0.676\n",
      "test accuracy on epoch 603: 0.692\n",
      "train loss on epoch 604 : 0.128\n",
      "train accuracy on epoch 604: 0.944\n",
      "test loss on epoch 604: 0.680\n",
      "test accuracy on epoch 604: 0.692\n",
      "train loss on epoch 605 : 0.070\n",
      "train accuracy on epoch 605: 1.000\n",
      "test loss on epoch 605: 0.682\n",
      "test accuracy on epoch 605: 0.692\n",
      "train loss on epoch 606 : 0.055\n",
      "train accuracy on epoch 606: 1.000\n",
      "test loss on epoch 606: 0.683\n",
      "test accuracy on epoch 606: 0.692\n",
      "train loss on epoch 607 : 0.092\n",
      "train accuracy on epoch 607: 0.889\n",
      "test loss on epoch 607: 0.684\n",
      "test accuracy on epoch 607: 0.692\n",
      "train loss on epoch 608 : 0.112\n",
      "train accuracy on epoch 608: 0.889\n",
      "test loss on epoch 608: 0.686\n",
      "test accuracy on epoch 608: 0.692\n",
      "train loss on epoch 609 : 0.178\n",
      "train accuracy on epoch 609: 0.889\n",
      "test loss on epoch 609: 0.688\n",
      "test accuracy on epoch 609: 0.692\n",
      "train loss on epoch 610 : 0.105\n",
      "train accuracy on epoch 610: 0.944\n",
      "test loss on epoch 610: 0.692\n",
      "test accuracy on epoch 610: 0.615\n",
      "train loss on epoch 611 : 0.112\n",
      "train accuracy on epoch 611: 0.944\n",
      "test loss on epoch 611: 0.693\n",
      "test accuracy on epoch 611: 0.615\n",
      "train loss on epoch 612 : 0.059\n",
      "train accuracy on epoch 612: 1.000\n",
      "test loss on epoch 612: 0.687\n",
      "test accuracy on epoch 612: 0.692\n",
      "train loss on epoch 613 : 0.157\n",
      "train accuracy on epoch 613: 0.889\n",
      "test loss on epoch 613: 0.672\n",
      "test accuracy on epoch 613: 0.692\n",
      "train loss on epoch 614 : 0.259\n",
      "train accuracy on epoch 614: 0.889\n",
      "test loss on epoch 614: 0.663\n",
      "test accuracy on epoch 614: 0.692\n",
      "train loss on epoch 615 : 0.099\n",
      "train accuracy on epoch 615: 0.944\n",
      "test loss on epoch 615: 0.659\n",
      "test accuracy on epoch 615: 0.692\n",
      "train loss on epoch 616 : 0.102\n",
      "train accuracy on epoch 616: 0.889\n",
      "test loss on epoch 616: 0.660\n",
      "test accuracy on epoch 616: 0.692\n",
      "train loss on epoch 617 : 0.071\n",
      "train accuracy on epoch 617: 1.000\n",
      "test loss on epoch 617: 0.667\n",
      "test accuracy on epoch 617: 0.692\n",
      "train loss on epoch 618 : 0.090\n",
      "train accuracy on epoch 618: 0.944\n",
      "test loss on epoch 618: 0.668\n",
      "test accuracy on epoch 618: 0.692\n",
      "train loss on epoch 619 : 0.067\n",
      "train accuracy on epoch 619: 0.944\n",
      "test loss on epoch 619: 0.672\n",
      "test accuracy on epoch 619: 0.692\n",
      "train loss on epoch 620 : 0.110\n",
      "train accuracy on epoch 620: 0.889\n",
      "test loss on epoch 620: 0.673\n",
      "test accuracy on epoch 620: 0.692\n",
      "train loss on epoch 621 : 0.100\n",
      "train accuracy on epoch 621: 0.889\n",
      "test loss on epoch 621: 0.677\n",
      "test accuracy on epoch 621: 0.692\n",
      "train loss on epoch 622 : 0.121\n",
      "train accuracy on epoch 622: 0.889\n",
      "test loss on epoch 622: 0.683\n",
      "test accuracy on epoch 622: 0.692\n",
      "train loss on epoch 623 : 0.109\n",
      "train accuracy on epoch 623: 0.944\n",
      "test loss on epoch 623: 0.682\n",
      "test accuracy on epoch 623: 0.692\n",
      "train loss on epoch 624 : 0.174\n",
      "train accuracy on epoch 624: 0.889\n",
      "test loss on epoch 624: 0.681\n",
      "test accuracy on epoch 624: 0.692\n",
      "train loss on epoch 625 : 0.082\n",
      "train accuracy on epoch 625: 0.944\n",
      "test loss on epoch 625: 0.676\n",
      "test accuracy on epoch 625: 0.692\n",
      "train loss on epoch 626 : 0.084\n",
      "train accuracy on epoch 626: 0.944\n",
      "test loss on epoch 626: 0.677\n",
      "test accuracy on epoch 626: 0.692\n",
      "train loss on epoch 627 : 0.172\n",
      "train accuracy on epoch 627: 0.889\n",
      "test loss on epoch 627: 0.676\n",
      "test accuracy on epoch 627: 0.692\n",
      "train loss on epoch 628 : 0.079\n",
      "train accuracy on epoch 628: 0.944\n",
      "test loss on epoch 628: 0.678\n",
      "test accuracy on epoch 628: 0.692\n",
      "train loss on epoch 629 : 0.059\n",
      "train accuracy on epoch 629: 1.000\n",
      "test loss on epoch 629: 0.681\n",
      "test accuracy on epoch 629: 0.692\n",
      "train loss on epoch 630 : 0.170\n",
      "train accuracy on epoch 630: 0.889\n",
      "test loss on epoch 630: 0.685\n",
      "test accuracy on epoch 630: 0.692\n",
      "train loss on epoch 631 : 0.139\n",
      "train accuracy on epoch 631: 0.944\n",
      "test loss on epoch 631: 0.682\n",
      "test accuracy on epoch 631: 0.692\n",
      "train loss on epoch 632 : 0.081\n",
      "train accuracy on epoch 632: 0.944\n",
      "test loss on epoch 632: 0.671\n",
      "test accuracy on epoch 632: 0.692\n",
      "train loss on epoch 633 : 0.095\n",
      "train accuracy on epoch 633: 0.944\n",
      "test loss on epoch 633: 0.660\n",
      "test accuracy on epoch 633: 0.692\n",
      "train loss on epoch 634 : 0.123\n",
      "train accuracy on epoch 634: 0.944\n",
      "test loss on epoch 634: 0.667\n",
      "test accuracy on epoch 634: 0.692\n",
      "train loss on epoch 635 : 0.062\n",
      "train accuracy on epoch 635: 1.000\n",
      "test loss on epoch 635: 0.673\n",
      "test accuracy on epoch 635: 0.692\n",
      "train loss on epoch 636 : 0.036\n",
      "train accuracy on epoch 636: 1.000\n",
      "test loss on epoch 636: 0.679\n",
      "test accuracy on epoch 636: 0.692\n",
      "train loss on epoch 637 : 0.120\n",
      "train accuracy on epoch 637: 0.944\n",
      "test loss on epoch 637: 0.688\n",
      "test accuracy on epoch 637: 0.692\n",
      "train loss on epoch 638 : 0.119\n",
      "train accuracy on epoch 638: 0.944\n",
      "test loss on epoch 638: 0.688\n",
      "test accuracy on epoch 638: 0.692\n",
      "train loss on epoch 639 : 0.134\n",
      "train accuracy on epoch 639: 0.889\n",
      "test loss on epoch 639: 0.687\n",
      "test accuracy on epoch 639: 0.692\n",
      "train loss on epoch 640 : 0.035\n",
      "train accuracy on epoch 640: 1.000\n",
      "test loss on epoch 640: 0.691\n",
      "test accuracy on epoch 640: 0.692\n",
      "train loss on epoch 641 : 0.079\n",
      "train accuracy on epoch 641: 0.944\n",
      "test loss on epoch 641: 0.688\n",
      "test accuracy on epoch 641: 0.692\n",
      "train loss on epoch 642 : 0.060\n",
      "train accuracy on epoch 642: 0.944\n",
      "test loss on epoch 642: 0.680\n",
      "test accuracy on epoch 642: 0.692\n",
      "train loss on epoch 643 : 0.087\n",
      "train accuracy on epoch 643: 0.944\n",
      "test loss on epoch 643: 0.677\n",
      "test accuracy on epoch 643: 0.692\n",
      "train loss on epoch 644 : 0.151\n",
      "train accuracy on epoch 644: 0.944\n",
      "test loss on epoch 644: 0.678\n",
      "test accuracy on epoch 644: 0.692\n",
      "train loss on epoch 645 : 0.116\n",
      "train accuracy on epoch 645: 0.944\n",
      "test loss on epoch 645: 0.687\n",
      "test accuracy on epoch 645: 0.692\n",
      "train loss on epoch 646 : 0.079\n",
      "train accuracy on epoch 646: 0.944\n",
      "test loss on epoch 646: 0.701\n",
      "test accuracy on epoch 646: 0.692\n",
      "train loss on epoch 647 : 0.109\n",
      "train accuracy on epoch 647: 0.889\n",
      "test loss on epoch 647: 0.706\n",
      "test accuracy on epoch 647: 0.692\n",
      "train loss on epoch 648 : 0.071\n",
      "train accuracy on epoch 648: 1.000\n",
      "test loss on epoch 648: 0.710\n",
      "test accuracy on epoch 648: 0.615\n",
      "train loss on epoch 649 : 0.123\n",
      "train accuracy on epoch 649: 0.944\n",
      "test loss on epoch 649: 0.716\n",
      "test accuracy on epoch 649: 0.615\n",
      "train loss on epoch 650 : 0.083\n",
      "train accuracy on epoch 650: 0.944\n",
      "test loss on epoch 650: 0.718\n",
      "test accuracy on epoch 650: 0.615\n",
      "train loss on epoch 651 : 0.154\n",
      "train accuracy on epoch 651: 0.889\n",
      "test loss on epoch 651: 0.725\n",
      "test accuracy on epoch 651: 0.615\n",
      "train loss on epoch 652 : 0.110\n",
      "train accuracy on epoch 652: 0.889\n",
      "test loss on epoch 652: 0.725\n",
      "test accuracy on epoch 652: 0.615\n",
      "train loss on epoch 653 : 0.036\n",
      "train accuracy on epoch 653: 1.000\n",
      "test loss on epoch 653: 0.715\n",
      "test accuracy on epoch 653: 0.615\n",
      "train loss on epoch 654 : 0.062\n",
      "train accuracy on epoch 654: 0.944\n",
      "test loss on epoch 654: 0.708\n",
      "test accuracy on epoch 654: 0.692\n",
      "train loss on epoch 655 : 0.081\n",
      "train accuracy on epoch 655: 0.944\n",
      "test loss on epoch 655: 0.690\n",
      "test accuracy on epoch 655: 0.692\n",
      "train loss on epoch 656 : 0.113\n",
      "train accuracy on epoch 656: 0.944\n",
      "test loss on epoch 656: 0.683\n",
      "test accuracy on epoch 656: 0.692\n",
      "train loss on epoch 657 : 0.181\n",
      "train accuracy on epoch 657: 0.944\n",
      "test loss on epoch 657: 0.682\n",
      "test accuracy on epoch 657: 0.692\n",
      "train loss on epoch 658 : 0.097\n",
      "train accuracy on epoch 658: 0.944\n",
      "test loss on epoch 658: 0.688\n",
      "test accuracy on epoch 658: 0.692\n",
      "train loss on epoch 659 : 0.228\n",
      "train accuracy on epoch 659: 0.944\n",
      "test loss on epoch 659: 0.710\n",
      "test accuracy on epoch 659: 0.692\n",
      "train loss on epoch 660 : 0.068\n",
      "train accuracy on epoch 660: 1.000\n",
      "test loss on epoch 660: 0.721\n",
      "test accuracy on epoch 660: 0.615\n",
      "train loss on epoch 661 : 0.108\n",
      "train accuracy on epoch 661: 0.944\n",
      "test loss on epoch 661: 0.724\n",
      "test accuracy on epoch 661: 0.615\n",
      "train loss on epoch 662 : 0.122\n",
      "train accuracy on epoch 662: 0.889\n",
      "test loss on epoch 662: 0.723\n",
      "test accuracy on epoch 662: 0.615\n",
      "train loss on epoch 663 : 0.070\n",
      "train accuracy on epoch 663: 1.000\n",
      "test loss on epoch 663: 0.729\n",
      "test accuracy on epoch 663: 0.615\n",
      "train loss on epoch 664 : 0.112\n",
      "train accuracy on epoch 664: 0.944\n",
      "test loss on epoch 664: 0.734\n",
      "test accuracy on epoch 664: 0.615\n",
      "train loss on epoch 665 : 0.024\n",
      "train accuracy on epoch 665: 1.000\n",
      "test loss on epoch 665: 0.738\n",
      "test accuracy on epoch 665: 0.615\n",
      "train loss on epoch 666 : 0.068\n",
      "train accuracy on epoch 666: 0.944\n",
      "test loss on epoch 666: 0.738\n",
      "test accuracy on epoch 666: 0.615\n",
      "train loss on epoch 667 : 0.155\n",
      "train accuracy on epoch 667: 0.944\n",
      "test loss on epoch 667: 0.736\n",
      "test accuracy on epoch 667: 0.615\n",
      "train loss on epoch 668 : 0.059\n",
      "train accuracy on epoch 668: 0.944\n",
      "test loss on epoch 668: 0.727\n",
      "test accuracy on epoch 668: 0.615\n",
      "train loss on epoch 669 : 0.082\n",
      "train accuracy on epoch 669: 0.944\n",
      "test loss on epoch 669: 0.718\n",
      "test accuracy on epoch 669: 0.615\n",
      "train loss on epoch 670 : 0.147\n",
      "train accuracy on epoch 670: 0.889\n",
      "test loss on epoch 670: 0.716\n",
      "test accuracy on epoch 670: 0.615\n",
      "train loss on epoch 671 : 0.081\n",
      "train accuracy on epoch 671: 0.944\n",
      "test loss on epoch 671: 0.698\n",
      "test accuracy on epoch 671: 0.692\n",
      "train loss on epoch 672 : 0.107\n",
      "train accuracy on epoch 672: 0.944\n",
      "test loss on epoch 672: 0.687\n",
      "test accuracy on epoch 672: 0.692\n",
      "train loss on epoch 673 : 0.098\n",
      "train accuracy on epoch 673: 0.889\n",
      "test loss on epoch 673: 0.676\n",
      "test accuracy on epoch 673: 0.692\n",
      "train loss on epoch 674 : 0.094\n",
      "train accuracy on epoch 674: 0.944\n",
      "test loss on epoch 674: 0.669\n",
      "test accuracy on epoch 674: 0.692\n",
      "train loss on epoch 675 : 0.129\n",
      "train accuracy on epoch 675: 0.944\n",
      "test loss on epoch 675: 0.672\n",
      "test accuracy on epoch 675: 0.692\n",
      "train loss on epoch 676 : 0.060\n",
      "train accuracy on epoch 676: 1.000\n",
      "test loss on epoch 676: 0.671\n",
      "test accuracy on epoch 676: 0.692\n",
      "train loss on epoch 677 : 0.050\n",
      "train accuracy on epoch 677: 1.000\n",
      "test loss on epoch 677: 0.667\n",
      "test accuracy on epoch 677: 0.692\n",
      "train loss on epoch 678 : 0.109\n",
      "train accuracy on epoch 678: 0.944\n",
      "test loss on epoch 678: 0.669\n",
      "test accuracy on epoch 678: 0.692\n",
      "train loss on epoch 679 : 0.072\n",
      "train accuracy on epoch 679: 1.000\n",
      "test loss on epoch 679: 0.670\n",
      "test accuracy on epoch 679: 0.692\n",
      "train loss on epoch 680 : 0.073\n",
      "train accuracy on epoch 680: 0.944\n",
      "test loss on epoch 680: 0.678\n",
      "test accuracy on epoch 680: 0.692\n",
      "train loss on epoch 681 : 0.086\n",
      "train accuracy on epoch 681: 1.000\n",
      "test loss on epoch 681: 0.684\n",
      "test accuracy on epoch 681: 0.692\n",
      "train loss on epoch 682 : 0.156\n",
      "train accuracy on epoch 682: 0.944\n",
      "test loss on epoch 682: 0.686\n",
      "test accuracy on epoch 682: 0.692\n",
      "train loss on epoch 683 : 0.086\n",
      "train accuracy on epoch 683: 0.944\n",
      "test loss on epoch 683: 0.684\n",
      "test accuracy on epoch 683: 0.692\n",
      "train loss on epoch 684 : 0.067\n",
      "train accuracy on epoch 684: 0.944\n",
      "test loss on epoch 684: 0.692\n",
      "test accuracy on epoch 684: 0.692\n",
      "train loss on epoch 685 : 0.085\n",
      "train accuracy on epoch 685: 0.944\n",
      "test loss on epoch 685: 0.699\n",
      "test accuracy on epoch 685: 0.692\n",
      "train loss on epoch 686 : 0.050\n",
      "train accuracy on epoch 686: 1.000\n",
      "test loss on epoch 686: 0.710\n",
      "test accuracy on epoch 686: 0.692\n",
      "train loss on epoch 687 : 0.125\n",
      "train accuracy on epoch 687: 0.944\n",
      "test loss on epoch 687: 0.710\n",
      "test accuracy on epoch 687: 0.692\n",
      "train loss on epoch 688 : 0.114\n",
      "train accuracy on epoch 688: 0.944\n",
      "test loss on epoch 688: 0.718\n",
      "test accuracy on epoch 688: 0.615\n",
      "train loss on epoch 689 : 0.091\n",
      "train accuracy on epoch 689: 0.944\n",
      "test loss on epoch 689: 0.736\n",
      "test accuracy on epoch 689: 0.615\n",
      "train loss on epoch 690 : 0.143\n",
      "train accuracy on epoch 690: 0.944\n",
      "test loss on epoch 690: 0.743\n",
      "test accuracy on epoch 690: 0.615\n",
      "train loss on epoch 691 : 0.090\n",
      "train accuracy on epoch 691: 0.944\n",
      "test loss on epoch 691: 0.747\n",
      "test accuracy on epoch 691: 0.615\n",
      "train loss on epoch 692 : 0.140\n",
      "train accuracy on epoch 692: 0.944\n",
      "test loss on epoch 692: 0.737\n",
      "test accuracy on epoch 692: 0.615\n",
      "train loss on epoch 693 : 0.038\n",
      "train accuracy on epoch 693: 1.000\n",
      "test loss on epoch 693: 0.727\n",
      "test accuracy on epoch 693: 0.615\n",
      "train loss on epoch 694 : 0.127\n",
      "train accuracy on epoch 694: 0.889\n",
      "test loss on epoch 694: 0.723\n",
      "test accuracy on epoch 694: 0.615\n",
      "train loss on epoch 695 : 0.060\n",
      "train accuracy on epoch 695: 1.000\n",
      "test loss on epoch 695: 0.716\n",
      "test accuracy on epoch 695: 0.615\n",
      "train loss on epoch 696 : 0.077\n",
      "train accuracy on epoch 696: 0.944\n",
      "test loss on epoch 696: 0.711\n",
      "test accuracy on epoch 696: 0.692\n",
      "train loss on epoch 697 : 0.099\n",
      "train accuracy on epoch 697: 0.944\n",
      "test loss on epoch 697: 0.703\n",
      "test accuracy on epoch 697: 0.692\n",
      "train loss on epoch 698 : 0.051\n",
      "train accuracy on epoch 698: 1.000\n",
      "test loss on epoch 698: 0.697\n",
      "test accuracy on epoch 698: 0.692\n",
      "train loss on epoch 699 : 0.061\n",
      "train accuracy on epoch 699: 0.944\n",
      "test loss on epoch 699: 0.697\n",
      "test accuracy on epoch 699: 0.692\n",
      "train loss on epoch 700 : 0.088\n",
      "train accuracy on epoch 700: 0.944\n",
      "test loss on epoch 700: 0.699\n",
      "test accuracy on epoch 700: 0.692\n",
      "train loss on epoch 701 : 0.062\n",
      "train accuracy on epoch 701: 0.944\n",
      "test loss on epoch 701: 0.705\n",
      "test accuracy on epoch 701: 0.692\n",
      "train loss on epoch 702 : 0.087\n",
      "train accuracy on epoch 702: 0.944\n",
      "test loss on epoch 702: 0.704\n",
      "test accuracy on epoch 702: 0.692\n",
      "train loss on epoch 703 : 0.074\n",
      "train accuracy on epoch 703: 1.000\n",
      "test loss on epoch 703: 0.708\n",
      "test accuracy on epoch 703: 0.692\n",
      "train loss on epoch 704 : 0.105\n",
      "train accuracy on epoch 704: 0.944\n",
      "test loss on epoch 704: 0.708\n",
      "test accuracy on epoch 704: 0.692\n",
      "train loss on epoch 705 : 0.051\n",
      "train accuracy on epoch 705: 1.000\n",
      "test loss on epoch 705: 0.709\n",
      "test accuracy on epoch 705: 0.692\n",
      "train loss on epoch 706 : 0.140\n",
      "train accuracy on epoch 706: 0.889\n",
      "test loss on epoch 706: 0.721\n",
      "test accuracy on epoch 706: 0.615\n",
      "train loss on epoch 707 : 0.048\n",
      "train accuracy on epoch 707: 1.000\n",
      "test loss on epoch 707: 0.722\n",
      "test accuracy on epoch 707: 0.615\n",
      "train loss on epoch 708 : 0.117\n",
      "train accuracy on epoch 708: 0.944\n",
      "test loss on epoch 708: 0.725\n",
      "test accuracy on epoch 708: 0.615\n",
      "train loss on epoch 709 : 0.084\n",
      "train accuracy on epoch 709: 0.944\n",
      "test loss on epoch 709: 0.729\n",
      "test accuracy on epoch 709: 0.615\n",
      "train loss on epoch 710 : 0.094\n",
      "train accuracy on epoch 710: 0.889\n",
      "test loss on epoch 710: 0.726\n",
      "test accuracy on epoch 710: 0.615\n",
      "train loss on epoch 711 : 0.104\n",
      "train accuracy on epoch 711: 0.944\n",
      "test loss on epoch 711: 0.733\n",
      "test accuracy on epoch 711: 0.615\n",
      "train loss on epoch 712 : 0.039\n",
      "train accuracy on epoch 712: 1.000\n",
      "test loss on epoch 712: 0.732\n",
      "test accuracy on epoch 712: 0.615\n",
      "train loss on epoch 713 : 0.101\n",
      "train accuracy on epoch 713: 0.944\n",
      "test loss on epoch 713: 0.731\n",
      "test accuracy on epoch 713: 0.615\n",
      "train loss on epoch 714 : 0.123\n",
      "train accuracy on epoch 714: 0.944\n",
      "test loss on epoch 714: 0.729\n",
      "test accuracy on epoch 714: 0.615\n",
      "train loss on epoch 715 : 0.104\n",
      "train accuracy on epoch 715: 0.944\n",
      "test loss on epoch 715: 0.726\n",
      "test accuracy on epoch 715: 0.615\n",
      "train loss on epoch 716 : 0.071\n",
      "train accuracy on epoch 716: 0.944\n",
      "test loss on epoch 716: 0.726\n",
      "test accuracy on epoch 716: 0.615\n",
      "train loss on epoch 717 : 0.096\n",
      "train accuracy on epoch 717: 0.889\n",
      "test loss on epoch 717: 0.727\n",
      "test accuracy on epoch 717: 0.615\n",
      "train loss on epoch 718 : 0.079\n",
      "train accuracy on epoch 718: 0.944\n",
      "test loss on epoch 718: 0.732\n",
      "test accuracy on epoch 718: 0.615\n",
      "train loss on epoch 719 : 0.080\n",
      "train accuracy on epoch 719: 1.000\n",
      "test loss on epoch 719: 0.736\n",
      "test accuracy on epoch 719: 0.615\n",
      "train loss on epoch 720 : 0.097\n",
      "train accuracy on epoch 720: 0.944\n",
      "test loss on epoch 720: 0.732\n",
      "test accuracy on epoch 720: 0.615\n",
      "train loss on epoch 721 : 0.148\n",
      "train accuracy on epoch 721: 0.889\n",
      "test loss on epoch 721: 0.723\n",
      "test accuracy on epoch 721: 0.615\n",
      "train loss on epoch 722 : 0.059\n",
      "train accuracy on epoch 722: 1.000\n",
      "test loss on epoch 722: 0.722\n",
      "test accuracy on epoch 722: 0.615\n",
      "train loss on epoch 723 : 0.116\n",
      "train accuracy on epoch 723: 0.944\n",
      "test loss on epoch 723: 0.716\n",
      "test accuracy on epoch 723: 0.615\n",
      "train loss on epoch 724 : 0.038\n",
      "train accuracy on epoch 724: 1.000\n",
      "test loss on epoch 724: 0.710\n",
      "test accuracy on epoch 724: 0.692\n",
      "train loss on epoch 725 : 0.056\n",
      "train accuracy on epoch 725: 0.944\n",
      "test loss on epoch 725: 0.703\n",
      "test accuracy on epoch 725: 0.692\n",
      "train loss on epoch 726 : 0.082\n",
      "train accuracy on epoch 726: 0.944\n",
      "test loss on epoch 726: 0.696\n",
      "test accuracy on epoch 726: 0.692\n",
      "train loss on epoch 727 : 0.083\n",
      "train accuracy on epoch 727: 0.944\n",
      "test loss on epoch 727: 0.700\n",
      "test accuracy on epoch 727: 0.692\n",
      "train loss on epoch 728 : 0.094\n",
      "train accuracy on epoch 728: 0.944\n",
      "test loss on epoch 728: 0.704\n",
      "test accuracy on epoch 728: 0.692\n",
      "train loss on epoch 729 : 0.104\n",
      "train accuracy on epoch 729: 0.944\n",
      "test loss on epoch 729: 0.697\n",
      "test accuracy on epoch 729: 0.692\n",
      "train loss on epoch 730 : 0.060\n",
      "train accuracy on epoch 730: 1.000\n",
      "test loss on epoch 730: 0.694\n",
      "test accuracy on epoch 730: 0.692\n",
      "train loss on epoch 731 : 0.085\n",
      "train accuracy on epoch 731: 0.944\n",
      "test loss on epoch 731: 0.690\n",
      "test accuracy on epoch 731: 0.692\n",
      "train loss on epoch 732 : 0.102\n",
      "train accuracy on epoch 732: 0.889\n",
      "test loss on epoch 732: 0.691\n",
      "test accuracy on epoch 732: 0.692\n",
      "train loss on epoch 733 : 0.126\n",
      "train accuracy on epoch 733: 0.944\n",
      "test loss on epoch 733: 0.697\n",
      "test accuracy on epoch 733: 0.692\n",
      "train loss on epoch 734 : 0.111\n",
      "train accuracy on epoch 734: 0.944\n",
      "test loss on epoch 734: 0.698\n",
      "test accuracy on epoch 734: 0.692\n",
      "train loss on epoch 735 : 0.124\n",
      "train accuracy on epoch 735: 0.944\n",
      "test loss on epoch 735: 0.701\n",
      "test accuracy on epoch 735: 0.692\n",
      "train loss on epoch 736 : 0.079\n",
      "train accuracy on epoch 736: 0.944\n",
      "test loss on epoch 736: 0.709\n",
      "test accuracy on epoch 736: 0.692\n",
      "train loss on epoch 737 : 0.076\n",
      "train accuracy on epoch 737: 0.944\n",
      "test loss on epoch 737: 0.713\n",
      "test accuracy on epoch 737: 0.692\n",
      "train loss on epoch 738 : 0.042\n",
      "train accuracy on epoch 738: 1.000\n",
      "test loss on epoch 738: 0.710\n",
      "test accuracy on epoch 738: 0.692\n",
      "train loss on epoch 739 : 0.085\n",
      "train accuracy on epoch 739: 0.944\n",
      "test loss on epoch 739: 0.704\n",
      "test accuracy on epoch 739: 0.692\n",
      "train loss on epoch 740 : 0.126\n",
      "train accuracy on epoch 740: 0.889\n",
      "test loss on epoch 740: 0.701\n",
      "test accuracy on epoch 740: 0.692\n",
      "train loss on epoch 741 : 0.137\n",
      "train accuracy on epoch 741: 0.889\n",
      "test loss on epoch 741: 0.710\n",
      "test accuracy on epoch 741: 0.692\n",
      "train loss on epoch 742 : 0.061\n",
      "train accuracy on epoch 742: 1.000\n",
      "test loss on epoch 742: 0.712\n",
      "test accuracy on epoch 742: 0.692\n",
      "train loss on epoch 743 : 0.055\n",
      "train accuracy on epoch 743: 1.000\n",
      "test loss on epoch 743: 0.715\n",
      "test accuracy on epoch 743: 0.692\n",
      "train loss on epoch 744 : 0.165\n",
      "train accuracy on epoch 744: 0.889\n",
      "test loss on epoch 744: 0.716\n",
      "test accuracy on epoch 744: 0.692\n",
      "train loss on epoch 745 : 0.114\n",
      "train accuracy on epoch 745: 0.944\n",
      "test loss on epoch 745: 0.714\n",
      "test accuracy on epoch 745: 0.692\n",
      "train loss on epoch 746 : 0.132\n",
      "train accuracy on epoch 746: 0.944\n",
      "test loss on epoch 746: 0.717\n",
      "test accuracy on epoch 746: 0.692\n",
      "train loss on epoch 747 : 0.056\n",
      "train accuracy on epoch 747: 1.000\n",
      "test loss on epoch 747: 0.706\n",
      "test accuracy on epoch 747: 0.692\n",
      "train loss on epoch 748 : 0.046\n",
      "train accuracy on epoch 748: 1.000\n",
      "test loss on epoch 748: 0.705\n",
      "test accuracy on epoch 748: 0.692\n",
      "train loss on epoch 749 : 0.071\n",
      "train accuracy on epoch 749: 1.000\n",
      "test loss on epoch 749: 0.703\n",
      "test accuracy on epoch 749: 0.692\n",
      "train loss on epoch 750 : 0.156\n",
      "train accuracy on epoch 750: 0.944\n",
      "test loss on epoch 750: 0.702\n",
      "test accuracy on epoch 750: 0.692\n",
      "train loss on epoch 751 : 0.067\n",
      "train accuracy on epoch 751: 0.944\n",
      "test loss on epoch 751: 0.695\n",
      "test accuracy on epoch 751: 0.692\n",
      "train loss on epoch 752 : 0.071\n",
      "train accuracy on epoch 752: 1.000\n",
      "test loss on epoch 752: 0.695\n",
      "test accuracy on epoch 752: 0.692\n",
      "train loss on epoch 753 : 0.084\n",
      "train accuracy on epoch 753: 0.944\n",
      "test loss on epoch 753: 0.693\n",
      "test accuracy on epoch 753: 0.692\n",
      "train loss on epoch 754 : 0.061\n",
      "train accuracy on epoch 754: 1.000\n",
      "test loss on epoch 754: 0.692\n",
      "test accuracy on epoch 754: 0.692\n",
      "train loss on epoch 755 : 0.051\n",
      "train accuracy on epoch 755: 1.000\n",
      "test loss on epoch 755: 0.692\n",
      "test accuracy on epoch 755: 0.692\n",
      "train loss on epoch 756 : 0.062\n",
      "train accuracy on epoch 756: 0.944\n",
      "test loss on epoch 756: 0.696\n",
      "test accuracy on epoch 756: 0.692\n",
      "train loss on epoch 757 : 0.086\n",
      "train accuracy on epoch 757: 0.944\n",
      "test loss on epoch 757: 0.699\n",
      "test accuracy on epoch 757: 0.692\n",
      "train loss on epoch 758 : 0.091\n",
      "train accuracy on epoch 758: 0.944\n",
      "test loss on epoch 758: 0.696\n",
      "test accuracy on epoch 758: 0.692\n",
      "train loss on epoch 759 : 0.061\n",
      "train accuracy on epoch 759: 0.944\n",
      "test loss on epoch 759: 0.697\n",
      "test accuracy on epoch 759: 0.692\n",
      "train loss on epoch 760 : 0.089\n",
      "train accuracy on epoch 760: 0.944\n",
      "test loss on epoch 760: 0.699\n",
      "test accuracy on epoch 760: 0.692\n",
      "train loss on epoch 761 : 0.071\n",
      "train accuracy on epoch 761: 0.944\n",
      "test loss on epoch 761: 0.703\n",
      "test accuracy on epoch 761: 0.692\n",
      "train loss on epoch 762 : 0.087\n",
      "train accuracy on epoch 762: 1.000\n",
      "test loss on epoch 762: 0.706\n",
      "test accuracy on epoch 762: 0.692\n",
      "train loss on epoch 763 : 0.106\n",
      "train accuracy on epoch 763: 0.944\n",
      "test loss on epoch 763: 0.700\n",
      "test accuracy on epoch 763: 0.692\n",
      "train loss on epoch 764 : 0.100\n",
      "train accuracy on epoch 764: 0.944\n",
      "test loss on epoch 764: 0.694\n",
      "test accuracy on epoch 764: 0.692\n",
      "train loss on epoch 765 : 0.067\n",
      "train accuracy on epoch 765: 1.000\n",
      "test loss on epoch 765: 0.693\n",
      "test accuracy on epoch 765: 0.692\n",
      "train loss on epoch 766 : 0.089\n",
      "train accuracy on epoch 766: 0.944\n",
      "test loss on epoch 766: 0.699\n",
      "test accuracy on epoch 766: 0.692\n",
      "train loss on epoch 767 : 0.098\n",
      "train accuracy on epoch 767: 0.944\n",
      "test loss on epoch 767: 0.699\n",
      "test accuracy on epoch 767: 0.692\n",
      "train loss on epoch 768 : 0.200\n",
      "train accuracy on epoch 768: 0.889\n",
      "test loss on epoch 768: 0.710\n",
      "test accuracy on epoch 768: 0.692\n",
      "train loss on epoch 769 : 0.165\n",
      "train accuracy on epoch 769: 0.889\n",
      "test loss on epoch 769: 0.720\n",
      "test accuracy on epoch 769: 0.692\n",
      "train loss on epoch 770 : 0.085\n",
      "train accuracy on epoch 770: 0.944\n",
      "test loss on epoch 770: 0.716\n",
      "test accuracy on epoch 770: 0.692\n",
      "train loss on epoch 771 : 0.128\n",
      "train accuracy on epoch 771: 0.889\n",
      "test loss on epoch 771: 0.715\n",
      "test accuracy on epoch 771: 0.692\n",
      "train loss on epoch 772 : 0.125\n",
      "train accuracy on epoch 772: 0.944\n",
      "test loss on epoch 772: 0.709\n",
      "test accuracy on epoch 772: 0.692\n",
      "train loss on epoch 773 : 0.119\n",
      "train accuracy on epoch 773: 0.889\n",
      "test loss on epoch 773: 0.705\n",
      "test accuracy on epoch 773: 0.692\n",
      "train loss on epoch 774 : 0.094\n",
      "train accuracy on epoch 774: 0.944\n",
      "test loss on epoch 774: 0.709\n",
      "test accuracy on epoch 774: 0.692\n",
      "train loss on epoch 775 : 0.060\n",
      "train accuracy on epoch 775: 1.000\n",
      "test loss on epoch 775: 0.711\n",
      "test accuracy on epoch 775: 0.692\n",
      "train loss on epoch 776 : 0.094\n",
      "train accuracy on epoch 776: 0.944\n",
      "test loss on epoch 776: 0.711\n",
      "test accuracy on epoch 776: 0.692\n",
      "train loss on epoch 777 : 0.161\n",
      "train accuracy on epoch 777: 0.944\n",
      "test loss on epoch 777: 0.714\n",
      "test accuracy on epoch 777: 0.692\n",
      "train loss on epoch 778 : 0.041\n",
      "train accuracy on epoch 778: 1.000\n",
      "test loss on epoch 778: 0.707\n",
      "test accuracy on epoch 778: 0.692\n",
      "train loss on epoch 779 : 0.079\n",
      "train accuracy on epoch 779: 0.944\n",
      "test loss on epoch 779: 0.710\n",
      "test accuracy on epoch 779: 0.692\n",
      "train loss on epoch 780 : 0.107\n",
      "train accuracy on epoch 780: 0.944\n",
      "test loss on epoch 780: 0.712\n",
      "test accuracy on epoch 780: 0.692\n",
      "train loss on epoch 781 : 0.155\n",
      "train accuracy on epoch 781: 0.944\n",
      "test loss on epoch 781: 0.712\n",
      "test accuracy on epoch 781: 0.692\n",
      "train loss on epoch 782 : 0.104\n",
      "train accuracy on epoch 782: 0.944\n",
      "test loss on epoch 782: 0.720\n",
      "test accuracy on epoch 782: 0.692\n",
      "train loss on epoch 783 : 0.051\n",
      "train accuracy on epoch 783: 1.000\n",
      "test loss on epoch 783: 0.731\n",
      "test accuracy on epoch 783: 0.692\n",
      "train loss on epoch 784 : 0.107\n",
      "train accuracy on epoch 784: 0.889\n",
      "test loss on epoch 784: 0.741\n",
      "test accuracy on epoch 784: 0.615\n",
      "train loss on epoch 785 : 0.062\n",
      "train accuracy on epoch 785: 1.000\n",
      "test loss on epoch 785: 0.744\n",
      "test accuracy on epoch 785: 0.615\n",
      "train loss on epoch 786 : 0.078\n",
      "train accuracy on epoch 786: 0.944\n",
      "test loss on epoch 786: 0.747\n",
      "test accuracy on epoch 786: 0.615\n",
      "train loss on epoch 787 : 0.041\n",
      "train accuracy on epoch 787: 1.000\n",
      "test loss on epoch 787: 0.751\n",
      "test accuracy on epoch 787: 0.615\n",
      "train loss on epoch 788 : 0.053\n",
      "train accuracy on epoch 788: 1.000\n",
      "test loss on epoch 788: 0.750\n",
      "test accuracy on epoch 788: 0.615\n",
      "train loss on epoch 789 : 0.189\n",
      "train accuracy on epoch 789: 0.889\n",
      "test loss on epoch 789: 0.744\n",
      "test accuracy on epoch 789: 0.615\n",
      "train loss on epoch 790 : 0.064\n",
      "train accuracy on epoch 790: 1.000\n",
      "test loss on epoch 790: 0.737\n",
      "test accuracy on epoch 790: 0.692\n",
      "train loss on epoch 791 : 0.064\n",
      "train accuracy on epoch 791: 0.944\n",
      "test loss on epoch 791: 0.734\n",
      "test accuracy on epoch 791: 0.692\n",
      "train loss on epoch 792 : 0.062\n",
      "train accuracy on epoch 792: 1.000\n",
      "test loss on epoch 792: 0.736\n",
      "test accuracy on epoch 792: 0.692\n",
      "train loss on epoch 793 : 0.106\n",
      "train accuracy on epoch 793: 0.889\n",
      "test loss on epoch 793: 0.738\n",
      "test accuracy on epoch 793: 0.692\n",
      "train loss on epoch 794 : 0.044\n",
      "train accuracy on epoch 794: 1.000\n",
      "test loss on epoch 794: 0.736\n",
      "test accuracy on epoch 794: 0.692\n",
      "train loss on epoch 795 : 0.086\n",
      "train accuracy on epoch 795: 0.944\n",
      "test loss on epoch 795: 0.728\n",
      "test accuracy on epoch 795: 0.692\n",
      "train loss on epoch 796 : 0.097\n",
      "train accuracy on epoch 796: 0.944\n",
      "test loss on epoch 796: 0.727\n",
      "test accuracy on epoch 796: 0.692\n",
      "train loss on epoch 797 : 0.102\n",
      "train accuracy on epoch 797: 0.944\n",
      "test loss on epoch 797: 0.736\n",
      "test accuracy on epoch 797: 0.692\n",
      "train loss on epoch 798 : 0.095\n",
      "train accuracy on epoch 798: 0.944\n",
      "test loss on epoch 798: 0.726\n",
      "test accuracy on epoch 798: 0.692\n",
      "train loss on epoch 799 : 0.064\n",
      "train accuracy on epoch 799: 1.000\n",
      "test loss on epoch 799: 0.720\n",
      "test accuracy on epoch 799: 0.692\n",
      "train loss on epoch 800 : 0.075\n",
      "train accuracy on epoch 800: 1.000\n",
      "test loss on epoch 800: 0.714\n",
      "test accuracy on epoch 800: 0.692\n",
      "train loss on epoch 801 : 0.076\n",
      "train accuracy on epoch 801: 0.944\n",
      "test loss on epoch 801: 0.717\n",
      "test accuracy on epoch 801: 0.692\n",
      "train loss on epoch 802 : 0.155\n",
      "train accuracy on epoch 802: 0.944\n",
      "test loss on epoch 802: 0.722\n",
      "test accuracy on epoch 802: 0.692\n",
      "train loss on epoch 803 : 0.044\n",
      "train accuracy on epoch 803: 1.000\n",
      "test loss on epoch 803: 0.735\n",
      "test accuracy on epoch 803: 0.692\n",
      "train loss on epoch 804 : 0.105\n",
      "train accuracy on epoch 804: 0.944\n",
      "test loss on epoch 804: 0.743\n",
      "test accuracy on epoch 804: 0.692\n",
      "train loss on epoch 805 : 0.144\n",
      "train accuracy on epoch 805: 0.889\n",
      "test loss on epoch 805: 0.750\n",
      "test accuracy on epoch 805: 0.615\n",
      "train loss on epoch 806 : 0.080\n",
      "train accuracy on epoch 806: 0.944\n",
      "test loss on epoch 806: 0.750\n",
      "test accuracy on epoch 806: 0.615\n",
      "train loss on epoch 807 : 0.115\n",
      "train accuracy on epoch 807: 0.889\n",
      "test loss on epoch 807: 0.747\n",
      "test accuracy on epoch 807: 0.615\n",
      "train loss on epoch 808 : 0.170\n",
      "train accuracy on epoch 808: 0.889\n",
      "test loss on epoch 808: 0.750\n",
      "test accuracy on epoch 808: 0.615\n",
      "train loss on epoch 809 : 0.102\n",
      "train accuracy on epoch 809: 0.889\n",
      "test loss on epoch 809: 0.744\n",
      "test accuracy on epoch 809: 0.615\n",
      "train loss on epoch 810 : 0.152\n",
      "train accuracy on epoch 810: 0.889\n",
      "test loss on epoch 810: 0.751\n",
      "test accuracy on epoch 810: 0.615\n",
      "train loss on epoch 811 : 0.095\n",
      "train accuracy on epoch 811: 0.944\n",
      "test loss on epoch 811: 0.752\n",
      "test accuracy on epoch 811: 0.615\n",
      "train loss on epoch 812 : 0.075\n",
      "train accuracy on epoch 812: 0.944\n",
      "test loss on epoch 812: 0.752\n",
      "test accuracy on epoch 812: 0.615\n",
      "train loss on epoch 813 : 0.143\n",
      "train accuracy on epoch 813: 0.889\n",
      "test loss on epoch 813: 0.755\n",
      "test accuracy on epoch 813: 0.615\n",
      "train loss on epoch 814 : 0.045\n",
      "train accuracy on epoch 814: 1.000\n",
      "test loss on epoch 814: 0.755\n",
      "test accuracy on epoch 814: 0.615\n",
      "train loss on epoch 815 : 0.107\n",
      "train accuracy on epoch 815: 0.944\n",
      "test loss on epoch 815: 0.757\n",
      "test accuracy on epoch 815: 0.615\n",
      "train loss on epoch 816 : 0.108\n",
      "train accuracy on epoch 816: 0.889\n",
      "test loss on epoch 816: 0.759\n",
      "test accuracy on epoch 816: 0.615\n",
      "train loss on epoch 817 : 0.069\n",
      "train accuracy on epoch 817: 1.000\n",
      "test loss on epoch 817: 0.764\n",
      "test accuracy on epoch 817: 0.615\n",
      "train loss on epoch 818 : 0.080\n",
      "train accuracy on epoch 818: 0.944\n",
      "test loss on epoch 818: 0.767\n",
      "test accuracy on epoch 818: 0.615\n",
      "train loss on epoch 819 : 0.070\n",
      "train accuracy on epoch 819: 1.000\n",
      "test loss on epoch 819: 0.772\n",
      "test accuracy on epoch 819: 0.615\n",
      "train loss on epoch 820 : 0.141\n",
      "train accuracy on epoch 820: 0.889\n",
      "test loss on epoch 820: 0.766\n",
      "test accuracy on epoch 820: 0.615\n",
      "train loss on epoch 821 : 0.103\n",
      "train accuracy on epoch 821: 0.944\n",
      "test loss on epoch 821: 0.758\n",
      "test accuracy on epoch 821: 0.615\n",
      "train loss on epoch 822 : 0.065\n",
      "train accuracy on epoch 822: 1.000\n",
      "test loss on epoch 822: 0.752\n",
      "test accuracy on epoch 822: 0.615\n",
      "train loss on epoch 823 : 0.090\n",
      "train accuracy on epoch 823: 0.944\n",
      "test loss on epoch 823: 0.747\n",
      "test accuracy on epoch 823: 0.615\n",
      "train loss on epoch 824 : 0.102\n",
      "train accuracy on epoch 824: 0.944\n",
      "test loss on epoch 824: 0.739\n",
      "test accuracy on epoch 824: 0.692\n",
      "train loss on epoch 825 : 0.110\n",
      "train accuracy on epoch 825: 0.944\n",
      "test loss on epoch 825: 0.732\n",
      "test accuracy on epoch 825: 0.692\n",
      "train loss on epoch 826 : 0.112\n",
      "train accuracy on epoch 826: 0.889\n",
      "test loss on epoch 826: 0.724\n",
      "test accuracy on epoch 826: 0.692\n",
      "train loss on epoch 827 : 0.114\n",
      "train accuracy on epoch 827: 0.889\n",
      "test loss on epoch 827: 0.719\n",
      "test accuracy on epoch 827: 0.692\n",
      "train loss on epoch 828 : 0.096\n",
      "train accuracy on epoch 828: 0.944\n",
      "test loss on epoch 828: 0.720\n",
      "test accuracy on epoch 828: 0.692\n",
      "train loss on epoch 829 : 0.073\n",
      "train accuracy on epoch 829: 0.944\n",
      "test loss on epoch 829: 0.723\n",
      "test accuracy on epoch 829: 0.692\n",
      "train loss on epoch 830 : 0.131\n",
      "train accuracy on epoch 830: 0.944\n",
      "test loss on epoch 830: 0.729\n",
      "test accuracy on epoch 830: 0.692\n",
      "train loss on epoch 831 : 0.101\n",
      "train accuracy on epoch 831: 0.944\n",
      "test loss on epoch 831: 0.732\n",
      "test accuracy on epoch 831: 0.692\n",
      "train loss on epoch 832 : 0.045\n",
      "train accuracy on epoch 832: 1.000\n",
      "test loss on epoch 832: 0.731\n",
      "test accuracy on epoch 832: 0.692\n",
      "train loss on epoch 833 : 0.110\n",
      "train accuracy on epoch 833: 0.944\n",
      "test loss on epoch 833: 0.738\n",
      "test accuracy on epoch 833: 0.615\n",
      "train loss on epoch 834 : 0.127\n",
      "train accuracy on epoch 834: 0.944\n",
      "test loss on epoch 834: 0.745\n",
      "test accuracy on epoch 834: 0.615\n",
      "train loss on epoch 835 : 0.156\n",
      "train accuracy on epoch 835: 0.889\n",
      "test loss on epoch 835: 0.754\n",
      "test accuracy on epoch 835: 0.615\n",
      "train loss on epoch 836 : 0.119\n",
      "train accuracy on epoch 836: 0.889\n",
      "test loss on epoch 836: 0.755\n",
      "test accuracy on epoch 836: 0.615\n",
      "train loss on epoch 837 : 0.098\n",
      "train accuracy on epoch 837: 0.944\n",
      "test loss on epoch 837: 0.740\n",
      "test accuracy on epoch 837: 0.615\n",
      "train loss on epoch 838 : 0.040\n",
      "train accuracy on epoch 838: 1.000\n",
      "test loss on epoch 838: 0.731\n",
      "test accuracy on epoch 838: 0.692\n",
      "train loss on epoch 839 : 0.103\n",
      "train accuracy on epoch 839: 0.944\n",
      "test loss on epoch 839: 0.731\n",
      "test accuracy on epoch 839: 0.692\n",
      "train loss on epoch 840 : 0.047\n",
      "train accuracy on epoch 840: 1.000\n",
      "test loss on epoch 840: 0.733\n",
      "test accuracy on epoch 840: 0.692\n",
      "train loss on epoch 841 : 0.077\n",
      "train accuracy on epoch 841: 0.944\n",
      "test loss on epoch 841: 0.734\n",
      "test accuracy on epoch 841: 0.615\n",
      "train loss on epoch 842 : 0.143\n",
      "train accuracy on epoch 842: 0.944\n",
      "test loss on epoch 842: 0.739\n",
      "test accuracy on epoch 842: 0.615\n",
      "train loss on epoch 843 : 0.111\n",
      "train accuracy on epoch 843: 0.944\n",
      "test loss on epoch 843: 0.749\n",
      "test accuracy on epoch 843: 0.615\n",
      "train loss on epoch 844 : 0.086\n",
      "train accuracy on epoch 844: 1.000\n",
      "test loss on epoch 844: 0.753\n",
      "test accuracy on epoch 844: 0.615\n",
      "train loss on epoch 845 : 0.056\n",
      "train accuracy on epoch 845: 1.000\n",
      "test loss on epoch 845: 0.754\n",
      "test accuracy on epoch 845: 0.615\n",
      "train loss on epoch 846 : 0.085\n",
      "train accuracy on epoch 846: 1.000\n",
      "test loss on epoch 846: 0.754\n",
      "test accuracy on epoch 846: 0.615\n",
      "train loss on epoch 847 : 0.035\n",
      "train accuracy on epoch 847: 1.000\n",
      "test loss on epoch 847: 0.749\n",
      "test accuracy on epoch 847: 0.615\n",
      "train loss on epoch 848 : 0.109\n",
      "train accuracy on epoch 848: 0.889\n",
      "test loss on epoch 848: 0.746\n",
      "test accuracy on epoch 848: 0.615\n",
      "train loss on epoch 849 : 0.094\n",
      "train accuracy on epoch 849: 0.944\n",
      "test loss on epoch 849: 0.740\n",
      "test accuracy on epoch 849: 0.615\n",
      "train loss on epoch 850 : 0.058\n",
      "train accuracy on epoch 850: 1.000\n",
      "test loss on epoch 850: 0.738\n",
      "test accuracy on epoch 850: 0.615\n",
      "train loss on epoch 851 : 0.073\n",
      "train accuracy on epoch 851: 0.944\n",
      "test loss on epoch 851: 0.728\n",
      "test accuracy on epoch 851: 0.692\n",
      "train loss on epoch 852 : 0.135\n",
      "train accuracy on epoch 852: 0.944\n",
      "test loss on epoch 852: 0.725\n",
      "test accuracy on epoch 852: 0.692\n",
      "train loss on epoch 853 : 0.164\n",
      "train accuracy on epoch 853: 0.889\n",
      "test loss on epoch 853: 0.725\n",
      "test accuracy on epoch 853: 0.692\n",
      "train loss on epoch 854 : 0.052\n",
      "train accuracy on epoch 854: 1.000\n",
      "test loss on epoch 854: 0.715\n",
      "test accuracy on epoch 854: 0.692\n",
      "train loss on epoch 855 : 0.128\n",
      "train accuracy on epoch 855: 0.944\n",
      "test loss on epoch 855: 0.717\n",
      "test accuracy on epoch 855: 0.692\n",
      "train loss on epoch 856 : 0.114\n",
      "train accuracy on epoch 856: 0.944\n",
      "test loss on epoch 856: 0.720\n",
      "test accuracy on epoch 856: 0.692\n",
      "train loss on epoch 857 : 0.087\n",
      "train accuracy on epoch 857: 0.944\n",
      "test loss on epoch 857: 0.728\n",
      "test accuracy on epoch 857: 0.692\n",
      "train loss on epoch 858 : 0.133\n",
      "train accuracy on epoch 858: 0.889\n",
      "test loss on epoch 858: 0.742\n",
      "test accuracy on epoch 858: 0.615\n",
      "train loss on epoch 859 : 0.078\n",
      "train accuracy on epoch 859: 0.944\n",
      "test loss on epoch 859: 0.752\n",
      "test accuracy on epoch 859: 0.615\n",
      "train loss on epoch 860 : 0.079\n",
      "train accuracy on epoch 860: 0.944\n",
      "test loss on epoch 860: 0.763\n",
      "test accuracy on epoch 860: 0.615\n",
      "train loss on epoch 861 : 0.098\n",
      "train accuracy on epoch 861: 0.889\n",
      "test loss on epoch 861: 0.772\n",
      "test accuracy on epoch 861: 0.615\n",
      "train loss on epoch 862 : 0.103\n",
      "train accuracy on epoch 862: 0.944\n",
      "test loss on epoch 862: 0.763\n",
      "test accuracy on epoch 862: 0.615\n",
      "train loss on epoch 863 : 0.098\n",
      "train accuracy on epoch 863: 0.944\n",
      "test loss on epoch 863: 0.764\n",
      "test accuracy on epoch 863: 0.615\n",
      "train loss on epoch 864 : 0.080\n",
      "train accuracy on epoch 864: 0.944\n",
      "test loss on epoch 864: 0.762\n",
      "test accuracy on epoch 864: 0.615\n",
      "train loss on epoch 865 : 0.156\n",
      "train accuracy on epoch 865: 0.889\n",
      "test loss on epoch 865: 0.759\n",
      "test accuracy on epoch 865: 0.615\n",
      "train loss on epoch 866 : 0.125\n",
      "train accuracy on epoch 866: 0.889\n",
      "test loss on epoch 866: 0.757\n",
      "test accuracy on epoch 866: 0.615\n",
      "train loss on epoch 867 : 0.153\n",
      "train accuracy on epoch 867: 0.889\n",
      "test loss on epoch 867: 0.755\n",
      "test accuracy on epoch 867: 0.615\n",
      "train loss on epoch 868 : 0.086\n",
      "train accuracy on epoch 868: 0.944\n",
      "test loss on epoch 868: 0.750\n",
      "test accuracy on epoch 868: 0.615\n",
      "train loss on epoch 869 : 0.072\n",
      "train accuracy on epoch 869: 1.000\n",
      "test loss on epoch 869: 0.748\n",
      "test accuracy on epoch 869: 0.615\n",
      "train loss on epoch 870 : 0.059\n",
      "train accuracy on epoch 870: 1.000\n",
      "test loss on epoch 870: 0.743\n",
      "test accuracy on epoch 870: 0.615\n",
      "train loss on epoch 871 : 0.059\n",
      "train accuracy on epoch 871: 1.000\n",
      "test loss on epoch 871: 0.742\n",
      "test accuracy on epoch 871: 0.615\n",
      "train loss on epoch 872 : 0.085\n",
      "train accuracy on epoch 872: 0.944\n",
      "test loss on epoch 872: 0.742\n",
      "test accuracy on epoch 872: 0.615\n",
      "train loss on epoch 873 : 0.148\n",
      "train accuracy on epoch 873: 0.889\n",
      "test loss on epoch 873: 0.743\n",
      "test accuracy on epoch 873: 0.615\n",
      "train loss on epoch 874 : 0.118\n",
      "train accuracy on epoch 874: 0.889\n",
      "test loss on epoch 874: 0.737\n",
      "test accuracy on epoch 874: 0.692\n",
      "train loss on epoch 875 : 0.124\n",
      "train accuracy on epoch 875: 0.889\n",
      "test loss on epoch 875: 0.734\n",
      "test accuracy on epoch 875: 0.692\n",
      "train loss on epoch 876 : 0.175\n",
      "train accuracy on epoch 876: 0.944\n",
      "test loss on epoch 876: 0.736\n",
      "test accuracy on epoch 876: 0.692\n",
      "train loss on epoch 877 : 0.067\n",
      "train accuracy on epoch 877: 1.000\n",
      "test loss on epoch 877: 0.741\n",
      "test accuracy on epoch 877: 0.692\n",
      "train loss on epoch 878 : 0.061\n",
      "train accuracy on epoch 878: 1.000\n",
      "test loss on epoch 878: 0.743\n",
      "test accuracy on epoch 878: 0.692\n",
      "train loss on epoch 879 : 0.074\n",
      "train accuracy on epoch 879: 1.000\n",
      "test loss on epoch 879: 0.747\n",
      "test accuracy on epoch 879: 0.615\n",
      "train loss on epoch 880 : 0.287\n",
      "train accuracy on epoch 880: 0.889\n",
      "test loss on epoch 880: 0.744\n",
      "test accuracy on epoch 880: 0.692\n",
      "train loss on epoch 881 : 0.109\n",
      "train accuracy on epoch 881: 0.944\n",
      "test loss on epoch 881: 0.745\n",
      "test accuracy on epoch 881: 0.692\n",
      "train loss on epoch 882 : 0.095\n",
      "train accuracy on epoch 882: 0.889\n",
      "test loss on epoch 882: 0.745\n",
      "test accuracy on epoch 882: 0.692\n",
      "train loss on epoch 883 : 0.074\n",
      "train accuracy on epoch 883: 1.000\n",
      "test loss on epoch 883: 0.744\n",
      "test accuracy on epoch 883: 0.692\n",
      "train loss on epoch 884 : 0.127\n",
      "train accuracy on epoch 884: 0.889\n",
      "test loss on epoch 884: 0.745\n",
      "test accuracy on epoch 884: 0.692\n",
      "train loss on epoch 885 : 0.067\n",
      "train accuracy on epoch 885: 0.944\n",
      "test loss on epoch 885: 0.743\n",
      "test accuracy on epoch 885: 0.692\n",
      "train loss on epoch 886 : 0.113\n",
      "train accuracy on epoch 886: 0.889\n",
      "test loss on epoch 886: 0.742\n",
      "test accuracy on epoch 886: 0.692\n",
      "train loss on epoch 887 : 0.061\n",
      "train accuracy on epoch 887: 1.000\n",
      "test loss on epoch 887: 0.742\n",
      "test accuracy on epoch 887: 0.692\n",
      "train loss on epoch 888 : 0.113\n",
      "train accuracy on epoch 888: 0.944\n",
      "test loss on epoch 888: 0.742\n",
      "test accuracy on epoch 888: 0.692\n",
      "train loss on epoch 889 : 0.084\n",
      "train accuracy on epoch 889: 0.944\n",
      "test loss on epoch 889: 0.742\n",
      "test accuracy on epoch 889: 0.692\n",
      "train loss on epoch 890 : 0.027\n",
      "train accuracy on epoch 890: 1.000\n",
      "test loss on epoch 890: 0.741\n",
      "test accuracy on epoch 890: 0.692\n",
      "train loss on epoch 891 : 0.141\n",
      "train accuracy on epoch 891: 0.889\n",
      "test loss on epoch 891: 0.735\n",
      "test accuracy on epoch 891: 0.692\n",
      "train loss on epoch 892 : 0.057\n",
      "train accuracy on epoch 892: 1.000\n",
      "test loss on epoch 892: 0.735\n",
      "test accuracy on epoch 892: 0.692\n",
      "train loss on epoch 893 : 0.084\n",
      "train accuracy on epoch 893: 0.944\n",
      "test loss on epoch 893: 0.734\n",
      "test accuracy on epoch 893: 0.692\n",
      "train loss on epoch 894 : 0.124\n",
      "train accuracy on epoch 894: 0.944\n",
      "test loss on epoch 894: 0.739\n",
      "test accuracy on epoch 894: 0.692\n",
      "train loss on epoch 895 : 0.047\n",
      "train accuracy on epoch 895: 1.000\n",
      "test loss on epoch 895: 0.739\n",
      "test accuracy on epoch 895: 0.692\n",
      "train loss on epoch 896 : 0.072\n",
      "train accuracy on epoch 896: 0.944\n",
      "test loss on epoch 896: 0.738\n",
      "test accuracy on epoch 896: 0.692\n",
      "train loss on epoch 897 : 0.066\n",
      "train accuracy on epoch 897: 0.944\n",
      "test loss on epoch 897: 0.741\n",
      "test accuracy on epoch 897: 0.692\n",
      "train loss on epoch 898 : 0.139\n",
      "train accuracy on epoch 898: 0.889\n",
      "test loss on epoch 898: 0.742\n",
      "test accuracy on epoch 898: 0.692\n",
      "train loss on epoch 899 : 0.086\n",
      "train accuracy on epoch 899: 0.944\n",
      "test loss on epoch 899: 0.745\n",
      "test accuracy on epoch 899: 0.692\n",
      "train loss on epoch 900 : 0.070\n",
      "train accuracy on epoch 900: 1.000\n",
      "test loss on epoch 900: 0.751\n",
      "test accuracy on epoch 900: 0.692\n",
      "train loss on epoch 901 : 0.113\n",
      "train accuracy on epoch 901: 0.889\n",
      "test loss on epoch 901: 0.754\n",
      "test accuracy on epoch 901: 0.692\n",
      "train loss on epoch 902 : 0.155\n",
      "train accuracy on epoch 902: 0.944\n",
      "test loss on epoch 902: 0.755\n",
      "test accuracy on epoch 902: 0.692\n",
      "train loss on epoch 903 : 0.097\n",
      "train accuracy on epoch 903: 0.944\n",
      "test loss on epoch 903: 0.746\n",
      "test accuracy on epoch 903: 0.692\n",
      "train loss on epoch 904 : 0.074\n",
      "train accuracy on epoch 904: 1.000\n",
      "test loss on epoch 904: 0.737\n",
      "test accuracy on epoch 904: 0.692\n",
      "train loss on epoch 905 : 0.138\n",
      "train accuracy on epoch 905: 0.889\n",
      "test loss on epoch 905: 0.732\n",
      "test accuracy on epoch 905: 0.692\n",
      "train loss on epoch 906 : 0.055\n",
      "train accuracy on epoch 906: 1.000\n",
      "test loss on epoch 906: 0.741\n",
      "test accuracy on epoch 906: 0.692\n",
      "train loss on epoch 907 : 0.088\n",
      "train accuracy on epoch 907: 0.944\n",
      "test loss on epoch 907: 0.748\n",
      "test accuracy on epoch 907: 0.692\n",
      "train loss on epoch 908 : 0.064\n",
      "train accuracy on epoch 908: 1.000\n",
      "test loss on epoch 908: 0.763\n",
      "test accuracy on epoch 908: 0.692\n",
      "train loss on epoch 909 : 0.082\n",
      "train accuracy on epoch 909: 0.944\n",
      "test loss on epoch 909: 0.777\n",
      "test accuracy on epoch 909: 0.615\n",
      "train loss on epoch 910 : 0.057\n",
      "train accuracy on epoch 910: 0.944\n",
      "test loss on epoch 910: 0.790\n",
      "test accuracy on epoch 910: 0.615\n",
      "train loss on epoch 911 : 0.125\n",
      "train accuracy on epoch 911: 0.944\n",
      "test loss on epoch 911: 0.782\n",
      "test accuracy on epoch 911: 0.615\n",
      "train loss on epoch 912 : 0.054\n",
      "train accuracy on epoch 912: 1.000\n",
      "test loss on epoch 912: 0.778\n",
      "test accuracy on epoch 912: 0.615\n",
      "train loss on epoch 913 : 0.121\n",
      "train accuracy on epoch 913: 0.889\n",
      "test loss on epoch 913: 0.776\n",
      "test accuracy on epoch 913: 0.615\n",
      "train loss on epoch 914 : 0.146\n",
      "train accuracy on epoch 914: 0.889\n",
      "test loss on epoch 914: 0.783\n",
      "test accuracy on epoch 914: 0.615\n",
      "train loss on epoch 915 : 0.072\n",
      "train accuracy on epoch 915: 0.944\n",
      "test loss on epoch 915: 0.785\n",
      "test accuracy on epoch 915: 0.615\n",
      "train loss on epoch 916 : 0.147\n",
      "train accuracy on epoch 916: 0.889\n",
      "test loss on epoch 916: 0.788\n",
      "test accuracy on epoch 916: 0.615\n",
      "train loss on epoch 917 : 0.134\n",
      "train accuracy on epoch 917: 0.889\n",
      "test loss on epoch 917: 0.791\n",
      "test accuracy on epoch 917: 0.615\n",
      "train loss on epoch 918 : 0.067\n",
      "train accuracy on epoch 918: 0.944\n",
      "test loss on epoch 918: 0.777\n",
      "test accuracy on epoch 918: 0.615\n",
      "train loss on epoch 919 : 0.102\n",
      "train accuracy on epoch 919: 0.889\n",
      "test loss on epoch 919: 0.764\n",
      "test accuracy on epoch 919: 0.615\n",
      "train loss on epoch 920 : 0.056\n",
      "train accuracy on epoch 920: 1.000\n",
      "test loss on epoch 920: 0.758\n",
      "test accuracy on epoch 920: 0.615\n",
      "train loss on epoch 921 : 0.097\n",
      "train accuracy on epoch 921: 0.944\n",
      "test loss on epoch 921: 0.756\n",
      "test accuracy on epoch 921: 0.615\n",
      "train loss on epoch 922 : 0.065\n",
      "train accuracy on epoch 922: 1.000\n",
      "test loss on epoch 922: 0.756\n",
      "test accuracy on epoch 922: 0.615\n",
      "train loss on epoch 923 : 0.071\n",
      "train accuracy on epoch 923: 1.000\n",
      "test loss on epoch 923: 0.757\n",
      "test accuracy on epoch 923: 0.615\n",
      "train loss on epoch 924 : 0.040\n",
      "train accuracy on epoch 924: 1.000\n",
      "test loss on epoch 924: 0.756\n",
      "test accuracy on epoch 924: 0.615\n",
      "train loss on epoch 925 : 0.115\n",
      "train accuracy on epoch 925: 0.944\n",
      "test loss on epoch 925: 0.757\n",
      "test accuracy on epoch 925: 0.615\n",
      "train loss on epoch 926 : 0.068\n",
      "train accuracy on epoch 926: 0.944\n",
      "test loss on epoch 926: 0.771\n",
      "test accuracy on epoch 926: 0.615\n",
      "train loss on epoch 927 : 0.112\n",
      "train accuracy on epoch 927: 0.944\n",
      "test loss on epoch 927: 0.778\n",
      "test accuracy on epoch 927: 0.615\n",
      "train loss on epoch 928 : 0.124\n",
      "train accuracy on epoch 928: 0.944\n",
      "test loss on epoch 928: 0.772\n",
      "test accuracy on epoch 928: 0.615\n",
      "train loss on epoch 929 : 0.102\n",
      "train accuracy on epoch 929: 0.944\n",
      "test loss on epoch 929: 0.767\n",
      "test accuracy on epoch 929: 0.615\n",
      "train loss on epoch 930 : 0.067\n",
      "train accuracy on epoch 930: 1.000\n",
      "test loss on epoch 930: 0.764\n",
      "test accuracy on epoch 930: 0.615\n",
      "train loss on epoch 931 : 0.069\n",
      "train accuracy on epoch 931: 0.944\n",
      "test loss on epoch 931: 0.766\n",
      "test accuracy on epoch 931: 0.615\n",
      "train loss on epoch 932 : 0.035\n",
      "train accuracy on epoch 932: 1.000\n",
      "test loss on epoch 932: 0.767\n",
      "test accuracy on epoch 932: 0.615\n",
      "train loss on epoch 933 : 0.097\n",
      "train accuracy on epoch 933: 0.889\n",
      "test loss on epoch 933: 0.768\n",
      "test accuracy on epoch 933: 0.615\n",
      "train loss on epoch 934 : 0.058\n",
      "train accuracy on epoch 934: 1.000\n",
      "test loss on epoch 934: 0.771\n",
      "test accuracy on epoch 934: 0.615\n",
      "train loss on epoch 935 : 0.078\n",
      "train accuracy on epoch 935: 0.944\n",
      "test loss on epoch 935: 0.765\n",
      "test accuracy on epoch 935: 0.615\n",
      "train loss on epoch 936 : 0.083\n",
      "train accuracy on epoch 936: 0.944\n",
      "test loss on epoch 936: 0.758\n",
      "test accuracy on epoch 936: 0.615\n",
      "train loss on epoch 937 : 0.068\n",
      "train accuracy on epoch 937: 0.944\n",
      "test loss on epoch 937: 0.749\n",
      "test accuracy on epoch 937: 0.692\n",
      "train loss on epoch 938 : 0.132\n",
      "train accuracy on epoch 938: 0.889\n",
      "test loss on epoch 938: 0.745\n",
      "test accuracy on epoch 938: 0.692\n",
      "train loss on epoch 939 : 0.110\n",
      "train accuracy on epoch 939: 0.889\n",
      "test loss on epoch 939: 0.742\n",
      "test accuracy on epoch 939: 0.692\n",
      "train loss on epoch 940 : 0.095\n",
      "train accuracy on epoch 940: 0.944\n",
      "test loss on epoch 940: 0.752\n",
      "test accuracy on epoch 940: 0.692\n",
      "train loss on epoch 941 : 0.156\n",
      "train accuracy on epoch 941: 0.944\n",
      "test loss on epoch 941: 0.769\n",
      "test accuracy on epoch 941: 0.615\n",
      "train loss on epoch 942 : 0.086\n",
      "train accuracy on epoch 942: 0.944\n",
      "test loss on epoch 942: 0.776\n",
      "test accuracy on epoch 942: 0.615\n",
      "train loss on epoch 943 : 0.110\n",
      "train accuracy on epoch 943: 0.889\n",
      "test loss on epoch 943: 0.778\n",
      "test accuracy on epoch 943: 0.615\n",
      "train loss on epoch 944 : 0.048\n",
      "train accuracy on epoch 944: 1.000\n",
      "test loss on epoch 944: 0.781\n",
      "test accuracy on epoch 944: 0.615\n",
      "train loss on epoch 945 : 0.077\n",
      "train accuracy on epoch 945: 0.944\n",
      "test loss on epoch 945: 0.778\n",
      "test accuracy on epoch 945: 0.615\n",
      "train loss on epoch 946 : 0.072\n",
      "train accuracy on epoch 946: 0.944\n",
      "test loss on epoch 946: 0.780\n",
      "test accuracy on epoch 946: 0.615\n",
      "train loss on epoch 947 : 0.075\n",
      "train accuracy on epoch 947: 0.944\n",
      "test loss on epoch 947: 0.777\n",
      "test accuracy on epoch 947: 0.615\n",
      "train loss on epoch 948 : 0.113\n",
      "train accuracy on epoch 948: 0.889\n",
      "test loss on epoch 948: 0.775\n",
      "test accuracy on epoch 948: 0.615\n",
      "train loss on epoch 949 : 0.108\n",
      "train accuracy on epoch 949: 0.944\n",
      "test loss on epoch 949: 0.774\n",
      "test accuracy on epoch 949: 0.615\n",
      "train loss on epoch 950 : 0.065\n",
      "train accuracy on epoch 950: 0.944\n",
      "test loss on epoch 950: 0.779\n",
      "test accuracy on epoch 950: 0.615\n",
      "train loss on epoch 951 : 0.139\n",
      "train accuracy on epoch 951: 0.944\n",
      "test loss on epoch 951: 0.775\n",
      "test accuracy on epoch 951: 0.615\n",
      "train loss on epoch 952 : 0.176\n",
      "train accuracy on epoch 952: 0.889\n",
      "test loss on epoch 952: 0.773\n",
      "test accuracy on epoch 952: 0.615\n",
      "train loss on epoch 953 : 0.096\n",
      "train accuracy on epoch 953: 0.889\n",
      "test loss on epoch 953: 0.768\n",
      "test accuracy on epoch 953: 0.615\n",
      "train loss on epoch 954 : 0.092\n",
      "train accuracy on epoch 954: 0.889\n",
      "test loss on epoch 954: 0.767\n",
      "test accuracy on epoch 954: 0.615\n",
      "train loss on epoch 955 : 0.058\n",
      "train accuracy on epoch 955: 1.000\n",
      "test loss on epoch 955: 0.766\n",
      "test accuracy on epoch 955: 0.615\n",
      "train loss on epoch 956 : 0.114\n",
      "train accuracy on epoch 956: 0.944\n",
      "test loss on epoch 956: 0.770\n",
      "test accuracy on epoch 956: 0.615\n",
      "train loss on epoch 957 : 0.157\n",
      "train accuracy on epoch 957: 0.889\n",
      "test loss on epoch 957: 0.786\n",
      "test accuracy on epoch 957: 0.615\n",
      "train loss on epoch 958 : 0.109\n",
      "train accuracy on epoch 958: 0.944\n",
      "test loss on epoch 958: 0.784\n",
      "test accuracy on epoch 958: 0.615\n",
      "train loss on epoch 959 : 0.117\n",
      "train accuracy on epoch 959: 0.944\n",
      "test loss on epoch 959: 0.774\n",
      "test accuracy on epoch 959: 0.615\n",
      "train loss on epoch 960 : 0.030\n",
      "train accuracy on epoch 960: 1.000\n",
      "test loss on epoch 960: 0.768\n",
      "test accuracy on epoch 960: 0.615\n",
      "train loss on epoch 961 : 0.099\n",
      "train accuracy on epoch 961: 0.944\n",
      "test loss on epoch 961: 0.773\n",
      "test accuracy on epoch 961: 0.615\n",
      "train loss on epoch 962 : 0.114\n",
      "train accuracy on epoch 962: 0.889\n",
      "test loss on epoch 962: 0.769\n",
      "test accuracy on epoch 962: 0.615\n",
      "train loss on epoch 963 : 0.142\n",
      "train accuracy on epoch 963: 0.889\n",
      "test loss on epoch 963: 0.761\n",
      "test accuracy on epoch 963: 0.692\n",
      "train loss on epoch 964 : 0.050\n",
      "train accuracy on epoch 964: 1.000\n",
      "test loss on epoch 964: 0.759\n",
      "test accuracy on epoch 964: 0.692\n",
      "train loss on epoch 965 : 0.185\n",
      "train accuracy on epoch 965: 0.889\n",
      "test loss on epoch 965: 0.755\n",
      "test accuracy on epoch 965: 0.692\n",
      "train loss on epoch 966 : 0.129\n",
      "train accuracy on epoch 966: 0.889\n",
      "test loss on epoch 966: 0.755\n",
      "test accuracy on epoch 966: 0.692\n",
      "train loss on epoch 967 : 0.104\n",
      "train accuracy on epoch 967: 0.944\n",
      "test loss on epoch 967: 0.755\n",
      "test accuracy on epoch 967: 0.692\n",
      "train loss on epoch 968 : 0.066\n",
      "train accuracy on epoch 968: 0.944\n",
      "test loss on epoch 968: 0.747\n",
      "test accuracy on epoch 968: 0.692\n",
      "train loss on epoch 969 : 0.073\n",
      "train accuracy on epoch 969: 0.944\n",
      "test loss on epoch 969: 0.747\n",
      "test accuracy on epoch 969: 0.692\n",
      "train loss on epoch 970 : 0.147\n",
      "train accuracy on epoch 970: 0.889\n",
      "test loss on epoch 970: 0.741\n",
      "test accuracy on epoch 970: 0.692\n",
      "train loss on epoch 971 : 0.096\n",
      "train accuracy on epoch 971: 0.944\n",
      "test loss on epoch 971: 0.736\n",
      "test accuracy on epoch 971: 0.692\n",
      "train loss on epoch 972 : 0.081\n",
      "train accuracy on epoch 972: 0.944\n",
      "test loss on epoch 972: 0.739\n",
      "test accuracy on epoch 972: 0.692\n",
      "train loss on epoch 973 : 0.089\n",
      "train accuracy on epoch 973: 0.944\n",
      "test loss on epoch 973: 0.738\n",
      "test accuracy on epoch 973: 0.692\n",
      "train loss on epoch 974 : 0.072\n",
      "train accuracy on epoch 974: 1.000\n",
      "test loss on epoch 974: 0.741\n",
      "test accuracy on epoch 974: 0.692\n",
      "train loss on epoch 975 : 0.139\n",
      "train accuracy on epoch 975: 0.889\n",
      "test loss on epoch 975: 0.744\n",
      "test accuracy on epoch 975: 0.692\n",
      "train loss on epoch 976 : 0.108\n",
      "train accuracy on epoch 976: 0.944\n",
      "test loss on epoch 976: 0.755\n",
      "test accuracy on epoch 976: 0.692\n",
      "train loss on epoch 977 : 0.126\n",
      "train accuracy on epoch 977: 0.944\n",
      "test loss on epoch 977: 0.777\n",
      "test accuracy on epoch 977: 0.615\n",
      "train loss on epoch 978 : 0.038\n",
      "train accuracy on epoch 978: 1.000\n",
      "test loss on epoch 978: 0.794\n",
      "test accuracy on epoch 978: 0.615\n",
      "train loss on epoch 979 : 0.070\n",
      "train accuracy on epoch 979: 1.000\n",
      "test loss on epoch 979: 0.799\n",
      "test accuracy on epoch 979: 0.615\n",
      "train loss on epoch 980 : 0.126\n",
      "train accuracy on epoch 980: 0.944\n",
      "test loss on epoch 980: 0.802\n",
      "test accuracy on epoch 980: 0.615\n",
      "train loss on epoch 981 : 0.096\n",
      "train accuracy on epoch 981: 0.944\n",
      "test loss on epoch 981: 0.804\n",
      "test accuracy on epoch 981: 0.615\n",
      "train loss on epoch 982 : 0.073\n",
      "train accuracy on epoch 982: 0.944\n",
      "test loss on epoch 982: 0.791\n",
      "test accuracy on epoch 982: 0.615\n",
      "train loss on epoch 983 : 0.061\n",
      "train accuracy on epoch 983: 0.944\n",
      "test loss on epoch 983: 0.782\n",
      "test accuracy on epoch 983: 0.615\n",
      "train loss on epoch 984 : 0.063\n",
      "train accuracy on epoch 984: 1.000\n",
      "test loss on epoch 984: 0.778\n",
      "test accuracy on epoch 984: 0.615\n",
      "train loss on epoch 985 : 0.071\n",
      "train accuracy on epoch 985: 0.944\n",
      "test loss on epoch 985: 0.773\n",
      "test accuracy on epoch 985: 0.615\n",
      "train loss on epoch 986 : 0.145\n",
      "train accuracy on epoch 986: 0.944\n",
      "test loss on epoch 986: 0.774\n",
      "test accuracy on epoch 986: 0.615\n",
      "train loss on epoch 987 : 0.079\n",
      "train accuracy on epoch 987: 1.000\n",
      "test loss on epoch 987: 0.788\n",
      "test accuracy on epoch 987: 0.615\n",
      "train loss on epoch 988 : 0.091\n",
      "train accuracy on epoch 988: 0.944\n",
      "test loss on epoch 988: 0.794\n",
      "test accuracy on epoch 988: 0.615\n",
      "train loss on epoch 989 : 0.088\n",
      "train accuracy on epoch 989: 0.944\n",
      "test loss on epoch 989: 0.802\n",
      "test accuracy on epoch 989: 0.615\n",
      "train loss on epoch 990 : 0.101\n",
      "train accuracy on epoch 990: 0.944\n",
      "test loss on epoch 990: 0.804\n",
      "test accuracy on epoch 990: 0.615\n",
      "train loss on epoch 991 : 0.217\n",
      "train accuracy on epoch 991: 0.889\n",
      "test loss on epoch 991: 0.801\n",
      "test accuracy on epoch 991: 0.615\n",
      "train loss on epoch 992 : 0.111\n",
      "train accuracy on epoch 992: 0.889\n",
      "test loss on epoch 992: 0.816\n",
      "test accuracy on epoch 992: 0.615\n",
      "train loss on epoch 993 : 0.090\n",
      "train accuracy on epoch 993: 0.944\n",
      "test loss on epoch 993: 0.820\n",
      "test accuracy on epoch 993: 0.615\n",
      "train loss on epoch 994 : 0.082\n",
      "train accuracy on epoch 994: 0.944\n",
      "test loss on epoch 994: 0.814\n",
      "test accuracy on epoch 994: 0.615\n",
      "train loss on epoch 995 : 0.146\n",
      "train accuracy on epoch 995: 0.889\n",
      "test loss on epoch 995: 0.811\n",
      "test accuracy on epoch 995: 0.615\n",
      "train loss on epoch 996 : 0.088\n",
      "train accuracy on epoch 996: 0.944\n",
      "test loss on epoch 996: 0.821\n",
      "test accuracy on epoch 996: 0.615\n",
      "train loss on epoch 997 : 0.145\n",
      "train accuracy on epoch 997: 0.944\n",
      "test loss on epoch 997: 0.822\n",
      "test accuracy on epoch 997: 0.615\n",
      "train loss on epoch 998 : 0.040\n",
      "train accuracy on epoch 998: 1.000\n",
      "test loss on epoch 998: 0.825\n",
      "test accuracy on epoch 998: 0.615\n",
      "train loss on epoch 999 : 0.079\n",
      "train accuracy on epoch 999: 0.944\n",
      "test loss on epoch 999: 0.819\n",
      "test accuracy on epoch 999: 0.615\n"
     ]
    }
   ],
   "source": [
    "\n",
    "distil_bert_embeddings = distil_bert.distilbert.embeddings.word_embeddings.weight\n",
    "\n",
    "net = MyModel(distil_bert_embeddings.size(0), distil_bert_embeddings.size(1), pad_idx, dropout=0.5)\n",
    "net.embedding.weight.requires_grad = False\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(net.parameters(), lr=0.01,weight_decay=0.000)\n",
    "epochs = 1000\n",
    "\n",
    "train_test(train_loader, test_loader, net, loss_function, optimizer, epochs) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "974783d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the bike drives on the road {'logits': tensor([[-2.7199,  2.7181]]), 'prediction': tensor([1.])} truth= 1\n",
      "a lion and a cat in a tree {'logits': tensor([[ 6.7135, -6.7122]]), 'prediction': tensor([0.])} truth= 0\n",
      "two cars crashed {'logits': tensor([[-2.5299,  2.5229]]), 'prediction': tensor([1.])} truth= 1\n",
      "i always go to work by bike {'logits': tensor([[-1.1376,  1.1252]]), 'prediction': tensor([1.])} truth= 1\n",
      "i have no animal at home {'logits': tensor([[-2.1499,  2.1602]]), 'prediction': tensor([1.])} truth= 0\n",
      "dogs like cheese {'logits': tensor([[-0.7416,  0.7390]]), 'prediction': tensor([1.])} truth= 0\n",
      "a pink flamingo {'logits': tensor([[-0.1438,  0.1405]]), 'prediction': tensor([1.])} truth= 0\n",
      "trucks {'logits': tensor([[-1.1913,  1.1865]]), 'prediction': tensor([1.])} truth= 1\n",
      "truckks {'logits': tensor([[-2.6506,  2.6527]]), 'prediction': tensor([1.])} truth= 1\n",
      "truckmegatruck {'logits': tensor([[-2.4114,  2.4116]]), 'prediction': tensor([1.])} truth= 1\n",
      "a text about trucks, not animals {'logits': tensor([[-0.8743,  0.8708]]), 'prediction': tensor([1.])} truth= 1\n",
      "a text about animals, not trucks {'logits': tensor([[-0.8743,  0.8708]]), 'prediction': tensor([1.])} truth= 0\n",
      "doggs {'logits': tensor([[-0.6398,  0.6322]]), 'prediction': tensor([1.])} truth= 0\n",
      "tensor([0.6154])\n"
     ]
    }
   ],
   "source": [
    "print(predict_from_pandas(vdf,net))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad2751e",
   "metadata": {},
   "source": [
    "Ce n'est pas vraiment mieux qu'avec les embeddings de mots (et même plutôt moins bien)\n",
    "\n",
    "# Adaptation d'un modèle pre-entraîné : DistillBert\n",
    "\n",
    "On va plutôt essayer de re-utiliser le modèle DistillBert complet, en ne fine-tunant que la dernière couche linéaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474f4cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load pre-trained DistilBERT model and tokenizer\n",
    "distil_bert_model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Assuming you have already loaded your datasets and defined DataLoader as train_loader and test_loader\n",
    "\n",
    "# Modify the last layer to match the number of classes in your task\n",
    "num_classes = 2  # Change this based on your classification task\n",
    "distil_bert_model.classifier = nn.Linear(distil_bert_model.config.hidden_size, num_classes)\n",
    "\n",
    "# Freeze all layers except the last one\n",
    "for param in distil_bert_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the last layer\n",
    "for param in distil_bert_model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(distil_bert_model.parameters(), lr=0.01, weight_decay=0.000)\n",
    "\n",
    "# Number of training epochs\n",
    "epochs = 5  # Adjust as needed\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    distil_bert_model.train()\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        inputs = {\"input_ids\": batch[\"input_ids\"], \"labels\": batch[\"labels\"]}\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = distil_bert_model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss = loss_function(logits, inputs[\"labels\"])\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        accuracy = (predictions == inputs[\"labels\"]).sum().item()\n",
    "        total_accuracy += accuracy\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    avg_accuracy = total_accuracy / len(train_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}: Train Loss - {avg_loss:.4f}, Train Accuracy - {avg_accuracy:.4f}\")\n",
    "\n",
    "# Evaluation loop\n",
    "distil_bert_model.eval()\n",
    "total_accuracy = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs = {\"input_ids\": batch[\"input_ids\"], \"labels\": batch[\"labels\"]}\n",
    "\n",
    "        outputs = distil_bert_model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        accuracy = (predictions == inputs[\"labels\"]).sum().item()\n",
    "        total_accuracy += accuracy\n",
    "\n",
    "avg_accuracy = total_accuracy / len(test_loader.dataset)\n",
    "print(f\"Test Accuracy: {avg_accuracy:.4f}\")\n",
    "\n",
    "avg_accuracy = total_accuracy / len(test_loader.dataset)\n",
    "print(f\"Test Accuracy: {avg_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "28d45edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the bike drives on the road {'logits': tensor([[-2.7199,  2.7181]]), 'prediction': tensor([1.])} truth= 1\n",
      "a lion and a cat in a tree {'logits': tensor([[ 6.7135, -6.7122]]), 'prediction': tensor([0.])} truth= 0\n",
      "two cars crashed {'logits': tensor([[-2.5299,  2.5229]]), 'prediction': tensor([1.])} truth= 1\n",
      "i always go to work by bike {'logits': tensor([[-1.1376,  1.1252]]), 'prediction': tensor([1.])} truth= 1\n",
      "i have no animal at home {'logits': tensor([[-2.1499,  2.1602]]), 'prediction': tensor([1.])} truth= 0\n",
      "dogs like cheese {'logits': tensor([[-0.7416,  0.7390]]), 'prediction': tensor([1.])} truth= 0\n",
      "a pink flamingo {'logits': tensor([[-0.1438,  0.1405]]), 'prediction': tensor([1.])} truth= 0\n",
      "trucks {'logits': tensor([[-1.1913,  1.1865]]), 'prediction': tensor([1.])} truth= 1\n",
      "truckks {'logits': tensor([[-2.6506,  2.6527]]), 'prediction': tensor([1.])} truth= 1\n",
      "truckmegatruck {'logits': tensor([[-2.4114,  2.4116]]), 'prediction': tensor([1.])} truth= 1\n",
      "a text about trucks, not animals {'logits': tensor([[-0.8743,  0.8708]]), 'prediction': tensor([1.])} truth= 1\n",
      "a text about animals, not trucks {'logits': tensor([[-0.8743,  0.8708]]), 'prediction': tensor([1.])} truth= 0\n",
      "doggs {'logits': tensor([[-0.6398,  0.6322]]), 'prediction': tensor([1.])} truth= 0\n",
      "tensor([0.6154])\n"
     ]
    }
   ],
   "source": [
    "print(predict_from_pandas(vdf,net))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d84ced0",
   "metadata": {},
   "source": [
    "Ok... on sent qu'avec beaucoup plus de données on pourrait arriver à une très bonne qualité mais le dataset d'apprentissage est vraiment trop petit pour un modèle aussi gros.\n",
    "\n",
    "# Module Transformer\n",
    "\n",
    "Essayons maintenant de créer un petit réseau Transformer d'un seul layer d'encoder, qui reutilise les embeddings de DistilBert (seulement ceux des mots dans un premier temps), avec 8 têtes de self-attention. Attention, par défaut la taille des séquences est la première dimension des entrées attendues par le module TransformerEncoderLayer. Pour donner plutôt sous une forme (taille du batch, longueur des sequences, taille des embeddings), utiliser l'option batch_first=True. Le token CLS à utiliser pour faire la classification est le premier de chaque sequence (c'est sur lui qu'on branche la tête de classification, qui correspond à une simple couche linéaire, précédée d'une activation tanh) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fddfbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "        \n",
    "class TransfoModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, pad_idx, dropout, num_heads):\n",
    "        super(TransfoModel, self).__init__()\n",
    "        \n",
    "        self.transformer_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim,\n",
    "            nhead=num_heads,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer=self.transformer_layer,\n",
    "            num_layers=1  # Using a single layer\n",
    "        )\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(embeds.weight)\n",
    "        \n",
    "        self.classification_head = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 2),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = embedded.permute(1, 0, 2)\n",
    "        transformer_output = self.transformer_encoder(embedded)\n",
    "        cls_token = transformer_output[0, :, :]\n",
    "        output = self.classification_head(cls_token)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "embeds=distil_bert.distilbert.embeddings.word_embeddings\n",
    "\n",
    "net = TransfoModel(vocab_size,embeds.weight.shape[-1],pad_idx,0.5,1)\n",
    "net.embedding=nn.Embedding.from_pretrained(embeds.weight)   # on charge les pre-train (en supposant que le module Embedding du modèle soit dans une variable embeddings)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(net.parameters(), lr=0.001,weight_decay=0.0)\n",
    "epochs = 50000\n",
    "\n",
    "train_test(train_loader, test_loader, net, loss_function, optimizer, epochs, clip=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f9dd63",
   "metadata": {},
   "source": [
    "Ca a l'air mieux ! Voyons les predictions sur nos exemples précédents : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901bc88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_from_pandas(vdf,net))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fab311",
   "metadata": {},
   "source": [
    "Ok, les fautes de frappes ont l'air mieux gérées. Le mot complexe également.. \n",
    "Par contre on a toujours la limite de l'ordre, avec les deux phrases \"a text about trucks, not animals\" et \"a text about animals, not trucks\" retournant exactemenent les mêmes prédictions. Normal, un transformer est de base invariant à l'ordre ! \n",
    "\n",
    "# Positionnal Embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0040ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "#from sentence_transformers import SentenceTransformer\n",
    "        \n",
    "class TransfoModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, pad_idx, dropout, nhead):\n",
    "        super(TransfoModel, self).__init__()\n",
    "\n",
    "        # Embedding layer (initialement sans positional embeddings)\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Positional embeddings\n",
    "        self.pos_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Transformer Encoder Layer\n",
    "        self.transformer_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim,\n",
    "            nhead=nhead,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        # Transformer Encoder\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            self.transformer_layer,\n",
    "            num_layers=1,  \n",
    "        )\n",
    "\n",
    "        # Classification head (couche linéaire suivie d'une activation tanh)\n",
    "        self.cls_head = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 2), \n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embedding\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        seq_len = embedded.size(1)\n",
    "        positions = torch.arange(seq_len, device=x.device).expand(x.size(0), seq_len)\n",
    "        pos_embedded = self.pos_embedding(positions)\n",
    "        embedded = embedded + pos_embedded\n",
    "\n",
    "        transformer_output = self.transformer_encoder(embedded)\n",
    "        cls_token = transformer_output[:, 0, :]\n",
    "        output = self.cls_head(cls_token)\n",
    "\n",
    "        return output\n",
    "    \n",
    "embeds=distil_bert_model.distilbert.embeddings.word_embeddings\n",
    "\n",
    "net = TransfoModel(vocab_size,embeds.weight.shape[-1],pad_idx,0.5,8)\n",
    "net.embedding=nn.Embedding.from_pretrained(embeds.weight)   # on charge les pre-train (en supposant que le module Embedding du modèle soit dans une variable embeddings)\n",
    "net.pos_embedding=nn.Embedding.from_pretrained(distil_bert_model.distilbert.embeddings.position_embeddings.weight)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(net.parameters(), lr=0.001,weight_decay=0.0)\n",
    "epochs = 51\n",
    "\n",
    "train_test(train_loader, test_loader, net, loss_function, optimizer, epochs,clip=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb642eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_from_pandas(vdf,net))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df43c9ee",
   "metadata": {},
   "source": [
    "Super ! Bon çà reste des resultats fragiles vue la taille du jeu de données, mais on semble quand même obtenir une capacité de traitement bien supérieure aux modèles précédents. On note en particulier la capacité à distinguer les phrases possédant des mots identiques mais dans un ordre différent. \n",
    "\n",
    "# Experimentations sur un Jeu de Données Réel : IMDB\n",
    "\n",
    "Pour terminer ce TP, on s'intéresse à l'adaptation du modèle DistillBert utilisé ci-dessus, sur un jeu de données  beaucoup plus conséquent: le corpus IMDB (base de données de commentaires sur des films). Pour cette partie, il est largement conseillé de travailler sur GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a0ddfaec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 4.31k/4.31k [00:00<00:00, 8.61MB/s]\n",
      "Downloading metadata: 100%|██████████| 2.17k/2.17k [00:00<00:00, 6.25MB/s]\n",
      "Downloading readme: 100%|██████████| 7.59k/7.59k [00:00<00:00, 14.5MB/s]\n",
      "Downloading data: 100%|██████████| 84.1M/84.1M [01:18<00:00, 1.07MB/s]  \n",
      "Generating train split: 100%|██████████| 25000/25000 [00:11<00:00, 2258.23 examples/s]\n",
      "Generating test split: 100%|██████████| 25000/25000 [00:10<00:00, 2287.15 examples/s]\n",
      "Generating unsupervised split: 100%|██████████| 50000/50000 [00:12<00:00, 3890.14 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "dataset = datasets.load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cb3ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d928beb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fraction=0.2            \n",
    "train_dataset=dataset[\"train\"]\n",
    "train_dataset = train_dataset.train_test_split(test_size=1-train_fraction)[\"train\"]\n",
    "test_fraction=0.002\n",
    "validation_dataset=dataset[\"test\"]\n",
    "validation_dataset = validation_dataset.train_test_split(test_size=1-test_fraction)[\"train\"]\n",
    "\n",
    "from transformers import AutoTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = tokenizer(examples[\"text\"], truncation=True, padding='max_length', max_length=256, return_tensors='pt')\n",
    "    inputs[\"labels\"] = torch.tensor([1 if label == 'Y' else 0 for label in examples[\"label\"]], dtype=torch.long)\n",
    "    return inputs\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True, remove_columns=train_dataset.features.keys(), load_from_cache_file=True)\n",
    "print(train_dataset)\n",
    "\n",
    "validation_dataset = validation_dataset.map(preprocess_function, batched=True, remove_columns=validation_dataset.features.keys(), load_from_cache_file=True)\n",
    "print(validation_dataset)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bc4e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_idx=tokenizer.pad_token_id  # On met à jour l'index de padding et on recrée les dataloaders\n",
    "\n",
    "batchsize=32\n",
    "megamul=4\n",
    "\n",
    "train_sampler = LengthGroupedSampler(batchsize, train_dataset, megabatch_mul)\n",
    "\n",
    "train_loader=DataLoader(train_dataset,batchsize,sampler=train_sampler,collate_fn=data_collator,pin_memory=True, shuffle=False, num_workers=0)\n",
    "\n",
    "for data in train_loader:\n",
    "    print(data)\n",
    "    print(tokenizer.batch_decode(data[\"input_ids\"]))\n",
    "    break\n",
    "\n",
    "test_sampler = LengthGroupedSampler(batchsize, validation_dataset, megabatch_mul)    \n",
    "    \n",
    "test_loader=DataLoader(validation_dataset,batchsize,sampler=test_sampler,collate_fn=data_collator,pin_memory=True, shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "for data in test_loader:\n",
    "    print(data)\n",
    "    print(tokenizer.batch_decode(data[\"input_ids\"]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5052182b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(trainloader, testloader, model, loss_function, optimizer, epochs, clip=-1, test_rate=1):\n",
    "  it=0\n",
    "\n",
    "  \n",
    "\n",
    "  for epoch in range(epochs):\n",
    "      \n",
    "      \n",
    "      epoch_loss = 0\n",
    "      epoch_accuracy = 0\n",
    "      \n",
    "     \n",
    "    \n",
    "      nb_samples=0\n",
    "      t = tqdm(iter(trainloader), total=len(trainloader), dynamic_ncols=True, position=0)\n",
    "      train_loss_log = tqdm(total=0, position=4, bar_format='{desc}')\n",
    "      test_log = tqdm(total=0, position=2, bar_format='{desc}')\n",
    "      accuracy_log = tqdm(total=0, position=3, bar_format='{desc}')\n",
    "      for batch in t:\n",
    "          it+=1\n",
    "          if it%test_rate==0:\n",
    "            test(testloader, model, loss_function, epoch,(test_log,accuracy_log))\n",
    "          model.train()\n",
    "          optimizer.zero_grad()\n",
    "          #print(\"text shape \",batch.text.T.shape)\n",
    "          prediction = model(batch[\"input_ids\"].to(model.device))\n",
    "          if not isinstance(prediction,torch.Tensor):  \n",
    "                prediction = prediction[\"logits\"]\n",
    "          #print(prediction)\n",
    "          loss = loss_function(prediction, batch[\"labels\"].to(model.device))\n",
    "          train_loss_log.set_description_str(\"loss train {:.3f} \".format(loss.item()))\n",
    "          loss.backward()\n",
    "          if clip>0:\n",
    "              torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "          optimizer.step()\n",
    "          nb_samples+=prediction.shape[0]\n",
    "          epoch_loss+=loss.item()*prediction.shape[0]\n",
    "          preds=(prediction[:,1]>prediction[:,0])*1.0\n",
    "          accuracy=(preds==batch[\"labels\"].to(model.device)).sum()\n",
    "          epoch_accuracy+=accuracy.item()\n",
    "      print('train loss on epoch {} : {:.3f}'.format(epoch, epoch_loss/nb_samples))\n",
    "      print('train accuracy on epoch {}: {:.3f}'.format(epoch, epoch_accuracy/nb_samples))\n",
    "      \n",
    "def test(testloader, model, loss_function, epoch, descs):\n",
    "      model.eval()\n",
    "      test_loss = 0\n",
    "      test_accuracy = 0\n",
    "      nb_samples=0\n",
    "      accuracy=0\n",
    "      t = tqdm(iter(testloader), total=len(testloader), position=1, leave=False)\n",
    "      tl,al=descs\n",
    "      #test_log = tqdm(total=0, position=2, bar_format='{desc}')\n",
    "      #accuracy_log = tqdm(total=0, position=3, bar_format='{desc}')\n",
    "      for batch in t:\n",
    "          #print(\"test \",batch)\n",
    "          with torch.no_grad():\n",
    "              optimizer.zero_grad()\n",
    "              inputs=batch[\"input_ids\"].to(model.device)\n",
    "              prediction = model(inputs)\n",
    "              if not isinstance(prediction,torch.Tensor):\n",
    "                    prediction = prediction[\"logits\"]\n",
    "              loss = loss_function(prediction, batch[\"labels\"].to(model.device))\n",
    "              nb_samples+=prediction.shape[0]\n",
    "              test_loss+=loss.item()*prediction.shape[0]\n",
    "              #print(batch_decode(batch[\"input_ids\"]))\n",
    "              preds=(prediction[:,1]>prediction[:,0])*1.0\n",
    "              accuracy=(preds==batch[\"labels\"].to(model.device)).sum()\n",
    "              test_accuracy+=accuracy.item()  \n",
    "              #print(preds,accuracy)\n",
    "      tl.set_description_str('test loss on epoch {}: {:.3f}'.format(epoch, test_loss/nb_samples))\n",
    "      al.set_description_str('test accuracy on epoch {}: {:.3f}'.format(epoch, test_accuracy/nb_samples))\n",
    "      #print('test loss on epoch {}: {:.3f}'.format(epoch, test_loss/nb_samples))\n",
    "      #print('test accuracy on epoch {}: {:.3f}'.format(epoch, test_accuracy/nb_samples))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7834920",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "distil_bert = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "net = distil_bert\n",
    "def set_parameter_requires_grad(module, requires_grad=True):\n",
    "    for param in module.parameters():\n",
    "        param.requires_grad = requires_grad\n",
    "\n",
    "\n",
    "net=net.to(device)\n",
    "\n",
    "# Fixons d'abord les poids du réseau :\n",
    "set_parameter_requires_grad(net,False)\n",
    "#set_parameter_requires_grad(net.distilbert.embeddings,False)\n",
    "set_parameter_requires_grad(net.classifier,True)\n",
    "\n",
    "params_to_update = []\n",
    "for name,param in net.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        params_to_update.append(param)\n",
    "        \n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(params_to_update, lr=0.0001,weight_decay=0.000)\n",
    "epochs = 50000\n",
    "\n",
    "\n",
    "train_test(train_loader, test_loader, net, loss_function, optimizer, epochs, test_rate=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
