{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TE2ItlsI956"
   },
   "source": [
    "# Deep Learning avec PyTorch\n",
    "\n",
    "## Fonctions Dérivables\n",
    "\n",
    "Supports adaptés de Sylvain Lamprier (sylvain.lamprier@univ-angers.fr), Nicolas Baskiotis (nicolas.baskiotis@sorbonne-univeriste.fr) et Benjamin Piwowarski (benjamin.piwowarski@sorbonne-universite.fr) -- MLIA/ISIR, Sorbonne Université"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3Y9YOOHHhJKY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La version de torch est :  2.1.0\n",
      "Le calcul GPU est disponible ?  False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"La version de torch est : \",torch.__version__)\n",
    "print(\"Le calcul GPU est disponible ? \", torch.cuda.is_available())\n",
    "\n",
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans la partie précédente, nous avons vu comment implémenter une regression linéaire en utilisant les structures Tensor de PyTorch. Cependant, nous exploitions pas du tout la puissance de PyTorch qui permet de faciliter le calcul des gradients via de l'auto-dérivation. Dans la partie précédente précédent nous avions défini un algorithme spécifique à de la regression pour un modèle (linéaire) et un coût (moindres carrés) figés, en définissant à la main le gradient du coût global pour l'ensemble des paramètres. Ce mode de programmation est très peu modulaire et est très difficilement étendable à des architectures plus complexes. Sachant que l'objectif est de développer des architectures neuronales avec des nombreux modules neuronaux enchaînés, il n'est pas possible de travailler de cette façon. \n",
    "\n",
    "Nous allons voir comment décomposer les choses pour rendre le code plus facilement généralisable. L'objectif est de comprendre le fonctionnement interne de PyTorch (sans en utiliser encore les facilités offertes par l'utilisation d'un graphe de calcul), basé sur l'implémentation d'objets Function.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctions\n",
    "\n",
    "PyTorch utilise une classe abstraite Function dont sont héritées toutes les fonctions et qui nécessite l'implémentation de ces deux méthodes :\n",
    "\n",
    "- méthode $\\texttt{forward(ctx, *inputs)}$ : calcule le résultat de l'application de la fonction\n",
    "- méthode $\\texttt{backward(ctx, *grad-outputs)}$ : calcule le gradient partiel par rapport à chaque entrée de la méthode $\\texttt{forward}$; le nombre de $\\texttt{grad-outputs}$ doit être égale aux nombre de sorties de $\\texttt{forward}$ et le nombre de sorties doit être égale aux nombres de $\\texttt{inputs}$ de $\\texttt{forward}$.\n",
    "\n",
    "Pour des raisons d'implémentation, les deux méthodes doivent être statiques. Le premier paramètre $\\texttt{ctx}$ permet de sauvegarder un contexte lors de la passe $\\texttt{forward}$ (par exemple les tenseurs d'entrées) et il est passé lors de la passe $\\texttt{backward}$ en paramètre afin de récupérer les valeurs. $\\textbf{Attention : }$ le contexte doit être unique pour chaque appel de $\\texttt{forward}$.\n",
    "\n",
    "Création des modules MSE (coût moindres carrés) et Linéaire. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Function\n",
    "from torch.autograd import gradcheck\n",
    "\n",
    "class Context:\n",
    "    \"\"\"Un objet contexte très simplifié pour simuler PyTorch\n",
    "    \n",
    "    Un contexte différent doit être utilisé à chaque forward\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self._saved_tensors = ()\n",
    "\n",
    "    def save_for_backward(self, *args):\n",
    "        self._saved_tensors = args\n",
    "\n",
    "    @property\n",
    "    def saved_tensors(self):\n",
    "        return self._saved_tensors\n",
    "\n",
    "\n",
    "class MSE(Function):\n",
    "    \"\"\"Début d'implémentation de la fonction MSE\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, yhat, y):\n",
    "        ## Garde les valeurs nécessaires pour le backward\n",
    "        ctx.save_for_backward(yhat, y)\n",
    "        return torch.pow(yhat - y, 2).mean()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        ## Calcul du gradient du module par rapport à chaque groupe d'entrées\n",
    "        yhat, y = ctx.saved_tensors\n",
    "        grad_yhat = 2 * (yhat - y) / yhat.numel()\n",
    "        grad_y = -2 * (yhat - y) / yhat.numel()\n",
    "        return grad_yhat * grad_output, grad_y * grad_output\n",
    "\n",
    "\n",
    "class Linear(Function):\n",
    "    \"\"\"Début d'implémentation de la fonction Linear\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, X, W, b):\n",
    "        ctx.save_for_backward(X, W, b)\n",
    "        return (X @ W) + b\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        X, W, b = ctx.saved_tensors\n",
    "        grad_X = grad_output @ W.T\n",
    "        grad_W = (grad_output.t() @ X).t()\n",
    "        grad_b = grad_output.sum(0)\n",
    "        return grad_X, grad_W, grad_b\n",
    "\n",
    "\n",
    "# Utiliser gradcheck\n",
    "\n",
    "mse = MSE.apply\n",
    "linear = Linear.apply\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test du gradient de MSE \n",
    "yhat = torch.randn(10,5, requires_grad=True, dtype=torch.float64)\n",
    "y = torch.randn(10,5, requires_grad=True, dtype=torch.float64)\n",
    "torch.autograd.gradcheck(mse, (yhat, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test du gradient de Linear (sur le même modèle que MSE)\n",
    "\n",
    "x = torch.randn(13, 5,requires_grad=True,dtype=torch.float64)\n",
    "w = torch.randn(5, 7,requires_grad=True,dtype=torch.float64)\n",
    "b = torch.randn(7,requires_grad=True,dtype=torch.float64)\n",
    "torch.autograd.gradcheck(linear,(x,w,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descente de Gradient\n",
    "\n",
    "Régression linéaire en utilisant les objets Function déclarés ci-dessus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'exemples :  20640 Dimension :  8\n",
      "Nom des attributs :  MedInc, HouseAge, AveRooms, AveBedrms, Population, AveOccup, Latitude, Longitude\n",
      "Itérations 0: loss 2788145.5\n",
      "Itérations 100: loss 4256.62841796875\n",
      "Itérations 200: loss 3293.343505859375\n",
      "Itérations 300: loss 2550.746337890625\n",
      "Itérations 400: loss 1978.269775390625\n",
      "Itérations 500: loss 1536.9329833984375\n",
      "Itérations 600: loss 1196.686279296875\n",
      "Itérations 700: loss 934.3665771484375\n",
      "Itérations 800: loss 732.1173706054688\n",
      "Itérations 900: loss 576.1751708984375\n",
      "Itérations 1000: loss 455.9288635253906\n",
      "Itérations 1100: loss 363.199462890625\n",
      "Itérations 1200: loss 291.68206787109375\n",
      "Itérations 1300: loss 236.5161590576172\n",
      "Itérations 1400: loss 193.9551239013672\n",
      "Itérations 1500: loss 161.1110382080078\n",
      "Itérations 1600: loss 135.75746154785156\n",
      "Itérations 1700: loss 116.1781997680664\n",
      "Itérations 1800: loss 101.0504379272461\n",
      "Itérations 1900: loss 89.35417938232422\n",
      "Itérations 2000: loss 80.30341339111328\n",
      "Itérations 2100: loss 73.29206085205078\n",
      "Itérations 2200: loss 67.85305786132812\n",
      "Itérations 2300: loss 63.62614440917969\n",
      "Itérations 2400: loss 60.333740234375\n",
      "Itérations 2500: loss 57.762149810791016\n",
      "Itérations 2600: loss 55.746131896972656\n",
      "Itérations 2700: loss 54.15855026245117\n",
      "Itérations 2800: loss 52.901466369628906\n",
      "Itérations 2900: loss 51.89924240112305\n",
      "Itérations 3000: loss 51.09370803833008\n",
      "Itérations 3100: loss 50.44007873535156\n",
      "Itérations 3200: loss 49.90358352661133\n",
      "Itérations 3300: loss 49.45755386352539\n",
      "Itérations 3400: loss 49.081424713134766\n",
      "Itérations 3500: loss 48.75931930541992\n",
      "Itérations 3600: loss 48.479034423828125\n",
      "Itérations 3700: loss 48.23107147216797\n",
      "Itérations 3800: loss 48.00823211669922\n",
      "Itérations 3900: loss 47.8049430847168\n",
      "Itérations 4000: loss 47.61692810058594\n",
      "Itérations 4100: loss 47.440677642822266\n",
      "Itérations 4200: loss 47.273651123046875\n",
      "Itérations 4300: loss 47.11418533325195\n",
      "Itérations 4400: loss 46.960540771484375\n",
      "Itérations 4500: loss 46.811405181884766\n",
      "Itérations 4600: loss 46.66587829589844\n",
      "Itérations 4700: loss 46.523433685302734\n",
      "Itérations 4800: loss 46.383602142333984\n",
      "Itérations 4900: loss 46.24576187133789\n",
      "Itérations 5000: loss 46.10974884033203\n",
      "Itérations 5100: loss 45.97513198852539\n",
      "Itérations 5200: loss 45.84178924560547\n",
      "Itérations 5300: loss 45.70956802368164\n",
      "Itérations 5400: loss 45.57835006713867\n",
      "Itérations 5500: loss 45.448020935058594\n",
      "Itérations 5600: loss 45.31855773925781\n",
      "Itérations 5700: loss 45.19001770019531\n",
      "Itérations 5800: loss 45.06212615966797\n",
      "Itérations 5900: loss 44.93497085571289\n",
      "Itérations 6000: loss 44.80841064453125\n",
      "Itérations 6100: loss 44.68269729614258\n",
      "Itérations 6200: loss 44.55756378173828\n",
      "Itérations 6300: loss 44.433101654052734\n",
      "Itérations 6400: loss 44.30937576293945\n",
      "Itérations 6500: loss 44.18638610839844\n",
      "Itérations 6600: loss 44.063926696777344\n",
      "Itérations 6700: loss 43.942115783691406\n",
      "Itérations 6800: loss 43.82083511352539\n",
      "Itérations 6900: loss 43.70018768310547\n",
      "Itérations 7000: loss 43.58014678955078\n",
      "Itérations 7100: loss 43.460872650146484\n",
      "Itérations 7200: loss 43.34211730957031\n",
      "Itérations 7300: loss 43.22396469116211\n",
      "Itérations 7400: loss 43.10634994506836\n",
      "Itérations 7500: loss 42.98934555053711\n",
      "Itérations 7600: loss 42.87287521362305\n",
      "Itérations 7700: loss 42.75697708129883\n",
      "Itérations 7800: loss 42.64163589477539\n",
      "Itérations 7900: loss 42.526859283447266\n",
      "Itérations 8000: loss 42.41263198852539\n",
      "Itérations 8100: loss 42.298946380615234\n",
      "Itérations 8200: loss 42.18584060668945\n",
      "Itérations 8300: loss 42.07325744628906\n",
      "Itérations 8400: loss 41.96122741699219\n",
      "Itérations 8500: loss 41.84972381591797\n",
      "Itérations 8600: loss 41.73878860473633\n",
      "Itérations 8700: loss 41.62836456298828\n",
      "Itérations 8800: loss 41.51848220825195\n",
      "Itérations 8900: loss 41.40911865234375\n",
      "Itérations 9000: loss 41.30032730102539\n",
      "Itérations 9100: loss 41.1920166015625\n",
      "Itérations 9200: loss 41.084232330322266\n",
      "Itérations 9300: loss 40.97698211669922\n",
      "Itérations 9400: loss 40.87027359008789\n",
      "Itérations 9500: loss 40.76412582397461\n",
      "Itérations 9600: loss 40.65851593017578\n",
      "Itérations 9700: loss 40.55337905883789\n",
      "Itérations 9800: loss 40.44878387451172\n",
      "Itérations 9900: loss 40.344703674316406\n",
      "Itérations 10000: loss 40.24110794067383\n",
      "Itérations 10100: loss 40.138004302978516\n",
      "Itérations 10200: loss 40.03539276123047\n",
      "Itérations 10300: loss 39.93331527709961\n",
      "Itérations 10400: loss 39.83169174194336\n",
      "Itérations 10500: loss 39.730594635009766\n",
      "Itérations 10600: loss 39.62993621826172\n",
      "Itérations 10700: loss 39.52979278564453\n",
      "Itérations 10800: loss 39.430118560791016\n",
      "Itérations 10900: loss 39.33097457885742\n",
      "Itérations 11000: loss 39.2323112487793\n",
      "Itérations 11100: loss 39.134300231933594\n",
      "Itérations 11200: loss 39.03667449951172\n",
      "Itérations 11300: loss 38.93959426879883\n",
      "Itérations 11400: loss 38.842891693115234\n",
      "Itérations 11500: loss 38.74677658081055\n",
      "Itérations 11600: loss 38.65104675292969\n",
      "Itérations 11700: loss 38.555843353271484\n",
      "Itérations 11800: loss 38.46104431152344\n",
      "Itérations 11900: loss 38.366737365722656\n",
      "Itérations 12000: loss 38.2728385925293\n",
      "Itérations 12100: loss 38.17941665649414\n",
      "Itérations 12200: loss 38.08642578125\n",
      "Itérations 12300: loss 37.993900299072266\n",
      "Itérations 12400: loss 37.90184020996094\n",
      "Itérations 12500: loss 37.81018829345703\n",
      "Itérations 12600: loss 37.71900939941406\n",
      "Itérations 12700: loss 37.62818908691406\n",
      "Itérations 12800: loss 37.53789520263672\n",
      "Itérations 12900: loss 37.44797897338867\n",
      "Itérations 13000: loss 37.35856628417969\n",
      "Itérations 13100: loss 37.26950454711914\n",
      "Itérations 13200: loss 37.180912017822266\n",
      "Itérations 13300: loss 37.092716217041016\n",
      "Itérations 13400: loss 37.004940032958984\n",
      "Itérations 13500: loss 36.917625427246094\n",
      "Itérations 13600: loss 36.8306884765625\n",
      "Itérations 13700: loss 36.74420928955078\n",
      "Itérations 13800: loss 36.658077239990234\n",
      "Itérations 13900: loss 36.57242202758789\n",
      "Itérations 14000: loss 36.48711395263672\n",
      "Itérations 14100: loss 36.40229034423828\n",
      "Itérations 14200: loss 36.31782150268555\n",
      "Itérations 14300: loss 36.23377990722656\n",
      "Itérations 14400: loss 36.15011978149414\n",
      "Itérations 14500: loss 36.066932678222656\n",
      "Itérations 14600: loss 35.984336853027344\n",
      "Itérations 14700: loss 35.902076721191406\n",
      "Itérations 14800: loss 35.82029724121094\n",
      "Itérations 14900: loss 35.73882293701172\n",
      "Itérations 15000: loss 35.65779113769531\n",
      "Itérations 15100: loss 35.57709884643555\n",
      "Itérations 15200: loss 35.49680709838867\n",
      "Itérations 15300: loss 35.41693115234375\n",
      "Itérations 15400: loss 35.33739471435547\n",
      "Itérations 15500: loss 35.258296966552734\n",
      "Itérations 15600: loss 35.17948532104492\n",
      "Itérations 15700: loss 35.10112762451172\n",
      "Itérations 15800: loss 35.02308654785156\n",
      "Itérations 15900: loss 34.94546127319336\n",
      "Itérations 16000: loss 34.86820602416992\n",
      "Itérations 16100: loss 34.79127502441406\n",
      "Itérations 16200: loss 34.71476745605469\n",
      "Itérations 16300: loss 34.638545989990234\n",
      "Itérations 16400: loss 34.5627555847168\n",
      "Itérations 16500: loss 34.4873046875\n",
      "Itérations 16600: loss 34.4122200012207\n",
      "Itérations 16700: loss 34.33749008178711\n",
      "Itérations 16800: loss 34.263065338134766\n",
      "Itérations 16900: loss 34.189064025878906\n",
      "Itérations 17000: loss 34.1153564453125\n",
      "Itérations 17100: loss 34.04206466674805\n",
      "Itérations 17200: loss 33.969093322753906\n",
      "Itérations 17300: loss 33.89644241333008\n",
      "Itérations 17400: loss 33.82417297363281\n",
      "Itérations 17500: loss 33.75218200683594\n",
      "Itérations 17600: loss 33.68058776855469\n",
      "Itérations 17700: loss 33.60932540893555\n",
      "Itérations 17800: loss 33.538394927978516\n",
      "Itérations 17900: loss 33.46787643432617\n",
      "Itérations 18000: loss 33.397727966308594\n",
      "Itérations 18100: loss 33.3280143737793\n",
      "Itérations 18200: loss 33.25856018066406\n",
      "Itérations 18300: loss 33.18946075439453\n",
      "Itérations 18400: loss 33.120689392089844\n",
      "Itérations 18500: loss 33.05218505859375\n",
      "Itérations 18600: loss 32.98408889770508\n",
      "Itérations 18700: loss 32.9162712097168\n",
      "Itérations 18800: loss 32.848812103271484\n",
      "Itérations 18900: loss 32.78166961669922\n",
      "Itérations 19000: loss 32.71479415893555\n",
      "Itérations 19100: loss 32.64828109741211\n",
      "Itérations 19200: loss 32.582054138183594\n",
      "Itérations 19300: loss 32.51613998413086\n",
      "Itérations 19400: loss 32.45054244995117\n",
      "Itérations 19500: loss 32.385250091552734\n",
      "Itérations 19600: loss 32.320289611816406\n",
      "Itérations 19700: loss 32.25562286376953\n",
      "Itérations 19800: loss 32.191261291503906\n",
      "Itérations 19900: loss 32.1272087097168\n",
      "Itérations 20000: loss 32.06344223022461\n",
      "Itérations 20100: loss 31.99997329711914\n",
      "Itérations 20200: loss 31.936782836914062\n",
      "Itérations 20300: loss 31.873933792114258\n",
      "Itérations 20400: loss 31.81138038635254\n",
      "Itérations 20500: loss 31.749107360839844\n",
      "Itérations 20600: loss 31.687114715576172\n",
      "Itérations 20700: loss 31.625415802001953\n",
      "Itérations 20800: loss 31.56400489807129\n",
      "Itérations 20900: loss 31.50287437438965\n",
      "Itérations 21000: loss 31.44205093383789\n",
      "Itérations 21100: loss 31.381502151489258\n",
      "Itérations 21200: loss 31.32125473022461\n",
      "Itérations 21300: loss 31.261276245117188\n",
      "Itérations 21400: loss 31.201568603515625\n",
      "Itérations 21500: loss 31.142126083374023\n",
      "Itérations 21600: loss 31.082975387573242\n",
      "Itérations 21700: loss 31.024124145507812\n",
      "Itérations 21800: loss 30.965538024902344\n",
      "Itérations 21900: loss 30.907209396362305\n",
      "Itérations 22000: loss 30.849172592163086\n",
      "Itérations 22100: loss 30.791397094726562\n",
      "Itérations 22200: loss 30.73387908935547\n",
      "Itérations 22300: loss 30.67661476135254\n",
      "Itérations 22400: loss 30.619657516479492\n",
      "Itérations 22500: loss 30.562963485717773\n",
      "Itérations 22600: loss 30.50651741027832\n",
      "Itérations 22700: loss 30.450336456298828\n",
      "Itérations 22800: loss 30.39441680908203\n",
      "Itérations 22900: loss 30.33875274658203\n",
      "Itérations 23000: loss 30.2833309173584\n",
      "Itérations 23100: loss 30.228195190429688\n",
      "Itérations 23200: loss 30.17331886291504\n",
      "Itérations 23300: loss 30.118688583374023\n",
      "Itérations 23400: loss 30.064302444458008\n",
      "Itérations 23500: loss 30.010181427001953\n",
      "Itérations 23600: loss 29.956298828125\n",
      "Itérations 23700: loss 29.902652740478516\n",
      "Itérations 23800: loss 29.84926986694336\n",
      "Itérations 23900: loss 29.796154022216797\n",
      "Itérations 24000: loss 29.743270874023438\n",
      "Itérations 24100: loss 29.690628051757812\n",
      "Itérations 24200: loss 29.638235092163086\n",
      "Itérations 24300: loss 29.58608055114746\n",
      "Itérations 24400: loss 29.534147262573242\n",
      "Itérations 24500: loss 29.482454299926758\n",
      "Itérations 24600: loss 29.43103790283203\n",
      "Itérations 24700: loss 29.379844665527344\n",
      "Itérations 24800: loss 29.328876495361328\n",
      "Itérations 24900: loss 29.27815818786621\n",
      "Itérations 25000: loss 29.2276611328125\n",
      "Itérations 25100: loss 29.177385330200195\n",
      "Itérations 25200: loss 29.127344131469727\n",
      "Itérations 25300: loss 29.077547073364258\n",
      "Itérations 25400: loss 29.027978897094727\n",
      "Itérations 25500: loss 28.978639602661133\n",
      "Itérations 25600: loss 28.929533004760742\n",
      "Itérations 25700: loss 28.88064193725586\n",
      "Itérations 25800: loss 28.83196449279785\n",
      "Itérations 25900: loss 28.783517837524414\n",
      "Itérations 26000: loss 28.73545265197754\n",
      "Itérations 26100: loss 28.687665939331055\n",
      "Itérations 26200: loss 28.640125274658203\n",
      "Itérations 26300: loss 28.592796325683594\n",
      "Itérations 26400: loss 28.545812606811523\n",
      "Itérations 26500: loss 28.499067306518555\n",
      "Itérations 26600: loss 28.452537536621094\n",
      "Itérations 26700: loss 28.40620994567871\n",
      "Itérations 26800: loss 28.36012077331543\n",
      "Itérations 26900: loss 28.31422996520996\n",
      "Itérations 27000: loss 28.268537521362305\n",
      "Itérations 27100: loss 28.223072052001953\n",
      "Itérations 27200: loss 28.177806854248047\n",
      "Itérations 27300: loss 28.132740020751953\n",
      "Itérations 27400: loss 28.087890625\n",
      "Itérations 27500: loss 28.043237686157227\n",
      "Itérations 27600: loss 27.9987735748291\n",
      "Itérations 27700: loss 27.954530715942383\n",
      "Itérations 27800: loss 27.91047477722168\n",
      "Itérations 27900: loss 27.86660385131836\n",
      "Itérations 28000: loss 27.82297134399414\n",
      "Itérations 28100: loss 27.779550552368164\n",
      "Itérations 28200: loss 27.736324310302734\n",
      "Itérations 28300: loss 27.693317413330078\n",
      "Itérations 28400: loss 27.650497436523438\n",
      "Itérations 28500: loss 27.60787010192871\n",
      "Itérations 28600: loss 27.565446853637695\n",
      "Itérations 28700: loss 27.52320671081543\n",
      "Itérations 28800: loss 27.481163024902344\n",
      "Itérations 28900: loss 27.439308166503906\n",
      "Itérations 29000: loss 27.397634506225586\n",
      "Itérations 29100: loss 27.35615348815918\n",
      "Itérations 29200: loss 27.31485939025879\n",
      "Itérations 29300: loss 27.27373504638672\n",
      "Itérations 29400: loss 27.232812881469727\n",
      "Itérations 29500: loss 27.192060470581055\n",
      "Itérations 29600: loss 27.1514835357666\n",
      "Itérations 29700: loss 27.111101150512695\n",
      "Itérations 29800: loss 27.070878982543945\n",
      "Itérations 29900: loss 27.030841827392578\n",
      "Itérations 30000: loss 26.991018295288086\n",
      "Itérations 30100: loss 26.95136833190918\n",
      "Itérations 30200: loss 26.911928176879883\n",
      "Itérations 30300: loss 26.872650146484375\n",
      "Itérations 30400: loss 26.833539962768555\n",
      "Itérations 30500: loss 26.79462432861328\n",
      "Itérations 30600: loss 26.755870819091797\n",
      "Itérations 30700: loss 26.717296600341797\n",
      "Itérations 30800: loss 26.67888832092285\n",
      "Itérations 30900: loss 26.64064598083496\n",
      "Itérations 31000: loss 26.602588653564453\n",
      "Itérations 31100: loss 26.56468391418457\n",
      "Itérations 31200: loss 26.52695083618164\n",
      "Itérations 31300: loss 26.4893856048584\n",
      "Itérations 31400: loss 26.45197105407715\n",
      "Itérations 31500: loss 26.41474151611328\n",
      "Itérations 31600: loss 26.37765884399414\n",
      "Itérations 31700: loss 26.340740203857422\n",
      "Itérations 31800: loss 26.30402183532715\n",
      "Itérations 31900: loss 26.26746368408203\n",
      "Itérations 32000: loss 26.23109245300293\n",
      "Itérations 32100: loss 26.194868087768555\n",
      "Itérations 32200: loss 26.158809661865234\n",
      "Itérations 32300: loss 26.122913360595703\n",
      "Itérations 32400: loss 26.087160110473633\n",
      "Itérations 32500: loss 26.05159568786621\n",
      "Itérations 32600: loss 26.016164779663086\n",
      "Itérations 32700: loss 25.980897903442383\n",
      "Itérations 32800: loss 25.945781707763672\n",
      "Itérations 32900: loss 25.910810470581055\n",
      "Itérations 33000: loss 25.87601089477539\n",
      "Itérations 33100: loss 25.841344833374023\n",
      "Itérations 33200: loss 25.806842803955078\n",
      "Itérations 33300: loss 25.772480010986328\n",
      "Itérations 33400: loss 25.738269805908203\n",
      "Itérations 33500: loss 25.704208374023438\n",
      "Itérations 33600: loss 25.670286178588867\n",
      "Itérations 33700: loss 25.636564254760742\n",
      "Itérations 33800: loss 25.60297966003418\n",
      "Itérations 33900: loss 25.569555282592773\n",
      "Itérations 34000: loss 25.536270141601562\n",
      "Itérations 34100: loss 25.50313377380371\n",
      "Itérations 34200: loss 25.47014808654785\n",
      "Itérations 34300: loss 25.437297821044922\n",
      "Itérations 34400: loss 25.404605865478516\n",
      "Itérations 34500: loss 25.37203598022461\n",
      "Itérations 34600: loss 25.339628219604492\n",
      "Itérations 34700: loss 25.30735206604004\n",
      "Itérations 34800: loss 25.275211334228516\n",
      "Itérations 34900: loss 25.24321746826172\n",
      "Itérations 35000: loss 25.211349487304688\n",
      "Itérations 35100: loss 25.17963218688965\n",
      "Itérations 35200: loss 25.148029327392578\n",
      "Itérations 35300: loss 25.116588592529297\n",
      "Itérations 35400: loss 25.08525848388672\n",
      "Itérations 35500: loss 25.054079055786133\n",
      "Itérations 35600: loss 25.023033142089844\n",
      "Itérations 35700: loss 24.992151260375977\n",
      "Itérations 35800: loss 24.96140480041504\n",
      "Itérations 35900: loss 24.9307918548584\n",
      "Itérations 36000: loss 24.900318145751953\n",
      "Itérations 36100: loss 24.869964599609375\n",
      "Itérations 36200: loss 24.83976173400879\n",
      "Itérations 36300: loss 24.809669494628906\n",
      "Itérations 36400: loss 24.779726028442383\n",
      "Itérations 36500: loss 24.749897003173828\n",
      "Itérations 36600: loss 24.7202091217041\n",
      "Itérations 36700: loss 24.69063949584961\n",
      "Itérations 36800: loss 24.661197662353516\n",
      "Itérations 36900: loss 24.631881713867188\n",
      "Itérations 37000: loss 24.602689743041992\n",
      "Itérations 37100: loss 24.573623657226562\n",
      "Itérations 37200: loss 24.54467010498047\n",
      "Itérations 37300: loss 24.515851974487305\n",
      "Itérations 37400: loss 24.48726463317871\n",
      "Itérations 37500: loss 24.458831787109375\n",
      "Itérations 37600: loss 24.430498123168945\n",
      "Itérations 37700: loss 24.402311325073242\n",
      "Itérations 37800: loss 24.374221801757812\n",
      "Itérations 37900: loss 24.34627914428711\n",
      "Itérations 38000: loss 24.318429946899414\n",
      "Itérations 38100: loss 24.290721893310547\n",
      "Itérations 38200: loss 24.263132095336914\n",
      "Itérations 38300: loss 24.235675811767578\n",
      "Itérations 38400: loss 24.20833396911621\n",
      "Itérations 38500: loss 24.181116104125977\n",
      "Itérations 38600: loss 24.154014587402344\n",
      "Itérations 38700: loss 24.12703514099121\n",
      "Itérations 38800: loss 24.10017204284668\n",
      "Itérations 38900: loss 24.073429107666016\n",
      "Itérations 39000: loss 24.04680061340332\n",
      "Itérations 39100: loss 24.020292282104492\n",
      "Itérations 39200: loss 23.993896484375\n",
      "Itérations 39300: loss 23.967613220214844\n",
      "Itérations 39400: loss 23.941444396972656\n",
      "Itérations 39500: loss 23.91538429260254\n",
      "Itérations 39600: loss 23.88943862915039\n",
      "Itérations 39700: loss 23.86359977722168\n",
      "Itérations 39800: loss 23.837879180908203\n",
      "Itérations 39900: loss 23.812253952026367\n",
      "Itérations 40000: loss 23.786745071411133\n",
      "Itérations 40100: loss 23.761341094970703\n",
      "Itérations 40200: loss 23.73604393005371\n",
      "Itérations 40300: loss 23.710845947265625\n",
      "Itérations 40400: loss 23.685762405395508\n",
      "Itérations 40500: loss 23.660776138305664\n",
      "Itérations 40600: loss 23.635892868041992\n",
      "Itérations 40700: loss 23.611116409301758\n",
      "Itérations 40800: loss 23.58643913269043\n",
      "Itérations 40900: loss 23.561861038208008\n",
      "Itérations 41000: loss 23.537384033203125\n",
      "Itérations 41100: loss 23.513010025024414\n",
      "Itérations 41200: loss 23.488750457763672\n",
      "Itérations 41300: loss 23.46461296081543\n",
      "Itérations 41400: loss 23.44056510925293\n",
      "Itérations 41500: loss 23.4166316986084\n",
      "Itérations 41600: loss 23.392824172973633\n",
      "Itérations 41700: loss 23.369298934936523\n",
      "Itérations 41800: loss 23.345861434936523\n",
      "Itérations 41900: loss 23.322540283203125\n",
      "Itérations 42000: loss 23.299297332763672\n",
      "Itérations 42100: loss 23.276172637939453\n",
      "Itérations 42200: loss 23.253124237060547\n",
      "Itérations 42300: loss 23.230194091796875\n",
      "Itérations 42400: loss 23.207340240478516\n",
      "Itérations 42500: loss 23.18459701538086\n",
      "Itérations 42600: loss 23.161937713623047\n",
      "Itérations 42700: loss 23.139379501342773\n",
      "Itérations 42800: loss 23.116905212402344\n",
      "Itérations 42900: loss 23.094532012939453\n",
      "Itérations 43000: loss 23.072246551513672\n",
      "Itérations 43100: loss 23.050050735473633\n",
      "Itérations 43200: loss 23.027950286865234\n",
      "Itérations 43300: loss 23.005929946899414\n",
      "Itérations 43400: loss 22.9840087890625\n",
      "Itérations 43500: loss 22.96216583251953\n",
      "Itérations 43600: loss 22.940425872802734\n",
      "Itérations 43700: loss 22.918750762939453\n",
      "Itérations 43800: loss 22.89719581604004\n",
      "Itérations 43900: loss 22.87572479248047\n",
      "Itérations 44000: loss 22.85436248779297\n",
      "Itérations 44100: loss 22.833078384399414\n",
      "Itérations 44200: loss 22.811887741088867\n",
      "Itérations 44300: loss 22.79078483581543\n",
      "Itérations 44400: loss 22.7697696685791\n",
      "Itérations 44500: loss 22.748849868774414\n",
      "Itérations 44600: loss 22.727998733520508\n",
      "Itérations 44700: loss 22.70725440979004\n",
      "Itérations 44800: loss 22.68657112121582\n",
      "Itérations 44900: loss 22.666000366210938\n",
      "Itérations 45000: loss 22.645490646362305\n",
      "Itérations 45100: loss 22.625078201293945\n",
      "Itérations 45200: loss 22.60474395751953\n",
      "Itérations 45300: loss 22.584487915039062\n",
      "Itérations 45400: loss 22.564319610595703\n",
      "Itérations 45500: loss 22.544221878051758\n",
      "Itérations 45600: loss 22.524219512939453\n",
      "Itérations 45700: loss 22.5042781829834\n",
      "Itérations 45800: loss 22.484437942504883\n",
      "Itérations 45900: loss 22.464662551879883\n",
      "Itérations 46000: loss 22.444971084594727\n",
      "Itérations 46100: loss 22.425355911254883\n",
      "Itérations 46200: loss 22.405813217163086\n",
      "Itérations 46300: loss 22.386354446411133\n",
      "Itérations 46400: loss 22.366962432861328\n",
      "Itérations 46500: loss 22.34768295288086\n",
      "Itérations 46600: loss 22.32846450805664\n",
      "Itérations 46700: loss 22.309343338012695\n",
      "Itérations 46800: loss 22.2902889251709\n",
      "Itérations 46900: loss 22.271312713623047\n",
      "Itérations 47000: loss 22.252426147460938\n",
      "Itérations 47100: loss 22.233600616455078\n",
      "Itérations 47200: loss 22.21487045288086\n",
      "Itérations 47300: loss 22.196195602416992\n",
      "Itérations 47400: loss 22.177610397338867\n",
      "Itérations 47500: loss 22.159095764160156\n",
      "Itérations 47600: loss 22.140649795532227\n",
      "Itérations 47700: loss 22.12228775024414\n",
      "Itérations 47800: loss 22.103981018066406\n",
      "Itérations 47900: loss 22.085769653320312\n",
      "Itérations 48000: loss 22.06761360168457\n",
      "Itérations 48100: loss 22.049535751342773\n",
      "Itérations 48200: loss 22.031530380249023\n",
      "Itérations 48300: loss 22.013580322265625\n",
      "Itérations 48400: loss 21.995723724365234\n",
      "Itérations 48500: loss 21.977914810180664\n",
      "Itérations 48600: loss 21.960189819335938\n",
      "Itérations 48700: loss 21.94252586364746\n",
      "Itérations 48800: loss 21.9249267578125\n",
      "Itérations 48900: loss 21.907405853271484\n",
      "Itérations 49000: loss 21.889938354492188\n",
      "Itérations 49100: loss 21.8725528717041\n",
      "Itérations 49200: loss 21.855226516723633\n",
      "Itérations 49300: loss 21.83797836303711\n",
      "Itérations 49400: loss 21.82080841064453\n",
      "Itérations 49500: loss 21.803695678710938\n",
      "Itérations 49600: loss 21.786666870117188\n",
      "Itérations 49700: loss 21.769697189331055\n",
      "Itérations 49800: loss 21.752792358398438\n",
      "Itérations 49900: loss 21.73596954345703\n"
     ]
    }
   ],
   "source": [
    "## Chargement des données Boston et transformation en tensor.\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "boston = fetch_california_housing() ## chargement des données\n",
    "x = torch.tensor(boston['data'],dtype=torch.float)\n",
    "y = torch.tensor(boston['target'],dtype=torch.float).view(-1,1)\n",
    "\n",
    "print(\"Nombre d'exemples : \",x.size(0), \"Dimension : \",x.size(1))\n",
    "print(\"Nom des attributs : \", \", \".join(boston['feature_names']))\n",
    "\n",
    "#initialisation aléatoire de w et b\n",
    "w = torch.randn(x.size(1),1)\n",
    "b =  torch.randn(1,1)\n",
    "\n",
    "EPOCHS = 50000\n",
    "EPS = 1e-7\n",
    "for n_iter in range(EPOCHS):\n",
    "    ## Calcul du forward (loss), avec création de nouveaux Context pour chaque module\n",
    "    ctx_mse = Context()\n",
    "    ctx_linear = Context()\n",
    "\n",
    "    y_pred = Linear.forward(ctx_linear, x, w, b)\n",
    "    loss = MSE.forward(ctx_mse, y_pred, y)\n",
    "\n",
    "    if n_iter % 100 == 0:\n",
    "        print(f\"Itérations {n_iter}: loss {loss.item()}\")\n",
    "\n",
    "    ## Calcul du backward (grad_w, grad_b)\n",
    "    grad_mse = MSE.backward(ctx_mse,1)\n",
    "    grad_X, grad_W, grad_b = Linear.backward(ctx_linear, grad_mse[0])\n",
    "\n",
    "    ## Mise à jour des paramètres du modèle\n",
    "    #x -= EPS * grad_X\n",
    "    w -= EPS * grad_W  # Update weights\n",
    "    b -= EPS * grad_b  # Update bias"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DeepLearning fc TP1 2020-2021-correction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
