{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TE2ItlsI956"
   },
   "source": [
    "# Deep Learning avec PyTorch\n",
    "\n",
    "## Prise en main de Pytorch\n",
    "\n",
    "Supports adaptés de Sylvain Lamprier (sylvain.lamprier@univ-angers.fr) , Nicolas Baskiotis (nicolas.baskiotis@sorbonne-univeriste.fr) et Benjamin Piwowarski (benjamin.piwowarski@sorbonne-universite.fr) -- MLIA/ISIR, Sorbonne Université"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Y9YOOHHhJKY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La version de torch est :  2.1.1+cu121\n",
      "Le calcul GPU est disponible ?  False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"La version de torch est : \",torch.__version__)\n",
    "print(\"Le calcul GPU est disponible ? \", torch.cuda.is_available())\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u2LqFo3wzwYP"
   },
   "source": [
    "### <span class=\"alert-success\"> Syntaxe\n",
    "\n",
    "Le principal objet manipulé sous Pytorch est **torch.Tensor** qui correspond à un tenseur mathématique (généralisation de la notion de matrice en $n$-dimensions), très proche dans l'utilisation de **numpy.array**.   Cet objet est optimisé pour les calculs sur GPU ce qui implique quelques contraintes plus importantes que sous **numpy**. En particulier :\n",
    "* le type du tenseur manipulé est très important et les conversions ne sont pas automatique (**FloatTensor** de type **torch.float**, **DoubleTensor** de type **torch.double**,  **ByteTensor** de type **torch.byte**, **IntTensor** de type **torch.int**, **LongTensor** de type **torch.long**). Pour un tenseur **t** La conversion se fait très simplement en utilisant les fonctions : **t.double()**, **t.float()**, **t.long()** ...\n",
    "* la plupart des opérations ont une version *inplace*, c'est-à-dire qui modifie le tenseur plutôt que de renvoyer un nouveau tenseur; elles sont suffixées par **_** (**add_** par exemple).\n",
    "\n",
    "[Documentation officielle](https://pytorch.org/docs/stable/tensors.html) pour la liste exhaustive des opérations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "id": "VZxNfy1b1u43",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Création de tenseurs et caractéristiques\n",
    "## Créer un tenseur (2,3) à partir d'une liste\n",
    "t1=torch.tensor(np.array([[1, 2, 3], [4, 5, 6]]))\n",
    "\n",
    "## Créer un tenseur  tenseur rempli de 1 de taille 2x3x4\n",
    "t2 = torch.ones([2,3,4])\n",
    "\n",
    "## tenseur de zéros de taille 2x3 de type float\n",
    "t3 = torch.zeros([2,3,4],dtype=torch.float)\n",
    "\n",
    "## tirage uniforme entier entre 10 et 15, \n",
    "## remarquez l'utilisation du _ dans random pour l'opération inplace\n",
    "t4 = torch.empty([2, 3],dtype=torch.int32).random_(10,16)\n",
    "\n",
    "## tirage suivant la loi normale\n",
    "t5 = torch.empty([3, 4],dtype=torch.float32).normal_()\n",
    "\n",
    "## equivalent à zeros(3,4).normal_\n",
    "t6 = torch.randn([3, 4])\n",
    "\n",
    "## Création d'un vecteur de 3 flottants selon la loi de normale\n",
    "t7 = torch.randn([1, 3])\n",
    "\n",
    "## concatenation de tenseurs sur la dimension 0\n",
    "t8_1 = torch.tensor(np.array([[1, 2, 3], [4, 5, 6]]))\n",
    "t8_2 = torch.tensor(np.array([[7, 8, 9], [10,11,12]]))\n",
    "t8 = torch.cat((t8_1,t8_2),0)\n",
    "\n",
    "## concatenation de tenseurs  sur la dimension 1\n",
    "t9_1 = torch.tensor(np.array([[1, 2, 3], [4, 5, 6]]))\n",
    "t9_2 = torch.tensor(np.array([[7, 8, 9], [10,11,12]]))\n",
    "t9 = torch.cat((t9_1,t9_2),1)\n",
    "\n",
    "## Taille des tenseurs/vecteurs\n",
    "t10 = torch.zeros([2,3,4],dtype=torch.float)\n",
    "taille = t10.shape\n",
    "\n",
    "## Conversion de type\n",
    "t11 = torch.ones([2,3,4],dtype=torch.float)\n",
    "t11_int = t11.to(torch.int32)\n",
    "\n",
    "# Opérations élémentaires sur les tenseurs\n",
    "## produit scalaire (et contrairement à numpy, que produit scalaire)\n",
    "t12_1 = torch.tensor([1, 2, 3])\n",
    "t12_2 = torch.tensor([4, 5, 6])\n",
    "t12 = torch.dot(t12_1, t12_2)\n",
    "\n",
    "## produit matriciel : utilisation de @ ou de la fonction mm\n",
    "t13_1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "t13_2 = torch.tensor([[7, 8], [9, 10], [11, 12]])\n",
    "t13 = t13_1@t13_2\n",
    "\n",
    "## transposé\n",
    "t14_1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "t14 = t14_1.t()\n",
    "\n",
    "## index du maximum selon une dimension\n",
    "t15 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "max_values, max_indices = torch.max(t15, dim=1)\n",
    "\n",
    "## somme selon une dimension/de tous les éléments\n",
    "t16 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "somme = torch.sum(t16,dim=1)\n",
    "\n",
    "## moyenne selon  une dimension/sur tous les éléments\n",
    "t17 = torch.tensor([[1, 2, 3], [4, 5, 12]]).float().mean(dim=1)\n",
    "\n",
    "## changer les dimensions du tenseur (la taille totale doit être inchangée)\n",
    "t18 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "t_resized = t18.view(3, 2) \n",
    "\n",
    "## somme/produit/puissance termes a termes\n",
    "t19_1 = torch.tensor([1, 2, 3])\n",
    "t19_2 = torch.tensor([4, 5, 6])\n",
    "\n",
    "sum_result = t19_1 + t19_2\n",
    "product_result = t19_1 * t19_2\n",
    "puiss_result = t19_1**2\n",
    "#print(\"sum \"+str(sum_result)+\" poduct \"+str(product_result)+\" puiss \"+str(puiss_result))\n",
    "\n",
    "## Soit un tenseur a de (2,3,4). Le recopier dans une version (2,3,3,4) avec les tenseurs (3,4) \n",
    "## a[0] et a[1] recopiés chacun 3 fois (avec expand)\n",
    "# Créer le tenseur initial a de forme (2, 3, 4)\n",
    "a = torch.rand((2,3,4))\n",
    "print(\"Tenseur initial (a) de forme (2, 3, 4):\")\n",
    "print(a)\n",
    "# Créer les tenseurs (3, 4)\n",
    "a=a.view(2,1,3,-1).expand(-1,3,-1,-1).contiguous().view(-1,3,4)\n",
    "print(\"Tenseur final:\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span class=\"alert-success\"> Régression linéaire  </span>\n",
    "\n",
    "On souhaite apprendre un modèle de régression linéaire $f$ du type:  $f(x,w,b)=x.w^t+b$  avec $x\\in \\mathbb{R}^{{d}}$ un vecteur d'observations pour lequel on souhaite prédire une sortie $\\hat{y} \\in \\mathbb{R}$, $w\\in\\mathbb{R}^{1,d}$ et $b\\in \\mathbb{R}$ les paramètres du modèle. \n",
    "\n",
    "Pour cela on dispose d'un jeu de données étiquetées $\\{(x,y)\\}$, que l'on découpe en jeu d'entraînement (80%) et de validation (20%). Dans cet exercice, on utilisera le jeu de données très classique *Boston*, le prix des loyers à Boston en fonction de caractéristiques socio-économiques des quartiers.\n",
    "\n",
    "On considèrera un coût moindres carrés pour apprendre le modèle sur le jeu d'entraînement (avec $N$ le nombre de données d'entraînement et $(x^i,y^i)$ le i-ème couple de cet ensemble): $$w^∗,b^∗=argmin_{w,b}\\frac{1}{N} \\sum_{i=1}^N \\|f(x^i,w,b)-y^i\\|^2$$\n",
    "\n",
    "\n",
    "* Fonction **flineaire(x,w,b)** qui calcule $f(x,w,b)=x.w^t+b$  avec $x\\in \\mathbb{R}^{{n\\times d}},~w\\in\\mathbb{R}^{1,d}, b\\in \\mathbb{R}$\n",
    "* Fonction **loss(x,w,b,y)** qui retourne le coût moindre carré du modèle linéaire utilisant **flineaire(x,w,b)** pour un batch de données $x$ et leurs images associées $y$. \n",
    "* Calcul du gradient de l'erreur en fonction de chacun des paramètres $w_i$ et b par la fonction **getGradient(x,w,b)**\n",
    "* Descente de gradient et apprentissage des paramètres optimaux pour la regression linéaire.\n",
    "* Entraînement sur 80% des données et test sur 20%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flineaire(x,w,b):\n",
    "    return (x@w.T) + b\n",
    "\n",
    "def getLoss(x,w,b,y):\n",
    "    y = y.view(-1,1) # n ligne et 1 colonne\n",
    "    return torch.pow(flineaire(x,w,b)-y,2).mean()/2.0 # pow -> puissance\n",
    "\n",
    "def getGradient(x, w, b, y):\n",
    "    # Calcul de la prédiction\n",
    "    y = y.view(-1,1)\n",
    "    yhat = flineaire(x,w,b)\n",
    "    diff = yhat - y    \n",
    "    '''\n",
    "    #version boucle\n",
    "    g= torch.zeros_like(w)\n",
    "    for i in range(w.size(-1)):\n",
    "        g[0,i] = (x[:,i].view(-1,1)*diff.mean())\n",
    "    return g, diff.mean()\n",
    "    '''\n",
    "    return ((x.t()@diff)/x.shape[0]).t(),(diff.mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Chargement des données California et transformation en tensor.\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing() ## chargement des données\n",
    "data_x = torch.tensor(housing['data'],dtype=torch.float)\n",
    "data_y = torch.tensor(housing['target'],dtype=torch.float)\n",
    "\n",
    "print(\"Nombre d'exemples : \",data_x.size(0), \"Dimension : \",data_x.size(1))\n",
    "print(data_x,data_y)\n",
    "\n",
    "torch.random.manual_seed(1)\n",
    "\n",
    "#initialisation aléatoire de w et b\n",
    "w = torch.randn(1,data_x.size(1),requires_grad=True)\n",
    "b =  torch.randn(1,1,requires_grad=True)\n",
    "\n",
    "#decoupage en train 80 et test 20\n",
    "train_x = data_x[:int(data_x.size(0)*0.8)]\n",
    "train_y = data_y[:int(data_y.size(0)*0.8)]\n",
    "\n",
    "test_x = data_x[int(data_x.size(0)*0.8):]\n",
    "test_y = data_y[int(data_y.size(0)*0.8):]\n",
    "\n",
    "print(\"Nombre d'exemples train : \",train_x.size(0), \"Dimension : \",train_x.size(0),\"x\",train_x.size(1))\n",
    "print(\"Nombre d'exemples test : \",test_x.size(0), \"Dimension : \",test_x.size(0),\"x\",test_x.size(1))\n",
    "\n",
    "\n",
    "EPOCHS = 100000\n",
    "EPS = 1e-7\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    grad_w,grad_b = getGradient(train_x,w,b,train_y)\n",
    "\n",
    "    w = w - EPS*grad_w\n",
    "    b = b - EPS*grad_b\n",
    "\n",
    "    loss = getLoss(train_x,w,b,train_y)\n",
    "    \n",
    "    if i%10000==0:\n",
    "        print(\"Epoch : \",i,\" Loss : \",loss.item())\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DeepLearning fc TP1 2020-2021-correction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
